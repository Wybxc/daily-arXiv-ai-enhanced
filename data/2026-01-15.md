<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 15]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [LAUDE: LLM-Assisted Unit Test Generation and Debugging of Hardware DEsigns](https://arxiv.org/abs/2601.08856)
*Deeksha Nandal,Riccardo Revalor,Soham Dan,Debjit Pal*

Main category: cs.SE

TL;DR: LAUDE：基于LLM的硬件设计单元测试生成与调试统一框架，通过结合设计语义理解和CoT推理能力，显著提升测试生成准确性和代码调试能力


<details>
  <summary>Details</summary>
Motivation: 硬件设计中单元测试开发需要深入理解设计功能和创造力，而调试过程通常繁琐且耗时。现有方法缺乏统一的测试生成与调试框架，需要自动化工具来提升硬件验证效率

Method: 提出LAUDE统一框架，结合设计源代码语义理解和大型语言模型的思维链推理能力，集成提示工程和设计执行信息来增强单元测试生成准确性和代码可调试性

Result: 在VerilogEval数据集上，LAUDE生成的单元测试在组合设计中检测到100%的bug，在时序设计中检测到93%的bug；调试成功率分别为93%和84%

Conclusion: LAUDE框架成功展示了LLM在硬件设计验证中的潜力，通过统一测试生成和调试流程显著提升了硬件设计的验证效率和准确性

Abstract: Unit tests are critical in the hardware design lifecycle to ensure that component design modules are functionally correct and conform to the specification before they are integrated at the system level. Thus developing unit tests targeting various design features requires deep understanding of the design functionality and creativity. When one or more unit tests expose a design failure, the debugging engineer needs to diagnose, localize, and debug the failure to ensure design correctness, which is often a painstaking and intense process. In this work, we introduce LAUDE, a unified unit-test generation and debugging framework for hardware designs that cross-pollinates the semantic understanding of the design source code with the Chain-of-Thought (CoT) reasoning capabilities of foundational Large-Language Models (LLMs). LAUDE integrates prompt engineering and design execution information to enhance its unit test generation accuracy and code debuggability. We apply LAUDE with closed- and open-source LLMs to a large corpus of buggy hardware design codes derived from the VerilogEval dataset, where generated unit tests detected bugs in up to 100% and 93% of combinational and sequential designs and debugged up to 93% and 84% of combinational and sequential designs, respectively.

</details>


### [2] [Revisiting Software Engineering Education in the Era of Large Language Models: A Curriculum Adaptation and Academic Integrity Framework](https://arxiv.org/abs/2601.08857)
*Mustafa Degerli*

Main category: cs.SE

TL;DR: 论文提出理论框架分析生成式AI如何改变软件工程核心能力，并设计LLM整合教育模型，特别关注土耳其计算机工程教育面临的挑战。


<details>
  <summary>Details</summary>
Motivation: LLM工具如ChatGPT和GitHub Copilot正在重塑软件工程实践，降低了代码生成、解释和测试的成本，但大多数软件工程和计算机工程课程仍沿用传统教学模式，将手动语法编写等同于技术能力，这种日益增长的不匹配引发了评估有效性、学习成果和基础技能发展的担忧。

Method: 采用概念研究方法，提出分析生成式AI如何改变核心软件工程能力的理论框架，并引入LLM整合教育的教学设计模型，特别关注土耳其计算机工程项目中集中监管、大班教学和考试导向评估实践加剧的挑战。

Result: 框架描述了问题分析、设计、实现和测试如何从构建转向批判、验证和人机协作管理，同时指出传统的抄袭检测完整性机制已不足，需要向过程透明度模型转变。

Conclusion: 虽然本文提供了课程适应的结构化建议，但仍属理论贡献；论文最后强调需要进行纵向实证研究来评估这些干预措施及其对学习的长期影响。

Abstract: The integration of Large Language Models (LLMs), such as ChatGPT and GitHub Copilot, into professional workflows is increasingly reshaping software engineering practices. These tools have lowered the cost of code generation, explanation, and testing, while introducing new forms of automation into routine development tasks. In contrast, most of the software engineering and computer engineering curricula remain closely aligned with pedagogical models that equate manual syntax production with technical competence. This growing misalignment raises concerns regarding assessment validity, learning outcomes, and the development of foundational skills. Adopting a conceptual research approach, this paper proposes a theoretical framework for analyzing how generative AI alters core software engineering competencies and introduces a pedagogical design model for LLM-integrated education. Attention is given to computer engineering programs in Turkey, where centralized regulation, large class sizes, and exam-oriented assessment practices amplify these challenges. The framework delineates how problem analysis, design, implementation, and testing increasingly shift from construction toward critique, validation, and human-AI stewardship. In addition, the paper argues that traditional plagiarism-centric integrity mechanisms are becoming insufficient, motivating a transition toward a process transparency model. While this work provides a structured proposal for curriculum adaptation, it remains a theoretical contribution; the paper concludes by outlining the need for longitudinal empirical studies to evaluate these interventions and their long-term impacts on learning.

</details>


### [3] [Adaptive Trust Metrics for Multi-LLM Systems: Enhancing Reliability in Regulated Industries](https://arxiv.org/abs/2601.08858)
*Tejaswini Bollikonda*

Main category: cs.SE

TL;DR: 该论文提出了一种用于多LLM生态系统的自适应信任度量框架，旨在量化并提升在监管约束下的模型可靠性，通过系统行为分析、不确定性评估和动态监控来实现可操作的信任度。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在医疗、金融、法律等敏感领域的部署日益增多，关于信任、问责和可靠性的担忧也随之增加。需要建立能够适应多LLM生态系统的信任度量方法，以确保AI在受监管行业中的安全、可扩展应用。

Method: 提出自适应信任度量框架，包括：1) 分析系统行为；2) 评估多个LLM的不确定性；3) 实施动态监控管道。通过金融合规和医疗诊断的案例研究来验证方法的实际应用性。

Result: 研究表明自适应信任度量能够为多LLM生态系统提供实用的信任量化方法，在金融合规和医疗诊断等实际场景中展现出适用性，为受监管行业中的AI采用提供了可操作的信任评估途径。

Conclusion: 自适应信任度量是受监管行业中安全、可扩展AI采用的基础推动者，能够解决LLM部署中的信任、问责和可靠性问题，为多LLM生态系统的可靠运行提供框架支持。

Abstract: Large Language Models (LLMs) are increasingly deployed in sensitive domains such as healthcare, finance, and law, yet their integration raises pressing concerns around trust, accountability, and reliability. This paper explores adaptive trust metrics for multi LLM ecosystems, proposing a framework for quantifying and improving model reliability under regulated constraints. By analyzing system behaviors, evaluating uncertainty across multiple LLMs, and implementing dynamic monitoring pipelines, the study demonstrates practical pathways for operational trustworthiness. Case studies from financial compliance and healthcare diagnostics illustrate the applicability of adaptive trust metrics in real world settings. The findings position adaptive trust measurement as a foundational enabler for safe and scalable AI adoption in regulated industries.

</details>


### [4] [EZInput: A Cross-Environment Python Library for Easy UI Generation in Scientific Computing](https://arxiv.org/abs/2601.08859)
*Bruno M. Saraiva,Iván Hidalgo-Cenalmor,António D. Brito,Damián Martínez,Tayla Shakespeare,Guillaume Jacquemet,Ricardo Henriques*

Main category: cs.SE

TL;DR: EZInput是一个Python库，能够自动为计算算法生成图形用户界面，使非编程专家也能使用这些工具，支持跨Jupyter、Colab和终端环境，提高可重复性和工作效率。


<details>
  <summary>Details</summary>
Motivation: 研究人员在使用需要参数配置的计算算法时面临多个障碍：需要编程技能、不同环境的接口不一致、参数设置难以在会话间保存。这种碎片化导致重复输入、迭代探索缓慢，且参数选择难以记录、共享和重用，损害了研究的可重复性。

Method: EZInput采用声明式规范系统，开发者只需定义一次输入需求和验证约束。库自动处理环境检测、界面渲染、参数验证和会话持久化，支持Jupyter笔记本、Google Colab和终端环境。参数持久化通过轻量级YAML文件实现，受ImageJ/FIJI启发并适配Python工作流。

Result: EZInput实现了"一次编写，随处运行"的架构，使研究人员能在笔记本中快速原型设计，然后在远程系统上部署相同的参数配置进行批量执行，无需代码更改或手动转录。支持科学计算所需的各种输入类型，内置验证确保数据完整性，提供清晰反馈减少用户摩擦。

Conclusion: EZInput通过自动生成跨环境GUI、提供参数持久化和验证功能，解决了计算算法参数配置的碎片化问题，使非编程专家也能访问复杂工具，显著提高了研究工作的可重复性和效率。

Abstract: Researchers face a persistent barrier when applying computational algorithms with parameter configuration typically demanding programming skills, interfaces differing across environments, and settings rarely persisting between sessions. This fragmentation forces repetitive input, slows iterative exploration, and undermines reproducibility because parameter choices are difficult to record, share, and reuse. We present EZInput, a cross-runtime environment Python library enabling algorithm developers to automatically generate graphical user interfaces that make their computational tools accessible to end-users without programming expertise. EZInput employs a declarative specification system where developers define input requirements and validation constraints once; the library then handles environment detection, interface rendering, parameter validation, and session persistence across Jupyter notebooks, Google Colab, and terminal environments. This "write once, run anywhere" architecture enables researchers to prototype in notebooks and deploy identical parameter configurations for batch execution on remote systems without code changes or manual transcription. Parameter persistence, inspired by ImageJ/FIJI and adapted to Python workflows, saves and restores user configurations via lightweight YAML files, eliminating redundant input and producing shareable records that enhance reproducibility. EZInput supports diverse input types essential for scientific computing and it also includes built-in validation that ensures data integrity and clear feedback that reduces user friction.

</details>


### [5] [Bridging the Gap: Empowering Small Models in Reliable OpenACC-based Parallelization via GEPA-Optimized Prompting](https://arxiv.org/abs/2601.08884)
*Samyak Jhaveri,Cristina V. Lopes*

Main category: cs.SE

TL;DR: 使用遗传-帕累托框架优化提示，显著提升小型LLM生成OpenACC并行代码的编译成功率和GPU加速效果


<details>
  <summary>Details</summary>
Motivation: OpenACC降低了GPU卸载的门槛，但编写高性能的pragma仍然复杂，需要深厚的内存层次、数据移动和并行化策略专业知识。大型语言模型为自动并行代码生成提供了有前景的解决方案，但简单的提示通常会导致语法错误的指令、无法编译的代码或性能无法超过CPU基线。

Method: 提出系统化的提示优化方法，利用GEPA（遗传-帕累托）框架，通过反思反馈循环迭代演化提示。该过程利用指令的交叉和变异，以专家策划的黄金示例为指导，并基于黄金和预测pragma之间的子句和子句参数级别不匹配提供结构化反馈。

Result: 在PolyBench套件评估中，使用优化提示生成的OpenACC pragma的程序编译成功率显著提高，特别是对于"nano"规模模型。GPT-4.1 Nano的编译成功率从66.7%提升到93.3%，GPT-5 Nano从86.7%提升到100%，匹配或超越了更大、更昂贵版本的能力。优化提示还使实现GPU加速超过CPU基线的程序数量增加了21%。

Conclusion: 提示优化有效解锁了更小、更便宜的LLM在编写稳定有效的GPU卸载指令方面的潜力，为HPC工作流中基于指令的自动并行化建立了成本效益高的途径。

Abstract: OpenACC lowers the barrier to GPU offloading, but writing high-performing pragma remains complex, requiring deep domain expertise in memory hierarchies, data movement, and parallelization strategies. Large Language Models (LLMs) present a promising potential solution for automated parallel code generation, but naive prompting often results in syntactically incorrect directives, uncompilable code, or performance that fails to exceed CPU baselines. We present a systematic prompt optimization approach to enhance OpenACC pragma generation without the prohibitive computational costs associated with model post-training. Leveraging the GEPA (GEnetic-PAreto) framework, we iteratively evolve prompts through a reflective feedback loop. This process utilizes crossover and mutation of instructions, guided by expert-curated gold examples and structured feedback based on clause- and clause parameter-level mismatches between the gold and predicted pragma. In our evaluation on the PolyBench suite, we observe an increase in compilation success rates for programs annotated with OpenACC pragma generated using the optimized prompts compared to those annotated using the simpler initial prompt, particularly for the "nano"-scale models. Specifically, with optimized prompts, the compilation success rate for GPT-4.1 Nano surged from 66.7% to 93.3%, and for GPT-5 Nano improved from 86.7% to 100%, matching or surpassing the capabilities of their significantly larger, more expensive versions. Beyond compilation, the optimized prompts resulted in a 21% increase in the number of programs that achieve functional GPU speedups over CPU baselines. These results demonstrate that prompt optimization effectively unlocks the potential of smaller, cheaper LLMs in writing stable and effective GPU-offloading directives, establishing a cost-effective pathway to automated directive-based parallelization in HPC workflows.

</details>


### [6] [Build Code is Still Code: Finding the Antidote for Pipeline Poisoning](https://arxiv.org/abs/2601.08995)
*Brent Pappas,Paul Gazzillo*

Main category: cs.SE

TL;DR: 论文提出开发阶段隔离策略，通过建模构建自动化的信息和行为权限来防止构建系统中毒，并开发了Foreman工具原型成功检测XZ Utils攻击中的中毒测试文件。


<details>
  <summary>Details</summary>
Motivation: C项目不仅包含C代码，还包含构建系统代码（如编译、测试、打包自动化）。现有技术只关注验证软件依赖或分析程序代码，忽略了构建系统本身的安全。XZ Utils和SolarWinds攻击表明构建系统容易中毒，且中毒的构建系统可以轻易绕过程序漏洞检测工具。

Method: 提出开发阶段隔离策略，将构建自动化的信息和行为权限建模为程序代码。开发了原型工具Foreman来实现这一方法，通过检查构建系统的权限来检测中毒风险。

Result: Foreman成功检测并警告了XZ Utils攻击中涉及的中毒测试文件。展示了该方法对实际攻击的有效性。

Conclusion: 构建系统安全检查器应该像程序代码检查器一样普及。未来计划通过自动检查开发阶段隔离来防止流水线中毒，提升软件供应链安全。

Abstract: Open source C code underpins society's computing infrastructure. Decades of work has helped harden C code against attackers, but C projects do not consist of only C code. C projects also contain build system code for automating development tasks like compilation, testing, and packaging. These build systems are critcal to software supply chain security and vulnerable to being poisoned, with the XZ Utils and SolarWinds attacks being recent examples. Existing techniques try to harden software supply chains by verifying software dependencies, but such methods ignore the build system itself. Similarly, classic software security checkers only analyze and monitor program code, not build system code. Moreover, poisoned build systems can easily circumvent tools for detecting program code vulnerabilities by disabling such checks. We present development phase isolation, a novel strategy for hardening build systems against poisoning by modeling the information and behavior permissions of build automation as if it were program code. We have prototyped this approach as a tool called Foreman, which successfully detects and warns about the poisoned test files involved in the XZ Utils attack. We outline our future plans to protect against pipeline poisoning by automatically checking development phase isolation. We envision a future where build system security checkers are as prevalent as program code checkers.

</details>


### [7] [On the Flakiness of LLM-Generated Tests for Industrial and Open-Source Database Management Systems](https://arxiv.org/abs/2601.08998)
*Alexander Berndt,Thomas Bach,Rainer Gemulla,Marcus Kessel,Sebastian Baltes*

Main category: cs.SE

TL;DR: LLM生成的测试用例比现有测试更容易出现flaky问题，主要原因是测试依赖无序集合的顺序，且LLM会通过提示上下文将现有测试的flakiness传递到新生成的测试中。


<details>
  <summary>Details</summary>
Motivation: 研究LLM生成的测试用例中flaky测试的普遍性和根本原因，特别是在关系数据库管理系统环境中，以了解LLM测试生成的质量问题。

Method: 使用GPT-4o和Mistral-Large-Instruct-2407两个LLM对四个数据库系统（SAP HANA、DuckDB、MySQL、SQLite）的测试套件进行扩增，然后分析生成的测试用例的flakiness，并通过手动检查识别根本原因。

Result: LLM生成的测试比现有测试有稍高的flaky比例；最常见的根本原因是测试依赖无序集合的顺序（占115个flaky测试中的72个，63%）；LLM通过提示上下文将现有测试的flakiness传递到新测试中；在闭源系统（如SAP HANA）中flakiness传递更普遍。

Conclusion: LLM生成的测试存在flakiness问题，开发者应关注无序集合依赖导致的flaky测试；在使用LLM进行测试生成时，提供精心设计的上下文非常重要，以避免flakiness传递。

Abstract: Flaky tests are a common problem in software testing. They produce inconsistent results when executed multiple times on the same code, invalidating the assumption that a test failure indicates a software defect. Recent work on LLM-based test generation has identified flakiness as a potential problem with generated tests. However, its prevalence and underlying causes are unclear. We examined the flakiness of LLM-generated tests in the context of four relational database management systems: SAP HANA, DuckDB, MySQL, and SQLite. We amplified test suites with two LLMs, GPT-4o and Mistral-Large-Instruct-2407, to assess the flakiness of the generated test cases. Our results suggest that generated tests have a slightly higher proportion of flaky tests compared to existing tests. Based on a manual inspection, we found that the most common root cause of flakiness was the reliance of a test on a certain order that is not guaranteed ("unordered collection"), which was present in 72 of 115 flaky tests (63%). Furthermore, both LLMs transferred the flakiness from the existing tests to the newly generated tests via the provided prompt context. Our experiments suggest that flakiness transfer is more prevalent in closed-source systems such as SAP HANA than in open-source systems. Our study informs developers on what types of flakiness to expect from LLM-generated tests. It also highlights the importance of providing LLMs with tailored context when employing LLMs for test generation.

</details>


### [8] [SafePlanner: Testing Safety of the Automated Driving System Plan Model](https://arxiv.org/abs/2601.09171)
*Dohyun Kim,Sanggu Han,Sangmin Woo,Joonha Jang,Jaehoon Kim,Changhun Song,Yongdae Kim*

Main category: cs.SE

TL;DR: SafePlanner是一个用于自动驾驶系统规划模型安全测试的系统化框架，通过结构分析和引导式模糊测试发现安全关键缺陷。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统规划模型的安全验证至关重要，但传统测试方法难以生成结构上有意义的测试场景并检测危险规划行为。

Method: 1) 对规划模型实现进行结构分析，提取可行的场景转换；2) 结合NPC行为组合测试场景；3) 应用引导式模糊测试探索规划模型的行为空间。

Result: 在百度Apollo L4级ADS上测试，生成20635个测试用例，检测到520个危险行为，归纳为15个根本原因。修复其中4个问题后，问题消失且无明显副作用。在规划模型上达到83.63%函数覆盖率和63.22%决策覆盖率。

Conclusion: SafePlanner在缺陷发现和测试效率方面优于基线方法，能够有效识别自动驾驶系统规划模型的安全关键缺陷。

Abstract: In this work, we present SafePlanner, a systematic testing framework for identifying safety-critical flaws in the Plan model of Automated Driving Systems (ADS). SafePlanner targets two core challenges: generating structurally meaningful test scenarios and detecting hazardous planning behaviors. To maximize coverage, SafePlanner performs a structural analysis of the Plan model implementation - specifically, its scene-transition logic and hierarchical control flow - and uses this insight to extract feasible scene transitions from code. It then composes test scenarios by combining these transitions with non-player vehicle (NPC) behaviors. Guided fuzzing is applied to explore the behavioral space of the Plan model under these scenarios. We evaluate SafePlanner on Baidu Apollo, a production-grade level 4 ADS. It generates 20635 test cases and detects 520 hazardous behaviors, grouped into 15 root causes through manual analysis. For four of these, we applied patches based on our analysis; the issues disappeared, and no apparent side effects were observed. SafePlanner achieves 83.63 percent function and 63.22 percent decision coverage on the Plan model, outperforming baselines in both bug discovery and efficiency.

</details>


### [9] [AI-NativeBench: An Open-Source White-Box Agentic Benchmark Suite for AI-Native Systems](https://arxiv.org/abs/2601.09393)
*Zirui Wang,Guangba Yu,Michael R. Lyu*

Main category: cs.SE

TL;DR: AI-NativeBench：首个基于MCP和A2A标准的应用中心化白盒AI原生基准测试套件，用于评估AI原生系统工程特性而非单纯模型能力


<details>
  <summary>Details</summary>
Motivation: 从云原生到AI原生的架构转变使传统黑盒评估范式失效，现有基准测试仅衡量原始模型能力而忽视系统级执行动态，需要新的评估方法

Method: 引入AI-NativeBench基准套件，基于模型上下文协议（MCP）和代理到代理（A2A）标准，将代理跨度作为分布式跟踪中的一等公民，实现细粒度工程特性分析

Result: 在21个系统变体上发现传统指标无法揭示的关键工程现实：参数悖论（轻量模型在协议遵从性上优于旗舰模型）、普遍推理主导（协议开销次要）、昂贵失败模式（自愈机制在不可行工作流上成为成本倍增器）

Conclusion: 该工作为从衡量模型能力转向工程化可靠AI原生系统提供了首个系统性证据，开源基准和数据集促进可复现性和进一步研究

Abstract: The transition from Cloud-Native to AI-Native architectures is fundamentally reshaping software engineering, replacing deterministic microservices with probabilistic agentic services. However, this shift renders traditional black-box evaluation paradigms insufficient: existing benchmarks measure raw model capabilities while remaining blind to system-level execution dynamics. To bridge this gap, we introduce AI-NativeBench, the first application-centric and white-box AI-Native benchmark suite grounded in Model Context Protocol (MCP) and Agent-to-Agent (A2A) standards. By treating agentic spans as first-class citizens within distributed traces, our methodology enables granular analysis of engineering characteristics beyond simple capabilities. Leveraging this benchmark across 21 system variants, we uncover critical engineering realities invisible to traditional metrics: a parameter paradox where lightweight models often surpass flagships in protocol adherence, a pervasive inference dominance that renders protocol overhead secondary, and an expensive failure pattern where self-healing mechanisms paradoxically act as cost multipliers on unviable workflows. This work provides the first systematic evidence to guide the transition from measuring model capability to engineering reliable AI-Native systems. To facilitate reproducibility and further research, we have open-sourced the benchmark and dataset.

</details>


### [10] [DepRadar: Agentic Coordination for Context Aware Defect Impact Analysis in Deep Learning Libraries](https://arxiv.org/abs/2601.09440)
*Yi Gao,Xing Hu,Tongtong Xu,Jiali Zhao,Xiaohu Yang,Xin Xia*

Main category: cs.SE

TL;DR: DepRadar是一个用于深度学习库缺陷影响分析的智能代理协调框架，通过多代理协作从代码变更中提取缺陷语义，分析触发条件，并检查下游程序是否受影响。


<details>
  <summary>Details</summary>
Motivation: 深度学习库（如Transformers、Megatron）广泛使用，但当这些库引入缺陷时，下游用户难以评估自己的程序是否受影响。影响分析需要理解缺陷语义并检查复杂的触发条件（配置标志、运行时环境、间接API使用等）。

Method: DepRadar协调四个专业代理完成三个步骤：1) PR Miner和Code Diff Analyzer从提交或PR中提取结构化缺陷语义；2) Orchestrator Agent将这些信号合成为统一的缺陷模式及触发条件；3) Impact Analyzer检查下游程序是否可能触发缺陷。框架结合静态分析和DL特定领域规则来提高准确性和可解释性。

Result: 在两个代表性DL库的157个PR和70个提交上评估，DepRadar在缺陷识别方面达到90%的精确度，生成高质量结构化字段（平均字段得分1.6）。在122个客户端程序上，识别受影响案例的召回率达到90%，精确度80%，显著优于其他基线方法。

Conclusion: DepRadar通过智能代理协调框架，有效解决了深度学习库缺陷影响分析的挑战，能够准确识别缺陷并分析其对下游程序的影响，为DL库用户提供了实用的影响评估工具。

Abstract: Deep learning libraries like Transformers and Megatron are now widely adopted in modern AI programs. However, when these libraries introduce defects, ranging from silent computation errors to subtle performance regressions, it is often challenging for downstream users to assess whether their own programs are affected. Such impact analysis requires not only understanding the defect semantics but also checking whether the client code satisfies complex triggering conditions involving configuration flags, runtime environments, and indirect API usage. We present DepRadar, an agent coordination framework for fine grained defect and impact analysis in DL library updates. DepRadar coordinates four specialized agents across three steps: 1. the PR Miner and Code Diff Analyzer extract structured defect semantics from commits or pull requests, 2. the Orchestrator Agent synthesizes these signals into a unified defect pattern with trigger conditions, and 3. the Impact Analyzer checks downstream programs to determine whether the defect can be triggered. To improve accuracy and explainability, DepRadar integrates static analysis with DL-specific domain rules for defect reasoning and client side tracing. We evaluate DepRadar on 157 PRs and 70 commits across two representative DL libraries. It achieves 90% precision in defect identification and generates high quality structured fields (average field score 1.6). On 122 client programs, DepRadar identifies affected cases with 90% recall and 80% precision, substantially outperforming other baselines.

</details>


### [11] [Towards a Metadata Schema for Energy Research Software](https://arxiv.org/abs/2601.09456)
*Stephan Ferenz,Oliver Werth,Astrid Nieße*

Main category: cs.SE

TL;DR: 开发了一个能源研究软件的元数据模式，通过需求分析和用户测试，平衡了形式化、互操作性和领域特定需求


<details>
  <summary>Details</summary>
Motivation: 许多领域（包括能源研究）缺乏既定的元数据模式，这阻碍了研究软件的可发现性和可重用性，不符合FAIR4RS原则

Method: 基于需求分析开发能源研究软件元数据模式，并通过用户测试进行评估

Result: 该模式在形式化、互操作性和能源研究人员的特定需求之间取得了平衡；测试显示良好的信息呈现对研究人员创建所需元数据至关重要

Conclusion: 本文提供了设计能源研究软件元数据模式的挑战和机遇的见解，强调了良好信息呈现的重要性

Abstract: Domain-specific metadata schemas are essential to improve the findability and reusability of research software and to follow the FAIR4RS principles. However, many domains, including energy research, lack established metadata schemas. To address this gap, we developed a metadata schema for energy research software based on a requirement analysis and evaluated it through user testing. Our results show that the schema balances the need for formalization and interoperability, while also meeting the specific needs of energy researchers. Meanwhile, the testing showed that a good presentation of the required information is key to enable researchers to create the required metadata. This paper provides insights into the challenges and opportunities of designing a metadata schema for energy research software.

</details>


### [12] [Analyzing GitHub Issues and Pull Requests in nf-core Pipelines: Insights into nf-core Pipeline Repositories](https://arxiv.org/abs/2601.09612)
*Khairul Alam,Banani Roy*

Main category: cs.SE

TL;DR: 对nf-core科学工作流管道开发维护中挑战的实证研究：分析了25,173个issue和PR，识别出13个关键挑战，发现标签和代码片段能显著提高问题解决率。


<details>
  <summary>Details</summary>
Motivation: 尽管Nextflow和nf-core社区在科学计算领域广泛采用，但用户在这些管道开发和维护过程中面临的具体挑战尚不清楚。需要实证研究来了解这些挑战、管理实践和感知困难，以提升管道的可用性、可持续性和可重复性。

Method: 使用BERTopic主题建模分析25,173个issue和pull request，识别关键挑战；统计分析问题解决动态，包括解决时间、解决率，以及标签和代码片段对解决可能性的影响。

Result: 识别出13个关键挑战：管道开发与集成、bug修复、基因组数据集成、CI配置管理、版本更新等；89.38%的问题最终被关闭，半数在3天内解决；标签（大效应，δ=0.94）和代码片段（中效应，δ=0.50）显著提高解决可能性；工具开发和仓库维护是最具挑战性的任务。

Conclusion: 本研究揭示了nf-core管道协作开发和维护中的关键挑战，为提升其可用性、可持续性和可重复性提供了可操作的见解，特别强调了标签系统和代码示例在问题解决中的重要性。

Abstract: Scientific Workflow Management Systems (SWfMSs) such as Nextflow have become essential software frameworks for conducting reproducible, scalable, and portable computational analyses in data-intensive fields like genomics, transcriptomics, and proteomics. Building on Nextflow, the nf-core community curates standardized, peer-reviewed pipelines that follow strict testing, documentation, and governance guidelines. Despite its broad adoption, little is known about the challenges users face during the development and maintenance of these pipelines. This paper presents an empirical study of 25,173 issues and pull requests from these pipelines to uncover recurring challenges, management practices, and perceived difficulties. Using BERTopic modeling, we identify 13 key challenges, including pipeline development and integration, bug fixing, integrating genomic data, managing CI configurations, and handling version updates. We then examine issue resolution dynamics, showing that 89.38\% of issues and pull requests are eventually closed, with half resolved within three days. Statistical analysis reveals that the presence of labels (large effect, $δ$ = 0.94) and code snippets (medium effect, $δ$ = 0.50) significantly improve resolution likelihood. Further analysis reveals that tool development and repository maintenance poses the most significant challenges, followed by testing pipelines and CI configurations, and debugging containerized pipelines. Overall, this study provides actionable insights into the collaborative development and maintenance of nf-core pipelines, highlighting opportunities to enhance their usability, sustainability, and reproducibility.

</details>


### [13] [SysPro: Reproducing System-level Concurrency Bugs from Bug Reports](https://arxiv.org/abs/2601.09616)
*Tarannum Shaila Zaman,Zhihui Yan,Chen Wang,Chadni Islam,Jiangfan Shi,Tingting Yu*

Main category: cs.SE

TL;DR: SysPro：一种从bug报告中自动提取系统调用信息、生成输入数据并重现系统级并发bug的方法


<details>
  <summary>Details</summary>
Motivation: 重现系统级并发bug需要输入数据和精确的系统调用交错顺序，但bug报告通常缺乏详细信息且用自然语言描述，现有工具无法有效处理系统调用级别的交错问题

Method: SysPro通过信息检索、正则表达式匹配和类别划分方法自动从bug报告中提取系统调用名称，在源代码中定位这些调用，生成输入数据，并通过动态源代码插桩来重现bug

Result: 在真实世界基准测试上的实证研究表明，SysPro在从bug报告中定位和重现系统级并发bug方面既有效又高效

Conclusion: SysPro为解决系统级并发bug重现的挑战提供了一种新颖有效的方法，能够自动处理自然语言bug报告并生成必要的执行环境

Abstract: Reproducing system-level concurrency bugs requires both input data and the precise interleaving order of system calls. This process is challenging because such bugs are non-deterministic, and bug reports often lack the detailed information needed. Additionally, the unstructured nature of reports written in natural language makes it difficult to extract necessary details. Existing tools are inadequate to reproduce these bugs due to their inability to manage the specific interleaving at the system call level. To address these challenges, we propose SysPro, a novel approach that automatically extracts relevant system call names from bug reports and identifies their locations in the source code. It generates input data by utilizing information retrieval, regular expression matching, and the category-partition method. This extracted input and interleaving data are then used to reproduce bugs through dynamic source code instrumentation. Our empirical study on real-world benchmarks demonstrates that SysPro is both effective and efficient at localizing and reproducing system-level concurrency bugs from bug reports.

</details>


### [14] [How well LLM-based test generation techniques perform with newer LLM versions?](https://arxiv.org/abs/2601.09695)
*Michael Konstantinou,Renzo Degiovanni,Mike Papadakis*

Main category: cs.SE

TL;DR: 研究发现，在单元测试生成中，使用最新LLM的简单方法（无后处理）比现有的复杂工具（HITS、SymPrompt、TestSpark、CoverUp）表现更好，在代码覆盖率、分支覆盖率和变异得分上分别提升17.72%、19.80%和20.92%，且成本相当。建议先针对类生成测试，再针对未覆盖的方法，可减少约20%的LLM请求。


<details>
  <summary>Details</summary>
Motivation: 现有LLM测试生成技术通常基于较弱的基准（旧版LLM和简单提示）进行评估，而新版强大LLM可能使这些复杂技术的优势消失。需要验证在最新LLM下，简单方法与复杂工具的相对有效性。

Method: 复制四种最先进的LLM测试生成工具（HITS、SymPrompt、TestSpark、CoverUp），将所有方法集成最新LLM版本，在393个类和3,657个方法上进行实验，比较简单LLM方法与这些工具的测试效果和效率。

Result: 简单LLM方法在所有测试有效性指标上均优于现有技术：行覆盖率提升17.72%，分支覆盖率提升19.80%，变异得分提升20.92%，且LLM查询成本相当。测试生成粒度对成本有显著影响，先针对类再针对未覆盖方法的策略可减少约20%的LLM请求。

Conclusion: 最新LLM的强大能力使得简单直接的测试生成方法优于复杂的工程化工具，建议采用更高效的生成策略（先类后方法）来减少成本。研究挑战了现有LLM测试生成技术的必要性，强调需要基于最新LLM进行重新评估。

Abstract: The rapid evolution of Large Language Models (LLMs) has strongly impacted software engineering, leading to a growing number of studies on automated unit test generation. However, the standalone use of LLMs without post-processing has proven insufficient, often producing tests that fail to compile or achieve high coverage. Several techniques have been proposed to address these issues, reporting improvements in test compilation and coverage. While important, LLM-based test generation techniques have been evaluated against relatively weak baselines (for todays' standards), i.e., old LLM versions and relatively weak prompts, which may exacerbate the performance contribution of the approaches. In other words, stronger (newer) LLMs may obviate any advantage these techniques bring. We investigate this issue by replicating four state-of-the-art LLM-based test generation tools, HITS, SymPrompt, TestSpark, and CoverUp that include engineering components aimed at guiding the test generation process through compilation and execution feedback, and evaluate their relative effectiveness and efficiency over a plain LLM test generation method. We integrate current LLM versions in all approaches and run an experiment on 393 classes and 3,657 methods. Our results show that the plain LLM approach can outperform previous state-of-the-art approaches in all test effectiveness metrics we used: line coverage (by 17.72%), branch coverage (by 19.80%) and mutation score (by 20.92%), and it does so at a comparable cost (LLM queries). We also observe that the granularity at which the plain LLM is applied has a significant impact on the cost. We therefore propose targeting first the program classes, where test generation is more efficient, and then the uncovered methods to reduce the number of LLM requests. This strategy achieves comparable (slightly higher) effectiveness while requiring about 20% fewer LLM requests.

</details>


### [15] [ShortCoder: Knowledge-Augmented Syntax Optimization for Token-Efficient Code Generation](https://arxiv.org/abs/2601.09703)
*Sicong Liu,Yanxian Huang,Mingwei Liu,Jiachi Chen,Ensheng Shi,Yuchi Ma,Hongyu Zhang,Yin Zhang,Yanlin Wang*

Main category: cs.SE

TL;DR: ShortCoder是一个知识注入框架，通过语法级简化规则、混合数据合成和微调策略，在不影响功能的前提下优化代码生成效率，减少18.1%的token使用。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在代码生成时每个token都需要完整推理过程，导致内存占用大、资源消耗高。虽然已有研究关注推理阶段优化，但生成阶段的效率问题尚未充分探索。

Method: 1) 提出10个Python语法级简化规则，基于AST保持转换；2) 构建混合数据合成管道，结合规则重写和LLM引导精炼，创建ShorterCodeBench语料库；3) 设计微调策略，向基础LLM注入简洁性意识。

Result: 在HumanEval基准测试中，ShortCoder始终优于最先进方法，生成效率比先前方法提高18.1%-37.8%，同时保持代码生成性能。

Conclusion: ShortCoder通过语法简化、数据合成和模型微调，有效优化了代码生成效率，在保持语义等价和可读性的前提下显著减少了token使用和资源消耗。

Abstract: Code generation tasks aim to automate the conversion of user requirements into executable code, significantly reducing manual development efforts and enhancing software productivity. The emergence of large language models (LLMs) has significantly advanced code generation, though their efficiency is still impacted by certain inherent architectural constraints. Each token generation necessitates a complete inference pass, requiring persistent retention of contextual information in memory and escalating resource consumption. While existing research prioritizes inference-phase optimizations such as prompt compression and model quantization, the generation phase remains underexplored. To tackle these challenges, we propose a knowledge-infused framework named ShortCoder, which optimizes code generation efficiency while preserving semantic equivalence and readability. In particular, we introduce: (1) ten syntax-level simplification rules for Python, derived from AST-preserving transformations, achieving 18.1% token reduction without functional compromise; (2) a hybrid data synthesis pipeline integrating rule-based rewriting with LLM-guided refinement, producing ShorterCodeBench, a corpus of validated tuples of original code and simplified code with semantic consistency; (3) a fine-tuning strategy that injects conciseness awareness into the base LLMs. Extensive experimental results demonstrate that ShortCoder consistently outperforms state-of-the-art methods on HumanEval, achieving an improvement of 18.1%-37.8% in generation efficiency over previous methods while ensuring the performance of code generation.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [16] [Query Languages for Machine-Learning Models](https://arxiv.org/abs/2601.09381)
*Martin Grohe*

Main category: cs.LO

TL;DR: 本文讨论两种用于加权有限结构的逻辑：带求和的一阶逻辑(FO(SUM))及其递归扩展IFP(SUM)，并将其作为机器学习模型（特别是神经网络）的查询语言进行研究。


<details>
  <summary>Details</summary>
Motivation: 神经网络等机器学习模型通常表示为加权图，需要形式化的查询语言来描述和分析这些结构。现有的加权结构逻辑（源自Grädel、Gurevich和Meer在1990年代的基础工作）为这一需求提供了理论基础。

Method: 研究FO(SUM)和IFP(SUM)两种逻辑作为神经网络查询语言的表达能力。通过具体示例展示如何用这些逻辑表达神经网络查询，并分析其表达能力和计算复杂性。

Result: 提供了神经网络查询在FO(SUM)和IFP(SUM)中的表达示例，获得了关于这些逻辑表达能力和计算复杂性的基本结果。

Conclusion: FO(SUM)和IFP(SUM)逻辑为加权有限结构（特别是神经网络）提供了有效的形式化查询语言，为机器学习模型的分析和查询提供了理论基础。

Abstract: In this paper, I discuss two logics for weighted finite structures: first-order logic with summation (FO(SUM)) and its recursive extension IFP(SUM). These logics originate from foundational work by Grädel, Gurevich, and Meer in the 1990s. In recent joint work with Standke, Steegmans, and Van den Bussche, we have investigated these logics as query languages for machine learning models, specifically neural networks, which are naturally represented as weighted graphs. I present illustrative examples of queries to neural networks that can be expressed in these logics and discuss fundamental results on their expressiveness and computational complexity.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [17] [Reversible Weighted Automata over Finite Rings and Monoids with Commuting Idempotents](https://arxiv.org/abs/2601.09409)
*Peter Kostolányi,Andrej Ravinger*

Main category: cs.FL

TL;DR: 本文引入可逆加权自动机，证明在局部有限交换环（如有限域）上，这类自动机实现的级数支撑集恰好是那些幂等元可交换的有理语言，特别在F2域上可直接识别语言，从而获得ECom伪簇的新自动机刻画，并得到可逆加权自动机实现的判定问题是可判定的。


<details>
  <summary>Details</summary>
Motivation: 研究可逆加权自动机在局部有限交换环上的性质，特别是要刻画这类自动机实现的级数的支撑集特征，并建立与特定有理语言类（对应幂等元可交换的有限幺半群）的联系，从而获得新的自动机理论刻画。

Method: 引入可逆加权自动机的概念，在局部有限交换环（如有限域）的框架下进行研究。通过分析这类自动机实现的级数的支撑集特征，证明这些支撑集恰好对应那些幂等元可交换的有理语言。特别关注F2域上的情况，其中级数可直接识别为语言。

Result: 证明了可逆加权自动机实现的级数的支撑集恰好是那些幂等元可交换的有理语言。在F2域上，实现了级数可直接识别为这类语言。获得了ECom伪簇对应的有理语言簇的新自动机刻画，该簇也是Pin意义下可逆语言的布尔闭包。由此得出在局部有限交换环上判定有理级数是否可由可逆加权自动机实现的问题是可判定的。

Conclusion: 可逆加权自动机在局部有限交换环上提供了对ECom伪簇对应的有理语言簇的新自动机刻画，建立了可逆加权自动机与幂等元可交换的有理语言之间的对应关系，并解决了相关判定问题的可判定性。

Abstract: Reversible weighted automata are introduced and considered in a specific setting where the weights are taken from a nontrivial locally finite commutative ring such as a finite field. It is shown that the supports of series realised by such automata are precisely the rational languages such that the idempotents in their syntactic monoids commute. In particular, this is true for reversible weighted automata over the finite field $\mathbb{F}_2$, where the realised series can be directly identified with such languages. A new automata-theoretic characterisation is thus obtained for the variety of rational languages corresponding to the pseudovariety of finite monoids $\mathbf{ECom}$, which also forms the Boolean closure of the reversible languages in the sense of J.-É. Pin. The problem of determining whether a rational series over a locally finite commutative ring can be realised by a reversible weighted automaton is decidable as a consequence.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [18] [Relational Hoare Logic for High-Level Synthesis of Hardware Accelerators](https://arxiv.org/abs/2601.09217)
*Izumi Tanaka,Ken Sakayori,Shinya Takamaeda-Yamazaki,Naoki Kobayashi*

Main category: cs.PL

TL;DR: 提出基于关系霍尔逻辑的形式化翻译框架，自动优化HLS程序的内存访问模式，通过插入片上缓冲和流处理提升性能


<details>
  <summary>Details</summary>
Motivation: 现有HLS工具的内存优化对编程风格敏感且缺乏透明度，需要手动调优，难以设计高效的内存系统

Method: 基于关系霍尔逻辑的形式化翻译框架，识别复杂内存访问模式，自动插入片上缓冲实现线性外部内存访问，将非顺序处理替换为流处理

Result: 原型翻译器结合商用HLS编译器和真实FPGA板卡实验，展示了显著的性能提升

Conclusion: 提出的形式化框架实现了稳健透明的HLS程序转换，自动优化内存访问模式，显著提升硬件加速器性能

Abstract: High-level synthesis (HLS) is a powerful tool for developing efficient hardware accelerators that rely on specialized memory systems to achieve sufficient on-chip data reuse and off-chip bandwidth utilization. However, even with HLS, designing such systems still requires careful manual tuning, as automatic optimizations provided by existing tools are highly sensitive to programming style and often lack transparency. To address these issues, we present a formal translation framework based on relational Hoare logic, which enables robust and transparent transformations. Our method recognizes complex memory access patterns in naïve HLS programs and automatically transforms them by inserting on-chip buffers to enforce linear access to off-chip memory, and by replacing non-sequential processing with stream processing, while preserving program semantics. Experiments using our prototype translator, combined with an off-the-shelf HLS compiler and a real FPGA board, have demonstrated significant performance improvements.

</details>


### [19] [MLIR-Forge: A Modular Framework for Language Smiths](https://arxiv.org/abs/2601.09583)
*Berke Ates,Philipp Schaad,Timo Schneider,Alexandru Calotoiu,Torsten Hoefler*

Main category: cs.PL

TL;DR: MLIR-Forge是一个基于MLIR的随机程序生成框架，通过分离语言特定组件和可重用程序创建逻辑，简化了编译器IR测试专用程序生成器的开发。


<details>
  <summary>Details</summary>
Motivation: 领域特定语言（DSL）使用高级中间表示（IR）进行优化，但测试这些IR很复杂。虽然随机程序生成器对编译器测试有效，但为特定IR开发专用生成器既困难又耗时。

Method: 提出MLIR-Forge框架，将生成过程分解为语言特定的基础构建块和可重用的程序创建逻辑。该框架利用MLIR的灵活性，隐藏复杂性，并使用通用工具定义语言特定组件。

Result: 成功为MLIR内置方言、WebAssembly和数据中心程序表示DaCe生成程序，每个仅需不到一周开发时间。通过差异测试发现9个MLIR、15个WebAssembly和774个DaCe的bug组。

Conclusion: MLIR-Forge有效简化了编译器IR专用随机程序生成器的开发，显著减少了开发时间，并成功发现了大量编译器bug，证明了其实际价值。

Abstract: Optimizing compilers are essential for the efficient and correct execution of software across various scientific fields. Domain-specific languages (DSL) typically use higher level intermediate representations (IR) in their compiler pipelines for domain-specific optimizations. As these IRs add to complexity, it is crucial to test them thoroughly. Random program generators have proven to be an effective tool to test compilers through differential and fuzz testing. However, developing specialized program generators for compiler IRs is not straightforward and demands considerable resources. We introduce MLIR-Forge, a novel random program generator framework that leverages the flexibility of MLIR, aiming to simplify the creation of specialized program generators. MLIR-Forge achieves this by splitting the generation process into fundamental building blocks that are language specific, and reusable program creation logic that constructs random programs from these building blocks. This hides complexity and furthermore, even the language specific components can be defined using a set of common tools. We demonstrate MLIR-Forge's capabilities by generating MLIR with built-in dialects, WebAssembly, and a data-centric program representation, DaCe -- requiring less than a week of development time in total for each of them. Using the generated programs we conduct differential testing and find 9 MLIR, 15 WebAssembly, and 774 DaCe groups of bugs with the corresponding program generators, after running them until the rate of new bugs stagnates.

</details>
