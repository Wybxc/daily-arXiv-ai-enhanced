<div id=toc></div>

# Table of Contents

- [cs.LO](#cs.LO) [Total: 2]
- [cs.SE](#cs.SE) [Total: 12]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [1] [The Design of an Interactive Proof Mode for Dafny](https://arxiv.org/abs/2512.20486)
*Ştefan Ciobâcă,K. Rustan M. Leino,Ştefan-Alexandru Mercaş,Roxana-Mihaela Timon*

Main category: cs.LO

TL;DR: 将Dafny系统扩展为交互式证明模式，包括设计选择与原型实现


<details>
  <summary>Details</summary>
Motivation: Dafny系统目前缺乏交互式证明能力，需要增强其验证过程的灵活性和用户友好性

Method: 扩展Dafny系统，设计交互式证明模式的工作机制，包括主要设计选择，并实现原型系统

Result: 开发了Dafny的交互式证明模式原型，展示了其工作方式和设计选择

Conclusion: 成功为Dafny系统添加了交互式证明能力，提高了验证过程的灵活性和可用性

Abstract: We propose to extend the Dafny system with an interactive proof mode. We present a motivating example, how the IPM works, including the main design choices we make, and a prototype implementation.

</details>


### [2] [The Limitations and Power of NP-Oracle-Based Functional Synthesis Techniques](https://arxiv.org/abs/2512.20572)
*Brendan Juba,Kuldeep S. Meel*

Main category: cs.LO

TL;DR: 该论文对基于NP预言机的函数综合方法进行了系统的理论分析，揭示了现有方法的局限性并建立了新的理论框架。


<details>
  <summary>Details</summary>
Motivation: 尽管函数综合工具在可扩展性方面取得了显著进展，能够处理数万个变量的问题，但这些方法主要依赖SAT求解器，缺乏对其理论局限性和能力的系统理解。

Method: 通过理论分析，研究基于NP预言机的函数综合方法的能力边界。包括分析比特级学习方法的失败原因、证明基于插值方法的根本局限性、证明NP预言机的必要性，并建立使用NP预言机合成小Skolem函数的新理论框架。

Result: 1) 即使存在小Skolem函数，比特级学习方法也会失败；2) 基于插值的方法必须产生指数级大小的电路；3) 访问NP预言机是高效综合的必要条件；4) 建立了使用NP预言机在多项式时间内合成小Skolem函数的理论结果。

Conclusion: 该工作为基于NP预言机的函数综合方法提供了系统的理论基础，揭示了现有方法的局限性，并建立了新的理论框架，为广泛的关系型规范提供了积极的理论结果。

Abstract: Given a Boolean relational specification between inputs and outputs, the problem of functional synthesis is to construct a function that maps each assignment of the input to an assignment of the output such that each tuple of input and output assignments meets the specification. The past decade has witnessed significant improvement in the scalability of functional synthesis tools, allowing them to handle problems with tens of thousands of variables. A common ingredient in these approaches is their reliance on SAT solvers, thereby exploiting the breakthrough advances in SAT solving over the past three decades. While the recent techniques have been shown to perform well in practice, there is little theoretical understanding of the limitations and power of these approaches.
  The primary contribution of this work is to initiate a systematic theoretical investigation into the power of functional synthesis approaches that rely on NP oracles. We first show that even when small Skolem functions exist, naive bit-by-bit learning approaches fail due to the relational nature of specifications. We establish fundamental limitations of interpolation-based approaches, proving that even when small Skolem functions exist, resolution-based interpolation must produce exponential-size circuits. We prove that access to an NP oracle is inherently necessary for efficient synthesis. Our main technical result shows that it is possible to use NP oracles to synthesize small Skolem functions in time polynomial in the size of the specification and the size of the smallest sufficient set of witnesses, establishing positive results for a broad class of relational specifications.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [Attention Distance: A Novel Metric for Directed Fuzzing with Large Language Models](https://arxiv.org/abs/2512.19758)
*Wang Bin,Ao Yang,Kedan Li,Aofan Liu,Hui Li,Guibo Luo,Weixiang Huang,Yan Zhuang*

Main category: cs.SE

TL;DR: 提出注意力距离新指标，利用大语言模型分析代码上下文关系，改进定向灰盒模糊测试，相比传统方法效率提升3.43倍。


<details>
  <summary>Details</summary>
Motivation: 现有定向灰盒模糊测试仅测量种子执行路径与目标位置的物理距离，忽略了代码段之间的逻辑关系，在复杂二进制文件中会产生冗余或误导性指导，削弱实际效果。

Method: 引入注意力距离新指标，利用大语言模型的上下文分析能力计算代码元素间的注意力分数，揭示其内在联系。在AFLGo配置下，仅将物理距离替换为注意力距离而不改变其他模糊测试组件。

Result: 在38个真实漏洞复现实验中，相比传统方法平均测试效率提升3.43倍；相比最先进的定向模糊测试器DAFL和WindRanger，分别提升2.89倍和7.13倍。将注意力距离集成到DAFL和WindRanger中也能持续提升其原始性能。

Conclusion: 注意力距离通过大语言模型的上下文分析有效捕捉代码逻辑关系，显著提升定向模糊测试效率，具有良好通用性，可集成到现有定向模糊测试器中增强性能。

Abstract: In the domain of software security testing, Directed Grey-Box Fuzzing (DGF) has garnered widespread attention for its efficient target localization and excellent detection performance. However, existing approaches measure only the physical distance between seed execution paths and target locations, overlooking logical relationships among code segments. This omission can yield redundant or misleading guidance in complex binaries, weakening DGF's real-world effectiveness. To address this, we introduce \textbf{attention distance}, a novel metric that leverages a large language model's contextual analysis to compute attention scores between code elements and reveal their intrinsic connections. Under the same AFLGo configuration -- without altering any fuzzing components other than the distance metric -- replacing physical distances with attention distances across 38 real vulnerability reproduction experiments delivers a \textbf{3.43$\times$} average increase in testing efficiency over the traditional method. Compared to state-of-the-art directed fuzzers DAFL and WindRanger, our approach achieves \textbf{2.89$\times$} and \textbf{7.13$\times$} improvements, respectively. To further validate the generalizability of attention distance, we integrate it into DAFL and WindRanger, where it also consistently enhances their original performance. All related code and datasets are publicly available at https://github.com/TheBinKing/Attention\_Distance.git.

</details>


### [4] [A Declarative Language for Building And Orchestrating LLM-Powered Agent Workflows](https://arxiv.org/abs/2512.19769)
*Ivan Daunis*

Main category: cs.SE

TL;DR: 提出声明式LLM代理系统，通过统一DSL分离工作流规范与实现，支持多语言后端和部署环境，显著降低开发时间和部署速度。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理系统将代理逻辑与特定编程语言和部署模型紧密耦合，导致开发复杂、部署困难，需要一种更灵活、可移植的解决方案。

Method: 设计声明式系统，使用统一领域特定语言(DSL)表达常见代理工作流模式（数据序列化、过滤、RAG检索、API编排），将工作流规范与实现分离，支持多后端语言和部署环境。

Result: 在PayPal真实电商工作流中处理每日数百万次交互，开发时间减少60%，部署速度提升3倍，非工程师可安全修改代理行为，编排开销低于100ms，复杂工作流仅需50行DSL代码（相比500+行命令式代码）。

Conclusion: 声明式DSL方法将代理开发从应用程序编程转变为配置管理，显著提高开发效率、部署速度和可维护性，同时支持A/B测试和跨环境执行。

Abstract: Building deployment-ready LLM agents requires complex orchestration of tools, data sources, and control flow logic, yet existing systems tightly couple agent logic to specific programming languages and deployment models. We present a declarative system that separates agent workflow specification from implementation, enabling the same pipeline definition to execute across multiple backend languages (Java, Python, Go) and deployment environments (cloud-native, on-premises).
  Our key insight is that most agent workflows consist of common patterns -- data serialization, filtering, RAG retrieval, API orchestration -- that can be expressed through a unified DSL rather than imperative code. This approach transforms agent development from application programming to configuration, where adding new tools or fine-tuning agent behaviors requires only pipeline specification changes, not code deployment. Our system natively supports A/B testing of agent strategies, allowing multiple pipeline variants to run on the same backend infrastructure with automatic metric collection and comparison.
  We evaluate our approach on real-world e-commerce workflows at PayPal, processing millions of daily interactions. Our results demonstrate 60% reduction in development time, and 3x improvement in deployment velocity compared to imperative implementations. The language's declarative approach enables non-engineers to modify agent behaviors safely, while maintaining sub-100ms orchestration overhead. We show that complex workflows involving product search, personalization, and cart management can be expressed in under 50 lines of DSL compared to 500+ lines of imperative code.

</details>


### [5] [Larger Is Not Always Better: Leveraging Structured Code Diffs for Comment Inconsistency Detection](https://arxiv.org/abs/2512.19883)
*Phong Nguyen,Anh M. T. Bui,Phuong T. Nguyen*

Main category: cs.SE

TL;DR: 基于CodeT5+的即时代码-注释不一致性检测方法，通过将代码变更分解为有序的修改活动序列，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 代码与注释的语义一致性对程序理解、调试和维护至关重要。现有方法忽略代码演化的结构复杂性，存在隐私和资源挑战。

Method: 基于CodeT5+框架，将代码变更分解为替换、删除、添加等有序修改活动序列，以更好地捕捉代码变更与过时注释之间的关联。

Result: 在JITDATA和CCIBENCH基准测试中，F1分数比现有最佳方法提升13.54%，比微调的大型语言模型（DeepSeek-Coder、CodeLlama、Qwen2.5-Coder）提升4.18%-10.94%。

Conclusion: 提出的即时CCI检测方法通过结构化处理代码变更活动，有效提升了检测性能，解决了现有方法的局限性。

Abstract: Ensuring semantic consistency between source code and its accompanying comments is crucial for program comprehension, effective debugging, and long-term maintainability. Comment inconsistency arises when developers modify code but neglect to update the corresponding comments, potentially misleading future maintainers and introducing errors. Recent approaches to code-comment inconsistency (CCI) detection leverage Large Language Models (LLMs) and rely on capturing the semantic relationship between code changes and outdated comments. However, they often ignore the structural complexity of code evolution, including historical change activities, and introduce privacy and resource challenges. In this paper, we propose a Just-In-Time CCI detection approach built upon the CodeT5+ backbone. Our method decomposes code changes into ordered sequences of modification activities such as replacing, deleting, and adding to more effectively capture the correlation between these changes and the corresponding outdated comments. Extensive experiments conducted on publicly available benchmark datasets-JITDATA and CCIBENCH--demonstrate that our proposed approach outperforms recent state-of-the-art models by up to 13.54% in F1-Score and achieves an improvement ranging from 4.18% to 10.94% over fine-tuned LLMs including DeepSeek-Coder, CodeLlama and Qwen2.5-Coder.

</details>


### [6] [Neuron-Guided Interpretation of Code LLMs: Where, Why, and How?](https://arxiv.org/abs/2512.19980)
*Zhe Yin,Xiaodong Gu,Beijun Shen*

Main category: cs.SE

TL;DR: 本文研究了代码大语言模型的神经元级可解释性，发现了语言特异性神经元和概念层，并展示了它们在代码生成、克隆检测和代码摘要等任务中的应用价值。


<details>
  <summary>Details</summary>
Motivation: 代码语言模型在代码智能任务上表现出色，但其内部可解释性研究不足。现有的NLP神经元解释技术不适用于源代码，因为编程语言具有形式化、层次化和可执行的特性。

Method: 在神经元级别实证研究代码LLMs，定位语言特异性神经元（对单一语言有选择性响应）和概念层（编码语言无关代码表示的feed-forward层）。分析Llama-3.1-8B和Qwen2.5-Coder-32B在C++、Java、Python、Go和JavaScript多语言输入上的表现，测量神经元选择性和生成过程中的层间贡献。

Result: 发现：(1) 存在专门处理单个语言的神经元，同时存在支持通用生成的通用神经元子集；(2) 低层主要编码语言特定语法，中层捕获跨语言共享的语义抽象，这些中层作为概念层出现。

Conclusion: 展示了在三个任务上的应用价值：基于神经元引导的代码生成微调、通过概念层嵌入进行克隆检测、以及概念层引导的代码摘要迁移，每种方法在多语言设置中都获得了稳定的性能提升。

Abstract: Code language models excel on code intelligence tasks, yet their internal interpretability is underexplored. Existing neuron interpretability techniques from NLP are suboptimal for source code due to programming languages formal, hierarchical, and executable nature. We empirically investigate code LLMs at the neuron level, localizing language-specific neurons (selectively responsive to one language) and concept layers (feed-forward layers encoding language-agnostic code representations). We analyze Llama-3.1-8B and Qwen2.5-Coder-32B on multilingual inputs in C++, Java, Python, Go, and JavaScript, measuring neuron selectivity and layerwise contributions during generation. We find (1) neurons specialized for individual languages alongside a universal subset supporting general-purpose generation; and (2) lower layers mainly encode language-specific syntax, while middle layers capture semantic abstractions shared across languages, emerging as concept layers. We demonstrate utility on three tasks: neuron-guided fine-tuning for code generation, clone detection via concept-layer embeddings, and concept-layer-guided transfer for code summarization, each yielding consistent gains in multilingual settings.

</details>


### [7] [Detecting Non-Optimal Decisions of Embodied Agents via Diversity-Guided Metamorphic Testing](https://arxiv.org/abs/2512.20083)
*Wenzhao Wu,Yahui Tang,Mingfei Cheng,Wenbing Tang,Yuan Zhou,Yang Liu*

Main category: cs.SE

TL;DR: 论文提出NoD-DGMT框架，通过多样性引导的蜕变测试检测具身智能体任务规划中的非最优决策问题，显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前具身智能体评估方法主要关注功能正确性，忽视了规划的非功能性最优问题，这可能导致显著的性能下降和资源浪费。需要系统性地检测非最优决策问题。

Method: 提出NoD-DGMT框架，基于蜕变测试原理设计四种新颖的蜕变关系来捕捉基本最优性属性，并引入多样性引导的选择策略来最大化检测效率。

Result: 在AI2-THOR模拟器上对四个最先进的规划模型进行实验，平均违规检测率达到31.9%，多样性引导过滤器将检测率提升4.3%，多样性得分提升3.3，显著优于六个基线方法。

Conclusion: NoD-DGMT框架能有效检测具身智能体规划中的非最优决策问题，为资源受限应用中的智能体优化提供了系统性的评估方法。

Abstract: As embodied agents advance toward real-world deployment, ensuring optimal decisions becomes critical for resource-constrained applications. Current evaluation methods focus primarily on functional correctness, overlooking the non-functional optimality of generated plans. This gap can lead to significant performance degradation and resource waste. We identify and formalize the problem of Non-optimal Decisions (NoDs), where agents complete tasks successfully but inefficiently. We present NoD-DGMT, a systematic framework for detecting NoDs in embodied agent task planning via diversity-guided metamorphic testing. Our key insight is that optimal planners should exhibit invariant behavioral properties under specific transformations. We design four novel metamorphic relations capturing fundamental optimality properties: position detour suboptimality, action optimality completeness, condition refinement monotonicity, and scene perturbation invariance. To maximize detection efficiency, we introduce a diversity-guided selection strategy that actively selects test cases exploring different violation categories, avoiding redundant evaluations while ensuring comprehensive diversity coverage. Extensive experiments on the AI2-THOR simulator with four state-of-the-art planning models demonstrate that NoD-DGMT achieves violation detection rates of 31.9% on average, with our diversity-guided filter improving rates by 4.3% and diversity scores by 3.3 on average. NoD-DGMT significantly outperforms six baseline methods, with 16.8% relative improvement over the best baseline, and demonstrates consistent superiority across different model architectures and task complexities.

</details>


### [8] [AXIOM: Benchmarking LLM-as-a-Judge for Code via Rule-Based Perturbation and Multisource Quality Calibration](https://arxiv.org/abs/2512.20159)
*Ruiqi Wang,Xinchen Wang,Cuiyun Gao,Chun Yong Chong,Xin Xia,Qing Liao*

Main category: cs.SE

TL;DR: AXIOM是一个基于扰动的代码评估基准框架，通过规则引导的扰动和多源质量校准，生成具有平衡分数分布的代码评估数据集，解决现有基准的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有代码评估基准存在严重局限性：粗粒度二元标签掩盖了细微错误，细粒度但主观的评价标准导致人工标注不可靠，以及不受控制的数据合成方法导致分数分布不平衡，无法真实反映现实世界的代码生成场景。

Method: AXIOM采用两阶段框架：1) 规则引导的扰动：使用LLMs根据预定义扰动规则修改高质量程序的功能和代码质量，精确控制目标分数以实现平衡分布；2) 多源质量校准：选择高质量子集进行人工标注，确保基准的可靠性和多样性。

Result: 论文提出了AXIOM框架，能够大规模合成具有平衡分数分布的代码评估基准，解决了现有基准在粒度、主观性和分布平衡性方面的局限性。

Conclusion: AXIOM通过创新的扰动框架和校准方法，为代码评估提供了更可靠、平衡和多样化的基准，有助于更准确地评估LLM生成的代码质量。

Abstract: Large language models (LLMs) have been increasingly deployed in real-world software engineering, fostering the development of code evaluation metrics to study the quality of LLM-generated code. Conventional rule-based metrics merely score programs based on their surface-level similarities with reference programs instead of analyzing functionality and code quality in depth. To address this limitation, researchers have developed LLM-as-a-judge metrics, prompting LLMs to evaluate and score code, and curated various code evaluation benchmarks to validate their effectiveness. However, these benchmarks suffer from critical limitations, hindering reliable assessments of evaluation capability: Some feature coarse-grained binary labels, which reduce rich code behavior to a single bit of information, obscuring subtle errors. Others propose fine-grained but subjective, vaguely-defined evaluation criteria, introducing unreliability in manually-annotated scores, which is the ground-truth they rely on. Furthermore, they often use uncontrolled data synthesis methods, leading to unbalanced score distributions that poorly represent real-world code generation scenarios.
  To curate a diverse benchmark with programs of well-balanced distributions across various quality levels and streamline the manual annotation procedure, we propose AXIOM, a novel perturbation-based framework for synthesizing code evaluation benchmarks at scale. It reframes program scores as the refinement effort needed for deployment, consisting of two stages: (1) Rule-guided perturbation, which prompts LLMs to apply sequences of predefined perturbation rules to existing high-quality programs to modify their functionality and code quality, enabling us to precisely control each program's target score to achieve balanced score distributions. (2) Multisource quality calibration, which first selects a subset of...

</details>


### [9] [Well Begun is Half Done: Location-Aware and Trace-Guided Iterative Automated Vulnerability Repair](https://arxiv.org/abs/2512.20203)
*Zhenlei Ye,Xiaobing Sun,Sicong Cao,Lili Bo,Bin Li*

Main category: cs.SE

TL;DR: 提出了一种名为\sysname的LLM-based漏洞修复方法，该方法不仅关注修复内容，还提供需要修补的位置信息，并通过质量评估改进迭代修复策略。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的漏洞修复方法存在两个主要限制：1）忽略需要修补的位置信息，只关注修复内容；2）在迭代过程中缺乏对生成候选补丁的质量评估。

Method: \sysname方法首先提供需要修补的位置信息，然后通过两个维度评估补丁质量：是否引入新漏洞和污点语句覆盖率，选择最佳补丁进行下一轮迭代。

Result: 在VulnLoc+数据集（包含40个C/C++漏洞）上评估，\sysname生成了27个合理补丁，比基线方法多8-22个；在正确补丁生成方面，比现有方法多修复8-13个漏洞。

Conclusion: \sysname通过结合位置信息和补丁质量评估，显著提升了基于LLM的漏洞修复效果，在真实世界漏洞修复任务中表现出优越性能。

Abstract: The advances of large language models (LLMs) have paved the way for automated software vulnerability repair approaches, which iteratively refine the patch until it becomes plausible. Nevertheless, existing LLM-based vulnerability repair approaches face notable limitations: 1) they ignore the concern of locations that need to be patched and focus solely on the repair content. 2) they lack quality assessment for generated candidate patches in the iterative process.
  To tackle the two limitations, we propose \sysname, an LLM-based approach that provides information about where should be patched first. Furthermore, \sysname improves the iterative repair strategy by assessing the quality of test-failing patches and selecting the best patch for the next iteration. We introduce two dimensions to assess the quality of patches: whether they introduce new vulnerabilities and the taint statement coverage. We evaluated \sysname on a real-world C/C++ vulnerability repair dataset VulnLoc+, which contains 40 vulnerabilities and their Proofs-of-Vulnerability. The experimental results demonstrate that \sysname exhibits substantial improvements compared with the Neural Machine Translation-based, Program Analysis-based, and LLM-based state-of-the-art vulnerability repair approaches. Specifically, \sysname is able to generate 27 plausible patches, which is comparable to or even 8 to 22 more plausible patches than the baselines. In terms of correct patch generation, \sysname repairs 8 to 13 additional vulnerabilities compared with existing approaches.

</details>


### [10] [Toward Explaining Large Language Models in Software Engineering Tasks](https://arxiv.org/abs/2512.20328)
*Antonio Vitale,Khai-Nguyen Nguyen,Denys Poshyvanyk,Rocco Oliveto,Simone Scalabrino,Antonio Mastropaolo*

Main category: cs.SE

TL;DR: FeatureSHAP是一个专门为软件工程任务设计的自动化、模型无关的可解释性框架，基于Shapley值，通过系统输入扰动和任务特定相似性比较来解释LLM输出，在代码生成和代码摘要任务中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在软件工程任务自动化方面取得进展，但其黑盒特性阻碍了在高风险和安全性关键领域的应用，现有可解释性方法缺乏与软件工程实践者推理方式对齐的领域特定解释。

Method: 基于Shapley值，通过系统输入扰动和任务特定相似性比较，将模型输出归因于高级输入特征，兼容开源和专有LLM，是首个完全自动化、模型无关的软件工程可解释性框架。

Result: 在代码生成和代码摘要任务中，FeatureSHAP比基线方法更少关注无关输入特征，产生更高保真度的解释；37名实践者调查显示，FeatureSHAP能帮助更好解释模型输出并做出更明智决策。

Conclusion: FeatureSHAP代表了软件工程实用可解释AI的重要进展，通过提供与领域实践者推理方式对齐的解释，提高了LLM在软件工程任务中的透明度和可信度。

Abstract: Recent progress in Large Language Models (LLMs) has substantially advanced the automation of software engineering (SE) tasks, enabling complex activities such as code generation and code summarization. However, the black-box nature of LLMs remains a major barrier to their adoption in high-stakes and safety-critical domains, where explainability and transparency are vital for trust, accountability, and effective human supervision. Despite increasing interest in explainable AI for software engineering, existing methods lack domain-specific explanations aligned with how practitioners reason about SE artifacts. To address this gap, we introduce FeatureSHAP, the first fully automated, model-agnostic explainability framework tailored to software engineering tasks. Based on Shapley values, FeatureSHAP attributes model outputs to high-level input features through systematic input perturbation and task-specific similarity comparisons, while remaining compatible with both open-source and proprietary LLMs. We evaluate FeatureSHAP on two bi-modal SE tasks: code generation and code summarization. The results show that FeatureSHAP assigns less importance to irrelevant input features and produces explanations with higher fidelity than baseline methods. A practitioner survey involving 37 participants shows that FeatureSHAP helps practitioners better interpret model outputs and make more informed decisions. Collectively, FeatureSHAP represents a meaningful step toward practical explainable AI in software engineering. FeatureSHAP is available at https://github.com/deviserlab/FeatureSHAP.

</details>


### [11] [Comment Traps: How Defective Commented-out Code Augment Defects in AI-Assisted Code Generation](https://arxiv.org/abs/2512.20334)
*Yuan Huang,Yukang Zhou,Xiangping Chen,Zibin Zheng*

Main category: cs.SE

TL;DR: AI编程助手（GitHub Copilot和Cursor）会受到注释代码中缺陷的影响，导致生成更多有缺陷的代码，即使有明确忽略指令，缺陷减少也不超过21.84%。


<details>
  <summary>Details</summary>
Motivation: 随着AI代码生成工具的普及，先前研究主要关注代码上下文对缺陷生成的影响，但忽视了注释代码中缺陷的影响。AI编程助手对注释代码的解读会影响其生成代码的质量。

Method: 评估GitHub Copilot和Cursor两种AI编程助手如何受有缺陷的注释代码影响，通过实验分析缺陷传播模式，包括测试助手是否简单复制缺陷、是否主动推理完成不完整的缺陷模式，以及在存在干扰（如错误缩进或标签）时是否继续生成缺陷代码。

Result: 有缺陷的注释代码会导致AI编程助手生成更多有缺陷的代码，最高可达58.17%。工具不会简单复制缺陷，而是主动推理完成不完整的缺陷模式，即使在干扰下也会继续生成缺陷代码。即使明确指示忽略有缺陷的注释代码，缺陷减少也不超过21.84%。

Conclusion: AI编程助手对注释代码中缺陷的敏感性表明需要改进其鲁棒性和安全措施，以减少缺陷传播风险。

Abstract: With the rapid development of large language models in code generation, AI-powered editors such as GitHub Copilot and Cursor are revolutionizing software development practices. At the same time, studies have identified potential defects in the generated code. Previous research has predominantly examined how code context influences the generation of defective code, often overlooking the impact of defects within commented-out code (CO code). AI coding assistants' interpretation of CO code in prompts affects the code they generate.
  This study evaluates how AI coding assistants, GitHub Copilot and Cursor, are influenced by defective CO code. The experimental results show that defective CO code in the context causes AI coding assistants to generate more defective code, reaching up to 58.17 percent. Our findings further demonstrate that the tools do not simply copy the defective code from the context. Instead, they actively reason to complete incomplete defect patterns and continue to produce defective code despite distractions such as incorrect indentation or tags. Even with explicit instructions to ignore the defective CO code, the reduction in defects does not exceed 21.84 percent. These findings underscore the need for improved robustness and security measures in AI coding assistants.

</details>


### [12] [A Comprehensive Study of Bugs in Modern Distributed Deep Learning Systems](https://arxiv.org/abs/2512.20345)
*Xiaoxue Ma,Wanwei Zhan,Jiale Chen,Yishu Li,Jacky Keung,Federica Sarro*

Main category: cs.SE

TL;DR: 首次对专用分布式深度学习框架进行大规模实证分析，研究DeepSpeed、Megatron-LM和Colossal-AI的849个真实问题，构建了包含34种错误症状、28种根本原因和6种修复模式的分类体系，揭示了分布式训练特有的挑战和解决方案。


<details>
  <summary>Details</summary>
Motivation: 在数据驱动时代，深度学习处理海量数据时单设备训练受计算和内存限制。虽然通用框架（如TensorFlow和PyTorch）提供分布式能力，但这些通常是附加功能，需要大量手动工作来实现高级并行，凸显了对专用框架的需求。然而，目前缺乏对专用分布式框架中实际挑战的系统性研究。

Method: 对DeepSpeed、Megatron-LM和Colossal-AI三个专用分布式深度学习框架的849个真实世界问题进行分析。构建了包含34种错误症状、28种根本原因和6种修复模式的分类体系。建立了错误症状、根本原因和修复方案在分布式训练各阶段之间的明确映射关系。

Result: 45.1%的错误症状是分布式框架特有的，其中设置失败、内存问题和性能异常最为普遍。95%的通信设置阶段问题仅出现在分布式环境中。超过60%的案例可以通过版本和依赖管理、分布式特性、API和通信调优来解决。

Conclusion: 专用分布式深度学习框架存在大量特有的挑战，特别是在通信设置阶段。通过系统性的分类和映射分析，为开发者提供了可操作的指导，包括版本管理、依赖控制、API设计和通信优化等方面的改进建议，有助于提高分布式训练的成功率和效率。

Abstract: In today's data-driven era, deep learning is vital for processing massive datasets, yet single-device training is constrained by computational and memory limits. Distributed deep learning overcomes these challenges by leveraging multiple GPUs or machines in parallel. While general-purpose frameworks (e.g., TensorFlow and PyTorch) provide distributed capabilities, these are often add-on features that demand significant manual effort for advanced parallelism, underscoring the need for specialized frameworks. This study conducts the first large-scale empirical analysis of practitioner challenges in dedicated distributed frameworks. We examine 849 real-world issues from DeepSpeed, Megatron-LM, and Colossal-AI and construct a taxonomy of 34 bug symptoms, 28 root causes, and 6 fix patterns. Crucially, we establish explicit mappings between symptoms, causes, and fixes across distributed training stages, enabling a systematic understanding of how issues emerge and are resolved. Our results show that 45.1\% of bug symptoms are unique to distributed frameworks, with setup failures, memory issues, and performance anomalies being the most prevalent. Moreover, 95\% of issues in the communication setup stage occur exclusively in distributed contexts. We also find over 60\% of cases can be resolved through version and dependency management, and distributed feature, API, and communication tuning. Based on these findings, we provide actionable implications.

</details>


### [13] [Identifying Appropriately-Sized Services with Deep Reinforcement Learning](https://arxiv.org/abs/2512.20381)
*Syeda Tasnim Fabiha,Saad Shafiq,Wesley Klewerton Guez Assunção,Nenad Medvidović*

Main category: cs.SE

TL;DR: Rake：一种基于深度强化学习的服务分解技术，直接从实现工件中识别适当规模的服务，无需特定文档或项目人员，在模块化质量和业务能力对齐方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 基于服务的架构（SBA）在现代化遗留系统方面受到关注，但定义适当规模的服务（捕获系统功能的凝聚子集）仍然具有挑战性。现有方法通常依赖文档、项目人员访问或预先知道目标服务数量等假设，这些假设在许多现实场景中不成立。

Method: Rake是一种基于强化学习的技术，利用可用的系统文档和源代码，在实现方法级别指导服务分解。该技术无需特定文档或项目人员访问，是语言无关的，并支持可定制的目标函数，平衡模块化质量和业务能力对齐。

Result: 在四个开源遗留项目上应用Rake，并与两种最先进技术进行比较。平均而言，Rake实现了7-14%更高的模块化质量和18-22%更强的业务能力对齐。结果还显示，仅优化业务上下文可能会在紧耦合系统中降低分解质量，突显了平衡目标的重要性。

Conclusion: Rake通过强化学习方法有效解决了服务分解的挑战，在无需特定文档或项目人员的情况下，实现了更好的模块化质量和业务能力对齐，为遗留系统现代化提供了实用解决方案。

Abstract: Service-based architecture (SBA) has gained attention in industry and academia as a means to modernize legacy systems. It refers to a design style that enables systems to be developed as suites of small, loosely coupled, and autonomous components (services) that encapsulate functionality and communicate via language-agnostic APIs. However, defining appropriately sized services that capture cohesive subsets of system functionality remains challenging. Existing work often relies on the availability of documentation, access to project personnel, or a priori knowledge of the target number of services, assumptions that do not hold in many real-world scenarios. Our work addresses these limitations using a deep reinforcement learning-based approach to identify appropriately sized services directly from implementation artifacts. We present Rake, a reinforcement learning-based technique that leverages available system documentation and source code to guide service decomposition at the level of implementation methods. Rake does not require specific documentation or access to project personnel and is language-agnostic. It also supports a customizable objective function that balances modularization quality and business capability alignment, i.e., the degree to which a service covers the targeted business capability. We applied Rake to four open-source legacy projects and compared it with two state-of-the-art techniques. On average, Rake achieved 7-14 percent higher modularization quality and 18-22 percent stronger business capability alignment. Our results further show that optimizing solely for business context can degrade decomposition quality in tightly coupled systems, highlighting the need for balanced objectives.

</details>


### [14] [SweRank+: Multilingual, Multi-Turn Code Ranking for Software Issue Localization](https://arxiv.org/abs/2512.20482)
*Revanth Gangi Reddy,Ye Liu,Wenting Zhao,JaeHyeok Doo,Tarun Suresh,Daniel Lee,Caiming Xiong,Yingbo Zhou,Semih Yavuz,Shafiq Joty*

Main category: cs.SE

TL;DR: SweRank+框架结合SweRankMulti（跨语言代码排序工具）和SweRankAgent（智能搜索代理），通过多轮迭代推理提升多语言代码库问题定位的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有问题定位方法多为Python中心化且采用单次搜索，难以准确映射自然语言错误描述到需要修改的相关函数，特别是在大规模多语言代码库中。

Method: SweRankMulti包含代码嵌入检索器和列表式LLM重排序器，使用多语言问题定位数据集训练；SweRankAgent采用智能搜索循环，通过记忆缓冲区在多轮中积累相关定位候选。

Result: 在多语言问题定位基准测试中，SweRankMulti达到新的最先进性能，而SweRankAgent进一步提升了单次排序的定位效果。

Conclusion: SweRank+框架通过结合跨语言代码排序和智能多轮推理，显著改进了多语言代码库的问题定位能力。

Abstract: Maintaining large-scale, multilingual codebases hinges on accurately localizing issues, which requires mapping natural-language error descriptions to the relevant functions that need to be modified. However, existing ranking approaches are often Python-centric and perform a single-pass search over the codebase. This work introduces SweRank+, a framework that couples SweRankMulti, a cross-lingual code ranking tool, with SweRankAgent, an agentic search setup, for iterative, multi-turn reasoning over the code repository. SweRankMulti comprises a code embedding retriever and a listwise LLM reranker, and is trained using a carefully curated large-scale issue localization dataset spanning multiple popular programming languages. SweRankAgent adopts an agentic search loop that moves beyond single-shot localization with a memory buffer to reason and accumulate relevant localization candidates over multiple turns. Our experiments on issue localization benchmarks spanning various languages demonstrate new state-of-the-art performance with SweRankMulti, while SweRankAgent further improves localization over single-pass ranking.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [15] [Error Localization, Certificates, and Hints for Probabilistic Program Verification via Slicing (Extended Version)](https://arxiv.org/abs/2512.20214)
*Philipp Schröer,Darion Haase,Joost-Pieter Katoen*

Main category: cs.PL

TL;DR: 论文提出基于切片技术的概率程序验证诊断方法，包括错误报告、证明简化和验证结果保持三种诊断类型，并在Caesar验证器中实现为Brutus工具。


<details>
  <summary>Details</summary>
Motivation: 概率程序验证过程中需要有效的用户诊断工具来帮助理解验证失败原因、简化证明过程，并保持成功的验证结果。

Method: 在HeyVL定量中间验证语言上形式化定义三种切片概念：错误报告切片、证明简化切片和验证结果保持切片。实现Brutus工具，采用二分搜索算法最小化错误见证切片，并比较基于不可满足核、最小不可满足子集枚举和直接SMT编码的不同算法。

Result: 在现有和新基准测试上的实证评估表明，Brutus能够找到既小又信息丰富的切片，有效支持定量断言错误定位、证明规则诊断和验证提示生成。

Conclusion: 提出的切片诊断方法为概率程序验证提供了有效的用户诊断工具，能够生成专门的错误消息和验证提示，并通过形式化证明确保正确性。

Abstract: This paper focuses on effective user diagnostics generated during the deductive verification of probabilistic programs. Our key principle is based on providing slices for (1) error reporting, (2) proof simplification, and (3) preserving successful verification results. By formally defining these different notions on HeyVL, an existing quantitative intermediate verification language (IVL), our concepts (and implementation) can be used to obtain diagnostics for a range of probabilistic programming languages. Slicing for error reporting is a novel notion of error localization for quantitative assertions. We demonstrate slicing-based diagnostics on a variety of proof rules such as quantitative versions of the specification statement and invariant-based loop rules, and formally prove the correctness of specialized error messages and verification hints.
  We implemented our user diagnostics into the deductive verifier Caesar. Our novel implementation -- called \emph{Brutus} -- can search for slices which do or do not verify, corresponding to each of the three diagnostic notions. For error reporting (1), it exploits a binary search-based algorithm that minimizes error-witnessing slices. To solve for slices that verify (2 and 3), we empirically compare different algorithms based on unsatisfiable cores, minimal unsatisfiable subset enumeration, and a direct SMT encoding of the slicing problem. Our empirical evaluation of Brutus on existing and new benchmarks shows that we can find slices that are both small and informative.

</details>
