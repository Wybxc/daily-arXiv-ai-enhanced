<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 19]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Analysis of AdvFusion: Adapter-based Multilingual Learning for Code Large Language Models](https://arxiv.org/abs/2511.02869)
*Amirreza Esmaeili,Fahd Seddik,Yongyi Ji,Fatemeh Fard,Fuxiang Chen*

Main category: cs.SE

TL;DR: AdvFusion是一种参数高效微调方法，在多语言代码模型上评估了代码生成、代码翻译和提交消息生成三个新任务，发现不同模型/任务表现出不同特性。


<details>
  <summary>Details</summary>
Motivation: 探索AdvFusion方法在代码大语言模型上的扩展应用，验证其在更多软件工程任务中的有效性。

Method: 使用AdvFusion参数高效微调架构，在代码大语言模型上进行多任务评估，包括代码生成、代码翻译和提交消息生成。

Result: 不同任务表现各异：代码生成中AdvFusion优于AdapterFusion但不如其他PEFT方法；提交消息生成中AdapterFusion更好；代码翻译中AdvFusion整体表现较差，且随着模型规模增大差距扩大。

Conclusion: AdvFusion在不同代码任务中的表现存在差异，需要根据具体任务选择合适的参数高效微调方法。

Abstract: Programming languages can benefit from one another by utilizing a language
model for software engineering tasks. Full fine-tuning and Parameter Efficient
Fine-Tuning (PEFT) of Code Language Models (Code-LMs) has been explored for
multilingual knowledge transfer. AdapterFusion is a PEFT architecture that aims
to enhance task performance by leveraging information from multiple programming
languages, but primarily focuses on the target programming language.
  In our previous work, we proposed AdvFusion, a novel PEFT-based approach that
effectively learns from other programming languages before adapting to the
target task. Though previous experiments showed that AdvFusion outperformed
AdapterFusion and LoRA, it was applied on pre-trained Code-LMs and was limited
to only two tasks, code summarization and method name prediction. In this
study, we expanded our work and investigated AdvFusion on Code Large Language
Models (Code-LLMs), considering three new tasks: code generation, code
translation, and commit message generation. We observed that different
Code-LLMs/tasks exhibit different characteristics. In code generation,
AdvFusion outperformed AdapterFusion but not other PEFT methods (LoRA,
Compacter, and TaskAdapter). In commit message generation, AdapterFusion
performed better than AdvFusion, and contrary to code generation, we found that
the other PEFT methods do not have better performance. In code translation,
AdvFusion performed worse than AdapterFusion overall, with the performance gap
marginally widening as the model size increases. However, consistent with code
generation, other PEFT methods showed better performance.

</details>


### [2] [SELF-REDRAFT: Eliciting Intrinsic Exploration-Exploitation Balance in Test-Time Scaling for Code Generation](https://arxiv.org/abs/2511.02854)
*Yixiang Chen,Tianshi Zheng,Shijue Huang,Zhitao He,Yi R. Fung*

Main category: cs.SE

TL;DR: 提出了SELF-REDRAFT框架，基于Self-Refine构建，通过鼓励模型为存在根本缺陷的解决方案提出新草稿，来探索LLM在测试时扩展中平衡利用和探索的内在能力。


<details>
  <summary>Details</summary>
Motivation: 在测试用例不可用的真实代码生成场景中，测试时扩展至关重要。现有方法要么依赖贪婪利用（迭代优化），要么依赖随机探索（基于采样的投票或重排），但这两者之间的平衡尚未得到充分探索。

Method: 引入SELF-REDRAFT框架，建立在Self-Refine基础上，鼓励模型为根本有缺陷的解决方案提出新草稿，以平衡利用和探索。

Result: SELF-REDRAFT在相同最大迭代次数下收敛时，始终比Self-Refine表现更好。但仍有显著改进空间，主要受限于生成指导性反馈的能力有限和判别判断脆弱。不同LLM的平衡策略差异显著。

Conclusion: 本研究为测试时扩展中的内在探索-利用平衡建立了基线，并确定反馈和判别能力是未来进步的关键领域。

Abstract: Test-time scaling without interpreter feedback is essential for real-world
code generation scenarios where test cases are not readily available. While
existing paradigms often rely on either greedy exploitation (i.e., iterative
refinement) or stochastic exploration (i.e., relying on sample-based voting or
reranking mechanisms), the balance between these two dimensions remains
underexplored. To investigate the LLM's intrinsic ability to balance
exploitation and exploration, we introduce SELF-REDRAFT, a framework built upon
Self-Refine that encourages the model to propose new drafts for solutions that
are fundamentally flawed. Our results show that SELF-REDRAFT consistently
achieves better performance than Self-Refine when converged under the same
maximum number of iterations. Still, we observe that significant room for
improvement remains, largely due to two core aspects of current self-redraft
capabilities: constrained capacity for generating instructive feedback and
fragile discriminative judgment. We also find that balancing strategies vary
notably across different LLMs, reflecting distinct, model-specific behaviors.
Overall, our study establishes a baseline for intrinsic
exploration-exploitation balancing in test-time scaling and identifies feedback
and discrimination as key areas with potential for future advances.

</details>


### [3] [The Evolution of Agile and Hybrid Project Management Methodologies: A Systematic Literature Review](https://arxiv.org/abs/2511.02859)
*Bianca Leech,Ridewaan Hanslo*

Main category: cs.SE

TL;DR: 系统文献综述分析敏捷方法论向混合框架的演变，识别实施挑战和成功因素，强调情境适应的重要性。


<details>
  <summary>Details</summary>
Motivation: IT项目的快速发展推动了项目管理方法论的转变，从传统瀑布式到敏捷框架，再到混合模型。研究旨在探索敏捷方法论如何演变为混合框架，分析其挑战和成功因素。

Method: 采用PRISMA指南对过去8年的同行评审研究进行系统文献分析，识别关键趋势。

Result: 混合方法论源于敏捷在大规模和受监管环境中的局限性，结合了迭代灵活性和结构化治理。成功因素包括领导支持、定制化流程整合和持续改进机制。

Conclusion: 研究强调情境适应比僵化框架更重要，为组织在混合转型过程中提供实践见解。

Abstract: The rapid evolution of IT projects has driven the transformation of project
management methodologies, from traditional waterfall approaches to agile
frameworks and, more recently, hybrid models. This systematic literature review
investigates the evolution of agile methodologies into hybrid frameworks,
analysing their implementation challenges and success factors. We identify key
trends through PRISMA-guided analysis of peer-reviewed studies from the last 8
years. Hybrid methodologies emerge from agile limitations in large-scale and
regulated environments, combining iterative flexibility with structured
governance. Agile has several implementation challenges, leading to hybrid
methods, and the success hinges on leadership support, tailored process
integration, and continuous improvement mechanisms. The study explores the need
for contextual adaptation over rigid frameworks, offering practical insights
for organisations navigating hybrid transitions.

</details>


### [4] [LM-Fix: Lightweight Bit-Flip Detection and Rapid Recovery Framework for Language Models](https://arxiv.org/abs/2511.02866)
*Ahmad Tahmasivand,Noureldin Zahran,Saba Al-Sayouri,Mohammed Fouda,Khaled N. Khasawneh*

Main category: cs.SE

TL;DR: LM-Fix是一个轻量级检测和快速恢复框架，用于处理大语言模型中的故障。它通过短测试向量和哈希引导检查检测位翻转故障，并进行局部修复，无需完全重新加载模型。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型完整性方法通常过于笨重或缓慢，无法满足现代LLM的需求。需要一种轻量级、快速的故障检测和恢复方案来确保LLM在生产环境中的可靠性。

Method: LM-Fix运行短测试向量并通过哈希引导检查来检测位翻转故障，然后进行局部修复而不需要完全重新加载模型。

Result: 在多个模型上，LM-Fix在TVL=200时检测到超过94%的单比特翻转和接近100%的多比特翻转，运行时开销约为1%到7.7%；恢复速度比重新加载快100倍以上。

Conclusion: LM-Fix提供了一个实用、低开销的解决方案，能够保持大语言模型在生产环境中的可靠性。

Abstract: This paper presents LM-Fix, a lightweight detection and rapid recovery
framework for faults in large language models (LLMs). Existing integrity
approaches are often heavy or slow for modern LLMs. LM-Fix runs a short
test-vector pass and uses hash-guided checks to detect bit-flip faults, then
repairs them locally without a full reload. Across multiple models, it detects
over 94% of single-bit flips at TVL=200 and nearly 100% of multi-bit flips with
approximately 1% to 7.7% runtime overhead; recovery is more than 100x faster
than reloading. These results show a practical, low-overhead solution to keep
LLMs reliable in production

</details>


### [5] [An Analysis of Early-Stage Functional Safety Analysis Methods and Their Integration into Model-Based Systems Engineering](https://arxiv.org/abs/2511.02874)
*Jannatul Shefa,Taylan G. Topcu*

Main category: cs.SE

TL;DR: 本文对比了FMEA、FHA和FFIP三种安全分析方法，并研究了它们在MBSE中的集成现状。研究发现FFIP更适合现代互联系统的安全需求，而MBSE集成主要集中在FMEA上，缺乏统一框架。


<details>
  <summary>Details</summary>
Motivation: 随着系统日益复杂，在系统生命周期早期进行有效的安全分析至关重要。本文旨在研究关键安全分析方法的能力及其在基于模型的系统工程中的集成现状。

Method: 采用两阶段方法：第一阶段对比FMEA、FHA和FFIP技术的程序、优缺点；第二阶段回顾这些方法在MBSE中的集成研究。

Result: 分析显示FFIP在识别系统涌现行为、二阶效应和故障传播方面能力更强；MBSE集成主要集中在FMEA，可分为四类集成方法，但缺乏统一框架。

Conclusion: 需要一种集成方法支持数字工程转型，实现更协同的生命周期安全管理方法和工具。

Abstract: As systems become increasingly complex, conducting effective safety analysis
in the earlier phases of a system's lifecycle is essential to identify and
mitigate risks before they escalate. To that end, this paper investigates the
capabilities of key safety analysis techniques, namely: Failure Mode and
Effects Analysis (FMEA), Functional Hazard Analysis (FHA), and Functional
Failure Identification and Propagation (FFIP), along with the current state of
the literature in terms of their integration into Model-Based Systems
Engineering (MBSE). A two-phase approach is adopted. The first phase is focused
on contrasting FMEA, FHA, and FFIP techniques, examining their procedures,
along with a documentation of their relative strengths and limitations. Our
analysis highlights FFIP's capability in identifying emergent system behaviors,
second-order effects, and fault propagation; thus, suggesting it is better
suited for the safety needs of modern interconnected systems. Second, we review
the existing research on the efforts to integrate each of these methods into
MBSE. We find that MBSE integration efforts primarily focus on FMEA, and
integration of FHA and FFIP is nascent. Additionally, FMEA-MBSE integration
efforts could be organized into four categories: model-to-model transformation,
use of external customized algorithms, built-in MBSE packages, and manual use
of standard MBSE diagrams. While our findings indicate a variety of MBSE
integration approaches, there is no universally established framework or
standard. This leaves room for an integration approach that could support the
ongoing Digital Engineering transformation efforts by enabling a more
synergistic lifecycle safety management methods and tools.

</details>


### [6] [CS Educator challenges and their solutions : A systematic mapping study](https://arxiv.org/abs/2511.02876)
*Anjali Chouhan,Sruti Srinivasa Ragavan,Amey Karkare*

Main category: cs.SE

TL;DR: 本文通过结构化文献综述分析了计算机科学教育中教师面临的挑战及应对措施，识别了十大主题领域中的重复性问题，并指出了研究不足的领域。


<details>
  <summary>Details</summary>
Motivation: 计算机科学教育快速发展，但教师在教学环境中面临持续挑战，缺乏系统性的挑战分类和应对措施综合研究，不清楚哪些领域已充分解决，哪些仍需关注。

Method: 对过去五年同行评审研究论文进行结构化文献综述，重点关注十大分类主题中的挑战和应对措施，包括教学法、情感、技术和制度等维度。

Result: 分析揭示了在评估实践、教师培训、课堂管理和情感健康等领域的重复性问题，以及为缓解这些问题采取的各种策略，同时发现若干研究不足的领域。

Conclusion: 本综述提供了对计算机科学教育现状的整合理解，为研究者、课程设计者和政策制定者改善教学效果和教师支持提供了有价值的见解。

Abstract: Computer Science (CS) education is expanding rapidly, but educators continue
to face persistent challenges in teaching and learning environments.Despite
growing interest, limited systematic work exists to categorize and synthesize
the specific challenges faced by CS educators and the remedies adopted in
response.This is problematic because it remains unclear which areas have been
thoroughly addressed and which still lack sufficient scholarly attention. In
this study, we conducted a structured literature review of peer-reviewed
research papers published over the last five years, focusing on challenges and
remedies across ten categorized themes, including pedagogical, emotional,
technological, and institutional dimensions.Our analysis revealed recurring
issues in areas such as assessment practices, teacher training, classroom
management, and emotional well-being, along with various strategies such as
professional development programs and policy interventions adopted to mitigate
them while also revealing several areas that have received insufficient
attention.This review offers a consolidated understanding of the CS education
landscape, providing valuable insights for researchers, curriculum designers,
and policymakers aiming to improve teaching effectiveness and educator support.

</details>


### [7] [AgentSLA : Towards a Service Level Agreement for AI Agents](https://arxiv.org/abs/2511.02885)
*Gwendal Jouneaux,Jordi Cabot*

Main category: cs.SE

TL;DR: 本文提出了基于ISO/IEC 25010标准的AI智能体质量模型和领域特定语言，用于支持AI智能体服务的SLA定义。


<details>
  <summary>Details</summary>
Motivation: AI智能体正成为各类软件系统的关键组件，从模型即服务转向智能体即服务范式，但AI组件的质量定义和SLA规范仍是一个开放挑战，缺乏质量保证的共识方法。

Method: 开发了基于ISO/IEC 25010标准的AI智能体质量模型，并创建了领域特定语言来支持AI智能体服务的SLA定义。

Result: 提出了系统化的质量模型和SLA定义工具，为解决AI智能体质量保证问题提供了标准化框架。

Conclusion: 该研究为AI智能体的质量保证和SLA规范提供了重要解决方案，有助于提升智能软件系统的开发质量。

Abstract: AI components are increasingly becoming a key element of all types of
software systems to enhance their functionality. These AI components are often
implemented as AI Agents, offering more autonomy than a plain integration of
Large Language Models (LLMs), moving from a Model-as-a-Service paradigm to an
Agent-as-a-Service one, bringing new challenges to the development of smart
software systems. Indeed, while support for the design, implementation, and
deployment of those agents exist, the specification of Quality of Service (QoS)
and definition of Service Level Agreements (SLAs) aspects for those agents,
important to ensure the quality of the resulting systems, remains an open
challenge. Part of this is due to the difficulty to clearly define quality in
the context of AI components, resulting in a lack of consensus on how to best
approach Quality Assurance (QA) for these types of systems. To address this
challenge, this paper proposes both a quality model for AI agents based on the
ISO/IEC 25010 standard, and a domain specific language to support the
definition of SLAs for the services provided by these AI agents.

</details>


### [8] [Comprehension-Performance Gap in GenAI-Assisted Brownfield Programming: A Replication and Extension](https://arxiv.org/abs/2511.02922)
*Yunhan Qiao,Christopher Hundhausen,Summit Haque,Md Istiak Hossain Shihab*

Main category: cs.SE

TL;DR: 生成式AI编码助手在遗留代码维护任务中能提高开发效率，但不会改善代码理解能力，存在理解与性能之间的差距。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI编码助手在遗留代码维护任务中对开发效率和代码理解能力的影响，验证是否存在理解-性能差距。

Method: 采用组内实验设计，18名计算机科学研究生在有无Copilot辅助下完成功能实现任务，比较任务时间、测试用例通过率和理解得分。

Result: Copilot显著减少任务时间并提高测试用例通过率，但理解得分没有差异，且理解与任务性能之间无相关性。

Conclusion: 生成式AI工具能加速遗留代码库的编程进度，但这种进步可能不会带来对代码库的更好理解，这对编程教育和AI工具设计有重要启示。

Abstract: Code comprehension is essential for brownfield programming tasks, in which
developers maintain and enhance legacy code bases. Generative AI (GenAI) coding
assistants such as GitHub Copilot have been shown to improve developer
productivity, but their impact on code understanding is less clear. We
replicate and extend a previous study by exploring both performance and
comprehension in GenAI-assisted brownfield programming tasks. In a
within-subjects experimental study, 18 computer science graduate students
completed feature implementation tasks with and without Copilot. Results show
that Copilot significantly reduced task time and increased the number of test
cases passed. However, comprehension scores did not differ across conditions,
revealing a comprehension-performance gap: participants passed more test cases
with Copilot, but did not demonstrate greater understanding of the legacy
codebase. Moreover, we failed to find a correlation between comprehension and
task performance. These findings suggest that while GenAI tools can accelerate
programming progress in a legacy codebase, such progress may come without an
improved understanding of that codebase. We consider the implications of these
findings for programming education and GenAI tool design.

</details>


### [9] [Risk Estimation in Differential Fuzzing via Extreme Value Theory](https://arxiv.org/abs/2511.02927)
*Rafael Baez,Alejandro Olivas,Nathan K. Diamond,Marcelo Frias,Yannic Noller,Saeid Tizpaz-Niari*

Main category: cs.SE

TL;DR: 该论文应用极值理论来评估差分模糊测试中遗漏或低估bug的风险，通过在真实Java库中进行实验，证明EVT方法能够有效预测风险并节省测试资源。


<details>
  <summary>Details</summary>
Motivation: 差分模糊测试作为动态分析无法保证bug的完全检测，需要一种方法来评估在未发现bug或差异较小时继续测试的风险。

Method: 应用极值理论分析差分模糊测试过程中观察到的最大差异分布，通过统计方法预测遗漏bug的风险。

Result: EVT方法在14.3%的情况下优于基线统计方法，在64.2%的情况下与基线持平，在真实Java库测试中平均节省了数千万字节码执行。

Conclusion: 极值理论为差分模糊测试提供了有效的风险评估框架，能够显著提高测试效率并减少资源消耗。

Abstract: Differential testing is a highly effective technique for automatically
detecting software bugs and vulnerabilities when the specifications involve an
analysis over multiple executions simultaneously. Differential fuzzing, in
particular, operates as a guided randomized search, aiming to find (similar)
inputs that lead to a maximum difference in software outputs or their
behaviors. However, fuzzing, as a dynamic analysis, lacks any guarantees on the
absence of bugs: from a differential fuzzing campaign that has observed no bugs
(or a minimal difference), what is the risk of observing a bug (or a larger
difference) if we run the fuzzer for one or more steps?
  This paper investigates the application of Extreme Value Theory (EVT) to
address the risk of missing or underestimating bugs in differential fuzzing.
The key observation is that differential fuzzing as a random process resembles
the maximum distribution of observed differences. Hence, EVT, a branch of
statistics dealing with extreme values, is an ideal framework to analyze the
tail of the differential fuzzing campaign to contain the risk. We perform
experiments on a set of real-world Java libraries and use differential fuzzing
to find information leaks via side channels in these libraries. We first
explore the feasibility of EVT for this task and the optimal hyperparameters
for EVT distributions. We then compare EVT-based extrapolation against baseline
statistical methods like Markov's as well as Chebyshev's inequalities, and the
Bayes factor. EVT-based extrapolations outperform the baseline techniques in
14.3% of cases and tie with the baseline in 64.2% of cases. Finally, we
evaluate the accuracy and performance gains of EVT-enabled differential fuzzing
in real-world Java libraries, where we reported an average saving of tens of
millions of bytecode executions by an early stop.

</details>


### [10] [Assurance Case Development for Evolving Software Product Lines: A Formal Approach](https://arxiv.org/abs/2511.03026)
*Logan Murphy,Torin Viger,Alessio Di Sandro,Aren A. Babikian,Marsha Chechik*

Main category: cs.SE

TL;DR: 提出了一种用于软件产品线(SPL)的形式化提升保证案例开发方法，支持变体感知的回归分析，以解决SPL中保证案例开发的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 在软件产品线中，为每个产品单独开发严格的保证案例不可行，且当产品线演进时难以评估变更影响，需要一种能够同时为整个产品线开发保证案例的方法。

Method: 形式化定义了用于SPL的变体感知保证案例语言，研究了基于模板的保证案例开发提升方法，定义了SPL演进对变体感知保证案例影响的回归分析。

Result: 开发了一个基于模型的保证管理工具，并通过医疗设备产品线的保证案例开发实例验证了所提方法。

Conclusion: 该方法能够有效支持软件产品线的提升保证案例开发和变体感知回归分析，解决了大规模产品线中保证案例开发的可扩展性问题。

Abstract: In critical software engineering, structured assurance cases (ACs) are used
to demonstrate how key system properties are supported by evidence (e.g., test
results, proofs). Creating rigorous ACs is particularly challenging in the
context of software product lines (SPLs), i.e, sets of software products with
overlapping but distinct features and behaviours. Since SPLs can encompass very
large numbers of products, developing a rigorous AC for each product
individually is infeasible. Moreover, if the SPL evolves, e.g., by the
modification or introduction of features, it can be infeasible to assess the
impact of this change. Instead, the development and maintenance of ACs ought to
be lifted such that a single AC can be developed for the entire SPL
simultaneously, and be analyzed for regression in a variability-aware fashion.
In this article, we describe a formal approach to lifted AC development and
regression analysis. We formalize a language of variability-aware ACs for SPLs
and study the lifting of template-based AC development. We also define a
regression analysis to determine the effects of SPL evolutions on
variability-aware ACs. We describe a model-based assurance management tool
which implements these techniques, and illustrate our contributions by
developing an AC for a product line of medical devices.

</details>


### [11] [Adaptive Detection of Software Aging under Workload Shift](https://arxiv.org/abs/2511.03103)
*Rafael José Moura,Maria Gizele Nascimento,Fumio Machida,Ermeson Andrade*

Main category: cs.SE

TL;DR: 提出基于机器学习的自适应软件老化检测方法，在动态工作负载条件下使用ADWIN自适应窗口技术，相比静态模型能保持高检测精度。


<details>
  <summary>Details</summary>
Motivation: 软件老化现象影响长期运行系统，导致性能逐渐下降和故障风险增加，需要解决动态工作负载条件下的检测问题。

Method: 比较静态模型与包含DDM和ADWIN自适应检测器的自适应模型，将概念漂移检测方法应用于处理工作负载变化。

Result: 自适应模型在突发、渐进和重复工作负载转换场景中保持高精度，F1分数均超过0.93，而静态模型在未见工作负载下性能显著下降。

Conclusion: 基于ADWIN的自适应模型能有效应对动态工作负载变化，在软件老化检测中表现出优越性能。

Abstract: Software aging is a phenomenon that affects long-running systems, leading to
progressive performance degradation and increasing the risk of failures. To
mitigate this problem, this work proposes an adaptive approach based on machine
learning for software aging detection in environments subject to dynamic
workload conditions. We evaluate and compare a static model with adaptive
models that incorporate adaptive detectors, specifically the Drift Detection
Method (DDM) and Adaptive Windowing (ADWIN), originally developed for concept
drift scenarios and applied in this work to handle workload shifts. Experiments
with simulated sudden, gradual, and recurring workload transitions show that
static models suffer a notable performance drop when applied to unseen workload
profiles, whereas the adaptive model with ADWIN maintains high accuracy,
achieving an F1-Score above 0.93 in all analyzed scenarios.

</details>


### [12] [Automated Prompt Generation for Code Intelligence: An Empirical study and Experience in WeChat](https://arxiv.org/abs/2511.03136)
*Kexing Ji,Shiyun Fu,Cuiyun Gao,Yujia Chen,Zezhou Yang,Chaozheng Wang,Yuetang Deng*

Main category: cs.SE

TL;DR: 该论文研究了大型代码模型中自动提示生成的重要性，通过实证研究指令生成和多步推理两种方法，并提出了一种结合两者优势的新方法，在多个代码智能任务上显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型代码模型的提示设计主要依赖人工，耗时且依赖特定模型和任务。虽然NLP领域已有自动提示生成方法，但在代码智能领域尚未充分探索，这给面对多样化任务和黑盒模型的开发者带来了挑战。

Method: 实证研究指令生成和多步推理两种自动提示生成方法，并在四种开源大型代码模型和三个代码智能任务上进行评估。基于评估结果，提出了一种结合两种方法最佳实践的新自动提示生成方法。

Result: 新方法在代码翻译任务上平均提升28.38%的CodeBLEU分数，在代码摘要任务上提升58.11%的ROUGE-L分数，在API推荐任务上提升84.53%的SuccessRate@1。在工业场景的WeChat-Bench数据集上，API推荐的MRR平均提升148.89%。

Conclusion: 自动提示生成能显著提升大型代码模型的性能，指令生成和多步推理是有效的自动提示生成方法，提出的新方法在实际应用中表现出色，为代码智能任务提供了有效的自动化解决方案。

Abstract: Large Code Models (LCMs) show potential in code intelligence, but their
effectiveness is greatly influenced by prompt quality. Current prompt design is
mostly manual, which is time-consuming and highly dependent on specific LCMs
and tasks. While automated prompt generation (APG) exists in NLP, it is
underexplored for code intelligence. This creates a gap, as automating the
prompt process is essential for developers facing diverse tasks and black-box
LCMs.
  To mitigate this, we empirically investigate two important parts of APG:
Instruction Generation (IG) and Multi-Step Reasoning (MSR). IG provides a
task-related description to instruct LCMs, while MSR guides them to produce
logical steps before the final answer. We evaluate widely-used APG methods for
each part on four open-source LCMs and three code intelligence tasks: code
translation (PL-PL), code summarization (PL-NL), and API recommendation
(NL-PL).Experimental results indicate that both IG and MSR dramatically enhance
performance compared to basic prompts. Based on these results, we propose a
novel APG approach combining the best methods of the two parts. Experiments
show our approach achieves average improvements of 28.38% in CodeBLEU (code
translation), 58.11% in ROUGE-L (code summarization), and 84.53% in
SuccessRate@1 (API recommendation) over basic prompts. To validate its
effectiveness in an industrial scenario, we evaluate our approach on
WeChat-Bench, a proprietary dataset, achieving an average MRR improvement of
148.89% for API recommendation.

</details>


### [13] [RefAgent: A Multi-agent LLM-based Framework for Automatic Software Refactoring](https://arxiv.org/abs/2511.03153)
*Khouloud Oueslati,Maxime Lamothe,Foutse Khomh*

Main category: cs.SE

TL;DR: RefAgent是一个基于LLM的多智能体框架，用于端到端的软件重构，通过专门的规划、执行、测试和迭代优化智能体，显著提升了代码质量和重构效果。


<details>
  <summary>Details</summary>
Motivation: 传统LLM在软件重构中依赖静态指令，而LLM智能体能够动态适应上下文并自主决策，探索其在重构活动中的潜力。

Method: 引入RefAgent多智能体框架，包含负责规划、执行、测试和迭代优化的专门智能体，利用自反思和工具调用能力进行重构。

Result: 在8个开源Java项目上评估，RefAgent达到90%的单元测试通过率，减少52.5%的代码坏味，提升8.6%的关键质量属性，与开发者重构和搜索工具相比分别获得79.15%和72.7%的F1分数。

Conclusion: 多智能体架构在推进自动化软件重构方面具有巨大潜力，相比单智能体方法显著提升了测试通过率和编译成功率。

Abstract: Large Language Models (LLMs) have substantially influenced various software
engineering tasks. Indeed, in the case of software refactoring, traditional
LLMs have shown the ability to reduce development time and enhance code
quality. However, these LLMs often rely on static, detailed instructions for
specific tasks. In contrast, LLM-based agents can dynamically adapt to evolving
contexts and autonomously make decisions by interacting with software tools and
executing workflows. In this paper, we explore the potential of LLM-based
agents in supporting refactoring activities. Specifically, we introduce
RefAgent, a multi-agent LLM-based framework for end-to-end software
refactoring. RefAgent consists of specialized agents responsible for planning,
executing, testing, and iteratively refining refactorings using self-reflection
and tool-calling capabilities. We evaluate RefAgent on eight open-source Java
projects, comparing its effectiveness against a single-agent approach, a
search-based refactoring tool, and historical developer refactorings. Our
assessment focuses on: (1) the impact of generated refactorings on software
quality, (2) the ability to identify refactoring opportunities, and (3) the
contribution of each LLM agent through an ablation study. Our results show that
RefAgent achieves a median unit test pass rate of 90%, reduces code smells by a
median of 52.5%, and improves key quality attributes (e.g., reusability) by a
median of 8.6%. Additionally, it closely aligns with developer refactorings and
the search-based tool in identifying refactoring opportunities, attaining a
median F1-score of 79.15% and 72.7%, respectively. Compared to single-agent
approaches, RefAgent improves the median unit test pass rate by 64.7% and the
median compilation success rate by 40.1%. These findings highlight the promise
of multi-agent architectures in advancing automated software refactoring.

</details>


### [14] [Understanding Robustness of Model Editing in Code LLMs: An Empirical Study](https://arxiv.org/abs/2511.03182)
*Vinaik Chhetri,A. B Siddique,Umar Farooq*

Main category: cs.SE

TL;DR: 该研究系统评估了5种模型编辑方法在代码LLMs中的应用，发现在API弃用场景下，即时编辑会导致模型性能显著下降，语法有效性最多下降86个百分点，功能正确性下降45点。顺序编辑进一步加剧性能退化，正确采用编辑的情况仅占约6%。


<details>
  <summary>Details</summary>
Motivation: LLMs在训练后保持静态，而编程语言和API持续演进，导致生成的代码可能已过时或不兼容。重新训练LLMs成本高昂，模型编辑作为一种轻量级替代方案，但尚不清楚其是否能实现真正的语法和语义适应。

Method: 对5种最先进的模型编辑方法（Constrained FT、GRACE、MEMIT、PMET、ROME）在3个开源代码LLMs（CodeLlama、CodeQwen1.5、DeepSeek-Coder）上进行系统研究，在受控API弃用场景下评估即时和顺序编辑设置。

Result: 即时编辑持续降低模型性能，语法有效性最多下降86个百分点，功能正确性下降45点。顺序编辑进一步放大这种退化，某些情况下模型性能完全崩溃。正确采用编辑的情况仅占约6%，大多数通过生成依赖变通方案而非正确采纳预期更改。

Conclusion: 当前模型编辑方法在代码LLMs中效果有限，主要产生表面修复而非真正的语法和语义适应，正确采用率极低，表明需要更有效的模型更新策略来应对编程环境的持续演进。

Abstract: Large language models (LLMs) are increasingly used in software development.
However, while LLMs remain static after pretraining, programming languages and
APIs continue to evolve, leading to the generation of deprecated or
incompatible code that undermines reliability. Retraining LLMs from scratch to
reflect such changes is computationally expensive, making model editing a
promising lightweight alternative that updates only a small subset of
parameters. Despite its potential, it remains unclear whether model editing
yields genuine syntactic and semantic adaptations or merely superficial fixes.
In this work, we present a systematic study of five state-of-the-art model
editing methods: Constrained Fine-Tuning (FT), GRACE, MEMIT, PMET, and ROME. We
apply these methods to three leading open-source code LLMs, CodeLlama,
CodeQwen1.5, and DeepSeek-Coder, under controlled API deprecation scenarios.
Our evaluation covers both instant and sequential editing settings, using three
disjoint evaluation sets designed to assess reliability, generalization, and
specificity. We measure model correctness at three levels: successful
compilation, partial test case pass, and full test pass. Our findings show that
instant edits consistently degrade model performance, with syntactic validity
dropping by up to 86 percentage points and functional correctness declining by
45 points even in the best-performing setting. Sequential edits further amplify
this degradation, and in some cases, model performance collapses entirely.
Across all models, most passing generations relied on workarounds rather than
correctly adopting the intended changes, while faulty adoptions that result in
test failures or compilation errors were significantly more frequent. Correct
adoptions, where the model correctly integrates the intended change, occurred
in only about 6% of cases.

</details>


### [15] [Towards Realistic Project-Level Code Generation via Multi-Agent Collaboration and Semantic Architecture Modeling](https://arxiv.org/abs/2511.03404)
*Qianhui Zhao,Li Zhang,Fang Liu,Junhang Cheng,Chengru Wu,Junchen Ai,Qiaoyuanhe Meng,Lichen Zhang,Xiaoli Lian,Shubin Song,Yuanping Guo*

Main category: cs.SE

TL;DR: 提出了ProjectGen多智能体框架和CodeProjectEval数据集，用于解决项目级代码生成中的语义鸿沟、层次依赖管理和质量维护问题，在真实项目数据集上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有项目级代码生成研究存在数据集不真实、评估指标不可靠、用户需求与机器可解释结构之间存在语义鸿沟、难以管理层次依赖和保持生成质量等问题。

Method: 提出ProjectGen多智能体框架，将项目分解为架构设计、骨架生成和代码填充三个阶段，采用迭代优化和基于记忆的上下文管理，引入语义软件架构树(SSAT)来桥接用户需求和源代码实现。

Result: 在DevBench小规模项目级代码生成数据集上通过52/124个测试用例，比基线方法提升57%；在CodeProjectEval数据集上通过310个测试用例，比基线方法提升约10倍。

Conclusion: ProjectGen框架和CodeProjectEval数据集有效解决了项目级代码生成的关键挑战，实现了最先进的性能，为真实软件开发环境中的自动化项目生成提供了可行方案。

Abstract: In recent years, Large Language Models (LLMs) have achieved remarkable
progress in automated code generation. In real-world software engineering, the
growing demand for rapid iteration and continuous delivery underscores the
importance of project-level code generation, where LLMs are expected to
generate complete software projects directly from complex user requirements.
Although existing studies have made initial explorations, they still face key
limitations, including unrealistic datasets and unreliable evaluation metrics
that fail to reflect real-world complexity, the semantic gap between
human-written requirements and machine-interpretable structures, and
difficulties in managing hierarchical dependencies and maintaining quality
throughout the generation process. To address these limitations, we first
introduce CodeProjectEval, a project-level code generation dataset built from
18 real-world repositories with 12.7 files and 2,388.6 lines of code per task
on average, supplemented with documentation and executable test cases for
automatic evaluation. We further propose ProjectGen, a multi-agent framework
that decomposes projects into architecture design, skeleton generation, and
code filling stages with iterative refinement and memory-based context
management. Within this framework, we introduce the Semantic Software
Architecture Tree (SSAT), a structured and semantically rich representation
that effectively bridges user requirements and source code implementation.
Experiments show that ProjectGen achieves state-of-the-art performance, passing
52/124 test cases on the small-scale project-level code generation dataset
DevBench, a 57% improvement over the baseline approaches, and 310 test cases on
CodeProjectEval, representing an improvement of roughly tenfold compared to the
baselines.

</details>


### [16] [Light over Heavy: Automated Performance Requirements Quantification with Linguistic Inducement](https://arxiv.org/abs/2511.03421)
*Shihai Wang,Tao Chen*

Main category: cs.SE

TL;DR: LQPR是一种高效的自动化性能需求量化方法，通过将量化问题转化为分类问题，使用轻量级语言诱导匹配机制，在性能和成本上都优于基于LLM的方法。


<details>
  <summary>Details</summary>
Motivation: 现有性能需求量化主要依赖人工方法，成本高且容易出错。需要开发自动化的量化方法来提高效率和准确性。

Method: LQPR将性能需求量化转化为分类问题，利用性能需求具有强模式和简洁性的特点，设计轻量级语言诱导匹配机制，而不是依赖大型语言模型。

Result: 在多样化数据集上与9种最先进的学习方法比较，LQPR在75%以上的情况下排名第一，且成本降低两个数量级。

Conclusion: 对于性能需求量化任务，专门化的方法比通用的LLM驱动方法更合适，LQPR证明了这一点。

Abstract: Elicited performance requirements need to be quantified for compliance in
different engineering tasks, e.g., configuration tuning and performance
testing. Much existing work has relied on manual quantification, which is
expensive and error-prone due to the imprecision. In this paper, we present
LQPR, a highly efficient automatic approach for performance requirements
quantification.LQPR relies on a new theoretical framework that converts
quantification as a classification problem. Despite the prevalent applications
of Large Language Models (LLMs) for requirement analytics, LQPR takes a
different perspective to address the classification: we observed that
performance requirements can exhibit strong patterns and are often
short/concise, therefore we design a lightweight linguistically induced
matching mechanism. We compare LQPR against nine state-of-the-art
learning-based approaches over diverse datasets, demonstrating that it is
ranked as the sole best for 75% or more cases with two orders less cost. Our
work proves that, at least for performance requirement quantification,
specialized methods can be more suitable than the general LLM-driven
approaches.

</details>


### [17] [U2F: Encouraging SWE-Agent to Seize Novelty without Losing Feasibility](https://arxiv.org/abs/2511.03517)
*Wencheng Ye,Yan Liu*

Main category: cs.SE

TL;DR: U2F是一个认知启发的多智能体框架，通过拥抱不确定性来发现软件工程中的创新解决方案，显著提升了解决方案的新颖性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的软件工程智能体主要解决定义明确的问题，但在开放世界软件环境中难以发现超越预定义框架的创新解决方案。

Method: U2F包含发现-探索-集成智能体系统，以及跨领域类比推理、逆向思维和外部验证三个维度的认知增强机制。

Result: 在218个真实软件工程任务中，U2F使整体新颖性提升14%，语义新颖性提升51%，可行性稳定在4.02/5.0。

Conclusion: 拥抱不确定性可以作为软件工程创新的催化剂，U2F框架展示了通过系统性发现未知未知来推动创新的潜力。

Abstract: Large language models (LLMs) have shown strong capabilities in software
engineering tasks, yet most existing LLM-based SWE-Agents mainly tackle
well-defined problems using conventional methods, often overlooking alternative
or innovative solutions beyond their predefined frameworks. This limitation is
evident in open-world software environments, where emerging challenges
transcend established paradigms.
  We propose U2F (Unknown Unknowns to Functional solutions), a
cognitive-inspired, uncertainty-embracing multi-agent framework that
systematically surfaces "Unknown Unknowns" - novel solution pathways absent
from initial formulations but holding innovative potential. U2F consists of two
key components: (1) a Discovery-Exploration-Integration agent system for
uncovering and synthesizing potential solutions, and (2) cognitive enhancement
mechanisms across three dimensions: cross-domain analogical reasoning, reverse
thinking, and external validation, which strategically reframe and extend
conventional solution boundaries.
  Applied to 218 real-world software enabler stories curated from authentic
engineering tasks, U2F achieved notable improvements: human experts reported a
14 percent increase in overall novelty, 51 percent improvement in semantic
novelty, and stable feasibility (4.02/5.0), corroborated by an LLM-based
evaluator. These results highlight the potential of embracing uncertainty as a
catalyst for innovation in software engineering.

</details>


### [18] [Uncovering Code Insights: Leveraging GitHub Artifacts for Deeper Code Understanding](https://arxiv.org/abs/2511.03549)
*Ziv Nevo,Orna Raz,Karen Yorav*

Main category: cs.SE

TL;DR: 提出了一种利用GitHub自然语言工件（如PR描述、issue讨论等）来增强LLM代码理解的新方法，通过提取GitHub上下文、生成代码目的解释和验证解释三个组件，提升代码解释的质量和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在生成代码解释时缺乏软件工程上下文基础，需要利用GitHub中的自然语言工件来提供更丰富的背景信息，从而提高代码理解的质量。

Method: 系统包含三个组件：提取和结构化GitHub上下文、基于上下文生成代码目的的高层解释、验证解释的有效性。实现了独立工具和MCP服务器两种形式。

Result: 用户研究表明，当系统生成见解时，这些见解通常是有帮助且非平凡的，并且没有出现幻觉问题。

Conclusion: 通过利用GitHub自然语言工件增强LLM的代码理解能力，可以有效提升代码解释的质量和实用性，为软件维护和现代化提供更好的支持。

Abstract: Understanding the purpose of source code is a critical task in software
maintenance, onboarding, and modernization. While large language models (LLMs)
have shown promise in generating code explanations, they often lack grounding
in the broader software engineering context. We propose a novel approach that
leverages natural language artifacts from GitHub -- such as pull request
descriptions, issue descriptions and discussions, and commit messages -- to
enhance LLM-based code understanding. Our system consists of three components:
one that extracts and structures relevant GitHub context, another that uses
this context to generate high-level explanations of the code's purpose, and a
third that validates the explanation. We implemented this as a standalone tool,
as well as a server within the Model Context Protocol (MCP), enabling
integration with other AI-assisted development tools. Our main use case is that
of enhancing a standard LLM-based code explanation with code insights that our
system generates. To evaluate explanations' quality, we conducted a small scale
user study, with developers of several open projects, as well as developers of
proprietary projects. Our user study indicates that when insights are generated
they often are helpful and non trivial, and are free from hallucinations.

</details>


### [19] [The OpenHands Software Agent SDK: A Composable and Extensible Foundation for Production Agents](https://arxiv.org/abs/2511.03690)
*Xingyao Wang,Simon Rosenberg,Juan Michelini,Calvin Smith,Hoang Tran,Engel Nyst,Rohit Malhotra,Xuhui Zhou,Valerie Chen,Robert Brennan,Graham Neubig*

Main category: cs.SE

TL;DR: OpenHands Software Agent SDK是一个用于构建软件工程代理的工具包，提供灵活性、安全性和用户交互功能，支持从简单到复杂的代理实现。


<details>
  <summary>Details</summary>
Motivation: 构建生产就绪的软件工程代理是一个复杂任务，需要灵活的实现在线、可靠安全的执行环境以及用户交互接口。

Method: 重新设计了OpenHands框架的代理组件，提供简单接口实现基本代理，可扩展支持自定义工具、内存管理等复杂功能，具备本地到远程执行的可移植性、集成REST/WebSocket服务。

Result: 在SWE-Bench Verified和GAIA基准测试中表现出色，能够可靠地大规模部署代理。

Conclusion: OpenHands Software Agent SDK为原型设计、解锁新型自定义应用和大规模可靠部署代理提供了实用基础。

Abstract: Agents are now used widely in the process of software development, but
building production-ready software engineering agents is a complex task.
Deploying software agents effectively requires flexibility in implementation
and experimentation, reliable and secure execution, and interfaces for users to
interact with agents. In this paper, we present the OpenHands Software Agent
SDK, a toolkit for implementing software development agents that satisfy these
desiderata. This toolkit is a complete architectural redesign of the agent
components of the popular OpenHands framework for software development agents,
which has 64k+ GitHub stars. To achieve flexibility, we design a simple
interface for implementing agents that requires only a few lines of code in the
default case, but is easily extensible to more complex, full-featured agents
with features such as custom tools, memory management, and more. For security
and reliability, it delivers seamless local-to-remote execution portability,
integrated REST/WebSocket services. For interaction with human users, it can
connect directly to a variety of interfaces, such as visual workspaces (VS
Code, VNC, browser), command-line interfaces, and APIs. Compared with existing
SDKs from OpenAI, Claude, and Google, OpenHands uniquely integrates native
sandboxed execution, lifecycle control, model-agnostic multi-LLM routing, and
built-in security analysis. Empirical results on SWE-Bench Verified and GAIA
benchmarks demonstrate strong performance. Put together, these elements allow
the OpenHands Software Agent SDK to provide a practical foundation for
prototyping, unlocking new classes of custom applications, and reliably
deploying agents at scale.

</details>
