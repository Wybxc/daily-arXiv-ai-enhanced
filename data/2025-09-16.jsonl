{"id": "2509.10572", "categories": ["cs.SE", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.10572", "abs": "https://arxiv.org/abs/2509.10572", "authors": ["Ashlesha Akella", "Akshar Kaul", "Krishnasuri Narayanam", "Sameep Mehta"], "title": "Quality Assessment of Tabular Data using Large Language Models and Code Generation", "comment": "EMNLP industry track submitted", "summary": "Reliable data quality is crucial for downstream analysis of tabular datasets,\nyet rule-based validation often struggles with inefficiency, human\nintervention, and high computational costs. We present a three-stage framework\nthat combines statistical inliner detection with LLM-driven rule and code\ngeneration. After filtering data samples through traditional clustering, we\niteratively prompt LLMs to produce semantically valid quality rules and\nsynthesize their executable validators through code-generating LLMs. To\ngenerate reliable quality rules, we aid LLMs with retrieval-augmented\ngeneration (RAG) by leveraging external knowledge sources and domain-specific\nfew-shot examples. Robust guardrails ensure the accuracy and consistency of\nboth rules and code snippets. Extensive evaluations on benchmark datasets\nconfirm the effectiveness of our approach."}
{"id": "2509.10649", "categories": ["cs.SE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.10649", "abs": "https://arxiv.org/abs/2509.10649", "authors": ["Johan Cederbladh", "Loek Cleophas", "Eduard Kamburjan", "Lucas Lima", "Rakshit Mittal", "Hans Vangheluwe"], "title": "Reasonable Experiments in Model-Based Systems Engineering", "comment": null, "summary": "With the current trend in Model-Based Systems Engineering towards Digital\nEngineering and early Validation & Verification, experiments are increasingly\nused to estimate system parameters and explore design decisions. Managing such\nexperimental configuration metadata and results is of utmost importance in\naccelerating overall design effort. In particular, we observe it is important\nto 'intelligent-ly' reuse experiment-related data to save time and effort by\nnot performing potentially superfluous, time-consuming, and resource-intensive\nexperiments. In this work, we present a framework for managing experiments on\ndigital and/or physical assets with a focus on case-based reasoning with domain\nknowledge to reuse experimental data efficiently by deciding whether an\nalready-performed experiment (or associated answer) can be reused to answer a\nnew (potentially different) question from the engineer/user without having to\nset up and perform a new experiment. We provide the general architecture for\nsuch an experiment manager and validate our approach using an industrial\nvehicular energy system-design case study."}
{"id": "2509.10819", "categories": ["cs.SE", "cs.CR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.10819", "abs": "https://arxiv.org/abs/2509.10819", "authors": ["Christoph Hochrainer", "Valentin Wüstholz", "Maria Christakis"], "title": "Arguzz: Testing zkVMs for Soundness and Completeness Bugs", "comment": null, "summary": "Zero-knowledge virtual machines (zkVMs) are increasingly deployed in\ndecentralized applications and blockchain rollups since they enable verifiable\noff-chain computation. These VMs execute general-purpose programs, frequently\nwritten in Rust, and produce succinct cryptographic proofs. However, zkVMs are\ncomplex, and bugs in their constraint systems or execution logic can cause\ncritical soundness (accepting invalid executions) or completeness (rejecting\nvalid ones) issues.\n  We present Arguzz, the first automated tool for testing zkVMs for soundness\nand completeness bugs. To detect such bugs, Arguzz combines a novel variant of\nmetamorphic testing with fault injection. In particular, it generates\nsemantically equivalent program pairs, merges them into a single Rust program\nwith a known output, and runs it inside a zkVM. By injecting faults into the\nVM, Arguzz mimics malicious or buggy provers to uncover overly weak\nconstraints.\n  We used Arguzz to test six real-world zkVMs (RISC Zero, Nexus, Jolt, SP1,\nOpenVM, and Pico) and found eleven bugs in three of them. One RISC Zero bug\nresulted in a $50,000 bounty, despite prior audits, demonstrating the critical\nneed for systematic testing of zkVMs."}
{"id": "2509.10920", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.10920", "abs": "https://arxiv.org/abs/2509.10920", "authors": ["Guan-Yan Yang", "Farn Wang", "You-Zong Gu", "Ya-Wen Teng", "Kuo-Hui Yeh", "Ping-Hsueh Ho", "Wei-Ling Wen"], "title": "TPSQLi: Test Prioritization for SQL Injection Vulnerability Detection in Web Applications", "comment": "20 pages; 8 figures", "summary": "The rapid proliferation of network applications has led to a significant\nincrease in network attacks. According to the OWASP Top 10 Projects report\nreleased in 2021, injection attacks rank among the top three vulnerabilities in\nsoftware projects. This growing threat landscape has increased the complexity\nand workload of software testing, necessitating advanced tools to support agile\ndevelopment cycles. This paper introduces a novel test prioritization method\nfor SQL injection vulnerabilities to enhance testing efficiency. By leveraging\nprevious test outcomes, our method adjusts defense strength vectors for\nsubsequent tests, optimizing the testing workflow and tailoring defense\nmechanisms to specific software needs. This approach aims to improve the\neffectiveness and efficiency of vulnerability detection and mitigation through\na flexible framework that incorporates dynamic adjustments and considers the\ntemporal aspects of vulnerability exposure."}
{"id": "2509.11877", "categories": ["cs.LO", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.11877", "abs": "https://arxiv.org/abs/2509.11877", "authors": ["Andrei Arusoaie", "Horaţiu Cheval", "Radu Iosif"], "title": "Proceedings 9th edition of Working Formal Methods Symposium", "comment": null, "summary": "This volume contains the proceedings of the 9th Working Formal Methods\nSymposium, which was held at the Alexandru Ioan Cuza University, Ia\\c{s}i,\nRomania on September 17-19, 2025."}
{"id": "2509.11418", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.11418", "abs": "https://arxiv.org/abs/2509.11418", "authors": ["Runming Li", "Yue Yao", "Robert Harper"], "title": "Mechanizing Synthetic Tait Computability in Istari", "comment": null, "summary": "Categorical gluing is a powerful technique for proving meta-theorems of type\ntheories such as canonicity and normalization. Synthetic Tait Computability\n(STC) provides an abstract treatment of the complex gluing models by\ninternalizing the gluing category into a modal dependent type theory with a\nphase distinction. This work presents a mechanization of STC in the Istari\nproof assistant. Istari is a Martin-L\\\"{o}f-style extensional type theory with\nequality reflection. Equality reflection eliminates the nuisance of transport\nreasoning typically found in intensional proof assistants. This work develops a\nreusable library for synthetic phase distinction, including modalities,\nextension types, and strict glue types, and applies it to two case studies: (1)\na canonicity model for dependent type theory with dependent products and\nbooleans with large elimination, and (2) a Kripke canonicity model for the\ncost-aware logical framework. Our results demonstrate that the core STC\nconstructions can be formalized essentially verbatim in Istari, preserving the\nelegance of the on-paper arguments while ensuring machine-checked correctness."}
{"id": "2509.12077", "categories": ["cs.FL"], "pdf": "https://arxiv.org/pdf/2509.12077", "abs": "https://arxiv.org/abs/2509.12077", "authors": ["Yvo Ad Meeres", "František Mráz"], "title": "A Unifying Approach to Picture Automata", "comment": "A full version of a paper for ITAT WAFNL 2025", "summary": "A directed acyclic graph (DAG) can represent a two-dimensional string or\npicture. We propose recognizing picture languages using DAG automata by\nencoding 2D inputs into DAGs. An encoding can be input-agnostic (based on input\nsize only) or input-driven (depending on symbols). Three distinct\ninput-agnostic encodings characterize classes of picture languages accepted by\nreturning finite automata, boustrophedon automata, and online tessellation\nautomata. Encoding a string as a simple directed path limits recognition to\nregular languages. However, input-driven encodings allow DAG automata to\nrecognize some context-sensitive string languages and outperform online\ntessellation automata in two dimensions."}
{"id": "2509.10946", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10946", "abs": "https://arxiv.org/abs/2509.10946", "authors": ["Roberto Morabito", "Guanghan Wu"], "title": "When the Code Autopilot Breaks: Why LLMs Falter in Embedded Machine Learning", "comment": "This paper has been accepted for publication in Computer (IEEE). Upon\n  publication, the copyright will be transferred to IEEE", "summary": "Large Language Models (LLMs) are increasingly used to automate software\ngeneration in embedded machine learning workflows, yet their outputs often fail\nsilently or behave unpredictably. This article presents an empirical\ninvestigation of failure modes in LLM-powered ML pipelines, based on an\nautopilot framework that orchestrates data preprocessing, model conversion, and\non-device inference code generation. We show how prompt format, model behavior,\nand structural assumptions influence both success rates and failure\ncharacteristics, often in ways that standard validation pipelines fail to\ndetect. Our analysis reveals a diverse set of error-prone behaviors, including\nformat-induced misinterpretations and runtime-disruptive code that compiles but\nbreaks downstream. We derive a taxonomy of failure categories and analyze\nerrors across multiple LLMs, highlighting common root causes and systemic\nfragilities. Though grounded in specific devices, our study reveals broader\nchallenges in LLM-based code generation. We conclude by discussing directions\nfor improving reliability and traceability in LLM-powered embedded ML systems."}
{"id": "2410.04581", "categories": ["cs.PL", "cs.DS", "cs.LO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2410.04581", "abs": "https://arxiv.org/abs/2410.04581", "authors": ["Lee Zheng Han", "Umang Mathur"], "title": "Efficient Decrease-and-Conquer Linearizability Monitoring", "comment": null, "summary": "Linearizability has become the de facto correctness specification for\nimplementations of concurrent data structures. While formally verifying such\nimplementations remains challenging, linearizability monitoring has emerged as\na promising first step to rule out early problems in the development of custom\nimplementations, and serves as a key component in approaches that stress test\nsuch implementations. In this work, we investigate linearizability monitoring\n-- check if an execution history of an implementation is linearizable. While\nthis problem is intractable in general, a systematic understanding of when it\nbecomes tractable has remained elusive. We revisit this problem and first\npresent a unified `decrease-and-conquer' algorithmic framework for\nlinearizability monitoring. At its heart, this framework asks to identify\nspecial linearizability-preserving values in a given history -- values whose\npresence yields an equilinearizable sub-history when removed, and whose absence\nindicates non-linearizability. We prove that a polynomial time algorithm for\nthe problem of identifying linearizability-preserving values, yields a\npolynomial time algorithm for linearizability monitoring, while conversely,\nintractability of this problem implies intractability of the monitoring\nproblem. We demonstrate our framework's effectiveness by instantiating it for\nseveral popular data types -- sets, stacks, queues and priority queues --\nderiving polynomial time algorithms for each, with the unambiguity restriction,\nwhere each insertion to the underlying data structure adds a distinct value. We\noptimize these algorithms to achieve the optimal log-linear time complexity by\namortizing the cost of solving sub-problems through efficient data structures.\nOur implementation and evaluation on publicly available implementations show\nthat our approach scales to large histories and outperforms existing tools."}
{"id": "2509.11901", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2509.11901", "abs": "https://arxiv.org/abs/2509.11901", "authors": ["Kentaro Kobayashi", "Yukiyoshi Kameyama"], "title": "Expressive Power of One-Shot Control Operators and Coroutines", "comment": "Full version of the paper accepted at APLAS 2025. Includes appendices\n  with proofs. 59 pages", "summary": "Control operators, such as exceptions and effect handlers, provide a means of\nrepresenting computational effects in programs abstractly and modularly. While\nmost theoretical studies have focused on multi-shot control operators, one-shot\ncontrol operators -- which restrict the use of captured continuations to at\nmost once -- are gaining attention for their balance between expressiveness and\nefficiency. This study aims to fill the gap. We present a mathematically\nrigorous comparison of the expressive power among one-shot control operators,\nincluding effect handlers, delimited continuations, and even asymmetric\ncoroutines. Following previous studies on multi-shot control operators, we\nadopt Felleisen's macro-expressiveness as our measure of expressiveness. We\nverify the folklore that one-shot effect handlers and one-shot\ndelimited-control operators can be macro-expressed by asymmetric coroutines,\nbut not vice versa. We explain why a previous informal argument fails, and how\nto revise it to make a valid macro-translation."}
{"id": "2504.10813", "categories": ["cs.PL", "cs.FL", "cs.LO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2504.10813", "abs": "https://arxiv.org/abs/2504.10813", "authors": ["Zhendong Ang", "Azadeh Farzan", "Umang Mathur"], "title": "Enhanced Data Race Prediction Through Modular Reasoning", "comment": null, "summary": "There are two orthogonal methodologies for efficient prediction of data races\nfrom concurrent program runs: commutativity and prefix reasoning. There are\nseveral instances of each methodology in the literature, with the goal of\npredicting data races using a streaming algorithm where the required memory\ndoes not grow proportional to the length of the observed run, but these\ninstances were mostly created in an ad hoc manner, without much attention to\ntheir unifying underlying principles. In this paper, we identify and formalize\nthese principles for each category with the ultimate goal of paving the way for\ncombining them into a new algorithm which shares their efficiency\ncharacteristics but offers strictly more prediction power. In particular, we\nformalize three distinct classes of races predictable using commutativity\nreasoning, and compare them. We identify three different styles of prefix\nreasoning, and prove that they predict the same class of races, which provably\ncontains all races predictable by any commutativity reasoning technique.\n  Our key contribution is combining prefix reasoning and commutativity\nreasoning in a modular way to introduce a new class of races, granular prefix\nraces, that are predictable in constant-space and linear time, in a streaming\nfashion. This class of races includes all races predictable using commutativity\nand prefix reasoning techniques. We present an improved constant-space\nalgorithm for prefix reasoning alone based on the idea of antichains (from\nlanguage theory). This improved algorithm is the stepping stone that is\nrequired to devise an efficient algorithm for prediction of granular prefix\nraces. We present experimental results to demonstrate the expressive power and\nperformance of our new algorithm."}
{"id": "2509.11000", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.11000", "abs": "https://arxiv.org/abs/2509.11000", "authors": ["Omid Gheibi", "Christian Kästner", "Pooyan Jamshidi"], "title": "Hardness, Structural Knowledge, and Opportunity: An Analytical Framework for Modular Performance Modeling", "comment": null, "summary": "Performance-influence models are beneficial for understanding how\nconfigurations affect system performance, but their creation is challenging due\nto the exponential growth of configuration spaces. While gray-box approaches\nleverage selective \"structural knowledge\" (like the module execution graph of\nthe system) to improve modeling, the relationship between this knowledge, a\nsystem's characteristics (we call them \"structural aspects\"), and potential\nmodel improvements is not well understood. This paper addresses this gap by\nformally investigating how variations in structural aspects (e.g., the number\nof modules and options per module) and the level of structural knowledge impact\nthe creation of \"opportunities\" for improved \"modular performance modeling\". We\nintroduce and quantify the concept of modeling \"hardness\", defined as the\ninherent difficulty of performance modeling. Through controlled experiments\nwith synthetic system models, we establish an \"analytical matrix\" to measure\nthese concepts. Our findings show that modeling hardness is primarily driven by\nthe number of modules and configuration options per module. More importantly,\nwe demonstrate that both higher levels of structural knowledge and increased\nmodeling hardness significantly enhance the opportunity for improvement. The\nimpact of these factors varies by performance metric; for ranking accuracy\n(e.g., in debugging task), structural knowledge is more dominant, while for\nprediction accuracy (e.g., in resource management task), hardness plays a\nstronger role. These results provide actionable insights for system designers,\nguiding them to strategically allocate time and select appropriate modeling\napproaches based on a system's characteristics and a given task's objectives."}
{"id": "2504.10813", "categories": ["cs.PL", "cs.FL", "cs.LO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2504.10813", "abs": "https://arxiv.org/abs/2504.10813", "authors": ["Zhendong Ang", "Azadeh Farzan", "Umang Mathur"], "title": "Enhanced Data Race Prediction Through Modular Reasoning", "comment": null, "summary": "There are two orthogonal methodologies for efficient prediction of data races\nfrom concurrent program runs: commutativity and prefix reasoning. There are\nseveral instances of each methodology in the literature, with the goal of\npredicting data races using a streaming algorithm where the required memory\ndoes not grow proportional to the length of the observed run, but these\ninstances were mostly created in an ad hoc manner, without much attention to\ntheir unifying underlying principles. In this paper, we identify and formalize\nthese principles for each category with the ultimate goal of paving the way for\ncombining them into a new algorithm which shares their efficiency\ncharacteristics but offers strictly more prediction power. In particular, we\nformalize three distinct classes of races predictable using commutativity\nreasoning, and compare them. We identify three different styles of prefix\nreasoning, and prove that they predict the same class of races, which provably\ncontains all races predictable by any commutativity reasoning technique.\n  Our key contribution is combining prefix reasoning and commutativity\nreasoning in a modular way to introduce a new class of races, granular prefix\nraces, that are predictable in constant-space and linear time, in a streaming\nfashion. This class of races includes all races predictable using commutativity\nand prefix reasoning techniques. We present an improved constant-space\nalgorithm for prefix reasoning alone based on the idea of antichains (from\nlanguage theory). This improved algorithm is the stepping stone that is\nrequired to devise an efficient algorithm for prediction of granular prefix\nraces. We present experimental results to demonstrate the expressive power and\nperformance of our new algorithm."}
{"id": "2201.06325", "categories": ["cs.LO", "cs.DC", "cs.DS", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2201.06325", "abs": "https://arxiv.org/abs/2201.06325", "authors": ["Umang Mathur", "Andreas Pavlogiannis", "Hünkar Can Tunç", "Mahesh Viswanathan"], "title": "A Tree Clock Data Structure for Causal Orderings in Concurrent Executions", "comment": null, "summary": "Dynamic techniques are a scalable and effective way to analyze concurrent\nprograms. Instead of analyzing all behaviors of a program, these techniques\ndetect errors by focusing on a single program execution. Often a crucial step\nin these techniques is to define a causal ordering between events in the\nexecution, which is then computed using vector clocks, a simple data structure\nthat stores logical times of threads. The two basic operations of vector\nclocks, namely join and copy, require $\\Theta(k)$ time, where $k$ is the number\nof threads. Thus they are a computational bottleneck when $k$ is large.\n  In this work, we introduce tree clocks, a new data structure that replaces\nvector clocks for computing causal orderings in program executions. Joining and\ncopying tree clocks takes time that is roughly proportional to the number of\nentries being modified, and hence the two operations do not suffer the a-priori\n$\\Theta(k)$ cost per application. We show that when used to compute the classic\nhappens-before (HB) partial order, tree clocks are optimal, in the sense that\nno other data structure can lead to smaller asymptotic running time. Moreover,\nwe demonstrate that tree clocks can be used to compute other partial orders,\nsuch as schedulable-happens-before (SHB) and the standard Mazurkiewicz (MAZ)\npartial order, and thus are a versatile data structure. Our experiments show\nthat just by replacing vector clocks with tree clocks, the computation becomes\nfrom $2.02 \\times$ faster (MAZ) to $2.66 \\times$ (SHB) and $2.97 \\times$ (HB)\non average per benchmark. These results illustrate that tree clocks have the\npotential to become a standard data structure with wide applications in\nconcurrent analyses."}
{"id": "2509.11065", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.11065", "abs": "https://arxiv.org/abs/2509.11065", "authors": ["Yuan Si", "Daming Li", "Hanyuan Shi", "Jialu Zhang"], "title": "ViScratch: Using Large Language Models and Gameplay Videos for Automated Feedback in Scratch", "comment": null, "summary": "Block-based programming environments such as Scratch are increasingly popular\nin programming education, in particular for young learners. While the use of\nblocks helps prevent syntax errors, semantic bugs remain common and difficult\nto debug. Existing tools for Scratch debugging rely heavily on predefined rules\nor user manual inputs, and crucially, they ignore the platform's inherently\nvisual nature.\n  We introduce ViScratch, the first multimodal feedback generation system for\nScratch that leverages both the project's block code and its generated gameplay\nvideo to diagnose and repair bugs. ViScratch uses a two-stage pipeline: a\nvision-language model first aligns visual symptoms with code structure to\nidentify a single critical issue, then proposes minimal, abstract syntax tree\nlevel repairs that are verified via execution in the Scratch virtual machine.\n  We evaluate ViScratch on a set of real-world Scratch projects against\nstate-of-the-art LLM-based tools and human testers. Results show that gameplay\nvideo is a crucial debugging signal: ViScratch substantially outperforms prior\ntools in both bug identification and repair quality, even without access to\nproject descriptions or goals. This work demonstrates that video can serve as a\nfirst-class specification in visual programming environments, opening new\ndirections for LLM-based debugging beyond symbolic code alone."}
{"id": "2509.11901", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2509.11901", "abs": "https://arxiv.org/abs/2509.11901", "authors": ["Kentaro Kobayashi", "Yukiyoshi Kameyama"], "title": "Expressive Power of One-Shot Control Operators and Coroutines", "comment": "Full version of the paper accepted at APLAS 2025. Includes appendices\n  with proofs. 59 pages", "summary": "Control operators, such as exceptions and effect handlers, provide a means of\nrepresenting computational effects in programs abstractly and modularly. While\nmost theoretical studies have focused on multi-shot control operators, one-shot\ncontrol operators -- which restrict the use of captured continuations to at\nmost once -- are gaining attention for their balance between expressiveness and\nefficiency. This study aims to fill the gap. We present a mathematically\nrigorous comparison of the expressive power among one-shot control operators,\nincluding effect handlers, delimited continuations, and even asymmetric\ncoroutines. Following previous studies on multi-shot control operators, we\nadopt Felleisen's macro-expressiveness as our measure of expressiveness. We\nverify the folklore that one-shot effect handlers and one-shot\ndelimited-control operators can be macro-expressed by asymmetric coroutines,\nbut not vice versa. We explain why a previous informal argument fails, and how\nto revise it to make a valid macro-translation."}
{"id": "2509.10819", "categories": ["cs.SE", "cs.CR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.10819", "abs": "https://arxiv.org/abs/2509.10819", "authors": ["Christoph Hochrainer", "Valentin Wüstholz", "Maria Christakis"], "title": "Arguzz: Testing zkVMs for Soundness and Completeness Bugs", "comment": null, "summary": "Zero-knowledge virtual machines (zkVMs) are increasingly deployed in\ndecentralized applications and blockchain rollups since they enable verifiable\noff-chain computation. These VMs execute general-purpose programs, frequently\nwritten in Rust, and produce succinct cryptographic proofs. However, zkVMs are\ncomplex, and bugs in their constraint systems or execution logic can cause\ncritical soundness (accepting invalid executions) or completeness (rejecting\nvalid ones) issues.\n  We present Arguzz, the first automated tool for testing zkVMs for soundness\nand completeness bugs. To detect such bugs, Arguzz combines a novel variant of\nmetamorphic testing with fault injection. In particular, it generates\nsemantically equivalent program pairs, merges them into a single Rust program\nwith a known output, and runs it inside a zkVM. By injecting faults into the\nVM, Arguzz mimics malicious or buggy provers to uncover overly weak\nconstraints.\n  We used Arguzz to test six real-world zkVMs (RISC Zero, Nexus, Jolt, SP1,\nOpenVM, and Pico) and found eleven bugs in three of them. One RISC Zero bug\nresulted in a $50,000 bounty, despite prior audits, demonstrating the critical\nneed for systematic testing of zkVMs."}
{"id": "2509.11132", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.11132", "abs": "https://arxiv.org/abs/2509.11132", "authors": ["Xiaoyu Zhang", "Weipeng Jiang", "Juan Zhai", "Shiqing Ma", "Qingshuang Bao", "Chenhao Lin", "Chao Shen", "Tianlin Li", "Yang Liu"], "title": "Rethinking Technology Stack Selection with AI Coding Proficiency", "comment": "23 pages", "summary": "Large language models (LLMs) are now an integral part of software development\nworkflows and are reshaping the whole process. Traditional technology stack\nselection has not caught up. Most of the existing selection methods focus\nsolely on the inherent attributes of the technology, overlooking whether the\nLLM can effectively leverage the chosen technology. For example, when\ngenerating code snippets using popular libraries like Selenium (one of the most\nwidely used test automation tools with over 33k GitHub stars), existing LLMs\nfrequently generate low-quality code snippets (e.g., using deprecated APIs and\nmethods, or containing syntax errors). As such, teams using LLM assistants risk\nchoosing technologies that cannot be used effectively by LLMs, yielding high\ndebugging effort and mounting technical debt. We foresee a practical question\nin the LLM era, is a technology ready for AI-assisted development? In this\npaper, we first propose the concept, AI coding proficiency, the degree to which\nLLMs can utilize a given technology to generate high-quality code snippets. We\nconduct the first comprehensive empirical study examining AI proficiency across\n170 third-party libraries and 61 task scenarios, evaluating six widely used\nLLMs. Our findings reveal that libraries with similar functionalities can\nexhibit up to 84% differences in the quality score of LLM-generated code, while\ndifferent models also exhibit quality gaps among their generation results using\nthe same library. These gaps translate into real engineering costs and can\nsteer developer choices toward a narrow set of libraries with high AI coding\nproficiency, threatening technological diversity in the ecosystem. We call on\nthe community to integrate AI proficiency assessments into technology selection\nframeworks and develop mitigation strategies, preserving competitive balance in\nAI-driven development."}
{"id": "2509.11065", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.11065", "abs": "https://arxiv.org/abs/2509.11065", "authors": ["Yuan Si", "Daming Li", "Hanyuan Shi", "Jialu Zhang"], "title": "ViScratch: Using Large Language Models and Gameplay Videos for Automated Feedback in Scratch", "comment": null, "summary": "Block-based programming environments such as Scratch are increasingly popular\nin programming education, in particular for young learners. While the use of\nblocks helps prevent syntax errors, semantic bugs remain common and difficult\nto debug. Existing tools for Scratch debugging rely heavily on predefined rules\nor user manual inputs, and crucially, they ignore the platform's inherently\nvisual nature.\n  We introduce ViScratch, the first multimodal feedback generation system for\nScratch that leverages both the project's block code and its generated gameplay\nvideo to diagnose and repair bugs. ViScratch uses a two-stage pipeline: a\nvision-language model first aligns visual symptoms with code structure to\nidentify a single critical issue, then proposes minimal, abstract syntax tree\nlevel repairs that are verified via execution in the Scratch virtual machine.\n  We evaluate ViScratch on a set of real-world Scratch projects against\nstate-of-the-art LLM-based tools and human testers. Results show that gameplay\nvideo is a crucial debugging signal: ViScratch substantially outperforms prior\ntools in both bug identification and repair quality, even without access to\nproject descriptions or goals. This work demonstrates that video can serve as a\nfirst-class specification in visual programming environments, opening new\ndirections for LLM-based debugging beyond symbolic code alone."}
{"id": "2509.11238", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.11238", "abs": "https://arxiv.org/abs/2509.11238", "authors": ["Dongming Jin", "Zhi Jin", "Yiran Zhang", "Zheng Fang", "Linyu Li", "Yuanpeng He", "Xiaohong Chen", "Weisong Sun"], "title": "UserTrace: User-Level Requirements Generation and Traceability Recovery from Software Project Repositories", "comment": "21page, 5 figures", "summary": "Software maintainability critically depends on high-quality requirements\ndescriptions and explicit traceability between requirements and code. Although\nautomated code summarization (ACS) and requirements traceability (RT)\ntechniques have been widely studied, existing ACS methods mainly generate\nimplementation-level (i.e., developer-oriented) requirements (IRs) for\nfine-grained units (e.g., methods), while RT techniques often overlook the\nimpact of project evolution. As a result, user-level (i.e., end user-oriented)\nrequirements (URs) and live trace links remain underexplored, despite their\nimportance for supporting user understanding and for validating whether\nAI-generated software aligns with user intent. To address this gap, we propose\nUserTrace, a multi-agent system that automatically generates URs and recovers\nlive trace links (from URs to IRs to code) from software repositories.\nUserTrace coordinates four specialized agents (i.e., Code Reviewer, Searcher,\nWriter, and Verifier) through a three-phase process: structuring repository\ndependencies, deriving IRs for code units, and synthesizing URs with\ndomain-specific context. Our comparative evaluation shows that UserTrace\nproduces URs with higher completeness, correctness, and helpfulness than an\nestablished baseline, and achieves superior precision in trace link recovery\ncompared to five state-of-the-art RT approaches. A user study further\ndemonstrates that UserTrace helps end users validate whether the AI-generated\nrepositories align with their intent."}
{"id": "2509.11877", "categories": ["cs.LO", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.11877", "abs": "https://arxiv.org/abs/2509.11877", "authors": ["Andrei Arusoaie", "Horaţiu Cheval", "Radu Iosif"], "title": "Proceedings 9th edition of Working Formal Methods Symposium", "comment": null, "summary": "This volume contains the proceedings of the 9th Working Formal Methods\nSymposium, which was held at the Alexandru Ioan Cuza University, Ia\\c{s}i,\nRomania on September 17-19, 2025."}
{"id": "2509.11252", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11252", "abs": "https://arxiv.org/abs/2509.11252", "authors": ["Chengze li", "Yitong Zhang", "Jia Li", "Liyi Cai", "Ge Li"], "title": "Beyond Autoregression: An Empirical Study of Diffusion Large Language Models for Code Generation", "comment": null, "summary": "LLMs have become the mainstream approaches to code generation. Existing LLMs\nmainly employ autoregressive generation, i.e. generating code token-by-token\nfrom left to right. However, the underlying autoregressive generation has two\nlimitations in code generation. First, autoregressive LLMs only generate a\ntoken at each step, showing low efficiency in practice. Second, programming is\na non-sequential process involving back-and-forth editing, while autoregressive\nLLMs only employ the left-to-right generation order. These two intrinsic\nlimitations hinder the further development of LLMs in code generation.\nRecently, diffusion LLMs have emerged as a promising alternative. Diffusion\nLLMs address the above limitations with two advances, including multi-token\nprediction (i.e. generating multiple tokens at each step) and flexible\ngeneration order (i.e. flexibly determining which positions to generate\ntokens). However, there is no systematic study exploring diffusion LLMs in code\ngeneration. To bridge the knowledge gap, we present the first empirical study\nof diffusion LLMs for code generation. Our study involves 9 representative\ndiffusion LLMs and conduct experiments on 4 widely used benchmarks. Based on\nthe results, we summarize the following findings. (1) Existing diffusion LLMs\nare competitive with autoregressive LLMs with similar sizes. (2) Diffusion LLMs\nhave a stronger length extrapolation ability than autoregressive LLMs and\nperform better in long code understanding. (3) We explore factors impacting the\neffectiveness and efficiency of diffusion LLMs, and provide practical guidance.\n(4) We discuss several promising further directions to improve diffusion LLMs\non code generation. We open-source all source code, data, and results to\nfacilitate the following research. The code is publicly available at\nhttps://github.com/zhangyitonggg/dllm4code."}
{"id": "2509.11258", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.11258", "abs": "https://arxiv.org/abs/2509.11258", "authors": ["Regan Meloche", "Durga Sivakumar", "Amal A. Anda", "Sofana Alfuhaid", "Daniel Amyot", "Luigi Logrippo", "John Mylopoulos"], "title": "A Web-Based Environment for the Specification and Generation of Smart Legal Contracts", "comment": "12 pages, 5 figures, 2 tables, conference", "summary": "Monitoring the compliance of contract performance against legal obligations\nis important in order to detect violations, ideally, as soon as they occur.\nSuch monitoring can nowadays be achieved through the use of smart contracts,\nwhich provide protection against tampering as well as some level of automation\nin handling violations. However, there exists a large gap between natural\nlanguage contracts and smart contract implementations. This paper introduces a\nWeb-based environment that partly fills that gap by supporting the\nuser-assisted refinement of Symboleo specifications corresponding to legal\ncontract templates, followed by the automated generation of monitoring smart\ncontracts deployable on the Hyperledger Fabric platform. This environment,\nillustrated using a sample contract from the transactive energy domain, shows\nmuch potential in accelerating the development of smart contracts in a legal\ncompliance context."}
{"id": "2509.11312", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11312", "abs": "https://arxiv.org/abs/2509.11312", "authors": ["Wenchao Gu", "Yupan Chen", "Yanlin Wang", "Hongyu Zhang", "Cuiyun Gao", "Michael R. Lyu"], "title": "Weakly Supervised Vulnerability Localization via Multiple Instance Learning", "comment": null, "summary": "Software vulnerability detection has emerged as a significant concern in the\nfield of software security recently, capturing the attention of numerous\nresearchers and developers. Most previous approaches focus on coarse-grained\nvulnerability detection, such as at the function or file level. However, the\ndevelopers would still encounter the challenge of manually inspecting a large\nvolume of code inside the vulnerable function to identify the specific\nvulnerable statements for modification, indicating the importance of\nvulnerability localization. Training the model for vulnerability localization\nusually requires ground-truth labels at the statement-level, and labeling\nvulnerable statements demands expert knowledge, which incurs high costs. Hence,\nthe demand for an approach that eliminates the need for additional labeling at\nthe statement-level is on the rise. To tackle this problem, we propose a novel\napproach called WAVES for WeAkly supervised Vulnerability Localization via\nmultiplE inStance learning, which does not need the additional statement-level\nlabels during the training. WAVES has the capability to determine whether a\nfunction is vulnerable (i.e., vulnerability detection) and pinpoint the\nvulnerable statements (i.e., vulnerability localization). Specifically,\ninspired by the concept of multiple instance learning, WAVES converts the\nground-truth label at the function-level into pseudo labels for individual\nstatements, eliminating the need for additional statement-level labeling. These\npseudo labels are utilized to train the classifiers for the function-level\nrepresentation vectors. Extensive experimentation on three popular benchmark\ndatasets demonstrates that, in comparison to previous baselines, our approach\nachieves comparable performance in vulnerability detection and state-of-the-art\nperformance in statement-level vulnerability localization."}
{"id": "2509.11446", "categories": ["cs.SE", "D.2.0; D.2.1; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.11446", "abs": "https://arxiv.org/abs/2509.11446", "authors": ["Mohammad Amin Zadenoori", "Jacek Dąbrowski", "Waad Alhoshan", "Liping Zhao", "Alessio Ferrari"], "title": "Large Language Models (LLMs) for Requirements Engineering (RE): A Systematic Literature Review", "comment": null, "summary": "Large Language Models (LLMs) are finding applications in numerous domains,\nand Requirements Engineering (RE) is increasingly benefiting from their\ncapabilities to assist with complex, language-intensive tasks. This paper\npresents a systematic literature review of 74 primary studies published between\n2023 and 2024, examining how LLMs are being applied in RE. The study\ncategorizes the literature according to several dimensions, including\npublication trends, RE activities, prompting strategies, and evaluation\nmethods. Our findings indicate notable patterns, among which we observe\nsubstantial differences compared to previous works leveraging standard Natural\nLanguage Processing (NLP) techniques. Most of the studies focus on using LLMs\nfor requirements elicitation and validation, rather than defect detection and\nclassification, which were dominant in the past. Researchers have also\nbroadened their focus and addressed novel tasks, e.g., test generation,\nexploring the integration of RE with other software engineering (SE)\ndisciplines. Although requirements specifications remain the primary focus,\nother artifacts are increasingly considered, including issues from issue\ntracking systems, regulations, and technical manuals. The studies mostly rely\non GPT-based models, and often use Zero-shot or Few-shot prompting. They are\nusually evaluated in controlled environments, with limited use in industry\nsettings and limited integration in complex workflows. Our study outlines\nimportant future directions, such as leveraging the potential to expand the\ninfluence of RE in SE, exploring less-studied tasks, improving prompting\nmethods, and testing in real-world environments. Our contribution also helps\nresearchers and practitioners use LLMs more effectively in RE, by providing a\nlist of identified tools leveraging LLMs for RE, as well as datasets."}
{"id": "2509.11523", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.11523", "abs": "https://arxiv.org/abs/2509.11523", "authors": ["Ziliang Wang", "Ge Li", "Jia Li", "Hao Zhu", "Zhi Jin"], "title": "VulAgent: Hypothesis-Validation based Multi-Agent Vulnerability Detection", "comment": null, "summary": "The application of language models to project-level vulnerability detection\nremains challenging, owing to the dual requirement of accurately localizing\nsecurity-sensitive code and correctly correlating and reasoning over complex\nprogram context. We present VulAgent, a multi-agent vulnerability detection\nframework based on hypothesis validation. Our design is inspired by how human\nauditors review code: when noticing a sensitive operation, they form a\nhypothesis about a possible vulnerability, consider potential trigger paths,\nand then verify the hypothesis against the surrounding context. VulAgent\nimplements a semantics-sensitive, multi-view detection pipeline: specialized\nagents, each aligned to a specific analysis perspective (e.g., memory,\nauthorization), collaboratively surface and precisely localize sensitive code\nsites with higher coverage. Building on this, VulAgent adopts a\nhypothesis-validation paradigm: for each vulnerability report, it builds\nhypothesis conditions and a trigger path, steering the LLM to target the\nrelevant program context and defensive checks during verification, which\nreduces false positives. On average across the two datasets, VulAgent improves\noverall accuracy by 6.6%, increases the correct identification rate of\nvulnerable--fixed code pairs by up to 450% (246% on average), and reduces the\nfalse positive rate by about 36% compared with state-of-the-art LLM-based\nbaselines."}
{"id": "2509.11566", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.11566", "abs": "https://arxiv.org/abs/2509.11566", "authors": ["Hua Guo", "Yunhong Ji", "Xuan Zhou"], "title": "Sedeve-Kit, a Specification-Driven Development Framework for Building Distributed Systems", "comment": null, "summary": "Developing distributed systems presents significant challenges, primarily due\nto the complexity introduced by non-deterministic concurrency and faults. To\naddress these, we propose a specification-driven development framework. Our\nmethod encompasses three key stages. The first stage defines system\nspecifications and invariants using TLA${^+}$. It allows us to perform model\nchecking on the algorithm's correctness and generate test cases for subsequent\ndevelopment phases. In the second stage, based on the established\nspecifications, we write code to ensure consistency and accuracy in the\nimplementation. Finally, after completing the coding process, we rigorously\ntest the system using the test cases generated in the initial stage. This\nprocess ensures system quality by maintaining a strong connection between the\nabstract design and the concrete implementation through continuous\nverification."}
{"id": "2509.11626", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11626", "abs": "https://arxiv.org/abs/2509.11626", "authors": ["Prerna Agarwal", "Himanshu Gupta", "Soujanya Soni", "Rohith Vallam", "Renuka Sindhgatta", "Sameep Mehta"], "title": "Automated Creation and Enrichment Framework for Improved Invocation of Enterprise APIs as Tools", "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) has lead to the\ndevelopment of agents capable of complex reasoning and interaction with\nexternal tools. In enterprise contexts, the effective use of such tools that\nare often enabled by application programming interfaces (APIs), is hindered by\npoor documentation, complex input or output schema, and large number of\noperations. These challenges make tool selection difficult and reduce the\naccuracy of payload formation by up to 25%. We propose ACE, an automated tool\ncreation and enrichment framework that transforms enterprise APIs into\nLLM-compatible tools. ACE, (i) generates enriched tool specifications with\nparameter descriptions and examples to improve selection and invocation\naccuracy, and (ii) incorporates a dynamic shortlisting mechanism that filters\nrelevant tools at runtime, reducing prompt complexity while maintaining\nscalability. We validate our framework on both proprietary and open-source APIs\nand demonstrate its integration with agentic frameworks. To the best of our\nknowledge, ACE is the first end-to-end framework that automates the creation,\nenrichment, and dynamic selection of enterprise API tools for LLM agents."}
{"id": "2509.11686", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.11686", "abs": "https://arxiv.org/abs/2509.11686", "authors": ["Jian Wang", "Xiaofei Xie", "Qiang Hu", "Shangqing Liu", "Yi Li"], "title": "Do Code Semantics Help? A Comprehensive Study on Execution Trace-Based Information for Code Large Language Models", "comment": "EMNLP2025-findings", "summary": "Code Large Language Models (Code LLMs) have opened a new era in programming\nwith their impressive capabilities. However, recent research has revealed\ncritical limitations in their ability to reason about runtime behavior and\nunderstand the actual functionality of programs, which poses significant\nchallenges for their post-training and practical deployment. Specifically, Code\nLLMs encounter two principal issues: (1) a lack of proficiency in reasoning\nabout program execution behavior, as they struggle to interpret what programs\nactually do during runtime, and (2) the inconsistent and fragmented\nrepresentation of semantic information, such as execution traces, across\nexisting methods, which hinders their ability to generalize and reason\neffectively. These challenges underscore the necessity for more systematic\napproaches to enhance the reasoning capabilities of Code LLMs. To address these\nissues, we introduce a generic framework to support integrating semantic\ninformation~(e.g., execution trace) to code task-relevant prompts, and conduct\na comprehensive study to explore the role of semantic information in enhancing\nthe reasoning ability of Code LLMs accordingly. Specifically, we focus on\ninvestigating the usefulness of trace-based semantic information in boosting\nsupervised fine-tuning~(SFT) and post-phase inference of Code LLMs. The\nexperimental results surprisingly disagree with previous works and demonstrate\nthat semantic information has limited usefulness for SFT and test time scaling\nof Code LLM."}
{"id": "2509.11691", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.11691", "abs": "https://arxiv.org/abs/2509.11691", "authors": ["Lukas Rauh", "Mel-Rick Süner", "Daniel Schel", "Thomas Bauernhansl"], "title": "AI Asset Management for Manufacturing (AIM4M): Development of a Process Model for Operationalization", "comment": "10 pages, 4 figures, submitted for revision review at International\n  Conference on Industry of the Future and Smart Manufacturing (ISM) 2025", "summary": "The benefits of adopting artificial intelligence (AI) in manufacturing are\nundeniable. However, operationalizing AI beyond the prototype, especially when\ninvolved with cyber-physical production systems (CPPS), remains a significant\nchallenge due to the technical system complexity, a lack of implementation\nstandards and fragmented organizational processes. To this end, this paper\nproposes a new process model for the lifecycle management of AI assets designed\nto address challenges in manufacturing and facilitate effective\noperationalization throughout the entire AI lifecycle. The process model, as a\ntheoretical contribution, builds on machine learning operations (MLOps)\nprinciples and refines three aspects to address the domain-specific\nrequirements from the CPPS context. As a result, the proposed process model\naims to support organizations in practice to systematically develop, deploy and\nmanage AI assets across their full lifecycle while aligning with CPPS-specific\nconstraints and regulatory demands."}
{"id": "2509.11708", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.11708", "abs": "https://arxiv.org/abs/2509.11708", "authors": ["Zhantong Xue", "Pingchuan Ma", "Zhaoyu Wang", "Shuai Wang"], "title": "From Evaluation to Enhancement: Large Language Models for Zero-Knowledge Proof Code Generation", "comment": null, "summary": "Zero-knowledge proofs (ZKPs) are increasingly deployed in domains such as\nprivacy-preserving authentication, blockchain scalability, and secure finance.\nHowever, authoring ZK programs remains challenging: unlike mainstream\nprogramming, ZK development requires reasoning about finite field arithmetic,\nconstraint systems, and gadgets, making it knowledge-intensive and error-prone.\nWhile large language models (LLMs) have demonstrated strong code generation\ncapabilities in general-purpose languages, their effectiveness for ZK\nprogramming, where correctness hinges on both language mastery and gadget-level\nreasoning, remains unexplored. To address this gap, we propose\n\\textsc{ZK-Eval}, a domain-specific evaluation pipeline that probes LLM\ncapabilities at three levels: language knowledge, gadget competence, and\nend-to-end program generation. Our evaluation of four state-of-the-art LLMs\nreveals that models excel at surface-level syntax but struggle with gadget\nusage and semantic correctness, often yielding incorrect programs. Based on\nthese insights, we introduce \\textsc{ZK-Coder}, an agentic framework that\naugments LLMs with constraint sketching, guided retrieval, and interactive\nrepair. Experiments on Circom and Noir show substantial gains, with success\nrates improving from 17.35\\% to 83.38\\% and from 32.21\\% to 90.05\\%,\nrespectively. With \\textsc{ZK-Eval} and \\textsc{ZK-Coder}, we establish a\nfoundation for systematically measuring and augmenting LLMs in ZK code\ngeneration to lower barriers for practitioners and advance trustworthy\ncomputation."}
{"id": "2509.11738", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.11738", "abs": "https://arxiv.org/abs/2509.11738", "authors": ["Maria Küüsvek", "Hina Anwar"], "title": "Toward Greener Background Processes -- Measuring Energy Cost of Autosave Feature", "comment": "Author version. Accepted for publication in the proceedings of the\n  International Conference on Product-Focused Software Process Improvement\n  (PROFES 2025)", "summary": "Background processes in desktop applications are often overlooked in energy\nconsumption studies, yet they represent continuous, automated workloads with\nsignificant cumulative impact. This paper introduces a reusable process for\nevaluating the energy behavior of such features at the level of operational\ndesign. The process works in three phases: 1) decomposing background\nfunctionality into core operations, 2) operational isolation, and 3) controlled\nmeasurements enabling comparative profiling. We instantiate the process in a\ncase study of autosave implementations across three open-source Python-based\ntext editors. Using 900 empirical software-based energy measurements, we\nidentify key design factors affecting energy use, including save frequency,\nbuffering strategy, and auxiliary logic such as change detection. We give four\nactionable recommendations for greener implementations of autosave features in\nPython to support sustainable software practices."}
{"id": "2509.11748", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.11748", "abs": "https://arxiv.org/abs/2509.11748", "authors": ["Marius Mignard", "Steven Costiou", "Nicolas Anquetil", "Anne Etien"], "title": "Analysing Python Machine Learning Notebooks with Moose", "comment": null, "summary": "Machine Learning (ML) code, particularly within notebooks, often exhibits\nlower quality compared to traditional software. Bad practices arise at three\ndistinct levels: general Python coding conventions, the organizational\nstructure of the notebook itself, and ML-specific aspects such as\nreproducibility and correct API usage. However, existing analysis tools\ntypically focus on only one of these levels and struggle to capture ML-specific\nsemantics, limiting their ability to detect issues. This paper introduces\nVespucci Linter, a static analysis tool with multi-level capabilities, built on\nMoose and designed to address this challenge. Leveraging a metamodeling\napproach that unifies the notebook's structural elements with Python code\nentities, our linter enables a more contextualized analysis to identify issues\nacross all three levels. We implemented 22 linting rules derived from the\nliterature and applied our tool to a corpus of 5,000 notebooks from the Kaggle\nplatform. The results reveal violations at all levels, validating the relevance\nof our multi-level approach and demonstrating Vespucci Linter's potential to\nimprove the quality and reliability of ML development in notebook environments."}
{"id": "2509.11787", "categories": ["cs.SE", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.11787", "abs": "https://arxiv.org/abs/2509.11787", "authors": ["Pascal Joos", "Islem Bouzenia", "Michael Pradel"], "title": "CodeCureAgent: Automatic Classification and Repair of Static Analysis Warnings", "comment": null, "summary": "Static analysis tools are widely used to detect bugs, vulnerabilities, and\ncode smells. Traditionally, developers must resolve these warnings manually.\nBecause this process is tedious, developers sometimes ignore warnings, leading\nto an accumulation of warnings and a degradation of code quality. This paper\npresents CodeCureAgent, an approach that harnesses LLM-based agents to\nautomatically analyze, classify, and repair static analysis warnings. Unlike\nprevious work, our method does not follow a predetermined algorithm. Instead,\nwe adopt an agentic framework that iteratively invokes tools to gather\nadditional information from the codebase (e.g., via code search) and edit the\ncodebase to resolve the warning. CodeCureAgent detects and suppresses false\npositives, while fixing true positives when identified. We equip CodeCureAgent\nwith a three-step heuristic to approve patches: (1) build the project, (2)\nverify that the warning disappears without introducing new warnings, and (3)\nrun the test suite. We evaluate CodeCureAgent on a dataset of 1,000 SonarQube\nwarnings found in 106 Java projects and covering 291 distinct rules. Our\napproach produces plausible fixes for 96.8% of the warnings, outperforming\nstate-of-the-art baseline approaches by 30.7% and 29.2% in plausible-fix rate,\nrespectively. Manual inspection of 291 cases reveals a correct-fix rate of\n86.3%, showing that CodeCureAgent can reliably repair static analysis warnings.\nThe approach incurs LLM costs of about 2.9 cents (USD) and an end-to-end\nprocessing time of about four minutes per warning. We envision CodeCureAgent\nhelping to clean existing codebases and being integrated into CI/CD pipelines\nto prevent the accumulation of static analysis warnings."}
{"id": "2509.11937", "categories": ["cs.SE", "cs.AI", "D.2.0; E.m"], "pdf": "https://arxiv.org/pdf/2509.11937", "abs": "https://arxiv.org/abs/2509.11937", "authors": ["Alexandre Sallinen", "Stefan Krsteski", "Paul Teiletche", "Marc-Antoine Allard", "Baptiste Lecoeur", "Michael Zhang", "Fabrice Nemo", "David Kalajdzic", "Matthias Meyer", "Mary-Anne Hartley"], "title": "MMORE: Massive Multimodal Open RAG & Extraction", "comment": "This paper was originally submitted to the CODEML workshop for ICML\n  2025. 9 pages (including references and appendices)", "summary": "We introduce MMORE, an open-source pipeline for Massive Multimodal Open\nRetrievalAugmented Generation and Extraction, designed to ingest, transform,\nand retrieve knowledge from heterogeneous document formats at scale. MMORE\nsupports more than fifteen file types, including text, tables, images, emails,\naudio, and video, and processes them into a unified format to enable downstream\napplications for LLMs. The architecture offers modular, distributed processing,\nenabling scalable parallelization across CPUs and GPUs. On processing\nbenchmarks, MMORE demonstrates a 3.8-fold speedup over single-node baselines\nand 40% higher accuracy than Docling on scanned PDFs. The pipeline integrates\nhybrid dense-sparse retrieval and supports both interactive APIs and batch RAG\nendpoints. Evaluated on PubMedQA, MMORE-augmented medical LLMs improve\nbiomedical QA accuracy with increasing retrieval depth. MMORE provides a\nrobust, extensible foundation for deploying task-agnostic RAG systems on\ndiverse, real-world multimodal data. The codebase is available at\nhttps://github.com/swiss-ai/mmore."}
{"id": "2509.11942", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.11942", "abs": "https://arxiv.org/abs/2509.11942", "authors": ["Luís F. Gomes", "Xin Zhou", "David Lo", "Rui Abreu"], "title": "VisDocSketcher: Towards Scalable Visual Documentation with Agentic Systems", "comment": null, "summary": "Visual documentation is an effective tool for reducing the cognitive barrier\ndevelopers face when understanding unfamiliar code, enabling more intuitive\ncomprehension. Compared to textual documentation, it provides a higher-level\nunderstanding of the system structure and data flow. Developers usually prefer\nvisual representations over lengthy textual descriptions for large software\nsystems. Visual documentation is both difficult to produce and challenging to\nevaluate. Manually creating it is time-consuming, and currently, no existing\napproach can automatically generate high-level visual documentation directly\nfrom code. Its evaluation is often subjective, making it difficult to\nstandardize and automate. To address these challenges, this paper presents the\nfirst exploration of using agentic LLM systems to automatically generate visual\ndocumentation. We introduce VisDocSketcher, the first agent-based approach that\ncombines static analysis with LLM agents to identify key elements in the code\nand produce corresponding visual representations. We propose a novel evaluation\nframework, AutoSketchEval, for assessing the quality of generated visual\ndocumentation using code-level metrics. The experimental results show that our\napproach can valid visual documentation for 74.4% of the samples. It shows an\nimprovement of 26.7-39.8% over a simple template-based baseline. Our evaluation\nframework can reliably distinguish high-quality (code-aligned) visual\ndocumentation from low-quality (non-aligned) ones, achieving an AUC exceeding\n0.87. Our work lays the foundation for future research on automated visual\ndocumentation by introducing practical tools that not only generate valid\nvisual representations but also reliably assess their quality."}
{"id": "2509.12021", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.12021", "abs": "https://arxiv.org/abs/2509.12021", "authors": ["Benedikt Fein", "Florian Obermüller", "Gordon Fraser"], "title": "LitterBox+: An Extensible Framework for LLM-enhanced Scratch Static Code Analysis", "comment": "ASE 2025 Tool Demonstration Track", "summary": "Large language models (LLMs) have become an essential tool to support\ndevelopers using traditional text-based programming languages, but the\ngraphical notation of the block-based Scratch programming environment inhibits\nthe use of LLMs. To overcome this limitation, we propose the LitterBox+\nframework that extends the Scratch static code analysis tool LitterBox with the\ngenerative abilities of LLMs. By converting block-based code to a textual\nrepresentation suitable for LLMs, LitterBox+ allows users to query LLMs about\ntheir programs, about quality issues reported by LitterBox, and it allows\ngenerating code fixes. Besides offering a programmatic API for these\nfunctionalities, LitterBox+ also extends the Scratch user interface to make\nthese functionalities available directly in the environment familiar to\nlearners. The framework is designed to be easily extensible with other prompts,\nLLM providers, and new features combining the program analysis capabilities of\nLitterBox with the generative features of LLMs. We provide a screencast\ndemonstrating the tool at https://youtu.be/RZ6E0xgrIgQ."}
{"id": "2509.12087", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.12087", "abs": "https://arxiv.org/abs/2509.12087", "authors": ["Pengyu Xue", "Kunwu Zheng", "Zhen Yang", "Yifei Pei", "Linhao Wu", "Jiahui Dong", "Xiapu Luo", "Yan Xiao", "Fei Liu", "Yuxuan Zhang", "Xiran Lyu", "Xianhang Li", "Xuanyu Zhu", "Chengyi Wang"], "title": "A New Benchmark for Evaluating Code Translation with Third-Party Libraries", "comment": null, "summary": "In recent years, Large Language Models (LLMs) have been widely studied in the\ncode translation field on the method, class, and even repository levels.\nHowever, most of these benchmarks are limited in terms of Third-Party Library\n(TPL) categories and scales, making TPL-related errors hard to expose and\nhindering the development of targeted solutions. Considering the high\ndependence (over 90%) on TPLs in practical programming, demystifying and\nanalyzing LLMs' code translation performance involving various TPLs becomes\nimperative. To address this gap, we construct TransLibEval, the first benchmark\ndedicated to library-centric code translation. It consists of 200 real-world\ntasks across Python, Java, and C++, each explicitly involving TPLs from diverse\ncategories such as data processing, machine learning, and web development, with\ncomprehensive dependency coverage and high-coverage test suites. We evaluate\nseven recent LLMs of commercial, general, and code-specialized families under\nsix translation strategies of three categories: Direct, IR-guided, and\nRetrieval-augmented. Experimental results show a dramatic performance drop\ncompared with library-free settings (average CA decline over 60%), while\ndiverse strategies demonstrate heterogeneous advantages. Furthermore, we\nanalyze 4,831 failed cases from GPT-4o, one of the State-of-the-Art (SOTA)\nLLMs, revealing numerous third-party reference errors that were obscured\npreviously. These findings highlight the unique challenges of library-centric\ntranslation and provide practical guidance for improving TPL-aware code\nintelligence."}
{"id": "2509.12159", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12159", "abs": "https://arxiv.org/abs/2509.12159", "authors": ["Jingyu Xiao", "Zhongyi Zhang", "Yuxuan Wan", "Yintong Huo", "Yang Liu", "Michael R. Lyu"], "title": "EfficientUICoder: Efficient MLLM-based UI Code Generation via Input and Output Token Compression", "comment": null, "summary": "Multimodal Large Language Models have demonstrated exceptional performance in\nUI2Code tasks, significantly enhancing website development efficiency. However,\nthese tasks incur substantially higher computational overhead than traditional\ncode generation due to the large number of input image tokens and extensive\noutput code tokens required. Our comprehensive study identifies significant\nredundancies in both image and code tokens that exacerbate computational\ncomplexity and hinder focus on key UI elements, resulting in excessively\nlengthy and often invalid HTML files. We propose EfficientUICoder, a\ncompression framework for efficient UI code generation with three key\ncomponents. First, Element and Layout-aware Token Compression preserves\nessential UI information by detecting element regions and constructing UI\nelement trees. Second, Region-aware Token Refinement leverages attention scores\nto discard low-attention tokens from selected regions while integrating\nhigh-attention tokens from unselected regions. Third, Adaptive Duplicate Token\nSuppression dynamically reduces repetitive generation by tracking HTML/CSS\nstructure frequencies and applying exponential penalties. Extensive experiments\nshow EfficientUICoderachieves a 55%-60% compression ratio without compromising\nwebpage quality and delivers superior efficiency improvements: reducing\ncomputational cost by 44.9%, generated tokens by 41.4%, prefill time by 46.6%,\nand inference time by 48.8% on 34B-level MLLMs. Code is available at\nhttps://github.com/WebPAI/EfficientUICoder."}
{"id": "2410.04581", "categories": ["cs.PL", "cs.DS", "cs.LO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2410.04581", "abs": "https://arxiv.org/abs/2410.04581", "authors": ["Lee Zheng Han", "Umang Mathur"], "title": "Efficient Decrease-and-Conquer Linearizability Monitoring", "comment": null, "summary": "Linearizability has become the de facto correctness specification for\nimplementations of concurrent data structures. While formally verifying such\nimplementations remains challenging, linearizability monitoring has emerged as\na promising first step to rule out early problems in the development of custom\nimplementations, and serves as a key component in approaches that stress test\nsuch implementations. In this work, we investigate linearizability monitoring\n-- check if an execution history of an implementation is linearizable. While\nthis problem is intractable in general, a systematic understanding of when it\nbecomes tractable has remained elusive. We revisit this problem and first\npresent a unified `decrease-and-conquer' algorithmic framework for\nlinearizability monitoring. At its heart, this framework asks to identify\nspecial linearizability-preserving values in a given history -- values whose\npresence yields an equilinearizable sub-history when removed, and whose absence\nindicates non-linearizability. We prove that a polynomial time algorithm for\nthe problem of identifying linearizability-preserving values, yields a\npolynomial time algorithm for linearizability monitoring, while conversely,\nintractability of this problem implies intractability of the monitoring\nproblem. We demonstrate our framework's effectiveness by instantiating it for\nseveral popular data types -- sets, stacks, queues and priority queues --\nderiving polynomial time algorithms for each, with the unambiguity restriction,\nwhere each insertion to the underlying data structure adds a distinct value. We\noptimize these algorithms to achieve the optimal log-linear time complexity by\namortizing the cost of solving sub-problems through efficient data structures.\nOur implementation and evaluation on publicly available implementations show\nthat our approach scales to large histories and outperforms existing tools."}
{"id": "2504.10813", "categories": ["cs.PL", "cs.FL", "cs.LO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2504.10813", "abs": "https://arxiv.org/abs/2504.10813", "authors": ["Zhendong Ang", "Azadeh Farzan", "Umang Mathur"], "title": "Enhanced Data Race Prediction Through Modular Reasoning", "comment": null, "summary": "There are two orthogonal methodologies for efficient prediction of data races\nfrom concurrent program runs: commutativity and prefix reasoning. There are\nseveral instances of each methodology in the literature, with the goal of\npredicting data races using a streaming algorithm where the required memory\ndoes not grow proportional to the length of the observed run, but these\ninstances were mostly created in an ad hoc manner, without much attention to\ntheir unifying underlying principles. In this paper, we identify and formalize\nthese principles for each category with the ultimate goal of paving the way for\ncombining them into a new algorithm which shares their efficiency\ncharacteristics but offers strictly more prediction power. In particular, we\nformalize three distinct classes of races predictable using commutativity\nreasoning, and compare them. We identify three different styles of prefix\nreasoning, and prove that they predict the same class of races, which provably\ncontains all races predictable by any commutativity reasoning technique.\n  Our key contribution is combining prefix reasoning and commutativity\nreasoning in a modular way to introduce a new class of races, granular prefix\nraces, that are predictable in constant-space and linear time, in a streaming\nfashion. This class of races includes all races predictable using commutativity\nand prefix reasoning techniques. We present an improved constant-space\nalgorithm for prefix reasoning alone based on the idea of antichains (from\nlanguage theory). This improved algorithm is the stepping stone that is\nrequired to devise an efficient algorithm for prediction of granular prefix\nraces. We present experimental results to demonstrate the expressive power and\nperformance of our new algorithm."}
{"id": "2509.11877", "categories": ["cs.LO", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.11877", "abs": "https://arxiv.org/abs/2509.11877", "authors": ["Andrei Arusoaie", "Horaţiu Cheval", "Radu Iosif"], "title": "Proceedings 9th edition of Working Formal Methods Symposium", "comment": null, "summary": "This volume contains the proceedings of the 9th Working Formal Methods\nSymposium, which was held at the Alexandru Ioan Cuza University, Ia\\c{s}i,\nRomania on September 17-19, 2025."}
