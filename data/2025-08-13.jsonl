{"id": "2508.08322", "categories": ["cs.SE", "cs.AI", "68T07, 68N01", "D.2.2; I.2.6; D.2.5; I.2.8"], "pdf": "https://arxiv.org/pdf/2508.08322", "abs": "https://arxiv.org/abs/2508.08322", "authors": ["Muhammad Haseeb"], "title": "Context Engineering for Multi-Agent LLM Code Assistants Using Elicit, NotebookLM, ChatGPT, and Claude Code", "comment": "15 pages, 5 figures, research paper on multi-agent LLM systems for\n  code generation", "summary": "Large Language Models (LLMs) have shown promise in automating code generation\nand software engineering tasks, yet they often struggle with complex,\nmulti-file projects due to context limitations and knowledge gaps. We propose a\nnovel context engineering workflow that combines multiple AI components: an\nIntent Translator (GPT-5) for clarifying user requirements, an Elicit-powered\nsemantic literature retrieval for injecting domain knowledge, NotebookLM-based\ndocument synthesis for contextual understanding, and a Claude Code multi-agent\nsystem for code generation and validation. Our integrated approach leverages\nintent clarification, retrieval-augmented generation, and specialized\nsub-agents orchestrated via Claude's agent framework. We demonstrate that this\nmethod significantly improves the accuracy and reliability of code assistants\nin real-world repositories, yielding higher single-shot success rates and\nbetter adherence to project context than baseline single-agent approaches.\nQualitative results on a large Next.js codebase show the multi-agent system\neffectively plans, edits, and tests complex features with minimal human\nintervention. We compare our system with recent frameworks like CodePlan,\nMASAI, and HyperAgent, highlighting how targeted context injection and agent\nrole decomposition lead to state-of-the-art performance. Finally, we discuss\nthe implications for deploying LLM-based coding assistants in production, along\nwith lessons learned on context management and future research directions."}
{"id": "2508.08332", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08332", "abs": "https://arxiv.org/abs/2508.08332", "authors": ["Humza Ashraf", "Syed Muhammad Danish", "Aris Leivadeas", "Yazan Otoum", "Zeeshan Sattar"], "title": "Energy-Aware Code Generation with LLMs: Benchmarking Small vs. Large Language Models for Sustainable AI Programming", "comment": null, "summary": "Large Language Models (LLMs) are widely used for code generation. However,\ncommercial models like ChatGPT require significant computing power, which leads\nto high energy use and carbon emissions. This has raised concerns about their\nenvironmental impact. In this study, we evaluate open-source Small Language\nModels (SLMs) trained explicitly for code generation and compare their\nperformance and energy efficiency against large LLMs and efficient\nhuman-written Python code. The goal is to investigate whether SLMs can match\nthe performance of LLMs on certain types of programming problems while\nproducing more energy-efficient code. We evaluate 150 coding problems from\nLeetCode, evenly distributed across three difficulty levels: easy, medium, and\nhard. Our comparison includes three small open-source models, StableCode-3B,\nStarCoderBase-3B, and Qwen2.5-Coder-3B-Instruct, and two large commercial\nmodels, GPT-4.0 and DeepSeek-Reasoner. The generated code is evaluated using\nfour key metrics: run-time, memory usage, energy consumption, and correctness.\nWe use human-written solutions as a baseline to assess the quality and\nefficiency of the model-generated code. Results indicate that LLMs achieve the\nhighest correctness across all difficulty levels, but SLMs are often more\nenergy-efficient when their outputs are correct. In over 52% of the evaluated\nproblems, SLMs consumed the same or less energy than LLMs."}
{"id": "2508.08342", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.08342", "abs": "https://arxiv.org/abs/2508.08342", "authors": ["Maximilian Jungwirth", "Martin Gruber", "Gordon Fraser"], "title": "Improving Merge Pipeline Throughput in Continuous Integration via Pull Request Prioritization", "comment": "This paper is accepted on the Industry Track of the 41st\n  International Conference on Software Maintenance and Evolution (ICSME 2025)", "summary": "Integrating changes into large monolithic software repositories is a critical\nstep in modern software development that substantially impacts the speed of\nfeature delivery, the stability of the codebase, and the overall productivity\nof development teams. To ensure the stability of the main branch, many\norganizations use merge pipelines that test software versions before the\nchanges are permanently integrated. However, the load on merge pipelines is\noften so high that they become bottlenecks, despite the use of parallelization.\nExisting optimizations frequently rely on specific build systems, limiting\ntheir generalizability and applicability. In this paper we propose to optimize\nthe order of PRs in merge pipelines using practical build predictions utilizing\nonly historical build data, PR metadata, and contextual information to estimate\nthe likelihood of successful builds in the merge pipeline. By dynamically\nprioritizing likely passing PRs during peak hours, this approach maximizes\nthroughput when it matters most. Experiments conducted on a real-world,\nlarge-scale project demonstrate that predictive ordering significantly\noutperforms traditional first-in-first-out (FIFO), as well as\nnon-learning-based ordering strategies. Unlike alternative optimizations, this\napproach is agnostic to the underlying build system and thus easily integrable\ninto existing automated merge pipelines."}
{"id": "2508.08545", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08545", "abs": "https://arxiv.org/abs/2508.08545", "authors": ["Youssef Esseddiq Ouatiti", "Mohammed Sayagh", "Bram Adams", "Ahmed E. Hassan"], "title": "OmniLLP: Enhancing LLM-based Log Level Prediction with Context-Aware Retrieval", "comment": null, "summary": "Developers insert logging statements in source code to capture relevant\nruntime information essential for maintenance and debugging activities. Log\nlevel choice is an integral, yet tricky part of the logging activity as it\ncontrols log verbosity and therefore influences systems' observability and\nperformance. Recent advances in ML-based log level prediction have leveraged\nlarge language models (LLMs) to propose log level predictors (LLPs) that\ndemonstrated promising performance improvements (AUC between 0.64 and 0.8).\nNevertheless, current LLM-based LLPs rely on randomly selected in-context\nexamples, overlooking the structure and the diverse logging practices within\nmodern software projects. In this paper, we propose OmniLLP, a novel LLP\nenhancement framework that clusters source files based on (1) semantic\nsimilarity reflecting the code's functional purpose, and (2) developer\nownership cohesion. By retrieving in-context learning examples exclusively from\nthese semantic and ownership aware clusters, we aim to provide more coherent\nprompts to LLPs leveraging LLMs, thereby improving their predictive accuracy.\nOur results show that both semantic and ownership-aware clusterings\nstatistically significantly improve the accuracy (by up to 8\\% AUC) of the\nevaluated LLM-based LLPs compared to random predictors (i.e., leveraging\nrandomly selected in-context examples from the whole project). Additionally,\nour approach that combines the semantic and ownership signal for in-context\nprediction achieves an impressive 0.88 to 0.96 AUC across our evaluated\nprojects. Our findings highlight the value of integrating software\nengineering-specific context, such as code semantic and developer ownership\nsignals into LLM-LLPs, offering developers a more accurate, contextually-aware\napproach to logging and therefore, enhancing system maintainability and\nobservability."}
{"id": "2508.08496", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2508.08496", "abs": "https://arxiv.org/abs/2508.08496", "authors": ["Mudathir Mohamed", "Nick Feng", "Andrew Reynolds", "Cesare Tinelli", "Clark Barrett", "Marsha Chechik"], "title": "Solving Set Constraints with Comprehensions and Bounded Quantifiers", "comment": null, "summary": "Many real applications problems can be encoded easily as quantified formulas\nin SMT. However, this simplicity comes at the cost of difficulty during solving\nby SMT solvers. Different strategies and quantifier instantiation techniques\nhave been developed to tackle this. However, SMT solvers still struggle with\nquantified formulas generated by some applications. In this paper, we discuss\nthe use of set-bounded quantifiers, quantifiers whose variable ranges over a\nfinite set. These quantifiers can be implemented using quantifier-free fragment\nof the theory of finite relations with a filter operator, a form of restricted\ncomprehension, that constructs a subset from a finite set using a predicate. We\nshow that this approach outperforms other quantification techniques in\nsatisfiable problems generated by the SLEEC tool, and is very competitive on\nunsatisfiable benchmarks compared to LEGOS, a specialized solver for SLEEC. We\nalso identify a decidable class of constraints with restricted applications of\nthe filter operator, while showing that unrestricted applications lead to\nundecidability."}
{"id": "2508.08661", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08661", "abs": "https://arxiv.org/abs/2508.08661", "authors": ["Chunhua Liu", "Hong Yi Lin", "Patanamon Thongtanunam"], "title": "Hallucinations in Code Change to Natural Language Generation: Prevalence and Evaluation of Detection Metrics", "comment": "8 main pages, 5 figures", "summary": "Language models have shown strong capabilities across a wide range of tasks\nin software engineering, such as code generation, yet they suffer from\nhallucinations. While hallucinations have been studied independently in natural\nlanguage and code generation, their occurrence in tasks involving code changes\nwhich have a structurally complex and context-dependent format of code remains\nlargely unexplored. This paper presents the first comprehensive analysis of\nhallucinations in two critical tasks involving code change to natural language\ngeneration: commit message generation and code review comment generation. We\nquantify the prevalence of hallucinations in recent language models and explore\na range of metric-based approaches to automatically detect them. Our findings\nreveal that approximately 50\\% of generated code reviews and 20\\% of generated\ncommit messages contain hallucinations. Whilst commonly used metrics are weak\ndetectors on their own, combining multiple metrics substantially improves\nperformance. Notably, model confidence and feature attribution metrics\neffectively contribute to hallucination detection, showing promise for\ninference-time detection.\\footnote{All code and data will be released upon\nacceptance."}
{"id": "2508.09053", "categories": ["cs.LO", "68Q10, 68N19"], "pdf": "https://arxiv.org/pdf/2508.09053", "abs": "https://arxiv.org/abs/2508.09053", "authors": ["Klaus-Dieter Schewe", "Flavio Ferrarotti"], "title": "Behavioural Theory of Reflective Algorithms II: Reflective Parallel Algorithms", "comment": "49 pages; short version (without proofs) published in LNCS vol.\n  15728: Rigorous State-Based Methods, 2025", "summary": "We develop a behavioural theory of reflective parallel algorithms (RAs), i.e.\nsynchronous parallel algorithms that can modify their own behaviour. The theory\ncomprises a set of postulates defining the class of RAs, an abstract machine\nmodel, and the proof that all RAs are captured by this machine model. RAs are\nsequential-time, parallel algorithms, where every state includes a\nrepresentation of the algorithm in that state, thus enabling linguistic\nreflection. Bounded exploration is preserved using multiset comprehension terms\nas values. The abstract machine model is defined by reflective Abstract State\nMachines (rASMs), which extend ASMs using extended states that include an\nupdatable representation of the main ASM rule to be executed by the machine in\nthat state."}
{"id": "2508.08868", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.08868", "abs": "https://arxiv.org/abs/2508.08868", "authors": ["Henning Femmer", "Frank Houdek", "Max Unterbusch", "Andreas Vogelsang"], "title": "Description and Comparative Analysis of QuRE: A New Industrial Requirements Quality Dataset", "comment": null, "summary": "Requirements quality is central to successful software and systems\nengineering. Empirical research on quality defects in natural language\nrequirements relies heavily on datasets, ideally as realistic and\nrepresentative as possible. However, such datasets are often inaccessible,\nsmall, or lack sufficient detail. This paper introduces QuRE (Quality in\nRequirements), a new dataset comprising 2,111 industrial requirements that have\nbeen annotated through a real-world review process. Previously used for over\nfive years as part of an industrial contract, this dataset is now being\nreleased to the research community. In this work, we furthermore provide\ndescriptive statistics on the dataset, including measures such as lexical\ndiversity and readability, and compare it to existing requirements datasets and\nsynthetically generated requirements. In contrast to synthetic datasets, QuRE\nis linguistically similar to existing ones. However, this dataset comes with a\ndetailed context description, and its labels have been created and used\nsystematically and extensively in an industrial context over a period of close\nto a decade. Our goal is to foster transparency, comparability, and empirical\nrigor by supporting the development of a common gold standard for requirements\nquality datasets. This, in turn, will enable more sound and collaborative\nresearch efforts in the field."}
{"id": "2508.08872", "categories": ["cs.SE", "D.2.5"], "pdf": "https://arxiv.org/pdf/2508.08872", "abs": "https://arxiv.org/abs/2508.08872", "authors": ["Dylan Callaghan", "Alexandra van der Spuy", "Bernd Fischer"], "title": "Empirical Analysis of Temporal and Spatial Fault Characteristics in Multi-Fault Bug Repositories", "comment": null, "summary": "Fixing software faults contributes significantly to the cost of software\nmaintenance and evolution. Techniques for reducing these costs require datasets\nof software faults, as well as an understanding of the faults, for optimal\ntesting and evaluation. In this paper, we present an empirical analysis of the\ntemporal and spatial characteristics of faults existing in 16 open-source Java\nand Python projects, which form part of the Defects4J and BugsInPy datasets,\nrespectively. Our findings show that many faults in these software systems are\nlong-lived, leading to the majority of software versions having multiple\ncoexisting faults. This is in contrast to the assumptions of the original\ndatasets, where the majority of versions only identify a single fault. In\naddition, we show that although the faults are found in only a small subset of\nthe systems, these faults are often evenly distributed amongst this subset,\nleading to relatively few bug hotspots."}
{"id": "2508.08952", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.08952", "abs": "https://arxiv.org/abs/2508.08952", "authors": ["Hyunwoo Kim", "Jaeseong Lee", "Sunpyo Hong", "Changmin Han"], "title": "Toward Automated Hypervisor Scenario Generation Based on VM Workload Profiling for Resource-Constrained Environments", "comment": null, "summary": "In the automotive industry, the rise of software-defined vehicles (SDVs) has\n  driven a shift toward virtualization-based architectures that consolidate\n  diverse automotive workloads on a shared hardware platform. To support this\n  evolution, chipset vendors provide board support packages (BSPs), hypervisor\n  setups, and resource allocation guidelines. However, adapting these static\n  configurations to varying system requirements and workloads remain a\n  significant challenge for Tier 1 integrators.\n  This paper presents an automated scenario generation framework, which helps\n  automotive vendors to allocate hardware resources efficiently across multiple\n  VMs. By profiling runtime behavior and integrating both theoretical models\nand\n  vendor heuristics, the proposed tool generates optimized hypervisor\n  configurations tailored to system constraints.\n  We compare two main approaches for modeling target QoS based on profiled data\n  and resource allocation: domain-guided parametric modeling and deep\n  learning-based modeling. We further describe our optimization strategy using\n  the selected QoS model to derive efficient resource allocations. Finally, we\n  report on real-world deployments to demonstrate the effectiveness of our\n  framework in improving integration efficiency and reducing development time\nin\n  resource-constrained environments."}
