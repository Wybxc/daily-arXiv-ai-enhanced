<div id=toc></div>

# Table of Contents

- [cs.LO](#cs.LO) [Total: 4]
- [cs.SE](#cs.SE) [Total: 11]


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [1] [A Note on Constructive Canonical Splitter Strategies in Nowhere Dense Graph Classes](https://arxiv.org/abs/2509.10062)
*Janne Fuchser,Nikolas Mählmann,Sebastian Siebertz*

Main category: cs.LO

TL;DR: 本文为半径r分割游戏提供了构造性上界，证明如果Splitter能在k轮内获胜，则最多有(2r+1)^(2^(k-1)-1)个进展性移动


<details>
  <summary>Details</summary>
Motivation: Ohlmann等人的先前证明基于紧致性定理，没有给出进展性移动数量的构造性界限。本文旨在提供简单构造性证明

Method: 使用构造性方法分析半径r分割游戏中Splitter的进展性移动，建立明确的数学上界

Result: 证明了如果Splitter能在k轮内赢得半径r游戏，则进展性移动的数量最多为(2r+1)^(2^(k-1)-1)

Conclusion: 本文提供了分割游戏中进展性移动数量的构造性上界，改进了先前非构造性证明的结果

Abstract: The radius-$r$ splitter game is played on a graph $G$ between two players:
Splitter and Connector. In each round, Connector selects a vertex $v$, and the
current game arena is restricted to the radius-$r$ neighborhood of $v$. Then
Splitter removes a vertex from this restricted subgraph. The game ends, and
Splitter wins, when the arena becomes empty. Splitter aims to end the game as
quickly as possible, while Connector tries to prolong it for as long as
possible. The splitter game was introduced by Grohe, Kreutzer and Siebertz to
characterize nowhere dense graph classes. They showed that a class
$\mathscr{C}$ of graphs is nowhere dense if and only if for every radius $r$
there exists a number $\ell$ such that Splitter has a strategy on every $G\in
\mathscr{C}$ to win the radius-$r$ splitter game in at most $\ell$ rounds. It
was recently proved by Ohlmann et al. that there are only a bounded number of
possible Splitter moves that are progressing, that is, moves that lead to an
arena where Splitter can win in one less round. The proof of Ohlmann et al. is
based on the compactness theorem and does not give a constructive bound on the
number of progressing moves. In this work, we give a simple constructive proof,
showing that if Splitter can force a win in the radius-$r$ game in $k$ rounds,
then there are at most $(2r+1)^{\,2^{k-1}-1}$ progressing moves.

</details>


### [2] [On Syntactical Simplification of Temporal Operators in Negation-free MTL](https://arxiv.org/abs/2509.10146)
*Mathijs van Noort,Femke Ongenae,Pieter Bonte*

Main category: cs.LO

TL;DR: 本文研究了无否定的MTL（度量时序逻辑）的表达能力，证明了"always"算子可以被"once"、"since"和"until"算子消除，甚至"once"算子也能被移除，仅保留"until"和"since"算子，挑战了否定在表达通用时序约束中的必要性假设。


<details>
  <summary>Details</summary>
Motivation: 在动态数据密集型环境中，传统基于否定的时序推理方法在开放分布式系统（如IoT网络和语义网）中不可靠，因为数据不完整和异步。因此需要研究无否定的时序规则系统，以保持单调性和可扩展推理。

Method: 研究无否定MTL的表达能力，通过算子消除技术证明"always"算子可以用"once"、"since"和"until"算子表示，进一步证明"once"算子也能被消除，最终得到仅基于"until"和"since"的片段。

Result: 成功证明了MTL中的"always"算子可以被消除，甚至"once"算子也能被移除，得到一个仅包含"until"和"since"算子的强大片段，能够捕获存在性和不变性时序模式。

Conclusion: 研究结果表明否定并非表达通用时序约束的必要条件，发现了一个强大的无否定片段，为MTL语法简化提供了理论基础，对理论研究和实现都有益处。

Abstract: Temporal reasoning in dynamic, data-intensive environments increasingly
demands expressive yet tractable logical frameworks. Traditional approaches
often rely on negation to express absence or contradiction. In such contexts,
Negation-as-Failure is commonly used to infer negative information from the
lack of positive evidence. However, open and distributed systems such as IoT
networks or the Semantic Web Negation-as-Failure semantics become unreliable
due to incomplete and asynchronous data. This has led to a growing interest in
negation-free fragments of temporal rule-based systems, which preserve
monotonicity and enable scalable reasoning.
  This paper investigates the expressive power of negation-free MTL, a temporal
logic framework designed for rule-based reasoning over time. We show that the
"always" operators of MTL, often treated as syntactic sugar for combinations of
other temporal constructs, can be eliminated using "once", "since" and "until"
operators. Remarkably, even the "once" operators can be removed, yielding a
fragment based solely on "until" and "since". These results challenge the
assumption that negation is necessary for expressing universal temporal
constraints, and reveal a robust fragment capable of capturing both existential
and invariant temporal patterns. Furthermore, the results induce a reduction in
the syntax of MTL, which in turn can provide benefits for both theoretical
study as well as implementation efforts.

</details>


### [3] [Initial Algebras of Domains via Quotient Inductive-Inductive Types](https://arxiv.org/abs/2509.10187)
*Simcha van Collem,Niels van der Weide,Herman Geuvers*

Main category: cs.LO

TL;DR: 本文提出了一个在域理论中构建代数效应的通用框架，使用DCPO（定向完全偏序）作为域，通过DCPO代数和商归纳-归纳类型来构造初始代数，并展示了多种经典DCPO构造都符合该框架。


<details>
  <summary>Details</summary>
Motivation: 域理论作为计算数学理论，为编程语言提供指称语义，但需要通用框架来统一建模各种代数效应（如非确定性、部分函数、副作用等）。

Method: 使用DCPO代数定义签名和不等式理论，通过商归纳-归纳类型（QIITs）构造初始DCPO代数，并在Cubical Agda中进行形式化验证。

Result: 成功构建了通用框架，证明多种经典DCPO构造（合并和、粉碎积、自由DCPO等）都符合该框架，统一了部分性和幂域等代数效应的建模。

Conclusion: 该框架为域理论中的代数效应提供了统一的数学基础，结合了同伦类型论的优势，为程序语义的形式化验证提供了有力工具。

Abstract: Domain theory has been developed as a mathematical theory of computation and
to give a denotational semantics to programming languages. It helps us to fix
the meaning of language concepts, to understand how programs behave and to
reason about programs. At the same time it serves as a great theory to model
various algebraic effects such as non-determinism, partial functions, side
effects and numerous other forms of computation.
  In the present paper, we present a general framework to construct algebraic
effects in domain theory, where our domains are DCPOs: directed complete
partial orders. We first describe so called DCPO algebras for a signature,
where the signature specifies the operations on the DCPO and the inequational
theory they obey. This provides a method to represent various algebraic
effects, like partiality. We then show that initial DCPO algebras exist by
defining them as so called Quotient Inductive-Inductive Types (QIITs), known
from homotopy type theory. A quotient inductive-inductive type allows one to
simultaneously define an inductive type and an inductive relation on that type,
together with equations on the type. We illustrate our approach by showing that
several well-known constructions of DCPOs fit our framework: coalesced sums,
smash products and free DCPOs (partiality and power domains). Our work makes
use of various features of homotopy type theory and is formalized in Cubical
Agda.

</details>


### [4] [Effects of the Strict-Tolerant Approach on Intuitionistic and Minimal Logic](https://arxiv.org/abs/2509.10322)
*Victor Barroso-Nascimento,German Mejia*

Main category: cs.LO

TL;DR: 本文将严格容忍逻辑方法应用于直觉主义和极小逻辑，发现直觉主义的严格容忍推理会坍缩为经典逻辑，而极小逻辑的严格容忍推理则没有有效推理，但在元推理层面三者产生不同的逻辑系统。


<details>
  <summary>Details</summary>
Motivation: 扩展严格容忍逻辑方法的应用范围，研究该方法在直觉主义和极小逻辑中的表现，探索不同逻辑层次（推理、元推理等）上的严格容忍定义如何影响逻辑系统的性质。

Method: 采用严格容忍逻辑方法，修改传统逻辑后果关系的定义，要求从前提的真必须推出结论的非假性。将此方法应用于直觉主义逻辑和极小逻辑，并在不同逻辑层次（推理、元推理、元元推理等）进行分析比较。

Result: 1) 直觉主义严格容忍推理坍缩为经典逻辑推理；2) 极小严格容忍逻辑没有有效推理（但在元推理层面不成立）；3) 直觉主义、极小编和经典逻辑在元推理层面产生三个不同的逻辑系统。

Conclusion: 严格容忍方法对不同的逻辑系统产生不同的影响：直觉主义逻辑在该方法下失去其非经典特性，而极小逻辑则表现出极端性质（无有效推理）。但在元推理层面，三种逻辑都保持了各自的独特性，表明严格容忍方法在不同逻辑层次上的效果是不同的。

Abstract: This paper extends the literature on the strict-tolerant logical approach by
applying its methods to intuitionistic and minimal logic. In short, the
strict-tolerant approach modifies the usual notion of logical consequence by
stipulating that, in order for an inference to be valid, from the truth of the
premises must follow the non-falsity of the conclusion. This notion can also be
generalized to define strict-tolerant metainferences, metametainferences and so
on, which may or may not generate logics distinct from those obtained on the
inferential level. It is already known that strict-tolerant definitions can
make the notion of inference for non-classical logics collapse into the
classical notion, but the strength of this effect is not yet fully known. This
paper shows that intuitionistic strict-tolerant inferences also collapse into
classical ones, but minimal ones do not. However, minimal strict-tolerant logic
has the property that no inferences are valid (which is not carried over to the
metainferential level). Additionally, it is shown that the logics obtained from
intuitionistic, minimal and classical logic at the metainferential level are
distinct from each other.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [5] [Stencil-Lifting: Hierarchical Recursive Lifting System for Extracting Summary of Stencil Kernel in Legacy Codes](https://arxiv.org/abs/2509.10236)
*Mingyi Li,Junmin Xiao,Siyan Chen,Hui Ma,Xi Chen,Peihua Bao,Liang Yuan,Guangming Tan*

Main category: cs.SE

TL;DR: Stencil-Lifting是一个自动将低级语言编写的模板内核转换为等效DSL实现的系统，通过层次递归提升理论和算法，相比现有系统实现了31.62倍和5.8倍的加速


<details>
  <summary>Details</summary>
Motivation: 针对现有验证提升系统在效率上的瓶颈，需要一种可扩展的方法来自动转换遗留代码中的模板内核到DSL实现，以弥合传统优化技术与现代DSL范式之间的差距

Method: 提出层次递归提升理论，使用不变子图表示嵌套循环结构的模板内核，每个顶点关联基于谓词的摘要；开发层次递归提升算法，通过收敛的递归过程保证终止，避免基于搜索的合成低效

Result: 在两个不同测试套件的多样化模板基准和四个真实应用上评估，相比最先进的STNG和Dexter系统分别实现了31.62倍和5.8倍的加速，同时保持完全语义等价

Conclusion: Stencil-Lifting显著提高了低级模板内核到DSL实现的转换效率，有效连接了传统优化技术和现代DSL范式

Abstract: We introduce Stencil-Lifting, a novel system for automatically converting
stencil kernels written in low-level languages in legacy code into semantically
equivalent Domain-Specific Language (DSL) implementations. Targeting the
efficiency bottlenecks of existing verified lifting systems, Stencil-Lifting
achieves scalable stencil kernel abstraction through two key innovations.
First, we propose a hierarchical recursive lifting theory that represents
stencil kernels, structured as nested loops, using invariant subgraphs, which
are customized data dependency graphs that capture loop-carried computation and
structural invariants. Each vertex in the invariant subgraph is associated with
a predicate-based summary, encoding its computational semantics. By enforcing
self-consistency across these summaries, Stencil-Lifting ensures the derivation
of correct loop invariants and postconditions for nested loops, eliminating the
need for external verification. Second, we develop a hierarchical recursive
lifting algorithm that guarantees termination through a convergent recursive
process, avoiding the inefficiencies of search-based synthesis. The algorithm
efficiently derives the valid summaries of stencil kernels, and its
completeness is formally proven. We evaluate Stencil-Lifting on diverse stencil
benchmarks from two different suites and on four real-world applications.
Experimental results demonstrate that Stencil-Lifting achieves 31.62$\times$
and 5.8$\times$ speedups compared to the state-of-the-art verified lifting
systems STNG and Dexter, respectively, while maintaining full semantic
equivalence. Our work significantly enhances the translation efficiency of
low-level stencil kernels to DSL implementations, effectively bridging the gap
between legacy optimization techniques and modern DSL-based paradigms.

</details>


### [6] [SWE-Effi: Re-Evaluating Software AI Agent System Effectiveness Under Resource Constraints](https://arxiv.org/abs/2509.09853)
*Zhiyu Fan,Kirill Vasilevski,Dayi Lin,Boyuan Chen,Yihao Chen,Zhiqing Zhong,Jie M. Zhang,Pinjia He,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: SWE-Effi是一套新的多维度评估指标，用于重新评估AI系统在软件工程任务中的整体效能，综合考虑准确性和资源消耗（token和时间）。


<details>
  <summary>Details</summary>
Motivation: 现有AI软件工程排行榜（如SWE-bench）只关注解决方案准确性，忽略了资源受限环境下的效能问题。任何AI系统不仅需要正确，还必须具有成本效益。

Method: 引入SWE-Effi指标，在SWE-bench基准的子集上重新评估流行的AI问题解决系统，从准确性和资源消耗两个维度进行多维度度量。

Result: 发现AI系统效能不仅取决于框架本身，还取决于与基础模型的整合程度；识别出"token雪球效应"和"昂贵失败"模式；观察到token预算和时间预算下的效能权衡。

Conclusion: 需要综合考虑准确性和资源消耗来评估AI系统效能，这对于实际部署和强化学习训练中的成本控制至关重要，系统整合质量是实现资源高效强性能的关键。

Abstract: The advancement of large language models (LLMs) and code agents has
demonstrated significant potential to assist software engineering (SWE) tasks,
such as autonomous issue resolution and feature addition. Existing AI for
software engineering leaderboards (e.g., SWE-bench) focus solely on solution
accuracy, ignoring the crucial factor of effectiveness in a
resource-constrained world. This is a universal problem that also exists beyond
software engineering tasks: any AI system should be more than correct - it must
also be cost-effective. To address this gap, we introduce SWE-Effi, a set of
new metrics to re-evaluate AI systems in terms of holistic effectiveness
scores. We define effectiveness as the balance between the accuracy of outcome
(e.g., issue resolve rate) and the resources consumed (e.g., token and time).
In this paper, we specifically focus on the software engineering scenario by
re-ranking popular AI systems for issue resolution on a subset of the SWE-bench
benchmark using our new multi-dimensional metrics. We found that AI system's
effectiveness depends not just on the scaffold itself, but on how well it
integrates with the base model, which is key to achieving strong performance in
a resource-efficient manner. We also identified systematic challenges such as
the "token snowball" effect and, more significantly, a pattern of "expensive
failures". In these cases, agents consume excessive resources while stuck on
unsolvable tasks - an issue that not only limits practical deployment but also
drives up the cost of failed rollouts during RL training. Lastly, we observed a
clear trade-off between effectiveness under the token budget and effectiveness
under the time budget, which plays a crucial role in managing project budgets
and enabling scalable reinforcement learning, where fast responses are
essential.

</details>


### [7] [From Hugging Face to GitHub: Tracing License Drift in the Open-Source AI Ecosystem](https://arxiv.org/abs/2509.09873)
*James Jewitt,Hao Li,Bram Adams,Gopi Krishnan Rajbahadur,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 对Hugging Face平台364千个数据集、160万个模型和140千个GitHub项目的首次端到端许可证审计，发现35.5%的模型到应用转换中存在系统性许可证违规，通过重新许可消除限制性条款。


<details>
  <summary>Details</summary>
Motivation: 开源AI生态系统中隐藏的许可证冲突存在严重的法律和伦理风险，但缺乏数据驱动的理解来了解这些冲突的发生频率、来源和受影响社区。

Method: 构建可扩展的规则引擎，编码近200个SPDX和模型特定条款来检测许可证冲突，覆盖大规模数据集、模型和GitHub项目。

Result: 发现35.5%的模型到应用转换存在许可证违规，原型引擎能解决86.4%的软件应用许可证冲突。

Conclusion: 许可证合规性是开源AI的关键治理挑战，研究提供了支持自动化、AI感知的大规模合规性所需的数据和工具。

Abstract: Hidden license conflicts in the open-source AI ecosystem pose serious legal
and ethical risks, exposing organizations to potential litigation and users to
undisclosed risk. However, the field lacks a data-driven understanding of how
frequently these conflicts occur, where they originate, and which communities
are most affected. We present the first end-to-end audit of licenses for
datasets and models on Hugging Face, as well as their downstream integration
into open-source software applications, covering 364 thousand datasets, 1.6
million models, and 140 thousand GitHub projects. Our empirical analysis
reveals systemic non-compliance in which 35.5% of model-to-application
transitions eliminate restrictive license clauses by relicensing under
permissive terms. In addition, we prototype an extensible rule engine that
encodes almost 200 SPDX and model-specific clauses for detecting license
conflicts, which can solve 86.4% of license conflicts in software applications.
To support future research, we release our dataset and the prototype engine.
Our study highlights license compliance as a critical governance challenge in
open-source AI and provides both the data and tools necessary to enable
automated, AI-aware compliance at scale.

</details>


### [8] [SLD-Spec: Enhancement LLM-assisted Specification Generation for Complex Loop Functions via Program Slicing and Logical Deletion](https://arxiv.org/abs/2509.09917)
*Zehan Chen,Long Zhang,Zhiwei Zhang,JingJing Zhang,Ruoyu Zhou,Yulong Shen,JianFeng Ma,Lin Yang*

Main category: cs.SE

TL;DR: SLD-Spec是一种针对复杂循环程序的LLM辅助规范生成方法，通过程序切片和逻辑删除两个新阶段，显著提高了生成规范的正确性、相关性和完整性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的方法在处理包含复杂循环结构的程序时往往产生不相关的规范，且验证工具的严格证明义务和设计约束会导致规范不完整和模糊。

Method: 引入两个新阶段：(1)切片阶段将函数分解为包含独立循环结构的代码片段；(2)逻辑删除阶段应用基于LLM的推理过滤错误候选规范。

Result: 在简单数据集上比最先进的AutoSpec多验证5个程序，运行时间减少23.73%；在复杂循环数据集上使95.1%的断言和90.91%的程序通过验证。

Conclusion: 逻辑删除对提升规范正确性和相关性至关重要，程序切片对规范完整性贡献显著，SLD-Spec能有效处理复杂循环程序的规范生成问题。

Abstract: Automatically generating formal specifications from program code can greatly
enhance the efficiency of program verification and enable end-to-end automation
from requirements to reliable software. However, existing LLM-based approaches
often struggle with programs that include complex loop structures, leading to
irrelevant specifications. Moreover, the rigorous proof obligations and design
constraints imposed by verification tools can further result in incomplete and
ambiguous specifications. To address these challenges, we propose SLD-Spec, an
LLM-assisted specification generation method tailored for programs with complex
loop constructs. SLD-Spec introduces two novel phases into the traditional
specification generation framework: (1) A slicing phase, which decomposes each
function into code fragments containing independent loop structures, thereby
reducing the complexity of specification generation; and (2) A logical deletion
phase, which applies LLM-based reasoning to filter out incorrect candidate
specifications--especially those not easily identified by verification
tool--while retaining valid ones. Experimental results show that on the simple
dataset, SLD-Spec successfully verifies five more programs than the
state-of-the-art AutoSpec and reduces runtime by 23.73%. To address the
limitations of existing research, we manually construct a dataset comprising
four categories of complex loop programs. On this dataset, SLD-Spec
significantly improves the correctness, relevance, and completeness of
generated specifications compared to baseline methods, enabling 95.1% of
assertions and 90.91% of programs to pass verification. Ablation studies
further reveal that logical deletion is critical for enhancing specification
correctness and relevance, while program slicing contributes significantly to
specification completeness. Our code and data are publicly available.

</details>


### [9] [WALL: A Web Application for Automated Quality Assurance using Large Language Models](https://arxiv.org/abs/2509.09918)
*Seyed Moein Abtahi,Akramul Azim*

Main category: cs.SE

TL;DR: WALL是一个集成SonarQube和大型语言模型的Web应用，通过三个模块自动化代码问题检测、修复和评估，在563个文件7599个问题上验证了有效性


<details>
  <summary>Details</summary>
Motivation: 随着软件项目复杂度增加，代码问题数量和种类大幅增长，需要高效的问题检测、解决和评估工具来应对这一挑战

Method: 开发WALL Web应用，集成SonarQube和LLMs（GPT-3.5 Turbo和GPT-4o），包含问题提取工具、代码问题修订器和代码比较工具三个模块

Result: 在563个文件7599个问题上实验证明，WALL能有效减少人工工作量并保持高质量修订，混合使用经济型和先进LLMs可显著降低成本并提高修订率

Conclusion: WALL展示了自动化代码质量管理的可行性，未来工作将集成开源LLMs并消除人工干预，实现完全自动化的代码质量管理

Abstract: As software projects become increasingly complex, the volume and variety of
issues in code files have grown substantially. Addressing this challenge
requires efficient issue detection, resolution, and evaluation tools. This
paper presents WALL, a web application that integrates SonarQube and large
language models (LLMs) such as GPT-3.5 Turbo and GPT-4o to automate these
tasks. WALL comprises three modules: an issue extraction tool, code issues
reviser, and code comparison tool. Together, they enable a seamless pipeline
for detecting software issues, generating automated code revisions, and
evaluating the accuracy of revisions. Our experiments, conducted on 563 files
with over 7,599 issues, demonstrate WALL's effectiveness in reducing human
effort while maintaining high-quality revisions. Results show that employing a
hybrid approach of cost-effective and advanced LLMs can significantly lower
costs and improve revision rates. Future work aims to enhance WALL's
capabilities by integrating open-source LLMs and eliminating human
intervention, paving the way for fully automated code quality management.

</details>


### [10] [Toward Green Code: Prompting Small Language Models for Energy-Efficient Code Generation](https://arxiv.org/abs/2509.09947)
*Humza Ashraf,Syed Muhammad Danish,Zeeshan Sattar*

Main category: cs.SE

TL;DR: 研究表明提示工程可以提升小型语言模型在代码生成中的能效，其中CoT提示对某些模型能带来一致的节能效果，但效果因模型而异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在软件开发中的高能耗和碳足迹引发环境担忧，小型语言模型作为更可持续的替代方案，需要研究如何通过提示工程提高其能效。

Method: 评估4个开源小型语言模型在150个LeetCode Python问题上的表现，使用4种提示策略（角色提示、零样本、少样本和思维链），测量运行时、内存使用和能耗。

Result: 思维链提示为Qwen2.5-Coder和StableCode-3B带来一致的节能效果，而CodeLlama-7B和Phi-3-Mini-4K在所有提示策略下均未能超越人工编写基准。

Conclusion: 提示工程的效益具有模型依赖性，精心设计的提示可以引导小型语言模型实现更环保的软件开发。

Abstract: There is a growing concern about the environmental impact of large language
models (LLMs) in software development, particularly due to their high energy
use and carbon footprint. Small Language Models (SLMs) offer a more sustainable
alternative, requiring fewer computational resources while remaining effective
for fundamental programming tasks. In this study, we investigate whether prompt
engineering can improve the energy efficiency of SLMs in code generation. We
evaluate four open-source SLMs, StableCode-Instruct-3B,
Qwen2.5-Coder-3B-Instruct, CodeLlama-7B-Instruct, and Phi-3-Mini-4K-Instruct,
across 150 Python problems from LeetCode, evenly distributed into easy, medium,
and hard categories. Each model is tested under four prompting strategies: role
prompting, zero-shot, few-shot, and chain-of-thought (CoT). For every generated
solution, we measure runtime, memory usage, and energy consumption, comparing
the results with a human-written baseline. Our findings show that CoT prompting
provides consistent energy savings for Qwen2.5-Coder and StableCode-3B, while
CodeLlama-7B and Phi-3-Mini-4K fail to outperform the baseline under any
prompting strategy. These results highlight that the benefits of prompting are
model-dependent and that carefully designed prompts can guide SLMs toward
greener software development.

</details>


### [11] [Development of Automated Software Design Document Review Methods Using Large Language Models](https://arxiv.org/abs/2509.09975)
*Takasaburo Fukuda,Takao Nakagawa,Keisuke Miyazaki,Susumu Tokumoto*

Main category: cs.SE

TL;DR: 使用LLM自动化软件设计文档评审过程的研究，通过分析11个评审视角并开发新技术，验证了LLM在识别设计文档不一致性方面的有效性


<details>
  <summary>Details</summary>
Motivation: 自动化软件设计文档评审过程，提高评审效率，利用LLM技术替代人工进行部分评审工作

Method: 分析设计文档评审方法并组织11个评审视角，开发新技术使LLM能够理解包含表格数据的复杂设计文档，使用GPT评估不同设计文档间设计项目和描述的一致性

Result: 实验证实LLM能够在评审过程中有效识别软件设计文档中的不一致性问题

Conclusion: 当前通用LLM可以替代人工完成部分设计文档评审工作，特别是在识别一致性问题上表现出良好效果

Abstract: In this study, we explored an approach to automate the review process of
software design documents by using LLM. We first analyzed the review methods of
design documents and organized 11 review perspectives. Additionally, we
analyzed the issues of utilizing LLMs for these 11 review perspectives and
determined which perspectives can be reviewed by current general-purpose LLMs
instead of humans. For the reviewable perspectives, we specifically developed
new techniques to enable LLMs to comprehend complex design documents that
include table data. For evaluation, we conducted experiments using GPT to
assess the consistency of design items and descriptions across different design
documents in the design process used in actual business operations. Our results
confirmed that LLMs can be utilized to identify inconsistencies in software
design documents during the review process.

</details>


### [12] [Sustaining Research Software: A Fitness Function Approach](https://arxiv.org/abs/2509.10085)
*Philipp Zech,Irdin Pekaric*

Main category: cs.SE

TL;DR: 本文提出使用进化架构中的适应度函数概念来提升研究软件的长期可持续性，通过自动化指标确保软件满足FAIR原则（可发现、可访问、可互操作、可重用）


<details>
  <summary>Details</summary>
Motivation: 研究软件通常面临可维护性差、缺乏适应性、最终过时等可持续性挑战，需要系统性的解决方案来确保其长期价值

Method: 定义针对研究软件FAIR原则的适应度函数集合，作为自动化持续评估指标，集成到开发生命周期中，促进模块化设计、完整文档、版本控制和与不断发展的技术生态系统的兼容性

Result: 案例研究和实验结果表明该方法能够有效增强研究软件的长期FAIR特性，弥合短期项目开发与持久科学影响之间的差距

Conclusion: 基于适应度函数的方法为研究软件可持续性提供了有效框架，通过在开发过程中嵌入自动化质量保障机制，能够培养研究社区的可持续文化

Abstract: The long-term sustainability of research software is a critical challenge, as
it usually suffers from poor maintainability, lack of adaptability, and
eventual obsolescence. This paper proposes a novel approach to addressing this
issue by leveraging the concept of fitness functions from evolutionary
architecture. Fitness functions are automated, continuously evaluated metrics
designed to ensure that software systems meet desired non-functional,
architectural qualities over time. We define a set of fitness functions
tailored to the unique requirements of research software, focusing on
findability, accessibility, interoperability and reusability (FAIR). These
fitness functions act as proactive safeguards, promoting practices such as
modular design, comprehensive documentation, version control, and compatibility
with evolving technological ecosystems. By integrating these metrics into the
development life cycle, we aim to foster a culture of sustainability within the
research community. Case studies and experimental results demonstrate the
potential of this approach to enhance the long-term FAIR of research software,
bridging the gap between ephemeral project-based development and enduring
scientific impact.

</details>


### [13] [Generating Energy-Efficient Code via Large-Language Models -- Where are we now?](https://arxiv.org/abs/2509.10099)
*Radu Apsan,Vincenzo Stoico,Michel Albonico,Rudra Dhar,Karthik Vaidhyanathan,Ivano Malavolta*

Main category: cs.SE

TL;DR: LLM生成的Python代码在能源效率上表现不一，在服务器上比人工代码低效16%，在树莓派上低效3%，但在PC上高效25%。绿色软件专家编写的代码在所有平台上都比LLM代码节能17-30%。


<details>
  <summary>Details</summary>
Motivation: 评估LLM生成的Python代码与人工编写代码在能源效率方面的差异，特别是在不同硬件平台上的表现，为绿色软件开发提供实证依据。

Method: 使用6个主流LLM和4种提示技术生成363个解决方案，针对EvoEval基准中的9个编程问题，在服务器、PC和树莓派三种硬件平台上测量能源消耗，总测试时间约881小时。

Result: 人工解决方案在服务器上节能16%，在树莓派上节能3%；LLM在PC上比人工开发者节能25%。提示技术对节能效果不一致，最节能的提示因硬件平台而异。绿色软件专家的代码在所有平台上都比所有LLM节能至少17-30%。

Conclusion: 尽管LLM展现出相对良好的代码生成能力，但没有一个LLM生成的代码比经验丰富的绿色软件开发者更节能，表明目前开发节能Python代码仍然需要人类专业知识。

Abstract: Context. The rise of Large Language Models (LLMs) has led to their widespread
adoption in development pipelines. Goal. We empirically assess the energy
efficiency of Python code generated by LLMs against human-written code and code
developed by a Green software expert. Method. We test 363 solutions to 9 coding
problems from the EvoEval benchmark using 6 widespread LLMs with 4 prompting
techniques, and comparing them to human-developed solutions. Energy consumption
is measured on three different hardware platforms: a server, a PC, and a
Raspberry Pi for a total of ~881h (36.7 days). Results. Human solutions are 16%
more energy-efficient on the server and 3% on the Raspberry Pi, while LLMs
outperform human developers by 25% on the PC. Prompting does not consistently
lead to energy savings, where the most energy-efficient prompts vary by
hardware platform. The code developed by a Green software expert is
consistently more energy-efficient by at least 17% to 30% against all LLMs on
all hardware platforms. Conclusions. Even though LLMs exhibit relatively good
code generation capabilities, no LLM-generated code was more energy-efficient
than that of an experienced Green software developer, suggesting that as of
today there is still a great need of human expertise for developing
energy-efficient Python code.

</details>


### [14] [Targeted Test Selection Approach in Continuous Integration](https://arxiv.org/abs/2509.10279)
*Pavel Plyusnin,Aleksey Antonov,Vasilii Ermakov,Aleksandr Khaybriev,Margarita Kikot,Ilseyar Alimova,Stanislav Moiseev*

Main category: cs.SE

TL;DR: T-TS是一种基于机器学习的工业测试选择方法，通过将代码提交表示为变更文件的词袋，结合跨文件特征，无需覆盖率映射，可减少85%的测试用例，执行时间降低5.9倍，检测超过95%的测试失败。


<details>
  <summary>Details</summary>
Motivation: 随着代码库扩展和测试套件增长，在高频率代码提交环境下，高效管理测试过程变得日益困难，需要更智能的测试选择方法。

Method: 提出Targeted Test Selection (T-TS)机器学习方法，使用变更文件的词袋表示提交，整合跨文件和其他预测特征，避免使用覆盖率映射。

Result: 在生产环境中部署后，T-TS仅选择15%的测试用例，执行时间减少5.9倍，流水线加速5.6倍，检测超过95%的测试失败。

Conclusion: T-TS在工业环境中表现出色，提供了高效的测试选择解决方案，实现已公开可用以支持进一步研究和实际应用。

Abstract: In modern software development change-based testing plays a crucial role.
However, as codebases expand and test suites grow, efficiently managing the
testing process becomes increasingly challenging, especially given the high
frequency of daily code commits. We propose Targeted Test Selection (T-TS), a
machine learning approach for industrial test selection. Our key innovation is
a data representation that represent commits as Bags-of-Words of changed files,
incorporates cross-file and additional predictive features, and notably avoids
the use of coverage maps. Deployed in production, T-TS was comprehensively
evaluated against industry standards and recent methods using both internal and
public datasets, measuring time efficiency and fault detection. On live
industrial data, T-TS selects only 15% of tests, reduces execution time by
$5.9\times$, accelerates the pipeline by $5.6\times$, and detects over 95% of
test failures. The implementation is publicly available to support further
research and practical adoption.

</details>


### [15] [Developer-LLM Conversations: An Empirical Study of Interactions and Generated Code Quality](https://arxiv.org/abs/2509.10402)
*Suzhen Zhong,Ying Zou,Bram Adams*

Main category: cs.SE

TL;DR: 基于82,845个真实开发者与LLM对话的分析显示，LLM响应长度是开发者提示的14倍，多轮对话占68%，Python和JavaScript代码存在大量未定义变量问题，Java缺少注释，C++缺少头文件，C#存在命名空间问题，但通过多轮对话可以改善文档质量和导入处理。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在软件开发中广泛应用，但开发者与LLM的实际交互方式、对话动态如何影响任务结果和代码质量的理解仍然有限。

Method: 利用CodeChat数据集（包含82,845个真实开发者-LLM对话，368,506个代码片段，覆盖20+编程语言），分析对话模式、代码质量问题，并在Python、JavaScript、C++、Java、C#五种语言中进行评估。

Result: 发现LLM响应显著长于开发者提示（中位数14:1），多轮对话占68%。各语言存在特定问题：Python和JavaScript未定义变量（83.4%/75.3%），Java缺少注释（75.9%），C++缺少头文件（41.1%），C#命名空间问题（49.2%）。多轮对话可改善Java文档质量（提升14.7%）和Python导入处理（改善3.7%）。

Conclusion: 明确指出先前代码错误并请求修复的提示最有效。多轮对话有助于改善代码质量，但语法和导入错误在多轮中持续存在，需要开发者积极参与纠错过程。

Abstract: Large Language Models (LLMs) are becoming integral to modern software
development workflows, assisting developers with code generation, API
explanation, and iterative problem-solving through natural language
conversations. Despite widespread adoption, there is limited understanding of
how developers interact with LLMs in practice and how these conversational
dynamics influence task outcomes, code quality, and software engineering
workflows. To address this, we leverage CodeChat, a large dataset comprising
82,845 real-world developer-LLM conversations, containing 368,506 code snippets
generated across over 20 programming languages, derived from the WildChat
dataset. We find that LLM responses are substantially longer than developer
prompts, with a median token-length ratio of 14:1. Multi-turn conversations
account for 68% of the dataset and often evolve due to shifting requirements,
incomplete prompts, or clarification requests. Topic analysis identifies web
design (9.6% of conversations) and neural network training (8.7% of
conversations) as the most frequent LLM-assisted tasks. Evaluation across five
languages (i.e., Python, JavaScript, C++, Java, and C#) reveals prevalent and
language-specific issues in LLM-generated code: generated Python and JavaScript
code often include undefined variables (83.4% and 75.3% of code snippets,
respectively); Java code lacks required comments (75.9%); C++ code frequently
omits headers (41.1%) and C# code shows unresolved namespaces (49.2%). During a
conversation, syntax and import errors persist across turns; however,
documentation quality in Java improves by up to 14.7%, and import handling in
Python improves by 3.7% over 5 turns. Prompts that point out mistakes in code
generated in prior turns and explicitly request a fix are most effective for
resolving errors.

</details>
