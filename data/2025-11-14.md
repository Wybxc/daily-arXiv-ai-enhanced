<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 5]
- [cs.SE](#cs.SE) [Total: 6]
- [cs.LO](#cs.LO) [Total: 3]
- [cs.FL](#cs.FL) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Cyclotron: Compilation of Recurrences to Distributed and Systolic Architectures](https://arxiv.org/abs/2511.09987)
*Shiv Sundram,Akhilesh Balasingam,Nathan Zhang,Kunle Olukotun,Fredrik Kjolstad*

Main category: cs.PL

TL;DR: Cyclotron是一个用于表达流式数据流算法的框架和编译器，使用递归方程来定义算法，然后可移植地编译到分布式处理器拓扑结构。


<details>
  <summary>Details</summary>
Motivation: 为了解决在分布式系统中高效执行流式数据流算法的需求，提供一种可移植的编译方法，将逻辑张量上的递归方程映射到实际的分布式硬件拓扑。

Method: 提供基于逻辑张量递归的输入语言，通过中间语言逐步降低到针对单个处理器的发送、接收和计算操作。优化IR使外部内存交互仅限于迭代空间边界，内部迭代空间中的数据访问变为本地化。

Result: 展示了框架的可移植性，能够编译到可重构的脉动阵列模拟器、芯片级分布式硬件以及分布式内存CPU集群。在分布式CPU设置中，生成的矩阵乘法和三角求解实现与ScaLAPACK竞争。

Conclusion: Cyclotron框架成功实现了使用递归方程表达流式数据流算法，并能够高效编译到多种分布式硬件平台，在性能上与传统库竞争。

Abstract: We present Cyclotron, a framework and compiler for using recurrence equations to express streaming dataflow algorithms, which then get portably compiled to distributed topologies of interlinked processors. Our framework provides an input language of recurrences over logical tensors, which then gets lowered into an intermediate language of recurrences over logical iteration spaces, and finally into programs of send, receive, and computation operations specific to each individual processor. In Cyclotron's IR, programs are optimized such that external memory interactions are confined to the boundaries of the iteration space. Within inner iteration spaces, all data accesses become local: data accesses target values residing in local fast memory or on neighboring processing units, avoiding costly memory movement. We provide a scheduling language allowing users to define how data gets streamed and broadcasted between processors, enabling pipelined execution of computation kernels over distributed topologies of processing elements. We demonstrate the portability of our approach by compiling our IR to a reconfigurable simulator of systolic arrays and chiplet style distributed hardware, as well as to distributed-memory CPU clusters. In the simulated reconfigurable setting, we use our compiler for hardware design space exploration in which link costs and latencies can be specified. In the distributed CPU setting, we show how to use recurrences and our scheduling language to express various matrix multiplication routines (Cannon, SUMMA, PUMMA, weight stationary) and solvers (Triangular solve and Cholesky). For matrix multiplication and the triangular solve, we generate distributed implementations competitive with ScaLAPACK.

</details>


### [2] [Omnidirectional type inference for ML: principality any way](https://arxiv.org/abs/2511.10343)
*Alistair O'Brien,Didier Rémy,Gabriel Scherer*

Main category: cs.PL

TL;DR: 提出了一种称为"全向类型推断"的新方法，通过动态信息流和挂起匹配约束来解决ML类型系统扩展中的主要性丧失问题，相比静态双向类型推断更具表达力。


<details>
  <summary>Details</summary>
Motivation: ML类型系统的成功归功于主要性（每个良类型表达式都有唯一最一般类型），但许多扩展（如GADTs、高阶多态、静态重载）通过引入脆弱构造破坏了主要性。现有方法使用固定顺序的推断算法，但静态推断顺序的刚性经常导致良类型程序被拒绝。

Method: 采用全向类型推断，类型信息以动态顺序流动。使用挂起匹配约束，在需要已知类型信息时暂停约束求解，在信息可用时恢复。引入增量实例化机制，允许包含挂起约束的部分解决类型方案被实例化，并在方案细化时增量更新实例。

Result: 在OCaml的两个根本不同特性上验证了该方法的通用性：记录标签和数据构造器的静态重载，以及半显式一等多态。在这两种情况下，都获得了比OCaml当前类型检查器更具表达力的主要类型推断算法。

Conclusion: 全向性为在存在脆弱特性时恢复主要性提供了一个通用框架，能够处理ML扩展中的类型推断问题，同时保持主要性和更好的表达能力。

Abstract: The Damas-Hindley-Milner (ML) type system owes its success to principality, the property that every well-typed expression has a unique most general type. This makes inference predictable and efficient. Unfortunately, many extensions of ML (GADTs, higher-rank polymorphism, and static overloading) endanger princpality by introducing _fragile_ constructs that resist principal inference. Existing approaches recover principality through directional inference algorithms, which propagate _known_ type information in a fixed (or static) order (e.g. as in bidirectional typing) to disambiguate such constructs. However, the rigidity of a static inference order often causes otherwise well-typed programs to be rejected.
  We propose _omnidirectional_ type inference, where type information flows in a dynamic order. Typing constraints may be solved in any order, suspending when progress requires known type information and resuming once it becomes available, using _suspended match constraints_. This approach is straightforward for simply typed systems, but extending it to ML is challenging due to let-generalization. Existing ML inference algorithms type let-bindings (let x = e1 in e2) in a fixed order: type e1, generalize its type, and then type e2. To overcome this, we introduce _incremental instantiation_, allowing partially solved type schemes containing suspended constraints to be instantiated, with a mechanism to incrementally update instances as the scheme is refined.
  Omnidirectionality provides a general framework for restoring principality in the presence of fragile features. We demonstrate its versatility on two fundamentally different features of OCaml: static overloading of record labels and datatype constructors and semi-explicit first-class polymorphism. In both cases, we obtain a principal type inference algorithm that is more expressive than OCaml's current typechecker.

</details>


### [3] [Lazy Linearity for a Core Functional Language](https://arxiv.org/abs/2511.10361)
*Rodrigo Mesquita,Bernardo Toninho*

Main category: cs.PL

TL;DR: 提出了Linear Core系统，在惰性求值语义下重新定义线性类型，解决了Haskell优化编译器中语法线性性被破坏但语义保持的问题。


<details>
  <summary>Details</summary>
Motivation: 传统线性类型系统中，语法出现等同于资源消耗，但在惰性求值环境下，语法出现不一定意味着实际资源使用。Haskell优化编译器会重写程序，破坏语法线性性但保持语义，这需要新的线性类型系统。

Method: 设计Linear Core系统，将线性性建立在惰性语义基础上，适用于GHC的Core中间语言。证明系统正确性，确保线性资源使用，并验证优化转换在Linear Core中保持线性性。

Result: 实现了Linear Core作为编译器插件，验证了系统在线性性重的库（如linear-base）中的有效性。

Conclusion: Linear Core成功解决了惰性语言中线性类型与优化编译的冲突，为Haskell等语言提供了语义正确的线性类型系统。

Abstract: Traditionally, in linearly typed languages, consuming a linear resource is synonymous with its syntactic occurrence in the program. However, under the lens of non-strict evaluation, linearity can be further understood semantically, where a syntactic occurrence of a resource does not necessarily entail using that resource when the program is executed. While this distinction has been largely unexplored, it turns out to be inescapable in Haskell's optimising compiler, which heavily rewrites the source program in ways that break syntactic linearity but preserve the program's semantics. We introduce Linear Core, a novel system which accepts the lazy semantics of linearity statically and is suitable for lazy languages such as the Core intermediate language of the Glasgow Haskell Compiler. We prove that Linear Core is sound, guaranteeing linear resource usage, and that multiple optimising transformations preserve linearity in Linear Core while failing to do so in Core. We have implemented Linear Core as a compiler plugin to validate the system against linearity-heavy libraries, including linear-base.

</details>


### [4] [Modeling Layout Abstractions Using Integer Set Relations](https://arxiv.org/abs/2511.10374)
*Somashekaracharya G Bhaskaracharya,Aravind Acharya,Bastian Hagedorn,Vinod Grover*

Main category: cs.PL

TL;DR: 本文提出使用整数集合库(ISL)为CuTe和Triton线性布局系统创建统一的数学表示，通过整数集合关系实现形式化分析、正确性验证和跨系统优化基础。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习编译器依赖布局抽象来管理逻辑张量结构与物理内存安排之间的复杂映射。CuTe和Triton线性布局是广泛采用的行业标准，但它们的数学基础不同，无法进行统一的形式化分析和跨系统推理。

Method: 使用ISL通过整数集合关系为两种布局系统创建统一数学表示：CuTe布局通过基于步长的计算建模从多维坐标到线性索引的转换，包括复杂的swizzle操作；Triton线性布局建模二进制向量空间变换，其中算术运算遵循有限域F_2规则。

Result: 实验评估表明，该系统能够处理从基本恒等变换到具有复杂步长配置和swizzle模式的复杂多维张量安排的完整布局复杂性谱系，验证了跨不同布局范式的数学建模方法。

Conclusion: 该方法成功地为CuTe和Triton布局系统建立了统一的数学基础，实现了严格的形式化分析、正确性验证，并为未来的跨系统优化策略奠定了基础。

Abstract: Modern deep learning compilers rely on layout abstractions to manage the complex mapping between logical tensor structures and physical memory arrangements. CuTe layouts and Triton linear layouts are widely adopted industry standards. However, these layout systems operate independently with distinct mathematical underpinnings, preventing unified formal analysis and cross-system reasoning. We bridge this gap by introducing a novel approach that leverages the Integer Set Library (ISL) to create a unified mathematical representation for both layout systems through integer set relations, thereby enabling rigorous formal analysis, correctness verification, and the foundation for future cross-system optimization strategies. Our approach models CuTe layouts through integer set relations that encode the transformation from multi-dimensional coordinates to linear indices using stride-based calculations, including sophisticated swizzle operations that perform bit-level manipulations for enhanced memory access patterns. For Triton linear layouts, we construct integer set relations that model the binary vector space transformations where arithmetic operations follow finite field F_2 rules. We implement a complete suite of layout manipulation algorithms for composition, inversion, complement using built-in operations in ISL to ensure mathematical correctness and preserve layout semantics. Experimental evaluation shows that the system handles the full spectrum of layout complexity, from elementary identity transformations to sophisticated multi-dimensional tensor arrangements with complex stride configurations and swizzle patterns, validating the mathematical modeling approach across different layout paradigms.

</details>


### [5] [zkStruDul: Programming zkSNARKs with Structural Duality](https://arxiv.org/abs/2511.10565)
*Rahul Krishnan,Ashley Samuelson,Emily Yao,Ethan Cecchetti*

Main category: cs.PL

TL;DR: zkStruDul是一个统一输入转换和谓词定义的语言，通过单一抽象消除重复代码和安全风险，支持递归证明等特性


<details>
  <summary>Details</summary>
Motivation: 现有NIZK工具将谓词定义和输入转换分开实现，导致逻辑重复和潜在的安全漏洞，需要统一的方法来避免这些问题

Method: 开发zkStruDul语言，将输入转换和谓词定义统一为单一抽象，编译器可从中投影出两个过程，消除重复代码

Result: 提供了源级语义并证明其与投影语义行为一致，支持递归证明等重要特性

Conclusion: zkStruDul通过统一抽象解决了NIZK应用中重复逻辑和安全不匹配的问题，为现有NIZK技术提供了高层抽象

Abstract: Non-Interactive Zero Knowledge (NIZK) proofs, such as zkSNARKS, let one prove knowledge of private data without revealing it or interacting with a verifier. While existing tooling focuses on specifying the predicate to be proven, real-world applications optimize predicate definitions to minimize proof generation overhead, but must correspondingly transform predicate inputs. Implementing these two steps separately duplicates logic that must precisely match to avoid catastrophic security flaws. We address this shortcoming with zkStruDul, a language that unifies input transformations and predicate definitions into a single combined abstraction from which a compiler can project both procedures, eliminating duplicate code and problematic mismatches. zkStruDul provides a high-level abstraction to layer on top of existing NIZK technology and supports important features like recursive proofs. We provide a source-level semantics and prove its behavior is identical to the projected semantics, allowing straightforward standard reasoning.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [6] [Evaluating Software Process Models for Multi-Agent Class-Level Code Generation](https://arxiv.org/abs/2511.09794)
*Wasique Islam Shafin,Md Nakhla Rafi,Zhenhao Li,Tse-Hsun Chen*

Main category: cs.SE

TL;DR: 多智能体LLM工作流通过瀑布式开发流程（需求、设计、实现、测试）重组而非增强模型性能，在代码可维护性和功能性之间产生权衡。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单智能体的函数级代码生成，缺乏对多智能体工作流中流程结构和角色专业化如何影响类级代码生成的系统研究。

Method: 使用三个LLM（GPT-4o-mini、DeepSeek-Chat、Claude-3.5-Haiku）在ClassEval基准的100个Python任务上模拟瀑布式开发周期。

Result: 瀑布式协作产生更清晰可维护的代码，但通常降低功能正确性（GPT-4o-mini -37.8%，DeepSeek-Chat -39.8%），Claude-3.5-Haiku是例外（+9.5%）。流程约束改变失败特征：结构问题减少，但语义和验证错误更频繁。

Conclusion: 软件流程结构从根本上改变LLM的推理、协作和失败方式，揭示了多智能体代码生成中严格工作流纪律与灵活问题解决之间的内在权衡。

Abstract: Modern software systems require code that is not only functional but also maintainable and well-structured. Although Large Language Models (LLMs) are increasingly used to automate software development, most studies focus on isolated, single-agent function-level generation. This work examines how process structure and role specialization shape multi-agent LLM workflows for class-level code generation. We simulate a Waterfall-style development cycle covering Requirement, Design, Implementation, and Testing using three LLMs (GPT-4o-mini, DeepSeek-Chat, and Claude-3.5-Haiku) on 100 Python tasks from the ClassEval benchmark. Our findings show that multi-agent workflows reorganize, rather than consistently enhance, model performance. Waterfall-style collaboration produces cleaner and more maintainable code but often reduces functional correctness (-37.8\% for GPT-4o-mini and -39.8\% for DeepSeek-Chat), with Claude-3.5-Haiku as a notable exception (+9.5\%). Importantly, process constraints shift failure characteristics: structural issues such as missing code decrease, while semantic and validation errors become more frequent. Among all stages, Testing exerts the strongest influence by improving verification coverage but also introducing new reasoning failures, whereas Requirement and Design have comparatively modest effects. Overall, this study provides empirical evidence that software process structure fundamentally alters how LLMs reason, collaborate, and fail, revealing inherent trade-offs between rigid workflow discipline and flexible problem-solving in multi-agent code generation.

</details>


### [7] [EnvTrace: Simulation-Based Semantic Evaluation of LLM Code via Execution Trace Alignment -- Demonstrated at Synchrotron Beamlines](https://arxiv.org/abs/2511.09964)
*Noah van der Vleuten,Anthony Flores,Shray Mathur,Max Rakitin,Thomas Hopkins,Kevin G. Yager,Esther H. R. Tsai*

Main category: cs.SE

TL;DR: EnvTrace是一种基于仿真的方法，通过评估执行轨迹来评估语义代码等价性，用于评估大语言模型在仪器控制方面的能力。该方法使用束线控制逻辑数字孪生进行演示，评估了30多个LLM，显示许多顶级模型在快速控制代码生成方面接近人类水平。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在仪器控制方面的能力需要超越标准无状态算法基准的方法，因为物理系统的行为不能仅通过单元测试来完全捕捉。

Method: 引入EnvTrace方法，使用基于仿真的执行轨迹评估来评估语义代码等价性，通过束线控制逻辑数字孪生进行演示，并使用轨迹对齐生成多维度功能正确性评分。

Result: 评估了30多个LLM，显示许多顶级模型在快速控制代码生成方面能够接近人类水平的性能。

Conclusion: 这是实现LLM和数字孪生共生协作愿景的第一步：LLM提供直观控制和智能编排，数字孪生提供安全高保真环境，为自主具身AI铺平道路。

Abstract: Evaluating large language models (LLMs) for instrument control requires methods that go beyond standard, stateless algorithmic benchmarks, since the behavior of physical systems cannot be fully captured by unit tests alone. Here we introduce EnvTrace, a simulation-based method that evaluates execution traces to assess semantic code equivalence. EnvTrace is demonstrated with a beamline control-logic digital twin to facilitate the evaluation of instrument control code, with the digital twin itself also enabling the pre-execution validation of live experiments. Over 30 LLMs were evaluated using trace alignment to generate a multi-faceted score for functional correctness across key behavioral dimensions, showing that many top-tier models can approach human-level performance in rapid control-code generation. This is a first step toward a broader vision where LLMs and digital twins work symbiotically: LLMs providing intuitive control and agentic orchestration, and digital twins offering safe and high-fidelity environments, paving the way towards autonomous embodied AI.

</details>


### [8] [Continuous Benchmark Generation for Evaluating Enterprise-scale LLM Agents](https://arxiv.org/abs/2511.10049)
*Divyanshu Saxena,Rishikesh Maurya,Xiaoxuan Ou,Gagan Somashekar,Shachee Mishra Gupta,Arun Iyer,Yu Kang,Chetan Bansal,Aditya Akella,Saravan Rajmohan*

Main category: cs.SE

TL;DR: 提出了一种用于评估演进AI代理的基准生成方法，通过半结构化文档和LLM生成基准，支持企业级代理的持续评估和改进。


<details>
  <summary>Details</summary>
Motivation: 传统固定基准无法满足企业级AI代理的需求，因为企业服务和要求持续演进，且真实案例稀疏，需要动态的评估方法。

Method: 使用半结构化文档表达高层意图，利用最先进的LLM从少量文档生成基准，建立可维护的评估框架。

Result: 在大型公共企业的服务迁移案例研究中成功应用该方法，实现了对演进AI代理的稳健评估。

Conclusion: 该方法能够生成可维护的评估框架，为AI代理性能提供快速反馈并促进针对性改进。

Abstract: The rapid adoption of AI agents across domains has made systematic evaluation crucial for ensuring their usefulness and successful production deployment. Evaluation of AI agents typically involves using a fixed set of benchmarks and computing multiple evaluation metrics for the agent. While sufficient for simple coding tasks, these benchmarks fall short for enterprise-scale agents, where services and requirements evolve continuously and ground-truth examples are sparse. We propose a process of benchmark generation that helps evolve the benchmarks as the requirements change and perform robust evaluation of evolving AI agents. We instantiate this approach for a case study of service migration from one deployment platform to another at a large public enterprise. Our approach relies on semi-structured documents where developers express the high-level intent, and uses state-of-the-art LLMs to generate benchmarks from just a small number of such documents. Overall, this process results in a maintainable evaluation framework, enabling rapid feedback on agent performance and facilitating targeted improvements.

</details>


### [9] [Quality Assurance of LLM-generated Code: Addressing Non-Functional Quality Characteristics](https://arxiv.org/abs/2511.10271)
*Xin Sun,Daniel Ståhl,Kristian Sandahl,Christoph Kessler*

Main category: cs.SE

TL;DR: 该研究系统评估了LLM生成代码的非功能性质量，发现学术关注点与行业需求存在不匹配，LLM生成的补丁在安全性、可维护性和性能效率方面存在权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LLM生成代码的功能正确性，但缺乏对其非功能性质量的系统理解和评估，而行业实践表明这些质量属性对软件长期维护至关重要。

Method: 采用三种互补研究方法：对108篇论文的系统综述、与行业从业者的两次研讨会、使用三个LLM对真实世界软件问题进行补丁生成的实证分析。

Result: 学术研究主要关注安全性和性能效率，而行业更重视可维护性和可读性；LLM生成的补丁在改进一个质量维度时往往以牺牲其他维度为代价；不同模型和优化策略在运行时和内存方面表现出高方差。

Conclusion: 需要在LLM代码生成管道中集成质量保证机制，确保生成的代码不仅通过测试，而且真正具有质量，以缓解学术关注点、行业优先级和模型性能之间的不匹配问题。

Abstract: In recent years, LLMs have been widely integrated into software engineering workflows, supporting tasks like code generation. However, while these models often generate functionally correct outputs, we still lack a systematic understanding and evaluation of their non-functional qualities. Existing studies focus mainly on whether generated code passes the tests rather than whether it passes with quality. Guided by the ISO/IEC 25010 quality model, this study conducted three complementary investigations: a systematic review of 108 papers, two industry workshops with practitioners from multiple organizations, and an empirical analysis of patching real-world software issues using three LLMs. Motivated by insights from both the literature and practitioners, the empirical study examined the quality of generated patches on security, maintainability, and performance efficiency. Across the literature, we found that security and performance efficiency dominate academic attention, while maintainability and other qualities are understudied. In contrast, industry experts prioritize maintainability and readability, warning that generated code may accelerate the accumulation of technical debt. In our evaluation of functionally correct patches generated by three LLMs, improvements in one quality dimension often come at the cost of others. Runtime and memory results further show high variance across models and optimization strategies. Overall, our findings reveal a mismatch between academic focus, industry priorities, and model performance, highlighting the urgent need to integrate quality assurance mechanisms into LLM code generation pipelines to ensure that future generated code not only passes tests but truly passes with quality.

</details>


### [10] [A Large-Scale Collection Of (Non-)Actionable Static Code Analysis Reports](https://arxiv.org/abs/2511.10323)
*Dávid Kószó,Tamás Aladics,Rudolf Ferenc,Péter Hegedűs*

Main category: cs.SE

TL;DR: 提出了NASCAR数据集，包含超过100万条Java静态代码分析警告，用于区分可操作和不可操作的警告，以解决警告疲劳问题。


<details>
  <summary>Details</summary>
Motivation: 静态代码分析工具会产生大量警告，其中许多是不可操作的，导致开发者出现"警告疲劳"，可能忽略关键问题，影响生产力和代码质量。目前缺乏大规模数据集来训练机器学习模型改进SCA工具。

Method: 提出了一种新颖的方法来收集和分类SCA警告，有效区分可操作和不可操作的警告，并基于此方法生成了大规模数据集。

Result: 生成了包含超过100万条Java源代码警告的NASCAR数据集，并将数据集和生成工具公开提供。

Conclusion: 该研究填补了Java静态代码分析警告数据集的空白，为后续研究提供了重要资源，有助于改进SCA工具的准确性和可用性，减轻警告疲劳的影响。

Abstract: Static Code Analysis (SCA) tools, while invaluable for identifying potential coding problems, functional bugs, or vulnerabilities, often generate an overwhelming number of warnings, many of which are non-actionable. This overload of alerts leads to ``alert fatigue'', a phenomenon where developers become desensitized to warnings, potentially overlooking critical issues and ultimately hindering productivity and code quality. Analyzing these warnings and training machine learning models to identify and filter them requires substantial datasets, which are currently scarce, particularly for Java. This scarcity impedes efforts to improve the accuracy and usability of SCA tools and mitigate the effects of alert fatigue. In this paper, we address this gap by introducing a novel methodology for collecting and categorizing SCA warnings, effectively distinguishing actionable from non-actionable ones. We further leverage this methodology to generate a large-scale dataset of over 1 million entries of Java source code warnings, named NASCAR: (Non-)Actionable Static Code Analysis Reports. To facilitate follow-up research in this domain, we make both the dataset and the tools used to generate it publicly available.

</details>


### [11] [Towards Comprehensive Sampling of SMT Solutions](https://arxiv.org/abs/2511.10326)
*Shuangyu Lyu,Chuan Luo,Ruizhi Shi,Wei Wu,Chanjuan Liu,Chunming Hu*

Main category: cs.SE

TL;DR: PanSampler是一种新颖的SMT采样器，通过多样性感知算法、AST引导评分函数和后采样优化技术，用少量解实现高覆盖率，显著提升软件测试和硬件验证效率。


<details>
  <summary>Details</summary>
Motivation: 在软件和硬件测试中，为SMT公式生成多样化解对于发现故障和检测安全违规至关重要。传统方法需要大量解才能达到高覆盖率，导致测试时间和资源消耗增加，影响效率。

Method: PanSampler结合三种新技术：多样性感知SMT算法、抽象语法树引导评分函数和后采样优化技术，通过迭代采样、候选解评估和局部搜索来精化解，确保用少量样本实现高覆盖率。

Result: 实验表明PanSampler在达到高目标覆盖率方面能力显著更强，且比现有采样器需要更少的解就能达到相同覆盖率。在实际软件系统测试中，PanSampler将所需测试用例数量减少了32.6%到76.4%，同时达到相同的故障检测效果。

Conclusion: PanSampler推进了SMT采样技术，显著降低了软件测试和硬件验证的成本，提高了测试效率。

Abstract: This work focuses on effectively generating diverse solutions for satisfiability modulo theories (SMT) formulas, targeting the theories of bit-vectors, arrays, and uninterpreted functions, which is a critical task in software and hardware testing. Generating diverse SMT solutions helps uncover faults and detect safety violations during the verification and testing process, resulting in the SMT sampling problem, i.e., constructing a small number of solutions while achieving comprehensive coverage of the constraint space. While high coverage is crucial for exploring system behaviors, reducing the number of solutions is of great importance, as excessive solutions increase testing time and resource usage, undermining efficiency. In this work, we introduce PanSampler, a novel SMT sampler that achieves high coverage with a small number of solutions. It incorporates three novel techniques, i.e., diversity-aware SMT algorithm, abstract syntax tree (AST)-guided scoring function and post-sampling optimization technology, enhancing its practical performance. It iteratively samples solutions, evaluates candidates, and employs local search to refine solutions, ensuring high coverage with a small number of samples. Extensive experiments on practical benchmarks demonstrate that PanSampler exhibits a significantly stronger capability to reach high target coverage, while requiring fewer solutions than current samplers to achieve the same coverage level. Furthermore, our empirical evaluation on practical subjects, which are collected from real-world software systems, shows that PanSampler achieves higher fault detection capability and reduces the number of required test cases from 32.6\% to 76.4\% to reach the same fault detection effectiveness, leading to a substantial improvement in testing efficiency. PanSampler advances SMT sampling, reducing the cost of software testing and hardware verification.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [12] [Contextual Refinement of Higher-Order Concurrent Probabilistic Programs](https://arxiv.org/abs/2511.10135)
*Kwing Hei Li,Alejandro Aguirre,Joseph Tassarotti,Lars Birkedal*

Main category: cs.LO

TL;DR: Foxtrot是第一个用于证明高阶并发概率程序上下文精化的高阶分离逻辑，结合了并发分离逻辑和概率推理原则，并在Rocq证明助手和Iris框架中实现。


<details>
  <summary>Details</summary>
Motivation: 需要开发一个能够处理高阶并发概率程序上下文精化的逻辑框架，以解决概率和并发组合带来的复杂性。

Method: 继承并发分离逻辑的推理原则（如不变量和幽灵资源），并支持高级概率推理原则（如磁带预采样和误差放大归纳），在Iris逻辑中依赖选择公理变体。

Result: 成功构建了Foxtrot逻辑，并在多个示例中展示了其表达能力，包括对抗性冯·诺依曼硬币和Sodium密码库的randombytes_uniform函数。

Conclusion: Foxtrot逻辑有效整合了并发和概率推理原则，为高阶并发概率程序的验证提供了强大的理论基础和实用工具。

Abstract: We present Foxtrot, the first higher-order separation logic for proving contextual refinement of higher-order concurrent probabilistic programs with higher-order local state. From a high level, Foxtrot inherits various concurrency reasoning principles from standard concurrent separation logic, e.g. invariants and ghost resources, and supports advanced probabilistic reasoning principles for reasoning about complex probability distributions induced by concurrent threads, e.g. tape presampling and induction by error amplification. The integration of these strong reasoning principles is highly non-trivial due to the combination of probability and concurrency in the language and the complexity of the Foxtrot model; the soundness of the logic relies on a version of the axiom of choice within the Iris logic, which is not used in earlier work on Iris-based logics. We demonstrate the expressiveness of Foxtrot on a wide range of examples, including the adversarial von Neumann coin and the $\mathsf{randombytes\_uniform}$ function of the Sodium cryptography software library.
  All results have been mechanized in the Rocq proof assistant and the Iris separation logic framework.

</details>


### [13] [Quantum modal logic](https://arxiv.org/abs/2511.10188)
*Kenji Tokuo*

Main category: cs.LO

TL;DR: 本文基于量子逻辑构建了一个模态逻辑框架，包括关系语义学和序列演算，并证明了其可靠性和完备性，为量子逻辑上的各种模态逻辑提供了基础。


<details>
  <summary>Details</summary>
Motivation: 为量子逻辑上的各种模态逻辑（如量子真势逻辑、量子时态逻辑、量子认知逻辑和量子动态逻辑）提供一个统一的形式化基础框架。

Method: 构建了基于量子逻辑的模态逻辑系统，包括关系语义学和序列演算两种形式化方法。

Result: 证明了该模态逻辑系统的可靠性和完备性定理，即语义和语法之间的等价关系。

Conclusion: 成功建立了一个基于量子逻辑的模态逻辑框架，为后续量子逻辑上的各种模态逻辑研究奠定了理论基础。

Abstract: A modal logic based on quantum logic is formalized in its simplest possible form. Specifically, a relational semantics and a sequent calculus are provided, and the soundness and the completeness theorems connecting both notions are demonstrated. This framework is intended to serve as a basis for formalizing various modal logics over quantum logic, such as quantum alethic logic, quantum temporal logic, quantum epistemic logic, and quantum dynamic logic.

</details>


### [14] [Certified Branch-and-Bound MaxSAT Solving (Extended Version)](https://arxiv.org/abs/2511.10273)
*Dieter Vandesande,Jordi Coll,Bart Bogaerts*

Main category: cs.LO

TL;DR: 本文展示了如何在分支定界MaxSAT求解器中实现证明记录，包括前瞻方法和基于多值决策图的伪布尔约束编码，实验证明该方法可行但证明检查仍有挑战


<details>
  <summary>Details</summary>
Motivation: 现代组合求解器虽然性能显著提升，但在关键应用中需要确保输出正确性。MaxSAT作为SAT的优化变体，其证明记录技术尚未取得突破性进展

Method: 在主导的分支定界求解器MaxCDCL中实现证明记录，包括前瞻方法和基于多值决策图的伪布尔约束高级子句编码

Result: 实验证明证明记录的开销有限，是可行的，但证明检查仍然是一个挑战

Conclusion: 成功实现了分支定界MaxSAT求解器的证明记录，为验证求解器输出正确性提供了有效方法

Abstract: Over the past few decades, combinatorial solvers have seen remarkable performance improvements, enabling their practical use in real-world applications. In some of these applications, ensuring the correctness of the solver's output is critical. However, the complexity of modern solvers makes them susceptible to bugs in their source code. In the domain of satisfiability checking (SAT), this issue has been addressed through proof logging, where the solver generates a formal proof of the correctness of its answer. For more expressive problems like MaxSAT, the optimization variant of SAT, proof logging had not seen a comparable breakthrough until recently.
  In this paper, we show how to achieve proof logging for state-of-the-art techniques in Branch-and-Bound MaxSAT solving. This includes certifying look-ahead methods used in such algorithms as well as advanced clausal encodings of pseudo-Boolean constraints based on so-called Multi-Valued Decision Diagrams (MDDs). We implement these ideas in MaxCDCL, the dominant branch-and-bound solver, and experimentally demonstrate that proof logging is feasible with limited overhead, while proof checking remains a challenge.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [15] [Spectral and combinatorial methods for efficiently computing the rank of unambiguous finite automata](https://arxiv.org/abs/2511.09703)
*Stefan Kiefer,Andrew Ryzhikov*

Main category: cs.FL

TL;DR: 该论文研究由0-1矩阵生成的幺半群中最小秩矩阵的计算问题，提出了多项式和准线性时间算法，并给出了Černý猜想的一个弱版本推广。


<details>
  <summary>Details</summary>
Motivation: 研究0-1矩阵幺半群的最小秩计算问题，这类矩阵可以表示非确定性有限自动机，是确定性有限自动机的重要推广，具有许多良好性质。

Method: 使用线性代数技术证明该问题属于NC类，并提出组合算法在O(n^{2+ω} + mn^4)时间内找到最小秩矩阵，其中ω是矩阵乘法指数。对于全DFA特例，算法复杂度为O(n^3 + mn^2)。

Result: 证明了最小秩计算问题在NC类中，可在O(mn^4)时间内解决。组合算法在O(n^{2+ω} + mn^4)时间内找到最小秩矩阵，并给出了Černý猜想的弱版本推广。

Conclusion: 0-1矩阵幺半群的最小秩计算问题具有高效算法，且存在大小为O(n^2)的直线程序描述得到最小秩矩阵的乘积，为Černý猜想提供了新的视角。

Abstract: A zero-one matrix is a matrix with entries from $\{0, 1\}$. We study monoids containing only such matrices. A finite set of zero-one matrices generating such a monoid can be seen as the matrix representation of an unambiguous finite automaton, an important generalisation of deterministic finite automata which shares many of their good properties.
  Let $\mathcal{A}$ be a finite set of $n \times n$ zero-one matrices generating a monoid of zero-one matrices, and $m$ be the cardinality of $\mathcal{A}$. We study the computational complexity of computing the minimum rank of a matrix in the monoid generated by $\mathcal{A}$. By using linear-algebraic techniques, we show that this problem is in $\textsf{NC}$ and can be solved in $\mathcal{O}(mn^4)$ time. We also provide a combinatorial algorithm finding a matrix of minimum rank in $\mathcal{O}(n^{2 + ω} + mn^4)$ time, where $2 \le ω\le 2.4$ is the matrix multiplication exponent. As a byproduct, we show a very weak version of a generalisation of the Černý conjecture: there always exists a straight line program of size $\mathcal{O}(n^2)$ describing a product resulting in a matrix of minimum rank.
  For the special case corresponding to total DFAs (that is, for the case where all matrices have exactly one 1 in each row), the minimum rank is the size of the smallest image of the set of all states under the action of a word. Our combinatorial algorithm finds a matrix of minimum rank in time $\mathcal{O}(n^3 + mn^2)$ in this case.

</details>
