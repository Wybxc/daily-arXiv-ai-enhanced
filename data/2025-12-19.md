<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.SE](#cs.SE) [Total: 11]
- [cs.LO](#cs.LO) [Total: 4]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [LOOPRAG: Enhancing Loop Transformation Optimization with Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2512.15766)
*Yijie Zhi,Yayu Cao,Jianhua Dai,Xiaoyang Han,Jingwen Pu,Qingran Wu,Sheng Cheng,Ming Cai*

Main category: cs.PL

TL;DR: LOOPRAG是一个基于检索增强生成的框架，通过参数驱动方法生成多样化的循环变换示例，结合循环感知检索算法和基于反馈的迭代机制，显著提升LLMs在循环优化任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管循环变换是广泛使用的语义保持优化技术，但找到最优的循环变换组合仍然具有挑战性。现有的大型语言模型在循环优化方面表现不佳，经常产生错误或次优结果，错失性能提升机会。

Method: 提出LOOPRAG框架：1) 参数驱动方法利用循环属性触发各种循环变换，生成多样化且合法的示例代码；2) 基于循环特征的循环感知检索算法平衡相似性和多样性；3) 基于反馈的迭代机制，结合编译、测试和性能结果作为反馈指导LLMs；4) 通过变异、覆盖和差分测试进行等价性检查。

Result: 在PolyBench、TSVC和LORE基准测试中，LOOPRAG相比基础编译器分别获得最高11.20×、14.34×和9.29×的加速比，相比基础LLMs分别获得最高11.97×、5.61×和11.59×的加速比。

Conclusion: LOOPRAG通过检索增强生成框架有效解决了LLMs在循环优化中的局限性，显著提升了循环变换优化的性能，为代码优化领域提供了新的解决方案。

Abstract: Loop transformations are semantics-preserving optimization techniques, widely used to maximize objectives such as parallelism. Despite decades of research, applying the optimal composition of loop transformations remains challenging due to inherent complexities, including cost modeling for optimization objectives. Recent studies have explored the potential of Large Language Models (LLMs) for code optimization. However, our key observation is that LLMs often struggle with effective loop transformation optimization, frequently leading to errors or suboptimal optimization, thereby missing opportunities for performance improvements. To bridge this gap, we propose LOOPRAG, a novel retrieval-augmented generation framework designed to guide LLMs in performing effective loop optimization on Static Control Part. We introduce a parameter-driven method to harness loop properties, which trigger various loop transformations, and generate diverse yet legal example codes serving as a demonstration source. To effectively obtain the most informative demonstrations, we propose a loop-aware algorithm based on loop features, which balances similarity and diversity for code retrieval. To enhance correct and efficient code generation, we introduce a feedback-based iterative mechanism that incorporates compilation, testing and performance results as feedback to guide LLMs. Each optimized code undergoes mutation, coverage and differential testing for equivalence checking. We evaluate LOOPRAG on PolyBench, TSVC and LORE benchmark suites, and compare it against compilers (GCC-Graphite, Clang-Polly, Perspective and ICX) and representative LLMs (DeepSeek and GPT-4). The results demonstrate average speedups over base compilers of up to 11.20$\times$, 14.34$\times$, and 9.29$\times$ for PolyBench, TSVC, and LORE, respectively, and speedups over base LLMs of up to 11.97$\times$, 5.61$\times$, and 11.59$\times$.

</details>


### [2] [Automated Formalization of Probabilistic Requirements from Structured Natural Language](https://arxiv.org/abs/2512.15788)
*Anastasia Mavridou,Marie Farrell,Gricel Vázquez,Tom Pressburger,Timothy E. Wang,Radu Calinescu,Michael Fisher*

Main category: cs.PL

TL;DR: 扩展NASA的FRET工具，支持用结构化自然语言编写概率需求，并自动转换为概率时序逻辑公式，使自主自适应系统的形式化分析更实用


<details>
  <summary>Details</summary>
Motivation: 自主自适应系统开发面临重大挑战，特别是需要明确捕捉环境或决策过程中的不确定性。安全关键和任务关键系统需要严格审查，但用概率构造编写需求很困难，而直接使用形式化逻辑对开发者来说不现实且容易出错。

Method: 扩展NASA的FRET工具的结构化自然语言，支持概率需求的规范；开发形式化、组合式、自动化的方法，将结构化自然语言需求转换为概率时序逻辑公式；建立自动化验证框架和形式化证明来确保生成公式的正确性。

Result: 扩展后的FRET工具使开发者能够用结构化自然语言指定概率需求，并自动转换为概率时序逻辑公式，提高了自主自适应系统形式化分析的实用性和可靠性。

Conclusion: 通过将结构化自然语言与概率时序逻辑相结合，为自主自适应系统提供了一种更实用、更不易出错的概率需求规范方法，使形式化分析更加可行。

Abstract: Integrating autonomous and adaptive behavior into software-intensive systems presents significant challenges for software development, as uncertainties in the environment or decision-making processes must be explicitly captured. These challenges are amplified in safety- and mission-critical systems, which must undergo rigorous scrutiny during design and development. Key among these challenges is the difficulty of specifying requirements that use probabilistic constructs to capture the uncertainty affecting these systems. To enable formal analysis, such requirements must be expressed in precise mathematical notations such as probabilistic logics. However, expecting developers to write requirements directly in complex formalisms is unrealistic and highly error-prone. We extend the structured natural language used by NASA's Formal Requirement Elicitation Tool (FRET) with support for the specification of unambiguous and correct probabilistic requirements, and develop an automated approach for translating these requirements into logical formulas. We propose and develop a formal, compositional, and automated approach for translating structured natural-language requirements into formulas in probabilistic temporal logic. To increase trust in our formalizations, we provide assurance that the generated formulas are well-formed and conform to the intended semantics through an automated validation framework and a formal proof. The extended FRET tool enables developers to specify probabilistic requirements in structured natural language, and to automatically translate them into probabilistic temporal logic, making the formal analysis of autonomous and adaptive systems more practical and less error-prone.

</details>


### [3] [A Neurosymbolic Approach to Loop Invariant Generation via Weakest Precondition Reasoning](https://arxiv.org/abs/2512.15816)
*Daragh King,Vasileios Koutavas,Laura Kovacs*

Main category: cs.PL

TL;DR: NeuroInv是一种神经符号方法，通过结合LLM的神经推理和符号验证来生成循环不变式，在150个Java程序上达到99.5%的成功率。


<details>
  <summary>Details</summary>
Motivation: 循环不变式生成是自动程序验证的关键瓶颈。现有基于LLM的方法缺乏可靠的结构化方法，且很少参考现有程序验证理论。

Method: NeuroInv包含两个关键模块：(1) 神经推理模块：利用LLM和霍尔逻辑通过后向链式最弱前置条件推理来推导和优化候选不变式；(2) 验证引导的符号模块：使用OpenJML的反例迭代修复不变式。

Result: 在包含单循环、多循环、多数组、随机分支和噪声代码段的150个Java程序基准测试中，NeuroInv达到99.5%的成功率，显著优于其他方法。在包含10个更大规模多循环程序（平均每个程序7个循环）的困难基准测试中也表现良好。

Conclusion: NeuroInv能够扩展到更复杂的验证场景，为循环不变式生成提供了一种可靠且结构化的神经符号方法。

Abstract: Loop invariant generation remains a critical bottleneck in automated program verification. Recent work has begun to explore the use of Large Language Models (LLMs) in this area, yet these approaches tend to lack a reliable and structured methodology, with little reference to existing program verification theory. This paper presents NeuroInv, a neurosymbolic approach to loop invariant generation. NeuroInv comprises two key modules: (1) a neural reasoning module that leverages LLMs and Hoare logic to derive and refine candidate invariants via backward-chaining weakest precondition reasoning, and (2) a verification-guided symbolic module that iteratively repairs invariants using counterexamples from OpenJML. We evaluate NeuroInv on a comprehensive benchmark of 150 Java programs, encompassing single and multiple (sequential) loops, multiple arrays, random branching, and noisy code segments. NeuroInv achieves a $99.5\%$ success rate, substantially outperforming the other evaluated approaches. Additionally, we introduce a hard benchmark of $10$ larger multi-loop programs (with an average of $7$ loops each); NeuroInv's performance in this setting demonstrates that it can scale to more complex verification scenarios.

</details>


### [4] [Optimizing Agentic Language Model Inference via Speculative Tool Calls](https://arxiv.org/abs/2512.15834)
*Daniel Nichols,Prajwal Singhania,Charles Jekel,Abhinav Bhatele,Harshitha Menon*

Main category: cs.PL

TL;DR: 该论文提出针对语言模型工具调用性能瓶颈的系统优化方案，通过推测工具调用和保持序列驻留来提升推理吞吐量，达到每秒数百token的改进。


<details>
  <summary>Details</summary>
Motivation: 语言模型越来越依赖外部工具（如文件搜索、代码执行、API调用等），这些工具虽然增强了模型能力，但在推理过程中引入了性能瓶颈，影响了推理吞吐量。

Method: 提出系统优化方案：1) 推测工具调用，提前预测并准备工具执行；2) 强制序列在推理引擎中保持驻留，减少开销；3) 理论分析推测配置；4) 建议新的"工具缓存"API端点。

Result: 优化方案显著提升了LM代理推理的吞吐量，达到每秒数百token的改进，并提供了理论分析指导最佳推测配置。

Conclusion: 通过系统优化解决LM工具调用性能瓶颈是可行的，提出的推测工具调用和序列驻留技术能显著提升推理性能，建议LM提供商采用工具缓存API来实施这些优化。

Abstract: Language models (LMs) are becoming increasingly dependent on external tools. LM-based agentic frameworks frequently interact with their environment via such tools to search files, run code, call APIs, etc. Further, modern reasoning-based LMs use tools such as web search and Python code execution to enhance their reasoning capabilities. While tools greatly improve the capabilities of LMs, they also introduce performance bottlenecks during the inference process. In this paper, we introduce novel systems optimizations to address such performance bottlenecks by speculating tool calls and forcing sequences to remain resident in the inference engine to minimize overheads. Our optimizations lead to throughput improvements of several hundred tokens per second when hosting inference for LM agents. We provide a theoretical analysis of our algorithms to provide insights into speculation configurations that will yield the best performance. Further, we recommend a new "tool cache" API endpoint to enable LM providers to easily adopt these optimizations.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [5] [XBIDetective: Leveraging Vision Language Models for Identifying Cross-Browser Visual Inconsistencies](https://arxiv.org/abs/2512.15804)
*Balreet Grewal,James Graham,Jeff Muizelaar,Jan Honza Odvarko,Suhaib Mujahid,Marco Castelluccio,Cor-Paul Bezemer*

Main category: cs.SE

TL;DR: XBIDetective使用视觉语言模型自动检测跨浏览器不一致性，通过对比Firefox和Chrome截图，在1052个网站上达到79%准确率，能有效识别动态元素和广告。


<details>
  <summary>Details</summary>
Motivation: 浏览器渲染bug难以检测，通常只在特定条件下触发。跨浏览器不一致性(XBIs)可作为检测此类bug的线索，但现有视觉和DOM分析方法难以处理动态交互元素。

Method: 开发XBIDetective工具，自动在Firefox和Chrome中捕获网站截图，使用视觉语言模型分析差异。评估了现成模型和微调模型在1052个网站上的表现。

Result: 微调后的VLM能识别跨浏览器差异，准确率达79%；检测动态元素和广告的准确率分别为84%和85%。工具在回归测试、大规模网站监控和bug报告分类方面有实用价值。

Conclusion: 视觉语言模型能有效检测跨浏览器不一致性，特别是针对动态内容。XBIDetective展示了在浏览器开发、质量保证和网站监控方面的实际应用潜力。

Abstract: Browser rendering bugs can be challenging to detect for browser developers, as they may be triggered by very specific conditions that are exhibited on only a very small subset of websites. Cross-browser inconsistencies (XBIs), variations in how a website is interpreted and displayed on different browsers, can be helpful guides to detect such rendering bugs. Although visual and Document Object Model (DOM)-based analysis techniques exist for detecting XBIs, they often struggle with dynamic and interactive elements. In this study, we discuss our industry experience with using vision language models (VLMs) to identify XBIs. We present the XBIDetective tool which automatically captures screenshots of a website in Mozilla Firefox and Google Chrome, and analyzes them with a VLM for XBIs. We evaluate XBIDetective's performance with an off-the-shelf and a fine-tuned VLM on 1,052 websites. We show that XBIDetective can identify cross-browser discrepancies with 79% accuracy and detect dynamic elements and advertisements with 84% and 85% accuracy, respectively, when using the fine-tuned VLM. We discuss important lessons learned, and we present several potential practical use cases for XBIDetective, including automated regression testing, large-scale monitoring of websites, and rapid triaging of XBI bug reports.

</details>


### [6] [CodeMem: Architecting Reproducible Agents via Dynamic MCP and Procedural Memory](https://arxiv.org/abs/2512.15813)
*Nishant Gaurav,Adit Akarsh,Tejas Ravishankar,Manoj Bajaj*

Main category: cs.SE

TL;DR: CodeMem提出通过代码实现程序性记忆的架构，用于构建可重复使用的确定性可靠智能体工作流，解决现有工具使用AI代理在重复任务中的概率不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 当前工具使用AI代理存在动作空间有限、上下文效率低和概率不稳定性问题，不适合处理重复任务。虽然已有工作通过Python语言扩展动作空间，但概率不稳定性问题仍然存在，需要程序性记忆来确保一致性和可靠性。

Method: 提出CodeMem架构，通过代码实现程序性记忆，用于构建和运行可重复使用的智能体工作流。该方法利用代码作为记忆形式，确保工作流的确定性可靠性。

Result: 论文提出了一个解决AI代理概率不稳定性问题的架构方案，能够构建具有确定性可靠性的可重复使用智能体工作流。

Conclusion: CodeMem通过代码实现程序性记忆，为构建可靠、一致的智能体工作流提供了解决方案，解决了现有工具使用AI代理在重复任务中的核心限制。

Abstract: Current tool-using AI agents suffer from limited action space, context inefficiency, and probabilistic instability that makes them unsuitable for handling repetitive tasks which are otherwise reliably and efficiently tackled by agentic workflows built on platforms like n8n and Zapier. Earlier works like CodeAct, DynaSaur, Code Mode have tried to tackle the first two issues by using the whole Python language as its action space: The number of tools that the agent can call becomes infinite. Python code blocks can execute complex actions into a single step and print only relevant results which helps in keeping the context lean. However, the probabilistic instability issue still remains, as for the same task in the same environment, the agent can follow different trajectories due to the probabilistic nature of LLMs. Therefore, we need procedural memory for consistency and reliability. This paper proposes CodeMem, an architecture to implement procedural memory via code which can be used to build and run reusable agentic workflows with deterministic reliability.

</details>


### [7] [OLAF: Towards Robust LLM-Based Annotation Framework in Empirical Software Engineering](https://arxiv.org/abs/2512.15979)
*Mia Mohammad Imran,Tarannum Shaila Zaman*

Main category: cs.SE

TL;DR: 本文提出了OLAF框架，将LLM标注视为测量过程而非纯自动化任务，强调可靠性、校准、漂移等关键概念，旨在提升软件工程研究中LLM标注的透明度和可复现性。


<details>
  <summary>Details</summary>
Motivation: 当前在实证软件工程中广泛使用LLM进行标注任务（如标记提交、问题等），但缺乏对可靠性、可复现性的系统研究。现有研究缺少标准化测量方法，且经常遗漏关键配置细节，需要建立方法论框架来规范LLM标注实践。

Method: 提出OLAF（LLM标注操作化框架）概念框架，组织六个核心构建：可靠性、校准、漂移、共识、聚合和透明度。将LLM标注视为测量过程，强调方法论讨论和未来实证工作。

Result: 建立了系统化的概念框架，为LLM在软件工程标注任务中的应用提供了方法论基础。框架明确了关键测量维度，为未来研究提供了标准化方向。

Conclusion: LLM标注应被视为测量过程而非纯自动化活动。OLAF框架为软件工程研究中的LLM标注提供了系统化方法论，旨在促进更透明、可复现的研究实践，需要进一步实证验证和完善。

Abstract: Large Language Models (LLMs) are increasingly used in empirical software engineering (ESE) to automate or assist annotation tasks such as labeling commits, issues, and qualitative artifacts. Yet the reliability and reproducibility of such annotations remain underexplored. Existing studies often lack standardized measures for reliability, calibration, and drift, and frequently omit essential configuration details. We argue that LLM-based annotation should be treated as a measurement process rather than a purely automated activity. In this position paper, we outline the \textbf{Operationalization for LLM-based Annotation Framework (OLAF)}, a conceptual framework that organizes key constructs: \textit{reliability, calibration, drift, consensus, aggregation}, and \textit{transparency}. The paper aims to motivate methodological discussion and future empirical work toward more transparent and reproducible LLM-based annotation in software engineering research.

</details>


### [8] [Embedding Software Intent: Lightweight Java Module Recovery](https://arxiv.org/abs/2512.15980)
*Yirui He,Yuqi Huai,Xingyu Chen,Joshua Garcia*

Main category: cs.SE

TL;DR: ClassLAR：一种基于类名和语言模型的轻量级方法，用于从单体Java系统中恢复Java模块，在20个项目中优于现有技术且速度更快


<details>
  <summary>Details</summary>
Motivation: 随着软件系统规模不断扩大，仅依赖代码级抽象变得不切实际。虽然架构抽象有助于管理系统，但保持其与实际代码的一致性一直存在问题。Java 9引入的JPMS（Java平台模块系统）通过在语言级别支持显式模块规范来解决这一限制，但现有架构恢复技术对模块恢复效果不佳，使得将现有单体项目模块化成为开放挑战。

Method: ClassLAR（基于类和语言模型的架构恢复）是一种新颖、轻量级且高效的方法，使用完全限定类名从单体Java系统恢复Java模块。该方法利用语言模型从包名和类名中提取语义信息，捕捉结构和功能意图。

Result: 在20个流行Java项目的评估中，ClassLAR在所有架构级相似性指标上都优于所有最先进技术，同时执行时间快了3.99到10.50倍。

Conclusion: ClassLAR为Java项目模块化提供了一种有效的解决方案，能够高效准确地恢复模块架构，有助于实现架构抽象与实际代码的一致性管理。

Abstract: As an increasing number of software systems reach unprecedented scale, relying solely on code-level abstractions is becoming impractical. While architectural abstractions offer a means to manage these systems, maintaining their consistency with the actual code has been problematic. The Java Platform Module System (JPMS), introduced in Java 9, addresses this limitation by enabling explicit module specification at the language level. JPMS enhances architectural implementation through improved encapsulation and direct specification of ground-truth architectures within Java projects. Although many projects are written in Java, modularizing existing monolithic projects to JPMS modules is an open challenge due to ineffective module recovery by existing architecture recovery techniques. To address this challenge, this paper presents ClassLAR (Class-and Language model-based Architectural Recovery), a novel, lightweight, and efficient approach that recovers Java modules from monolithic Java systems using fully-qualified class names. ClassLAR leverages language models to extract semantic information from package and class names, capturing both structural and functional intent. In evaluations across 20 popular Java projects, ClassLAR outperformed all state-of-the-art techniques in architectural-level similarity metrics while achieving execution times that were 3.99 to 10.50 times faster.

</details>


### [9] [LLM4Perf: Large Language Models Are Effective Samplers for Multi-Objective Performance Modeling (Copy)](https://arxiv.org/abs/2512.16070)
*Xin Wang,Zhenhao Li,Zishuo Ding*

Main category: cs.SE

TL;DR: LLM4Perf框架利用大语言模型进行多目标性能建模采样，在68.8%场景中优于传统方法，通过配置空间剪枝和反馈驱动策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统性能高度依赖复杂配置选项，现有采样方法在多目标优化和利用文档语义信息方面存在不足，而大语言模型在理解语义方面的成功激发了探索其作为性能建模采样器的可能性。

Method: 提出LLM4Perf反馈框架，系统评估LLM引导的采样过程，在四个真实可配置系统中进行实验，分析LLM在配置空间剪枝和策略优化方面的能力。

Result: LLM4Perf在112个评估场景中的77个（68.8%）取得最佳性能，其剪枝策略在448个案例中的410个（91.5%）提升了基线方法性能，证明了LLM在性能工程中的有效性。

Conclusion: LLM作为性能建模采样器具有显著优势，其有效性源于配置空间剪枝和反馈驱动策略优化能力，为性能工程提供了新的有效工具和机制洞察。

Abstract: The performance of modern software systems is critically dependent on their complex configuration options. Building accurate performance models to navigate this vast space requires effective sampling strategies, yet existing methods often struggle with multi-objective optimization and cannot leverage semantic information from documentation. The recent success of Large Language Models (LLMs) motivates the central question of this work: Can LLMs serve as effective samplers for multi-objective performance modeling? To explore this, we present a comprehensive empirical study investigating the capabilities and characteristics of LLM-driven sampling. We design and implement LLM4Perf, a feedback-based framework, and use it to systematically evaluate the LLM-guided sampling process across four highly configurable, real-world systems. Our study reveals that the LLM-guided approach outperforms traditional baselines in most cases. Quantitatively, LLM4Perf achieves the best performance in nearly 68.8% (77 out of 112) of all evaluation scenarios, demonstrating its superior effectiveness. We find this effectiveness stems from the LLM's dual capabilities of configuration space pruning and feedback-driven strategy refinement. The effectiveness of this pruning is further validated by the fact that it also improves the performance of the baseline methods in nearly 91.5% (410 out of 448) of cases. Furthermore, we show how the LLM choices for each component and hyperparameters within LLM4Perf affect its effectiveness. Overall, this paper provides strong evidence for the effectiveness of LLMs in performance engineering and offers concrete insights into the mechanisms that drive their success.

</details>


### [10] [Analysis of Design Patterns and Benchmark Practices in Apache Kafka Event-Streaming Systems](https://arxiv.org/abs/2512.16146)
*Muzeeb Mohammad*

Main category: cs.SE

TL;DR: 该论文对2015-2025年间42项关于Apache Kafka的同行评审研究进行了结构化综述，识别出9种常见设计模式，分析了使用趋势和基准测试实践，揭示了配置披露和可重复性方面的问题，并提供了分类法、基准矩阵和决策启发式方法。


<details>
  <summary>Details</summary>
Motivation: 尽管Apache Kafka已成为高吞吐量事件流处理的基础平台，并在实时分析、金融交易处理、工业遥测和大规模数据驱动系统中广泛应用，但关于可重用架构设计模式和可重复基准测试方法的研究仍然分散在学术和工业出版物中，缺乏系统性的整理。

Method: 对2015-2025年间42项同行评审研究进行结构化综述，识别出9种常见的Kafka设计模式，分析其共同使用趋势、领域特定部署和实证基准测试实践，包括使用TPCx Kafka、Yahoo Streaming Benchmark等标准套件以及自定义工作负载。

Result: 研究发现配置披露、评估严谨性和可重复性方面存在显著不一致，限制了跨研究比较和实际复现。研究提供了统一的分类法、模式基准矩阵和可操作的决策启发式方法。

Conclusion: 该研究为架构师和研究人员设计可重复、高性能和容错的基于Kafka的事件流系统提供了实用指导，通过统一的分类法和基准框架促进了更严谨和可比较的研究实践。

Abstract: Apache Kafka has become a foundational platform for high throughput event streaming, enabling real time analytics, financial transaction processing, industrial telemetry, and large scale data driven systems. Despite its maturity and widespread adoption, consolidated research on reusable architectural design patterns and reproducible benchmarking methodologies remains fragmented across academic and industrial publications. This paper presents a structured synthesis of forty two peer reviewed studies published between 2015 and 2025, identifying nine recurring Kafka design patterns including log compaction, CQRS bus, exactly once pipelines, change data capture, stream table joins, saga orchestration, tiered storage, multi tenant topics, and event sourcing replay. The analysis examines co usage trends, domain specific deployments, and empirical benchmarking practices using standard suites such as TPCx Kafka and the Yahoo Streaming Benchmark, as well as custom workloads. The study highlights significant inconsistencies in configuration disclosure, evaluation rigor, and reproducibility that limit cross study comparison and practical replication. By providing a unified taxonomy, pattern benchmark matrix, and actionable decision heuristics, this work offers practical guidance for architects and researchers designing reproducible, high performance, and fault tolerant Kafka based event streaming systems.

</details>


### [11] [Beyond Blind Spots: Analytic Hints for Mitigating LLM-Based Evaluation Pitfalls](https://arxiv.org/abs/2512.16272)
*Ora Nova Fandina,Eitan Farchi,Shmulik Froimovich,Raviv Gal,Wesam Ibraheem,Rami Katan,Alice Podolsky*

Main category: cs.SE

TL;DR: 研究发现LLM作为代码生成评估器存在领域盲点，通过结合轻量级分析检查器提供提示，可将错误检测率从45%提升至94%


<details>
  <summary>Details</summary>
Motivation: LLM作为代码生成评估器(LaaJ)在实际部署中存在局限性，特别是在领域特定问题上容易遗漏关键错误，这对其在关键评估任务中的可靠性提出了担忧

Method: 1) 分析生成的COBOL程序和LLM评估结果，构建初步分类法；2) 开发轻量级分析检查器工具，可检测30多种领域特定问题；3) 将检查器输出作为分析提示动态注入LLM评估提示中；4) 在100个程序测试集上使用4个生产级LLM进行实验

Result: 单独使用LLM仅能检测约45%的错误，单独使用分析检查器缺乏解释深度。结合使用时，LLM+提示配置最高可达到94%的覆盖率，并能产生质量更高、更准确的解释

Conclusion: 分析检查器与LLM的混合方法可以显著提高部署管道中的评估可靠性，为工业应用中的代码生成评估提供了有效解决方案

Abstract: Large Language Models are increasingly deployed as judges (LaaJ) in code generation pipelines. While attractive for scalability, LaaJs tend to overlook domain specific issues raising concerns about their reliability in critical evaluation tasks. To better understand these limitations in practice, we examine LaaJ behavior in a concrete industrial use case: legacy code modernization via COBOL code generation. In this setting, we find that even production deployed LaaJs can miss domain critical errors, revealing consistent blind spots in their evaluation capabilities.
  To better understand these blind spots, we analyze generated COBOL programs and associated LaaJs judgments, drawing on expert knowledge to construct a preliminary taxonomy. Based on this taxonomy, we develop a lightweight analytic checker tool that flags over 30 domain specific issues observed in practice. We use its outputs as analytic hints, dynamically injecting them into the judges prompt to encourage LaaJ to revisit aspects it may have overlooked.
  Experiments on a test set of 100 programs using four production level LaaJs show that LaaJ alone detects only about 45% of the errors present in the code (in all judges we tested), while the analytic checker alone lacks explanatory depth. When combined, the LaaJ+Hints configuration achieves up to 94% coverage (for the best performing judge and injection prompt) and produces qualitatively richer, more accurate explanations, demonstrating that analytic-LLM hybrids can substantially enhance evaluation reliability in deployed pipelines. We release the dataset and all used prompts.

</details>


### [12] [Using a Sledgehammer to Crack a Nut? Revisiting Automated Compiler Fault Isolation](https://arxiv.org/abs/2512.16335)
*Yibiao Yang,Qingyang Li,Maolin Sun,Jiangchang Wu,Yuming Zhou*

Main category: cs.SE

TL;DR: 比较编译器故障定位中基于bug-inducing commit的Basic策略与谱基故障定位技术，发现Basic策略在实际效果上相当甚至优于现有SBFL方法。


<details>
  <summary>Details</summary>
Motivation: 编译器故障对软件开发有严重影响，需要有效的定位方法。虽然已有许多谱基故障定位技术，但尚未评估它们与实践中广泛采用的基于bug-inducing commit策略的效果对比。

Method: Basic策略通过识别最近的好版本和最早的坏版本，使用二分查找定位bug-inducing commit，将该提交中修改的所有文件标记为潜在故障文件。在包含60个GCC bug和60个LLVM bug的基准测试中，将Basic与代表性SBFL技术进行严格比较。

Result: Basic策略在性能上与最先进的SBFL技术相当，在许多情况下甚至优于它们，特别是在关键的Top-1和Top-5排名指标上表现更好。

Conclusion: 这项研究为SBFL技术在实际编译器调试场景中的有效性提供了新见解。建议未来研究在开发和评估新的编译器故障隔离方法时，将Basic作为基准。

Abstract: Background: Compilers are fundamental to software development, translating high-level source code into executable software systems. Faults in compilers can have severe consequences and thus effective localization and resolution of compiler bugs are crucial. Problem: In practice, developers often examine version history to identify and investigate bug-inducing commit (BIC) for fixing bugs. However, while numerous sophisticated Spectrum-Based Fault Localization (SBFL) techniques have been proposed for compiler fault isolation, their effectiveness has not been evaluated against the BIC-based strategies widely adopted in practice. Objective: This study aims to bridge this gap by directly comparing a BIC-based strategy, Basic, with representative SBFL techniques in the context of compiler fault localization. The BIC-based strategy closely aligns with common developer practices, as it directly identifies the BIC and treats the files modified in that commit as faulty candidates. Method: The Basic identifies the most recent good release and earliest bad release, and then employs a binary search to pinpoint the bug-inducing commit. All files modified in the identified commit are flagged as potentially faulty. We rigorously compare Basic against SBFL-based techniques using a benchmark consisting of 60 GCC bugs and 60 LLVM bugs. Result: Our analysis reveals that Basic performs comparably to, and in many cases outperforms, state-of-the-art SBFL-based techniques, particularly on the critical Top-1 and Top-5 ranking metrics. Conclusion: This study provides new insights into the practical effectiveness of SBFL-based techniques in real-world compiler debugging scenarios. We recommend that future research adopt Basic as a baseline when developing and evaluating new compiler fault isolation methods.

</details>


### [13] [An Empirical Study of the Realism of Mutants in Deep Learning](https://arxiv.org/abs/2512.16741)
*Zaheed Ahmed,Philip Makedonski,Jens Grabowski*

Main category: cs.SE

TL;DR: 首次实证比较DL中预训练与后训练变异方法的真实性，发现预训练变异体与真实缺陷耦合更强、行为更相似，但计算成本高，需要开发更有效的后训练变异算子。


<details>
  <summary>Details</summary>
Motivation: 变异分析在传统软件中已成熟，但在深度学习中的应用假设（变异体行为类似真实缺陷）尚未验证。需要实证比较预训练和后训练变异方法的真实性。

Method: 引入统计框架量化变异体与真实缺陷的耦合强度和行为相似性，使用公开缺陷数据集（CleanML、DeepFD、DeepLocalize、defect4ML），用最先进工具生成预训练和后训练变异体。

Result: 预训练变异体比后训练变异体表现出更强的耦合和更高的行为相似性，表明其真实性更高。但预训练变异计算成本显著更高。

Conclusion: 预训练变异方法更真实，但计算成本高，需要开发更有效的后训练变异算子以达到或超过预训练变异体的真实性水平。

Abstract: Mutation analysis is a well-established technique for assessing test quality in the traditional software development paradigm by injecting artificial faults into programs. Its application to deep learning (DL) has expanded beyond classical testing to support tasks such as fault localization, repair, data generation, and model robustness evaluation. The core assumption is that mutants behave similarly to real faults, an assumption well established in traditional software systems but largely unverified for DL.
  This study presents the first empirical comparison of pre-training and post-training mutation approaches in DL with respect to realism. We introduce a statistical framework to quantify their coupling strength and behavioral similarity to real faults using publicly available bugs datasets: CleanML, DeepFD, DeepLocalize, and defect4ML. Mutants are generated using state-of-the-art tools representing both approaches.
  Results show that pre-training mutants exhibit consistently stronger coupling and higher behavioral similarity to real faults than post-training mutants, indicating greater realism. However, the substantial computational cost of pre-training mutation underscores the need for more effective post-training operators that match or exceed the realism demonstrated by pre-training mutants.

</details>


### [14] [Inside Out: Uncovering How Comment Internalization Steers LLMs for Better or Worse](https://arxiv.org/abs/2512.16790)
*Aaron Imani,Mohammad Moshirpour,Iftekhar Ahmed*

Main category: cs.SE

TL;DR: 本文首次对LLM在软件工程任务中的概念级可解释性进行研究，通过概念激活向量分析发现LLM将注释内部化为不同的潜在概念，并能区分注释子类型。通过激活/去激活这些概念，观察到性能在-90%到+67%之间的显著变化，且代码总结任务最能触发注释概念激活。


<details>
  <summary>Details</summary>
Motivation: 尽管注释是源代码的非功能性元素，但大型语言模型在执行软件工程任务时经常依赖它们。然而，模型在何处依赖注释以及这种依赖如何影响性能，目前仍知之甚少。需要理解LLM内部如何表示和处理注释信息。

Method: 使用概念激活向量（CAV）进行概念级可解释性研究，分析三个任务（代码补全、翻译和精炼）中的内部注释表示。通过系统性地激活和去激活LLM嵌入空间中的注释概念，观察性能变化。还进行了对照实验，使用相同的代码输入，让LLM执行10个不同的软件工程任务，同时测量其潜在表示中注释概念的激活程度。

Result: LLM不仅将注释内部化为不同的潜在概念，还能区分Javadocs、内联注释和多行注释等子类型。激活/去激活这些概念会导致显著、模型特定且任务依赖的性能变化（范围从-90%到+67%）。代码总结任务最能触发注释概念的激活，而代码补全任务对注释的敏感性最弱。

Conclusion: 这些结果为构建新的软件工程工具和模型开辟了新方向，这些工具和模型可以推理和操作内部概念表示，而不仅仅依赖于表面级别的输入。研究展示了概念级可解释性在理解LLM如何执行软件工程任务方面的重要性。

Abstract: While comments are non-functional elements of source code, Large Language Models (LLM) frequently rely on them to perform Software Engineering (SE) tasks. Yet, where in the model this reliance resides, and how it affects performance, remains poorly understood. We present the first concept-level interpretability study of LLMs in SE, analyzing three tasks - code completion, translation, and refinement - through the lens of internal comment representation. Using Concept Activation Vectors (CAV), we show that LLMs not only internalize comments as distinct latent concepts but also differentiate between subtypes such as Javadocs, inline, and multiline comments. By systematically activating and deactivating these concepts in the LLMs' embedding space, we observed significant, model-specific, and task-dependent shifts in performance ranging from -90% to +67%. Finally, we conducted a controlled experiment using the same set of code inputs, prompting LLMs to perform 10 distinct SE tasks while measuring the activation of the comment concept within their latent representations. We found that code summarization consistently triggered the strongest activation of comment concepts, whereas code completion elicited the weakest sensitivity. These results open a new direction for building SE tools and models that reason about and manipulate internal concept representations rather than relying solely on surface-level input.

</details>


### [15] [Toward Systematic Counterfactual Fairness Evaluation of Large Language Models: The CAFFE Framework](https://arxiv.org/abs/2512.16816)
*Alessandra Parziale,Gianmario Voria,Valeria Pontillo,Gemma Catolino,Andrea De Lucia,Fabio Palomba*

Main category: cs.SE

TL;DR: CAFFE是一个用于评估LLM反事实公平性的结构化测试框架，通过定义明确的测试组件、自动生成测试数据和语义相似度评估，比现有方法能更广泛地检测偏见。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在现代软件系统中的广泛应用，公平性问题日益突出。现有基于蜕变测试的方法存在局限性，需要更结构化、意图感知的框架来系统性地评估LLM的反事实公平性。

Method: CAFFE框架包含三个核心部分：(1) 形式化LLM公平性测试用例，明确定义提示意图、对话上下文、输入变体、预期公平阈值和测试环境配置；(2) 自动生成针对性测试数据；(3) 使用语义相似度指标评估模型响应。

Result: 在三种不同架构的LLM上进行的实验表明，CAFFE相比现有的蜕变测试方法，能够实现更广泛的偏见覆盖，并更可靠地检测不公平行为。

Conclusion: CAFFE为LLM反事实公平性评估提供了一个结构化、意图感知的测试框架，能够更有效地检测和评估模型中的偏见问题，为LLM的公平性测试提供了新的视角和方法。

Abstract: Nowadays, Large Language Models (LLMs) are foundational components of modern software systems. As their influence grows, concerns about fairness have become increasingly pressing. Prior work has proposed metamorphic testing to detect fairness issues, applying input transformations to uncover inconsistencies in model behavior. This paper introduces an alternative perspective for testing counterfactual fairness in LLMs, proposing a structured and intent-aware framework coined CAFFE (Counterfactual Assessment Framework for Fairness Evaluation). Inspired by traditional non-functional testing, CAFFE (1) formalizes LLM-Fairness test cases through explicitly defined components, including prompt intent, conversational context, input variants, expected fairness thresholds, and test environment configuration, (2) assists testers by automatically generating targeted test data, and (3) evaluates model responses using semantic similarity metrics. Our experiments, conducted on three different architectural families of LLM, demonstrate that CAFFE achieves broader bias coverage and more reliable detection of unfair behavior than existing metamorphic approaches.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [16] [Mechanizing Operads with Event-B](https://arxiv.org/abs/2512.16342)
*Christian Attiogbé*

Main category: cs.LO

TL;DR: 使用Event-B方法实现代数operad的完整精化链，为符号计算应用提供数学结构支持


<details>
  <summary>Details</summary>
Motivation: 自然和工业系统的严格建模面临抽象、方法和工具方面的挑战，operad作为数学结构能提供组合对象并保证良好形式化的抽象方法

Method: 采用Event-B方法开发完整的精化链，实现代数operad及其基本操作

Result: 建立了可实际使用的operad实现，为符号计算问题提供方法论支持

Conclusion: 该工作为处理符号计算问题的类似实现提供了方法论指导，并支持基于operad结构的符号计算应用推理

Abstract: Rigorous modelling of natural and industrial systems still conveys various challenges related to abstractions, methods to proceed with and easy-to-use tools to build, compose and reason on models. Operads are mathematical structures that provide such abstractions to compose various objects and garanteeing well-formedness.
  Concrete implementations of operads will offer practical means to exploit operads and to use them for various technical applications. Going from the mathematical structures, we develop with Event-B a complete refinement chain that implements algebraic operads and their basic operations.
  The result of this work, can be used from the methodological point of view to handle similar implementations for symbolic computation questions, and also to reason on symbolic computation applications supported by operads structures.

</details>


### [17] [(Pointed) Univalence in Universe Category Models of Type Theory](https://arxiv.org/abs/2512.16697)
*Chris Kapulkin,Yufeng Li*

Main category: cs.LO

TL;DR: 提出在宇宙范畴模型中表述单值公理的新方法，并发展其强化版本——点单值公理，验证其在Artin-Wraith粘合和逆图形成下的封闭性


<details>
  <summary>Details</summary>
Motivation: 在依赖类型论的宇宙范畴模型中，需要一种便于在同伦理论环境中验证的单值公理表述方法。同时，现有的单值公理在计算和语义方面存在改进空间。

Method: 1. 在宇宙范畴模型中重新表述单值公理，使其更易于在同伦理论环境中验证
2. 提出点单值公理作为单值公理的强化版本，兼具计算优势和语义自然性
3. 验证点单值公理在Artin-Wraith粘合和逆图形成操作下的封闭性

Result: 成功建立了便于同伦理论验证的单值公理表述，并证明了点单值公理在Artin-Wraith粘合和逆图形成下保持封闭性，为依赖类型论的同伦语义提供了更好的理论基础。

Conclusion: 点单值公理作为单值公理的强化版本，不仅计算上更理想，语义上更自然，而且在重要的范畴构造操作下保持封闭性，为同伦类型论的进一步发展提供了有力工具。

Abstract: We provide a formulation of the univalence axiom in a universe category model of dependent type theory that is convenient to verify in homotopy-theoretic settings. We further develop a strengthening of the univalence axiom, called pointed univalence, that is both computationally desirable and semantically natural, and verify its closure under Artin-Wraith gluing and formation of inverse diagrams.

</details>


### [18] [Towards Mass Spectrum Analysis with ASP](https://arxiv.org/abs/2512.16780)
*Nils Küchenmeister,Alex Ivliev,Markus Krötzsch*

Main category: cs.LO

TL;DR: 使用答案集编程（ASP）通过质谱数据推断分子结构，开发了分子结构的规范表示法来约束搜索空间，并与现有方法进行了比较。


<details>
  <summary>Details</summary>
Motivation: 质谱分析中，根据元素和结构片段的相对丰度推断分子结构是一个组合优化问题，搜索空间呈指数增长，需要有效的方法来约束搜索。

Method: 开发了分子结构的规范表示法，并基于此实现了ASP系统，利用对称性破缺技术来约束组合搜索空间。

Result: 在大规模已知分子结构数据集上验证了正确性，与其它ASP对称性破缺方法及商业化学分析工具相比，在质量和性能上表现出色。

Conclusion: ASP结合规范表示法能有效解决分子结构推断问题，为质谱数据分析提供了有前景的计算方法。

Abstract: We present a new use of Answer Set Programming (ASP) to discover the molecular structure of chemical samples based on the relative abundance of elements and structural fragments, as measured in mass spectrometry. To constrain the exponential search space for this combinatorial problem, we develop canonical representations of molecular structures and an ASP implemen- tation that uses these definitions. We evaluate the correctness of our implementation over a large set of known molecular structures, and we compare its quality and performance to other ASP symmetry-breaking methods and to a commercial tool from analytical chemistry. Under consideration in Theory and Practice of Logic Programming (TPLP).

</details>


### [19] [Checking the HAL Interface Specification Continuously, Right from the Start](https://arxiv.org/abs/2512.16897)
*Manuel Bentele,Onur Altinordu,Jan Körner,Andreas Podelski,Axel Sikora*

Main category: cs.LO

TL;DR: 提出一种在嵌入式开发中持续检查HAL接口规范的新方法，通过迭代开发确保每个步骤都能成功进行软件模型检查


<details>
  <summary>Details</summary>
Motivation: 硬件抽象层(HAL)接口的正确使用对嵌入式应用至关重要，但工业实践中软件模型检查的应用受到不可预测性的阻碍（无法确定对特定应用是否成功）

Method: 采用迭代开发方法：从仅调用HAL函数的程序骨架开始，逐步添加实际功能，在每一步都检查HAL接口规范，利用软件模型检查的抽象计算特性在不同步骤间传递

Result: 初步实验评估结果非常乐观，按照该方法检查在每个步骤都能成功，包括最终的应用程序

Conclusion: 通过从开发开始就持续检查HAL接口规范的迭代方法，可以有效解决软件模型检查在工业实践中的不可预测性问题

Abstract: The correct use of a Hardware Abstraction Layer (HAL) interface in embedded applications is crucial to prevent malfunctions, crashes, or even hardware damage. Software model checking has been successfully applied to check interface specifications in application programs, but its employment in industrial practice is hindered by its unpredictability (whether it succeeds for a given application program or not). In this paper, we present a novel approach to address this problem by checking the HAL interface specification continuously and right from the start of the development. I.e., we develop an embedded application in several iterations without a formal connection between the steps. The steps start from a program skeleton which does nothing but calling HAL functions. Actual functionality is added consecutively. The HAL interface specification is checked in each step of the sequence. The idea of the approach is to exploit a specific feature of software model checking: Its attempt to compute exactly the abstraction that is needed for the check to succeed may carry over from one step to the next, even if there is no formal connection between the steps. The experience from a preliminary experimental evaluation of our approach in the development of embedded applications is very promising. Following our approach, the check succeeds in each step and in particular in the final application program.

</details>
