<div id=toc></div>

# Table of Contents

- [cs.FL](#cs.FL) [Total: 1]
- [cs.LO](#cs.LO) [Total: 5]
- [cs.SE](#cs.SE) [Total: 15]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [1] [Hexagonal Picture Scanning Automata](https://arxiv.org/abs/2508.07779)
*Deepalakshmi D,Lisa Mathew*

Main category: cs.FL

TL;DR: 论文介绍了两种新型有限自动机，分析了其计算特性及语言家族关系，扩展了二维自动机理论。


<details>
  <summary>Details</summary>
Motivation: 将经典有限自动机概念扩展到六边形几何结构，探索其计算能力。

Method: 引入并分析了两种新型六边形有限自动机模型。

Result: 建立了这些自动机的理论基础，研究了其语言家族的关系与等价性。

Conclusion: 研究扩展了二维自动机理论，为六边形结构提供了新的计算模型。

Abstract: Two new classes of finite automata, called General hexagonal Boustrophedon
finite automata and General hexagonal returning finite automata operating on
hexagonal grids, are introduced and analyzed. The work establishes the
theoretical foundations for these automata models, examines their computational
properties, and investigates the relationships and equivalences between the
language families they define. The research contributes to the broader
understanding of two-dimensional automata theory by extending classical finite
automaton concepts to hexagonal geometric structures with specialized traversal
patterns.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [2] [On the fault diameter and wide diameter of the exchanged 3-ary $n$-cube](https://arxiv.org/abs/2508.07174)
*Rongshuan Geng,Wantao Ning*

Main category: cs.LO

TL;DR: 研究了交换3-ary $n$-立方体$E3C(r, s, t)$的故障直径和宽直径，发现其$(2r + 1)$-故障直径和$(2r + 2)$-宽直径在$n + 3$到$n + 5$之间。


<details>
  <summary>Details</summary>
Motivation: 故障直径和宽直径是评估互连网络通信性能的关键参数，研究其在新提出的交换3-ary $n$-立方体中的表现具有重要意义。

Method: 分析了交换3-ary $n$-立方体$E3C(r, s, t)$的结构特性，推导其故障直径和宽直径的上下界。

Result: 发现$(2r + 1)$-故障直径和$(2r + 2)$-宽直径的范围为$n + 3$到$n + 5$。

Conclusion: 交换3-ary $n$-立方体在故障容忍和传输效率方面表现良好，为网络设计提供了理论支持。

Abstract: Fault diameter and wide diameter are two critical parameters for evaluating
communication performance in interconnection networks. They measure the fault
tolerance and transmission efficiency of networks. The exchanged 3-ary $n$-cube
is a recently proposed variant of the hypercube, denoted by $E3C(r, s, t)$. In
this work, we obtain that the $(2r + 1)$-fault diameter and $(2r + 2)$-wide
diameter of $E3C(r, s, t)$ are bounded between $n + 3$ and $n + 5$ for $1 \leq
r \leq s \leq t$.

</details>


### [3] [Presburger Functional Synthesis: Complexity and Tractable Normal Forms](https://arxiv.org/abs/2508.07207)
*S. Akshay,A. R. Balasubramanian,Supratik Chakraborty,Georg Zetzsche*

Main category: cs.LO

TL;DR: 本文研究了Presburger算术理论中的函数合成问题（PFnS），证明了其可在EXPTIME内解决，并提供了匹配的指数下界。同时，提出了一种特殊范式PSyNF，保证多项式时间求解。


<details>
  <summary>Details</summary>
Motivation: 研究Presburger算术理论中的函数合成问题，填补现有理论空白，并探索高效求解方法。

Method: 通过理论分析，证明PFnS的复杂度，并设计特殊范式PSyNF以实现多项式时间求解。

Result: PFnS可在EXPTIME内解决，PSyNF范式保证多项式时间和多项式规模求解。

Conclusion: PFnS的复杂度为EXPTIME，PSyNF是一种高效的求解范式，适用于特定条件下的函数合成。

Abstract: Given a relational specification between inputs and outputs as a logic
formula, the problem of functional synthesis is to automatically synthesize a
function from inputs to outputs satisfying the relation. Recently, a rich line
of work has emerged tackling this problem for specifications in different
theories, from Boolean to general first-order logic. In this paper, we launch
an investigation of this problem for the theory of Presburger Arithmetic, that
we call Presburger Functional Synthesis (PFnS). We show that PFnS can be solved
in EXPTIME and provide a matching exponential lower bound. This is unlike the
case for Boolean functional synthesis (BFnS), where only conditional
exponential lower bounds are known. Further, we show that PFnS for one input
and one output variable is as hard as BFnS in general. We then identify a
special normal form, called PSyNF, for the specification formula that
guarantees poly-time and poly-size solvability of PFnS. We prove several
properties of PSyNF, including how to check and compile to this form, and
conditions under which any other form that guarantees poly-time solvability of
PFnS can be compiled in poly-time to PSyNF. Finally, we identify a syntactic
normal form that is easier to check but is exponentially less succinct than
PSyNF.

</details>


### [4] [From Knowledge to Conjectures: A Modal Framework for Reasoning about Hypotheses](https://arxiv.org/abs/2508.07304)
*Fabio Vitali*

Main category: cs.LO

TL;DR: 本文提出了一种新的认知模态逻辑家族，用于形式化推测推理，通过假设扩展已知事实以探索其后果。与传统信念和认知系统不同，该逻辑基于Axiom C，避免模态崩溃，并采用弱Kleene逻辑或描述逻辑的语义框架。


<details>
  <summary>Details</summary>
Motivation: 传统模态逻辑在推测推理中存在局限性，尤其是Axiom C与模态崩溃的关联。本文旨在设计一种新逻辑系统，避免崩溃并区分事实与推测。

Method: 采用弱Kleene逻辑或描述逻辑的语义框架，避免Axiom T，定义新模态系统（如KC和KDC），并引入动态操作settle(φ)。

Result: 新系统（KC和KDC）具有完备性、可判定性，并在部分知识下保持鲁棒性。动态操作settle(φ)形式化了从推测到事实的转换。

Conclusion: 本文提出的逻辑系统有效解决了推测推理中的模态崩溃问题，并提供了动态更新认知状态的方法。

Abstract: This paper introduces a new family of cognitive modal logics designed to
formalize conjectural reasoning: a modal system in which cognitive contexts
extend known facts with hypothetical assumptions to explore their consequences.
Unlike traditional doxastic and epistemic systems, conjectural logics rely on a
principle, called Axiom C ($\varphi \rightarrow \Box\varphi$), that ensures
that all established facts are preserved across hypothetical layers. While
Axiom C was dismissed in the past due to its association with modal collapse,
we show that the collapse only arises under classical and bivalent assumptions,
and specifically in the presence of Axiom T. Hence we avoid Axiom T and adopt a
paracomplete semantic framework, grounded in Weak Kleene logic or Description
Logic, where undefined propositions coexist with modal assertions. This
prevents the modal collapse and guarantees a layering to distinguish between
factual and conjectural statements. Under this framework we define new modal
systems, e.g., KC and KDC, and show that they are complete, decidable, and
robust under partial knowledge. Finally, we introduce a dynamic operation,
$\mathsf{settle}(\varphi)$, which formalizes the transition from conjecture to
accepted fact, capturing the event of the update of a world's cognitive state
through the resolution of uncertainty.

</details>


### [5] [A Rule-Based Approach to Specifying Preferences over Conflicting Facts and Querying Inconsistent Knowledge Bases](https://arxiv.org/abs/2508.07742)
*Meghyn Bienvenu,Camille Bourgaux,Katsumi Inoue,Robin Jean*

Main category: cs.LO

TL;DR: 论文提出了一种基于规则的声明性框架，用于指定和计算冲突事实之间的优先级关系，并探讨了如何消除偏好规则中的循环问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽然利用事实间的优先级关系选择最优修复，但如何指定这种偏好仍未解决。

Method: 引入声明性规则框架，结合循环消除技术，利用答案集编程实现优先级关系和查询回答。

Result: 提出了一个初步实现和实验评估的系统，支持不一致知识库的查询。

Conclusion: 该框架为不一致知识库的查询提供了有效的优先级关系指定和循环处理方法。

Abstract: Repair-based semantics have been extensively studied as a means of obtaining
meaningful answers to queries posed over inconsistent knowledge bases (KBs).
While several works have considered how to exploit a priority relation between
facts to select optimal repairs, the question of how to specify such
preferences remains largely unaddressed. This motivates us to introduce a
declarative rule-based framework for specifying and computing a priority
relation between conflicting facts. As the expressed preferences may contain
undesirable cycles, we consider the problem of determining when a set of
preference rules always yields an acyclic relation, and we also explore a
pragmatic approach that extracts an acyclic relation by applying various cycle
removal techniques. Towards an end-to-end system for querying inconsistent KBs,
we present a preliminary implementation and experimental evaluation of the
framework, which employs answer set programming to evaluate the preference
rules, apply the desired cycle resolution techniques to obtain a priority
relation, and answer queries under prioritized-repair semantics.

</details>


### [6] [Runtime Verification for LTL in Stochastic Systems](https://arxiv.org/abs/2508.07963)
*Javier Esparza,Vincent Fischer*

Main category: cs.LO

TL;DR: 提出了一种基于概率预测和置信度评分的新型运行时验证方法，用于解决传统LTL监控器对活性质无法给出确定结论的问题。


<details>
  <summary>Details</summary>
Motivation: 传统LTL监控器只能输出真、假或不确定，对于活性质无法从有限前缀得出结论，限制了其应用范围。

Method: 采用概率预测和置信度评分替代硬性判决，确保预测的最终正确性，并保证置信度从某点开始无限增长。

Result: 新方法能够为活性质提供渐进准确的预测，并保证置信度的持续提升。

Conclusion: 该方法扩展了运行时验证的能力，适用于更广泛的LTL公式，尤其是活性质。

Abstract: Runtime verification encompasses several lightweight techniques for checking
whether a system's current execution satisfies a given specification. We focus
on runtime verification for Linear Temporal Logic (LTL). Previous work
describes monitors which produce, at every time step one of three outputs -
true, false, or inconclusive - depending on whether the observed execution
prefix definitively determines satisfaction of the formula. However, for many
LTL formulas, such as liveness properties, satisfaction cannot be concluded
from any finite prefix. For these properties traditional monitors will always
output inconclusive. In this work, we propose a novel monitoring approach that
replaces hard verdicts with probabilistic predictions and an associated
confidence score. Our method guarantees eventual correctness of the prediction
and ensures that confidence increases without bound from that point on.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [7] [Refactoring-Aware Patch Integration Across Structurally Divergent Java Forks](https://arxiv.org/abs/2508.06718)
*Daniel Ogenrwot,John Businge*

Main category: cs.SE

TL;DR: 论文研究了GitHub上长期分叉（变体）的代码库中补丁集成的挑战，提出了RePatch系统，通过重构感知的方法成功解决了部分补丁集成失败的问题。


<details>
  <summary>Details</summary>
Motivation: 长期分叉的代码库（变体）在独立开发过程中会出现结构漂移，导致补丁集成困难，传统工具如Git cherry-pick因缺乏语义推理而失败率高。

Method: RePatch扩展了RefMerge框架，支持非对称补丁传输，通过反转重构操作对齐补丁上下文，再应用补丁并重放变换以保留变体意图。

Result: 在478个补丁请求中，Git cherry-pick失败率为64.4%，而RePatch成功集成了52.8%的失败补丁。

Conclusion: 研究揭示了基于语法工具的局限性，强调了语义推理在变体感知补丁传播中的重要性。

Abstract: While most forks on platforms like GitHub are short-lived and used for social
collaboration, a smaller but impactful subset evolve into long-lived forks,
referred to here as variants, that maintain independent development
trajectories. Integrating bug-fix patches across such divergent variants poses
challenges due to structural drift, including refactorings that rename,
relocate, or reorganize code elements and obscure semantic correspondence. This
paper presents an empirical study of patch integration failures in 14 divergent
pair of variants and introduces RePatch, a refactoring-aware integration system
for Java repositories. RePatch extends the RefMerge framework, originally
designed for symmetric merges, by supporting asymmetric patch transfer. RePatch
inverts refactorings in both the source and target to realign the patch
context, applies the patch, and replays the transformations to preserve the
intent of the variant. In our evaluation of 478 bug-fix pull requests, Git
cherry-pick fails in 64.4% of cases due to structural misalignments, while
RePatch successfully integrates 52.8% of the previously failing patches. These
results highlight the limitations of syntax-based tools and the need for
semantic reasoning in variant-aware patch propagation.

</details>


### [8] [Quo Vadis, Code Review? Exploring the Future of Code Review](https://arxiv.org/abs/2508.06879)
*Michael Dorner,Andreas Bauer,Darja Šmite,Lukas Thode,Daniel Mendez,Ricardo Britto,Stephan Lukasczyk,Ehsan Zabardast,Michael Kormann*

Main category: cs.SE

TL;DR: 研究探讨了开发者对代码审查现状的反思及未来变化的预期，并分析了这些变化对代码审查长期发展的潜在风险。


<details>
  <summary>Details</summary>
Motivation: 探索开发者对代码审查的现状反思及未来预期，以理解其对协作软件工程的影响。

Method: 通过调查和分析开发者的观点，探讨代码审查的未来变化及其潜在风险。

Result: 揭示了开发者对代码审查未来变化的预期，并识别了可能影响其长期发展的风险。

Conclusion: 代码审查的未来变化可能带来风险，需进一步研究以保障其在协作软件工程中的核心作用。

Abstract: Code review has long been a core practice in collaborative software
engineering. In this research, we explore how practitioners reflect on code
review today and what changes they anticipate in the near future. We then
discuss the potential long-term risks of these anticipated changes for the
evolution of code review and its role in collaborative software engineering.

</details>


### [9] [Multi-Modal Requirements Data-based Acceptance Criteria Generation using LLMs](https://arxiv.org/abs/2508.06888)
*Fanyu Wang,Chetan Arora,Yonghui Liu,Kaicheng Huang,Chakkrit Tantithamthavorn,Aldeida Aleti,Dishan Sambathkumar,David Lo*

Main category: cs.SE

TL;DR: 提出了一种基于多模态RAG的方法RAGcceptance M2RE，用于从文本和视觉UI信息生成验收标准，显著提升了相关性和正确性。


<details>
  <summary>Details</summary>
Motivation: 手动创建准确、全面且无歧义的验收标准具有挑战性，尤其是在依赖领域知识和视觉上下文的用户界面密集型应用中。

Method: 利用检索增强生成（RAG）技术，结合多模态需求数据（文本和UI视觉信息）生成验收标准。

Result: 工业案例研究表明，多模态信息显著提升了生成验收标准的相关性、正确性和可理解性，并减少了人工工作量。

Conclusion: 多模态RAG技术在优化软件验证流程和提高开发效率方面具有显著潜力。

Abstract: Acceptance criteria (ACs) play a critical role in software development by
clearly defining the conditions under which a software feature satisfies
stakeholder expectations. However, manually creating accurate, comprehensive,
and unambiguous acceptance criteria is challenging, particularly in user
interface-intensive applications, due to the reliance on domain-specific
knowledge and visual context that is not always captured by textual
requirements alone. To address these challenges, we propose RAGcceptance M2RE,
a novel approach that leverages Retrieval-Augmented Generation (RAG) to
generate acceptance criteria from multi-modal requirements data, including both
textual documentation and visual UI information. We systematically evaluated
our approach in an industrial case study involving an education-focused
software system used by approximately 100,000 users. The results indicate that
integrating multi-modal information significantly enhances the relevance,
correctness, and comprehensibility of the generated ACs. Moreover, practitioner
evaluations confirm that our approach effectively reduces manual effort,
captures nuanced stakeholder intent, and provides valuable criteria that domain
experts may overlook, demonstrating practical utility and significant potential
for industry adoption. This research underscores the potential of multi-modal
RAG techniques in streamlining software validation processes and improving
development efficiency. We also make our implementation and a dataset
available.

</details>


### [10] [Integrating Rules and Semantics for LLM-Based C-to-Rust Translation](https://arxiv.org/abs/2508.06926)
*Feng Luo,Kexing Ji,Cuiyun Gao,Shuzheng Gao,Jia Feng,Kui Liu,Xin Xia,Michael R. Lyu*

Main category: cs.SE

TL;DR: IRENE是一个基于LLM的框架，通过整合规则和语义提升C到Rust的代码翻译质量，解决了现有方法在语法规则和语义一致性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有C到Rust的翻译方法（如静态规则或直接提示LLM）在语法规则适应性和语义一致性上表现不佳，IRENE旨在解决这些问题。

Method: IRENE包含三个模块：规则增强检索模块、结构化摘要模块和错误驱动翻译模块，分别提升规则处理、语义理解和翻译迭代优化。

Result: 在xCodeEval和HW-Bench数据集上评估，IRENE在翻译准确性和安全性上表现优于现有方法。

Conclusion: IRENE通过整合规则和语义，显著提升了C到Rust代码翻译的质量和安全性。

Abstract: Automated translation of legacy C code into Rust aims to ensure memory safety
while reducing the burden of manual migration. Early approaches in code
translation rely on static rule-based methods, but they suffer from limited
coverage due to dependence on predefined rule patterns. Recent works regard the
task as a sequence-to-sequence problem by leveraging large language models
(LLMs). Although these LLM-based methods are capable of reducing unsafe code
blocks, the translated code often exhibits issues in following Rust rules and
maintaining semantic consistency. On one hand, existing methods adopt a direct
prompting strategy to translate the C code, which struggles to accommodate the
syntactic rules between C and Rust. On the other hand, this strategy makes it
difficult for LLMs to accurately capture the semantics of complex code. To
address these challenges, we propose IRENE, an LLM-based framework that
Integrates RulEs aNd sEmantics to enhance translation. IRENE consists of three
modules: 1) a rule-augmented retrieval module that selects relevant translation
examples based on rules generated from a static analyzer developed by us,
thereby improving the handling of Rust rules; 2) a structured summarization
module that produces a structured summary for guiding LLMs to enhance the
semantic understanding of C code; 3) an error-driven translation module that
leverages compiler diagnostics to iteratively refine translations. We evaluate
IRENE on two datasets (xCodeEval, a public dataset, and HW-Bench, an industrial
dataset provided by Huawei) and eight LLMs, focusing on translation accuracy
and safety.

</details>


### [11] [When Prompt Engineering Meets Software Engineering: CNL-P as Natural and Robust "APIs'' for Human-AI Interaction](https://arxiv.org/abs/2508.06942)
*Zhenchang Xing,Yang Liu,Zhuo Cheng,Qing Huang,Dehai Zhao,Daniel Sun,Chenhua Liu*

Main category: cs.SE

TL;DR: 论文提出了一种名为CNL-P的受控自然语言，结合了提示工程和软件工程的原则，以减少自然语言的歧义并提升LLM的输出质量。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）能力的增强，其在智能客服、代码生成等领域的应用日益广泛，但自然语言提示的歧义性限制了交互效果。

Method: 提出CNL-P，引入精确的语法结构和语义规范，并开发了NL2CNL-P转换工具和语法检查工具。

Result: 实验表明，CNL-P通过结合提示工程和软件工程，显著提升了LLM的输出质量。

Conclusion: CNL-P为以自然语言为中心的新编程范式奠定了基础，弥合了提示工程与传统软件工程之间的差距。

Abstract: With the growing capabilities of large language models (LLMs), they are
increasingly applied in areas like intelligent customer service, code
generation, and knowledge management. Natural language (NL) prompts act as the
``APIs'' for human-LLM interaction. To improve prompt quality, best practices
for prompt engineering (PE) have been developed, including writing guidelines
and templates. Building on this, we propose Controlled NL for Prompt (CNL-P),
which not only incorporates PE best practices but also draws on key principles
from software engineering (SE). CNL-P introduces precise grammar structures and
strict semantic norms, further eliminating NL's ambiguity, allowing for a
declarative but structured and accurate expression of user intent. This helps
LLMs better interpret and execute the prompts, leading to more consistent and
higher-quality outputs. We also introduce an NL2CNL-P conversion tool based on
LLMs, enabling users to write prompts in NL, which are then transformed into
CNL-P format, thus lowering the learning curve of CNL-P. In particular, we
develop a linting tool that checks CNL-P prompts for syntactic and semantic
accuracy, applying static analysis techniques to NL for the first time.
Extensive experiments demonstrate that CNL-P enhances the quality of LLM
responses through the novel and organic synergy of PE and SE. We believe that
CNL-P can bridge the gap between emerging PE and traditional SE, laying the
foundation for a new programming paradigm centered around NL.

</details>


### [12] [An Empirical Study on Method-Level Performance Evolution in Open-Source Java Projects](https://arxiv.org/abs/2508.07084)
*Kaveh Shahedi,Nana Gyambrah,Heng Li,Maxime Lamothe,Foutse Khomh*

Main category: cs.SE

TL;DR: 该研究通过大规模实证分析，探讨了方法级代码变更对性能演化的影响，发现32.7%的变更对性能有显著影响，且性能退化比改进更常见。研究挑战了传统观念，并提出了自动化性能测试的重要性。


<details>
  <summary>Details</summary>
Motivation: 性能是软件开发中的关键质量属性，但方法级代码变更对性能演化的影响缺乏细粒度的实证研究。

Method: 分析了15个成熟开源Java项目的739个提交中的1,499个方法级变更，使用JMH进行性能测量和统计量化，并通过字节码插桩捕获方法执行指标。

Result: 32.7%的变更对性能有显著影响，性能退化比改进多1.3倍；算法变更的改进潜力最大但风险也高；资深开发者的变更更稳定。

Conclusion: 研究支持将自动化性能测试集成到持续集成流程中，并挑战了传统的风险分层开发策略。

Abstract: Performance is a critical quality attribute in software development, yet the
impact of method-level code changes on performance evolution remains poorly
understood. While developers often make intuitive assumptions about which types
of modifications are likely to cause performance regressions or improvements,
these beliefs lack empirical validation at a fine-grained level. We conducted a
large-scale empirical study analyzing performance evolution in 15 mature
open-source Java projects hosted on GitHub. Our analysis encompassed 739
commits containing 1,499 method-level code changes, using Java Microbenchmark
Harness (JMH) for precise performance measurement and rigorous statistical
analysis to quantify both the significance and magnitude of performance
variations. We employed bytecode instrumentation to capture method-specific
execution metrics and systematically analyzed four key aspects: temporal
performance patterns, code change type correlations, developer and complexity
factors, and domain-size interactions. Our findings reveal that 32.7% of
method-level changes result in measurable performance impacts, with regressions
occurring 1.3 times more frequently than improvements. Contrary to conventional
wisdom, we found no significant differences in performance impact distributions
across code change categories, challenging risk-stratified development
strategies. Algorithmic changes demonstrate the highest improvement potential
but carry substantial regression risk. Senior developers produce more stable
changes with fewer extreme variations, while code complexity correlates with
increased regression likelihood. Domain-size interactions reveal significant
patterns, with web server + small projects exhibiting the highest performance
instability. Our study provides empirical evidence for integrating automated
performance testing into continuous integration pipelines.

</details>


### [13] [From Noise to Knowledge: Interactive Summaries for Developer Alerts](https://arxiv.org/abs/2508.07169)
*Burak Yetiştiren,Hong Jin Kang,Miryung Kim*

Main category: cs.SE

TL;DR: CLARITY是一个交互式工具，通过总结规则和主动反馈帮助程序员更高效地理解和分类静态分析工具生成的警告。


<details>
  <summary>Details</summary>
Motivation: 程序员通常需要逐条检查静态分析工具生成的警告，而识别重复主题和关系可以提升理解效率。

Method: CLARITY通过交互式查询和规则推断算法，支持自定义分组和主动反馈，帮助用户标记警告并发现共同模式。

Result: 在用户研究中，CLARITY显著提升了用户对警告根因的理解速度和信心，且个性化分组需求明显。模拟实验显示，主动反馈减少了规则对齐所需的交互次数。

Conclusion: CLARITY通过主动学习和总结规则，有效提升了警告理解的交互性和效率。

Abstract: Programmers using bug-finding tools often review their reported warnings one
by one. Based on the insight that identifying recurring themes and
relationships can enhance the cognitive process of sensemaking, we propose
CLARITY, which supports interpreting tool-generated warnings through
interactive inquiry. CLARITY derives summary rules for custom grouping of
related warnings with active feedback. As users mark warnings as interesting or
uninteresting, CLARITY's rule inference algorithm surfaces common symptoms,
highlighting structural similarities in containment, subtyping, invoked
methods, accessed fields, and expressions.
  We demonstrate CLARITY on Infer and SpotBugs warnings across two mature Java
projects. In a within-subject user study with 14 participants, users
articulated root causes for similar uninteresting warnings faster and with more
confidence using CLARITY. We observed significant individual variation in
desired grouping, reinforcing the need for customizable sensemaking. Simulation
shows that with rule-level feedback, only 11.8 interactions are needed on
average to align all inferred rules with a simulated user's labels (vs. 17.8
without). Our evaluation suggests that CLARITY's active learning-based
summarization enhances interactive warning sensemaking.

</details>


### [14] [Dynamic Benchmark Construction for Evaluating Large Language Models on Real-World Codes](https://arxiv.org/abs/2508.07180)
*Zhe Zhang,Runlin Liu,Aishan Liu,Xingyu Liu,Xiang Gao,Hailong Sun*

Main category: cs.SE

TL;DR: CODE2BENCH是一个动态构建代码生成基准的端到端管道，解决了现有基准的数据污染和测试不足问题，并通过实验揭示了LLM在复杂任务中的表现差异。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在软件开发中的广泛应用，现有基准存在数据污染和测试不足的问题，亟需一种更可靠的评估方法。

Method: CODE2BENCH通过自动动态更新、依赖分析和属性测试，构建了污染抵抗的基准CODE2BENCH-2505，包含1,163个任务。

Result: 评估16个LLM后发现，模型在需要复杂逻辑的跨语言任务（SC）上表现较差，而在允许库使用的Python任务（WSC）上表现较好。

Conclusion: CODE2BENCH提供了一种动态、语言无关的基准构建方法，为LLM在真实软件开发任务中的评估提供了可靠基础。

Abstract: As large language models LLMs) become increasingly integrated into software
development workflows, rigorously evaluating their performance on complex,
real-world code generation tasks has become essential. However, existing
benchmarks often suffer from data contamination and limited test rigor,
constraining their ability to reveal model failures effectively. To address
these, we present CODE2BENCH, a end-to-end pipeline for dynamically
constructing robust and contamination-resistant benchmarks from real-world
GitHub repositories. Specifically, CODE2BENCH introduces three key innovations:
(1) Automated Dynamism, achieved through periodic ingestion of recent code to
minimize training data contamination; (2) Scope Graph-based dependency
analysis, which enables structured classification of functions into benchmark
instances with controlled dependency levels (distinguishing between
Self-Contained (SC) tasks for cross-language evaluation and Weakly
Self-Contained (WSC) tasks involving permitted library usage); and (3)
Property-Based Testing (PBT) for the automated synthesis of rigorous test
suites to enable thorough functional verification. Using this pipeline, we
construct CODE2BENCH-2505, the first benchmark derived from 880 recent Python
projects spanning diverse domains, comprising 1,163 code generation tasks with
100% average branch coverage on ground-truth implementations. Extensive
evaluation of 16 LLMs using CODE2BENCH-2505 reveals that models consistently
struggle with SC tasks requiring complex, non-standard logic and cross-language
transfer, while showing relatively stronger performance on WSC tasks in Python.
Our work introduces a contamination-resistant, language-agnostic methodology
for dynamic benchmark construction, offering a principled foundation for the
comprehensive and realistic evaluation of LLMs on real-world software
development tasks.

</details>


### [15] [TraceLens: Question-Driven Debugging for Taint Flow Understanding](https://arxiv.org/abs/2508.07198)
*Burak Yetiştiren,Hong Jin Kang,Miryung Kim*

Main category: cs.SE

TL;DR: TraceLens是一种面向终端用户的问答式污点分析调试工具，通过支持用户提出“为什么”、“为什么不”和“假设”问题，显著提升了调试效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有污点分析工具缺乏对终端用户调试的支持，无法回答关于数据流的问题，且可视化方式难以全局分析多源和汇点的连接性。

Method: 提出TraceLens，一个问答式调试界面，支持用户提问并执行假设性分析，以理解数据流和第三方库模型的影响。

Result: 用户研究表明，TraceLens用户比CodeQL用户平均准确率高21%，心理负担减少45%，且对识别相关数据流的信心更高。

Conclusion: TraceLens通过问答式交互和假设分析，显著提升了污点分析的调试效率和用户体验。

Abstract: Taint analysis is a security analysis technique used to track the flow of
potentially dangerous data through an application and its dependent libraries.
Investigating why certain unexpected flows appear and why expected flows are
missing is an important sensemaking process during end-user taint analysis.
Existing taint analysis tools often do not provide this end-user debugging
capability, where developers can ask why, why-not, and what-if questions about
dataflows and reason about the impact of configuring sources and sinks, and
models of 3rd-party libraries that abstract permissible and impermissible data
flows. Furthermore, a tree-view or a list-view used in existing
taint-analyzer's visualization makes it difficult to reason about the global
impact on connectivity between multiple sources and sinks.
  Inspired by the insight that sensemaking tool-generated results can be
significantly improved by a QA inquiry process, we propose TraceLens, a first
end-user question-answer style debugging interface for taint analysis. It
enables a user to ask why, why-not, and what-if questions to investigate the
existence of suspicious flows, the non-existence of expected flows, and the
global impact of third-party library models. TraceLens performs speculative
what-if analysis, to help a user in debugging how different connectivity
assumptions affect overall results. A user study with 12 participants shows
that participants using TraceLens achieved 21% higher accuracy on average,
compared to CodeQL. They also reported a 45% reduction in mental demand
(NASA-TLX) and rated higher confidence in identifying relevant flows using
TraceLens.

</details>


### [16] [AutoAssert 1: A LoRA Fine-Tuned LLM Model for Efficient Automated Assertion Generation](https://arxiv.org/abs/2508.07371)
*Yi Zhong,Hongchao Liu,Di ZHao*

Main category: cs.SE

TL;DR: 提出了一种基于硬件描述语言（HDL）的断言生成方法，结合轻量级可调参数的大语言模型（LLM）和Unsloth平台，自动生成测试用例，显著降低训练成本且不牺牲准确性。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统复杂性增加，对自动化测试和维护工具的需求急剧增长。

Method: 结合轻量级可调参数LLM与Unsloth平台，自动生成符合硬件逻辑的断言。

Result: 实证评估表明，该方法能高效生成严格符合硬件逻辑的断言。

Conclusion: 该框架为现代软件测试和维护提供了稳健灵活的解决方案。

Abstract: As the complexity of software systems continues to increase, the demand for
automated testing and maintenance tools is growing exponentially. To meet this
urgent need, we propose a new assertion generation method based on Hardware
Description Language (HDL). This method combines a lightweight,
parameter-adjustable large language model (LLM) with the Unsloth platform to
automatically generate test cases, thereby significantly reducing training
costs without sacrificing accuracy or generalization performance. Empirical
evaluation shows that our method can efficiently generate assertions that
strictly conform to the hardware logic. This framework provides a robust and
flexible solution to modern software testing and maintenance challenges.
https://github.com/liusu-orange/AutoAssert-1 and
https://gitee.com/OpenBPU/auto-assert1 are the locations of the source code.

</details>


### [17] [Extracting Overlapping Microservices from Monolithic Code via Deep Semantic Embeddings and Graph Neural Network-Based Soft Clustering](https://arxiv.org/abs/2508.07486)
*Morteza Ziabakhsh,Kiyan Rezaee,Sadegh Eskandari,Seyed Amir Hossein Tabatabaei,Mohammad M. Ghassemi*

Main category: cs.SE

TL;DR: Mo2oM框架通过软聚类方法将单体应用分解为微服务，显著提升了结构模块化和通信效率。


<details>
  <summary>Details</summary>
Motivation: 现有微服务提取方法采用硬聚类，导致服务间耦合增加和服务内内聚降低。

Method: 结合深度语义嵌入和结构依赖，使用图神经网络进行软聚类。

Result: 在四个开源基准测试中，Mo2oM在结构模块化、通信开销等方面显著优于现有方法。

Conclusion: Mo2oM通过软聚类方法有效解决了微服务分解中的耦合和内聚问题。

Abstract: Modern software systems are increasingly shifting from monolithic
architectures to microservices to enhance scalability, maintainability, and
deployment flexibility. Existing microservice extraction methods typically rely
on hard clustering, assigning each software component to a single microservice.
This approach often increases inter-service coupling and reduces intra-service
cohesion. We propose Mo2oM (Monolithic to Overlapping Microservices), a
framework that formulates microservice extraction as a soft clustering problem,
allowing components to belong probabilistically to multiple microservices. This
approach is inspired by expert-driven decompositions, where practitioners
intentionally replicate certain software components across services to reduce
communication overhead. Mo2oM combines deep semantic embeddings with structural
dependencies extracted from methodcall graphs to capture both functional and
architectural relationships. A graph neural network-based soft clustering
algorithm then generates the final set of microservices. We evaluate Mo2oM on
four open-source monolithic benchmarks and compare it against eight
state-of-the-art baselines. Our results demonstrate that Mo2oM achieves
improvements of up to 40.97% in structural modularity (balancing cohesion and
coupling), 58% in inter-service call percentage (communication overhead),
26.16% in interface number (modularity and decoupling), and 38.96% in
non-extreme distribution (service size balance) across all benchmarks.

</details>


### [18] [Adopting Road-Weather Open Data in Route Recommendation Engine](https://arxiv.org/abs/2508.07881)
*Henna Tammia,Benjamin Kämä,Ella Peltonen*

Main category: cs.SE

TL;DR: 本文探讨了如何高效利用芬兰DigiTraffic提供的全国道路传感器数据，提出了一种基于机器学习的个性化路线推荐引擎方法。


<details>
  <summary>Details</summary>
Motivation: DigiTraffic提供了大量实时道路数据，但如何高效利用这些数据并应用于实际场景（如个性化路线推荐）仍具挑战性。

Method: 通过分析道路天气相关属性，提出数据预处理和机器学习工具的方法，并构建了一个简单的路由应用。

Result: 基于真实数据验证，能够为三种不同驾驶者配置文件高效识别和推荐个性化路线。

Conclusion: 该方法为大规模道路数据的实际应用提供了有效解决方案。

Abstract: Digitraffic, Finland's open road data interface, provides access to
nationwide road sensors with more than 2,300 real-time attributes from 1,814
stations. However, efficiently utilizing such a versatile data API for a
practical application requires a deeper understanding of the data qualities,
preprocessing phases, and machine learning tools. This paper discusses the
challenges of large-scale road weather and traffic data. We go through the
road-weather-related attributes from DigiTraffic as a practical example of
processes required to work with such a dataset. In addition, we provide a
methodology for efficient data utilization for the target application, a
personalized road recommendation engine based on a simple routing application.
We validate our solution based on real-world data, showing we can efficiently
identify and recommend personalized routes for three different driver profiles.

</details>


### [19] [SHIELDA: Structured Handling of Exceptions in LLM-Driven Agentic Workflows](https://arxiv.org/abs/2508.07935)
*Jingwen Zhou,Jieshan Chen,Qinghua Lu,Dehai Zhao,Liming Zhu*

Main category: cs.SE

TL;DR: 论文提出SHIELDA框架，用于结构化处理LLM驱动的工作流中的异常，通过分类和模块化策略实现跨阶段恢复。


<details>
  <summary>Details</summary>
Motivation: 现有异常处理方法对执行阶段的异常处理较浅，缺乏对推理阶段根本原因的追踪，且恢复逻辑脆弱。

Method: 提出SHIELDA框架，包括异常分类器、处理模式注册表和结构化执行器，实现阶段感知的恢复。

Result: 通过AutoPR代理的案例研究验证了SHIELDA的有效性，成功从推理引发的异常中恢复。

Conclusion: SHIELDA为LLM代理工作流提供了模块化、结构化的异常处理方案，显著提升了恢复能力。

Abstract: Large Language Model (LLM) agentic systems are software systems powered by
LLMs that autonomously reason, plan, and execute multi-step workflows to
achieve human goals, rather than merely executing predefined steps. During
execution, these workflows frequently encounter exceptions. Existing exception
handling solutions often treat exceptions superficially, failing to trace
execution-phase exceptions to their reasoning-phase root causes. Furthermore,
their recovery logic is brittle, lacking structured escalation pathways when
initial attempts fail. To tackle these challenges, we first present a
comprehensive taxonomy of 36 exception types across 12 agent artifacts.
Building on this, we propose SHIELDA (Structured Handling of Exceptions in
LLM-Driven Agentic Workflows), a modular runtime exception handling framework
for LLM agentic workflows. SHIELDA uses an exception classifier to select a
predefined exception handling pattern from a handling pattern registry. These
patterns are then executed via a structured handling executor, comprising local
handling, flow control, and state recovery, to enable phase-aware recovery by
linking exceptions to their root causes and facilitating composable strategies.
We validate SHIELDA's effectiveness through a case study on the AutoPR agent,
demonstrating effective, cross-phase recovery from a reasoning-induced
exception.

</details>


### [20] [Exploring the Challenges and Opportunities of AI-assisted Codebase Generation](https://arxiv.org/abs/2508.07966)
*Philipp Eibl,Sadra Sabouri,Souti Chattopadhyay*

Main category: cs.SE

TL;DR: 论文研究了代码库AI助手（CBAs）的使用情况，发现开发者对其生成代码的满意度较低，主要问题包括功能不足、代码质量差和沟通问题。


<details>
  <summary>Details</summary>
Motivation: 探索开发者如何与CBAs互动，以及CBAs未能满足开发者需求的原因。

Method: 通过用户研究（n=16）和访谈，分析开发者使用CBAs时的行为和反馈。

Result: 开发者对生成代码的满意度低（均分2.8/5），主要问题为功能不足（77%）、代码质量差（42%）和沟通问题（25%）。

Conclusion: 提出了六项挑战和五项障碍，并比较了21款商业CBAs的能力，为改进CBAs提供了设计机会。

Abstract: Recent AI code assistants have significantly improved their ability to
process more complex contexts and generate entire codebases based on a textual
description, compared to the popular snippet-level generation. These codebase
AI assistants (CBAs) can also extend or adapt codebases, allowing users to
focus on higher-level design and deployment decisions. While prior work has
extensively studied the impact of snippet-level code generation, this new class
of codebase generation models is relatively unexplored. Despite initial
anecdotal reports of excitement about these agents, they remain less frequently
adopted compared to snippet-level code assistants. To utilize CBAs better, we
need to understand how developers interact with CBAs, and how and why CBAs fall
short of developers' needs. In this paper, we explored these gaps through a
counterbalanced user study and interview with (n = 16) students and developers
working on coding tasks with CBAs. We found that participants varied the
information in their prompts, like problem description (48% of prompts),
required functionality (98% of prompts), code structure (48% of prompts), and
their prompt writing process. Despite various strategies, the overall
satisfaction score with generated codebases remained low (mean = 2.8, median =
3, on a scale of one to five). Participants mentioned functionality as the most
common factor for dissatisfaction (77% of instances), alongside poor code
quality (42% of instances) and communication issues (25% of instances). We
delve deeper into participants' dissatisfaction to identify six underlying
challenges that participants faced when using CBAs, and extracted five barriers
to incorporating CBAs into their workflows. Finally, we surveyed 21 commercial
CBAs to compare their capabilities with participant challenges and present
design opportunities for more efficient and useful CBAs.

</details>


### [21] [PyVeritas: On Verifying Python via LLM-Based Transpilation and Bounded Model Checking for C](https://arxiv.org/abs/2508.08171)
*Pedro Orvalho,Marta Kwiatkowska*

Main category: cs.SE

TL;DR: PyVeritas利用LLMs将Python代码转译为C语言，结合模型检查和MaxSAT技术，实现Python程序的验证和错误定位。


<details>
  <summary>Details</summary>
Motivation: Python缺乏成熟的验证工具，而C语言已有成熟工具（如CBMC）。PyVeritas旨在填补这一空白。

Method: 通过LLMs将Python转译为C，再利用C的模型检查工具进行验证和错误定位。

Result: 实验表明，某些LLMs的转译准确率可达80-90%，支持小规模Python程序的验证和诊断。

Conclusion: PyVeritas为Python程序提供了有效的验证和错误定位方案。

Abstract: Python has become the dominant language for general-purpose programming, yet
it lacks robust tools for formal verification. In contrast, programmers working
in languages such as C benefit from mature model checkers, for example CBMC,
which enable exhaustive symbolic reasoning and fault localisation. The inherent
complexity of Python, coupled with the verbosity and low-level nature of
existing transpilers (e.g., Cython), have historically limited the
applicability of formal verification to Python programs.
  In this paper, we propose PyVeritas, a novel framework that leverages Large
Language Models (LLMs) for high-level transpilation from Python to C, followed
by bounded model checking and MaxSAT-based fault localisation in the generated
C code. PyVeritas enables verification and bug localisation for Python code
using existing model checking tools for C. Our empirical evaluation on two
Python benchmarks demonstrates that LLM-based transpilation can achieve a high
degree of accuracy, up to 80--90% for some LLMs, enabling effective development
environment that supports assertion-based verification and interpretable fault
diagnosis for small yet non-trivial Python programs.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [22] [Checking Consistency of Event-driven Traces](https://arxiv.org/abs/2508.07855)
*Parosh Aziz Abdulla,Mohamed Faouzi Atig,R. Govind,Samuel Grahn,Ramanathan S. Thinniyam*

Main category: cs.PL

TL;DR: 本文提出了事件驱动程序的公理化语义，证明了其与操作语义的等价性，将一致性问题转化为事件驱动一致性问题，并分析了其计算复杂度（NP完全），同时识别了一个多项式时间可解的片段（无嵌套发布）。


<details>
  <summary>Details</summary>
Motivation: 事件驱动编程中，检查候选执行是否符合语义是一个核心问题，需要一种形式化方法来验证一致性。

Method: 基于执行图（traces）提出公理化语义，证明其与操作语义的等价性，将一致性问题转化为事件驱动一致性问题。

Result: 事件驱动一致性问题为NP完全问题，但在无嵌套发布的情况下可多项式时间求解。

Conclusion: 通过公理化语义和复杂度分析，为事件驱动程序的一致性检查提供了理论基础和实用工具。

Abstract: Event-driven programming is a popular paradigm where the flow of execution is
controlled by two features: (1) shared memory and (2) sending and receiving of
messages between multiple handler threads (just called handler). Each handler
has a mailbox (modelled as a queue) for receiving messages, with the constraint
that the handler processes its messages sequentially. Executions of messages by
different handlers may be interleaved. A central problem in this setting is
checking whether a candidate execution is consistent with the semantics of
event-driven programs. In this paper, we propose an axiomatic semantics for
eventdriven programs based on the standard notion of traces (also known as
execution graphs). We prove the equivalence of axiomatic and operational
semantics. This allows us to rephrase the consistency problem axiomatically,
resulting in the event-driven consistency problem: checking whether a given
trace is consistent. We analyze the computational complexity of this problem
and show that it is NP-complete, even when the number of handler threads is
bounded. We then identify a tractable fragment: in the absence of nested
posting, where handlers do not post new messages while processing a message,
consistency checking can be performed in polynomial time. Finally, we implement
our approach in a prototype tool and report on experimental results on a wide
range of benchmarks.

</details>
