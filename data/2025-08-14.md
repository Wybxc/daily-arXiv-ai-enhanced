<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 10]
- [cs.LO](#cs.LO) [Total: 4]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Teaching Code Refactoring Using LLMs](https://arxiv.org/abs/2508.09332)
*Anshul Khairnar,Aarya Rajoju,Edward F. Gehringer*

Main category: cs.SE

TL;DR: LLMs用于实时、上下文感知的反馈，提升软件工程课程中代码重构的教学效果。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如代码审查和静态分析工具）反馈有限且不一致，难以教授复杂代码库的重构。

Method: 通过结构化提示将LLM辅助重构整合到课程项目中，帮助学生识别和解决代码异味。

Result: 初步结果表明，LLMs能弥合理论与实践，加深学生对可维护性和重构原则的理解。

Conclusion: LLMs在代码重构教学中具有潜力，能有效提升学习效果。

Abstract: This Innovative Practice full paper explores how Large Language Models (LLMs)
can enhance the teaching of code refactoring in software engineering courses
through real-time, context-aware feedback. Refactoring improves code quality
but is difficult to teach, especially with complex, real-world codebases.
Traditional methods like code reviews and static analysis tools offer limited,
inconsistent feedback. Our approach integrates LLM-assisted refactoring into a
course project using structured prompts to help students identify and address
code smells such as long methods and low cohesion. Implemented in Spring 2025
in a long-lived OSS project, the intervention is evaluated through student
feedback and planned analysis of code quality improvements. Findings suggest
that LLMs can bridge theoretical and practical learning, supporting a deeper
understanding of maintainability and refactoring principles.

</details>


### [2] [Plug it and Play on Logs: A Configuration-Free Statistic-Based Log Parser](https://arxiv.org/abs/2508.09366)
*Qiaolin Qin,Xingfang Wu,Heng Li,Ettore Merlo*

Main category: cs.SE

TL;DR: PIPLUP是一种新型的基于统计的日志解析器，挑战了语义解析器更优的普遍观点，表现出高准确性和通用性。


<details>
  <summary>Details</summary>
Motivation: 现有基于统计的日志解析器在准确性和通用性上不足，而语义解析器依赖外部知识。PIPLUP旨在证明统计方法也能高效且实用。

Method: PIPLUP消除了对常量令牌位置的预设，采用数据不敏感参数，实现“即插即用”。

Result: 在实验中，PIPLUP优于现有统计解析器（如Drain），并与最佳无监督语义解析器（LUNAR）竞争。

Conclusion: PIPLUP高效、低耗且隐私友好，适用于实际场景，尤其在成本和隐私敏感的环境中。

Abstract: Log parsing is an essential task in log analysis, and many tools have been
designed to accomplish it. Existing log parsers can be categorized into
statistic-based and semantic-based approaches. In comparison to semantic-based
parsers, existing statistic-based parsers tend to be more efficient, require
lower computational costs, and be more privacy-preserving thanks to on-premise
deployment, but often fall short in their accuracy (e.g., grouping or parsing
accuracy) and generalizability. Therefore, it became a common belief that
statistic-based parsers cannot be as effective as semantic-based parsers since
the latter could take advantage of external knowledge supported by pretrained
language models. Our work, however, challenges this belief with a novel
statistic-based parser, PIPLUP. PIPLUP eliminates the pre-assumption of the
position of constant tokens for log grouping and relies on data-insensitive
parameters to overcome the generalizability challenge, allowing "plug and play"
on given log files. According to our experiments on an open-sourced large log
dataset, PIPLUP shows promising accuracy and generalizability with the
data-insensitive default parameter set. PIPLUP not only outperforms the
state-of-the-art statistic-based log parsers, Drain and its variants, but also
obtains a competitive performance compared to the best unsupervised
semantic-based log parser (i.e., LUNAR). Further, PIPLUP exhibits low time
consumption without GPU acceleration and external API usage; our simple,
efficient, and effective approach makes it more practical in real-world
adoptions, especially when costs and privacy are of major concerns.

</details>


### [3] [Your Coding Intent is Secretly in the Context and You Should Deliberately Infer It Before Completion](https://arxiv.org/abs/2508.09537)
*Yanzhou Li,Tianlin Li,Yiran Zhang,Shangqing Liu,Aishan Liu,Yang Liu*

Main category: cs.SE

TL;DR: 论文提出了一种三阶段方法，通过意图推断、交互式精炼和代码生成，提升大语言模型在无注释代码库中的功能补全能力，实验显示性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现实代码库中常缺乏显式注释（如文档字符串），导致大语言模型性能下降，需解决无注释场景下的功能补全问题。

Method: 三阶段流程：1) 意图推断，从前置代码中提取功能线索；2) 交互式精炼，开发者可选或编辑候选意图；3) 基于最终意图生成代码。

Result: 在DevEval和ComplexCodeEval上，方法显著提升性能，相对增益超20%，交互式精炼进一步优化结果。

Conclusion: 三阶段方法有效解决了无注释代码补全问题，交互式精炼增强了意图匹配的准确性。

Abstract: Large Language Models (LLMs) are increasingly used for function completion in
repository-scale codebases. Prior studies demonstrate that when explicit
instructions--such as docstrings--are provided, these models can generate
highly accurate implementations. However, in real-world repositories, such
annotations are frequently absent, and performance drops substantially without
them. To address this gap, we frame the task as a three-stage process. The
first stage focuses on intent inference, where the model analyzes the code
preceding the target function to uncover cues about the desired functionality.
Such preceding context often encodes subtle but critical information, and we
design a reasoning-based prompting framework to guide the LLM through
step-by-step extraction and synthesis of these signals before any code is
generated. The second stage introduces an optional interactive refinement
mechanism to handle cases where preceding context alone is insufficient for
intent recovery. In this stage, the model proposes a small set of candidate
intentions, enabling the developer to select or edit them so that the inferred
intent closely matches the actual requirement. Finally, in the third stage, the
LLM generates the target function conditioned on the finalized intent. To
support this pipeline, we curate a dataset of 40,000 examples annotated with
intermediate reasoning traces and corresponding docstrings. Extensive
experiments on DevEval and ComplexCodeEval show that our approach consistently
boosts multiple LLMs, achieving over 20\% relative gains in both
reference-based and execution-based metrics, with the interactive refinement
stage delivering additional improvements beyond these gains.

</details>


### [4] [ReqInOne: A Large Language Model-Based Agent for Software Requirements Specification Generation](https://arxiv.org/abs/2508.09648)
*Taohong Zhu,Lucas C. Cordeiro,Youcheng Sun*

Main category: cs.SE

TL;DR: ReqInOne是一个基于LLM的代理，通过模块化设计将自然语言转换为结构化SRS，相比现有方法更准确且结构清晰。


<details>
  <summary>Details</summary>
Motivation: 手动编写SRS耗时且易产生歧义，现有自动化方法依赖人工分析或存在幻觉和可控性问题。

Method: ReqInOne将SRS生成分解为摘要、需求提取和需求分类三个任务，每个任务使用定制提示模板。

Result: 实验表明ReqInOne生成的SRS更准确且结构更好，其需求分类组件性能优于现有模型。

Conclusion: 模块化设计使ReqInOne在SRS生成中表现优异，为自动化需求工程提供了新思路。

Abstract: Software Requirements Specification (SRS) is one of the most important
documents in software projects, but writing it manually is time-consuming and
often leads to ambiguity. Existing automated methods rely heavily on manual
analysis, while recent Large Language Model (LLM)-based approaches suffer from
hallucinations and limited controllability. In this paper, we propose ReqInOne,
an LLM-based agent that follows the common steps taken by human requirements
engineers when writing an SRS to convert natural language into a structured
SRS. ReqInOne adopts a modular architecture by decomposing SRS generation into
three tasks: summary, requirement extraction, and requirement classification,
each supported by tailored prompt templates to improve the quality and
consistency of LLM outputs.
  We evaluate ReqInOne using GPT-4o, LLaMA 3, and DeepSeek-R1, and compare the
generated SRSs against those produced by the holistic GPT-4-based method from
prior work as well as by entry-level requirements engineers. Expert evaluations
show that ReqInOne produces more accurate and well-structured SRS documents.
The performance advantage of ReqInOne benefits from its modular design, and
experimental results further demonstrate that its requirement classification
component achieves comparable or even better results than the state-of-the-art
requirement classification model.

</details>


### [5] [DeputyDev -- AI Powered Developer Assistant: Breaking the Code Review Logjam through Contextual AI to Boost Developer Productivity](https://arxiv.org/abs/2508.09676)
*Vishal Khare,Vijay Saini,Deepak Sharma,Anand Kumar,Ankit Rana,Anshul Yadav*

Main category: cs.SE

TL;DR: 研究探讨了AI代码审查助手DeputyDev在提升软件开发效率方面的实施与效果，显著减少了代码审查时间。


<details>
  <summary>Details</summary>
Motivation: 解决代码审查过程中的低效问题，如耗时长、反馈不一致和质量不足。

Method: 开发DeputyDev的自动化代码审查功能，并通过双盲A/B实验评估其对审查时间的影响。

Result: DeputyDev显著减少了每PR（23.09%）和每行代码（40.13%）的审查时间。

Conclusion: DeputyDev成功提升了开发流程效率，并已作为SaaS解决方案推广至外部公司。

Abstract: This study investigates the implementation and efficacy of DeputyDev, an
AI-powered code review assistant developed to address inefficiencies in the
software development process. The process of code review is highly inefficient
for several reasons, such as it being a time-consuming process, inconsistent
feedback, and review quality not being at par most of the time. Using our
telemetry data, we observed that at TATA 1mg, pull request (PR) processing
exhibits significant inefficiencies, with average pick-up and review times of
73 and 82 hours, respectively, resulting in a 6.2 day closure cycle. The review
cycle was marked by prolonged iterative communication between the reviewing and
submitting parties. Research from the University of California, Irvine
indicates that interruptions can lead to an average of 23 minutes of lost
focus, critically affecting code quality and timely delivery. To address these
challenges, we developed DeputyDev's PR review capabilities by providing
automated, contextual code reviews. We conducted a rigorous double-controlled
A/B experiment involving over 200 engineers to evaluate DeputyDev's impact on
review times. The results demonstrated a statistically significant reduction in
both average per PR (23.09%) and average per-line-of-code (40.13%) review
durations. After implementing safeguards to exclude outliers, DeputyDev has
been effectively rolled out across the entire organisation. Additionally, it
has been made available to external companies as a Software-as-a-Service (SaaS)
solution, currently supporting the daily work of numerous engineering
professionals. This study explores the implementation and effectiveness of
AI-assisted code reviews in improving development workflow timelines and code.

</details>


### [6] [Inclusive Employment Pathways: Career Success Factors for Autistic Individuals in Software Engineering](https://arxiv.org/abs/2508.09680)
*Orvila Sarker,Mona Jamshaid,M. Ali Babar*

Main category: cs.SE

TL;DR: 论文总结了自闭症个体在ICT领域的潜力与挑战，提出了18个成功因素，分为四类，为教育机构、雇主和工具开发者提供了基于证据的建议。


<details>
  <summary>Details</summary>
Motivation: 自闭症个体在ICT领域有独特优势，但面临多重障碍。研究旨在填补从教育到职场包容的完整路径的知识空白。

Method: 通过系统综述30项研究，识别出18个成功因素，分为四类主题。

Result: 提出了四类成功因素，包括教育、职业培训、工作环境和辅助技术，并给出了具体建议。

Conclusion: 研究为提升自闭症个体在软件工程领域的包容性提供了实用指南，强调环境设计的重要性。

Abstract: Research has highlighted the valuable contributions of autistic individuals
in the Information and Communication Technology (ICT) sector, particularly in
areas such as software development, testing, and cybersecurity. Their strengths
in information processing, attention to detail, innovative thinking, and
commitment to high-quality outcomes in the ICT domain are well-documented.
However, despite their potential, autistic individuals often face barriers in
Software Engineering (SE) roles due to a lack of personalised tools, complex
work environments, non-inclusive recruitment practices, limited co-worker
support, challenging social dynamics and so on. Motivated by the ethical
framework of the neurodiversity movement and the success of pioneering
initiatives like the Dandelion program, corporate Diversity, Equity, and
Inclusion (DEI) in the ICT sector has increasingly focused on autistic talent.
This movement fundamentally reframes challenges not as individual deficits but
as failures of environments designed for a neurotypical majority. Despite this
progress, there is no synthesis of knowledge reporting the full pathway from
software engineering education through to sustainable workplace inclusion. To
address this, we conducted a Systematic Review of 30 studies and identified 18
success factors grouped into four thematic categories: (1) Software Engineering
Education, (2) Career and Employment Training, (3) Work Environment, and (4)
Tools and Assistive Technologies. Our findings offer evidence-based
recommendations for educational institutions, employers, organisations, and
tool developers to enhance the inclusion of autistic individuals in SE. These
include strategies for inclusive meeting and collaboration practices,
accessible and structured work environments, clear role and responsibility
definitions, and the provision of tailored workplace accommodations.

</details>


### [7] [LibRec: Benchmarking Retrieval-Augmented LLMs for Library Migration Recommendations](https://arxiv.org/abs/2508.09791)
*Junxiao Han,Yarong Wang,Xiaodong Gu,Cuiyun Gao,Yao Wan,Song Han,David Lo,Shuiguang Deng*

Main category: cs.SE

TL;DR: LibRec是一个结合LLMs和RAG技术的框架，用于自动化推荐替代库，并通过上下文学习提取迁移意图以提高推荐准确性。评估使用LibEval基准，包含2,888条迁移记录。


<details>
  <summary>Details</summary>
Motivation: 解决库迁移推荐任务中自动化推荐和意图提取的挑战。

Method: 结合LLMs和RAG技术，利用上下文学习提取迁移意图，并通过LibEval基准评估。

Result: 评估了10种流行LLMs，分析了框架关键组件、提示策略和意图类型的影响，并进行了失败案例分析。

Conclusion: LibRec在库迁移推荐任务中表现出色，LibEval为评估提供了有效基准。

Abstract: In this paper, we propose LibRec, a novel framework that integrates the
capabilities of LLMs with retrieval-augmented generation(RAG) techniques to
automate the recommendation of alternative libraries. The framework further
employs in-context learning to extract migration intents from commit messages
to enhance the accuracy of its recommendations. To evaluate the effectiveness
of LibRec, we introduce LibEval, a benchmark designed to assess the performance
in the library migration recommendation task. LibEval comprises 2,888 migration
records associated with 2,368 libraries extracted from 2,324 Python
repositories. Each migration record captures source-target library pairs, along
with their corresponding migration intents and intent types. Based on LibEval,
we evaluated the effectiveness of ten popular LLMs within our framework,
conducted an ablation study to examine the contributions of key components
within our framework, explored the impact of various prompt strategies on the
framework's performance, assessed its effectiveness across various intent
types, and performed detailed failure case analyses.

</details>


### [8] [Fast and Accurate Heuristics for Bus-Factor Estimation](https://arxiv.org/abs/2508.09828)
*Sebastiano Antonio Piccolo*

Main category: cs.SE

TL;DR: 该论文提出了两种基于图剥离的启发式方法（最小覆盖和最大覆盖），用于近似计算软件项目的总线因子（bus-factor），显著优于传统方法，并在大规模图上验证了其高效性和准确性。


<details>
  <summary>Details</summary>
Motivation: 总线因子是衡量项目关键知识或功能风险的重要指标，但精确计算是NP难问题，现有方法难以扩展到大系统。

Method: 将软件项目建模为开发者和任务的双分图，提出两种基于迭代图剥离的启发式方法（最小覆盖和最大覆盖）。

Result: 在1000多个合成幂律图上验证，新方法比传统方法更准确且可扩展到百万级节点图。

Conclusion: 提出的启发式方法不仅更准确，还能适应开发任务分配图的结构变化，已开源实现以支持研究和应用。

Abstract: The bus-factor is a critical risk indicator that quantifies how many key
contributors a project can afford to lose before core knowledge or
functionality is compromised. Despite its practical importance, accurately
computing the bus-factor is NP-Hard under established formalizations, making
scalable analysis infeasible for large software systems.
  In this paper, we model software projects as bipartite graphs of developers
and tasks and propose two novel approximation heuristics, Minimum Coverage and
Maximum Coverage, based on iterative graph peeling, for two influential
bus-factor formalizations. Our methods significantly outperform the widely
adopted degree-based heuristic, which we show can yield severely inflated
estimates.
  We conduct a comprehensive empirical evaluation on over $1\,000$ synthetic
power-law graphs and demonstrate that our heuristics provide tighter estimates
while scaling to graphs with millions of nodes and edges in minutes. Our
results reveal that the proposed heuristics are not only more accurate but also
robust to structural variations in developer-task assignment graph. We release
our implementation as open-source software to support future research and
practical adoption.

</details>


### [9] [Exploring the Potential of Large Language Models in Fine-Grained Review Comment Classification](https://arxiv.org/abs/2508.09832)
*Linh Nguyen,Chunhua Liu,Hong Yi Lin,Patanamon Thongtanunam*

Main category: cs.SE

TL;DR: 本文探讨了使用大型语言模型（LLMs）对代码审查评论进行分类的潜力，结果显示LLMs在分类性能上优于现有深度学习方法，尤其在低频类别中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有基于监督学习的代码审查评论分类方法需要大量人工标注，限制了其可扩展性。本文旨在探索LLMs是否能提供更高效的解决方案。

Method: 通过评估LLMs对17类代码审查评论的分类性能，并与现有深度学习方法对比。

Result: LLMs在分类性能上优于现有方法，尤其在低频类别中表现更优，且无需依赖特定的小规模训练数据分布。

Conclusion: LLMs为代码审查分析提供了可扩展的解决方案，有助于提升代码审查效率。

Abstract: Code review is a crucial practice in software development. As code review
nowadays is lightweight, various issues can be identified, and sometimes, they
can be trivial. Research has investigated automated approaches to classify
review comments to gauge the effectiveness of code reviews. However, previous
studies have primarily relied on supervised machine learning, which requires
extensive manual annotation to train the models effectively. To address this
limitation, we explore the potential of using Large Language Models (LLMs) to
classify code review comments. We assess the performance of LLMs to classify 17
categories of code review comments. Our results show that LLMs can classify
code review comments, outperforming the state-of-the-art approach using a
trained deep learning model. In particular, LLMs achieve better accuracy in
classifying the five most useful categories, which the state-of-the-art
approach struggles with due to low training examples. Rather than relying
solely on a specific small training data distribution, our results show that
LLMs provide balanced performance across high- and low-frequency categories.
These results suggest that the LLMs could offer a scalable solution for code
review analytics to improve the effectiveness of the code review process.

</details>


### [10] [An Empirical Study of CGO Usage in Go Projects -- Distribution, Purposes, Patterns and Critical Issues](https://arxiv.org/abs/2508.09875)
*Jinbao Chen,Boyao Ding,Yu Zhang,Qingwei Li,Fugen Tang*

Main category: cs.SE

TL;DR: 本文通过实证研究分析了Go语言中CGO的使用情况，揭示了其分布、模式、目的及关键问题，并提出了改进工具链的建议。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视了Go语言中新兴的FFI工具CGO，而CGO可能带来独特风险，因此需要深入研究其使用情况和潜在问题。

Method: 研究分析了920个开源Go项目，开发了CGOAnalyzer工具来识别和量化CGO相关特征。

Result: 研究发现11.3%的项目使用CGO，主要用于系统级交互和性能优化，同时发现了19种CGO相关问题，包括可能导致运行时崩溃的关键问题。

Conclusion: 研究为开发者和Go团队提供了宝贵见解，提出了临时和永久解决方案，提升了开发效率和工具链的稳健性。

Abstract: Multilingual software development integrates multiple languages into a single
application, with the Foreign Function Interface (FFI) enabling seamless
interaction. While FFI boosts efficiency and extensibility, it also introduces
risks. Existing studies focus on FFIs in languages like Python and Java,
neglecting CGO, the emerging FFI in Go, which poses unique risks.
  To address these concerns, we conduct an empirical study of CGO usage across
920 open-source Go projects. Our study aims to reveal the distribution,
patterns, purposes, and critical issues associated with CGO, offering insights
for developers and the Go team. We develop CGOAnalyzer, a tool to efficiently
identify and quantify CGO-related features. Our findings reveal that: (1) 11.3%
of analyzed Go projects utilize CGO, with usage concentrated in a subset of
projects; (2) CGO serves 4 primary purposes, including system-level
interactions and performance optimizations, with 15 distinct usage patterns
observed; (3) 19 types of CGO-related issues exist, including one critical
issue involving unnecessary pointer checks that pose risks of runtime crashes
due to limitations in the current Go compilation toolchain; (4) a temporary
solution reduces unnecessary pointer checks, mitigating crash risks, and (5) we
submitted a proposal to improve the Go toolchain for a permanent fix, which has
been grouped within an accepted proposal for future resolution. Our findings
provide valuable insights for developers and the Go team, enhancing development
efficiency and reliability while improving the robustness of the Go toolchain.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [11] [TPTP World Infrastructure for Non-classical Logics](https://arxiv.org/abs/2508.09318)
*Alexander Steen,Geoff Sutcliffe*

Main category: cs.LO

TL;DR: 本文概述了TPTP World基础设施在非经典逻辑中的扩展与应用，重点介绍了语言扩展、问题与解决方案及工具支持，并以量化正规多模态逻辑为例详细说明。


<details>
  <summary>Details</summary>
Motivation: TPTP World作为自动化定理证明（ATP）系统的核心基础设施，已支持非经典逻辑，本文旨在全面介绍其在这一领域的扩展与应用。

Method: 通过非经典语言扩展、问题与解决方案的设计，以及工具支持的实现，详细展示了TPTP World在非经典逻辑中的应用。

Result: TPTP World成功扩展至非经典逻辑领域，提供了完整的语言和工具支持，并以量化正规多模态逻辑为例验证了其有效性。

Conclusion: TPTP World为非经典逻辑的ATP研究提供了强大支持，未来可进一步扩展至更多逻辑类型。

Abstract: The TPTP World is the well established infrastructure that supports research,
development, and deployment of Automated Theorem Proving (ATP) systems. The
TPTP World supports a range of classical logics, and since release v9.0.0 has
supported non-classical logics. This paper provides a self-contained
comprehensive overview of the TPTP World infrastructure for ATP in
non-classical logics: the non-classical language extension, problems and
solutions, and tool support. A detailed description of use of the
infrastructure for quantified normal multi-modal logic is given.

</details>


### [12] [On Middle Grounds for Preference Statements](https://arxiv.org/abs/2508.09553)
*Anne-Marie George,Ana Ozaki*

Main category: cs.LO

TL;DR: 论文研究了在群体决策中如何用逻辑表达冲突意见，并提出了一种形式化的中间立场概念。通过偏好陈述的实例化，证明了中间立场的存在性和唯一性，并提供了相关算法。


<details>
  <summary>Details</summary>
Motivation: 群体决策中常遇到冲突意见，需要一种逻辑表达方式和中间立场的概念来解决分歧。

Method: 基于偏好陈述（如“我偏好a胜于b”）的形式化框架，结合分层和词典序模型，研究中间立场的理论性质和算法。

Result: 证明中间立场可能不存在或不唯一，并提供了判断存在性和寻找中间立场的算法。

Conclusion: 该研究为群体决策中的冲突解决提供了理论支持和实用工具。

Abstract: In group decisions or deliberations, stakeholders are often confronted with
conflicting opinions. We investigate a logic-based way of expressing such
opinions and a formal general notion of a middle ground between stakeholders.
Inspired by the literature on preferences with hierarchical and lexicographic
models, we instantiate our general framework to the case where stakeholders
express their opinions using preference statements of the form I prefer 'a' to
'b', where 'a' and 'b' are alternatives expressed over some attributes, e.g.,
in a trolley problem, one can express I prefer to save 1 adult and 1 child to 2
adults (and 0 children). We prove theoretical results on the existence and
uniqueness of middle grounds. In particular, we show that, for preference
statements, middle grounds may not exist and may not be unique. We also provide
algorithms for deciding the existence and finding middle grounds.

</details>


### [13] [Short proofs without interference](https://arxiv.org/abs/2508.09851)
*Adrian Rebola-Pardo*

Main category: cs.LO

TL;DR: 提出了一种消除SAT求解中干扰现象的框架，基于命题动态逻辑，保留了干扰证明的表达能力，并初步构建了类似RUP的决策过程。


<details>
  <summary>Details</summary>
Motivation: 干扰现象在SAT求解的证明系统中既反直觉又影响开发，现有系统均存在此问题，需解决。

Method: 基于命题动态逻辑提出框架，消除干扰，并构建类似RUP的决策过程。

Result: 框架成功消除干扰，同时保留原有表达能力，为有效证明检查方法奠定基础。

Conclusion: 新框架解决了干扰问题，为SAT求解的证明系统提供了更优方案。

Abstract: Interference is a phenomenon on proof systems for SAT solving that is both
counter-intuitive and bothersome when developing proof-logging techniques.
However, all existing proof systems that can produce short proofs for all
inprocessing techniques deployed by SAT present this feature. Based on insights
from propositional dynamic logic, we propose a framework that eliminates
interference while preserving the same expressive power of interference-based
proofs. Furthermore, we propose a first building blocks towards RUP-like
decision procedures for our dynamic logic-based frameworks, which are essential
to developing effective proof checking methods.

</details>


### [14] [Efficient Volume Computation for SMT Formulas](https://arxiv.org/abs/2508.09934)
*Arijit Shaw,Uddalok Sarkar,Kuldeep S. Meel*

Main category: cs.LO

TL;DR: 论文提出了一种高效算法ttc，扩展了SMT求解器在计算线性实数算术公式可满足区域体积方面的能力，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: SMT在自动推理问题中表现强大，但传统方法无法处理定量问题，如计算可满足区域的体积。这在软件验证、信息物理系统和神经网络等领域具有重要意义。

Method: ttc算法将SMT线性实数算术公式的解空间分解为重叠凸多面体的并集，计算其体积并求并集。结合了流模式集合并、体积计算算法和AllSAT技术。

Result: 实验评估表明，ttc算法在性能上显著优于现有最先进方法。

Conclusion: ttc算法为SMT求解器提供了高效的体积计算能力，解决了定量验证问题，具有广泛的应用前景。

Abstract: Satisfiability Modulo Theory (SMT) has recently emerged as a powerful tool
for solving various automated reasoning problems across diverse domains. Unlike
traditional satisfiability methods confined to Boolean variables, SMT can
reason on real-life variables like bitvectors, integers, and reals. A natural
extension in this context is to ask quantitative questions. One such query in
the SMT theory of Linear Real Arithmetic (LRA) is computing the volume of the
entire satisfiable region defined by SMT formulas. This problem is important in
solving different quantitative verification queries in software verification,
cyber-physical systems, and neural networks, to mention a few.
  We introduce ttc, an efficient algorithm that extends the capabilities of SMT
solvers to volume computation. Our method decomposes the solution space of SMT
Linear Real Arithmetic formulas into a union of overlapping convex polytopes,
then computes their volumes and calculates their union. Our algorithm builds on
recent developments in streaming-mode set unions, volume computation
algorithms, and AllSAT techniques. Experimental evaluations demonstrate
significant performance improvements over existing state-of-the-art approaches.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [15] [Invertible Syntax without the Tuples (Functional Pearl)](https://arxiv.org/abs/2508.09856)
*Mathieu Boespflug,Arnaud Spiwack*

Main category: cs.PL

TL;DR: 本文主张Danvy的延续传递风格（CPS）在解析和打印结构化数据中仍具价值，提出了三种基于CPS的解决方案。


<details>
  <summary>Details</summary>
Motivation: 探讨延续传递风格在解析和打印结构化数据中的适用性，证明其优于依赖类型和单子聚合方法。

Method: 提出三种基于延续传递风格的解决方案，逐步提升表达能力。

Result: 展示了CPS方法在解析和打印结构化数据中的有效性，支持列表和树等归纳结构。

Conclusion: 延续传递风格在通用场景中仍具优势，是依赖类型和单子聚合的可行替代方案。

Abstract: In the seminal paper Functional unparsing, Olivier Danvy used continuation
passing to reanalyse printf-like format strings as combinators. In the
intervening decades, the conversation shifted towards a concurrent line of work
-- applicative, monadic or arrow-based combinator libraries -- in an effort to
find combinators for invertible syntax descriptions that simultaneously
determine a parser as well as a printer, and with more expressive power, able
to handle inductive structures such as lists and trees. Along the way,
continuation passing got lost. This paper argues that Danvy's insight remains
as relevant to the general setting as it was to the restricted setting of his
original paper. Like him, we present three solutions that exploit
continuation-passing style as an alternative to both dependent types and
monoidal aggregation via nested pairs, in our case to parse and print
structured data with increasing expressive power.

</details>
