<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 13]
- [cs.PL](#cs.PL) [Total: 4]
- [cs.LO](#cs.LO) [Total: 4]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [AutoCode: LLMs as Problem Setters for Competitive Programming](https://arxiv.org/abs/2510.12803)
*Shang Zhou,Zihan Zheng,Kaiyuan Liu,Zeyu Shen,Zerui Cheng,Zexing Chen,Hansen He,Jianzhu Yao,Huanzhi Mao,Qiuyang Mang,Tianfu Fu,Beichen Li,Dongruixuan Li,Wenhao Chai,Zhuang Liu,Aleksandra Korolova,Peter Henderson,Natasha Jaques,Pramod Viswanath,Saining Xie,Jingbo Shang*

Main category: cs.SE

TL;DR: AutoCode是一个使用多轮验证生成竞赛级编程题目和测试用例的系统，在保留问题上测试套件与官方评判的一致性接近99%，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 编写竞赛编程题目需要设置约束条件、输入分布和边界情况，针对特定算法，并校准超出大多数参赛者能力的复杂度，这使其成为测试大语言模型通用能力的理想场景。

Method: AutoCode使用多轮验证生成题目陈述和测试用例，从随机种子问题创建新颖变体，并通过交叉验证生成的参考和暴力解决方案与测试用例来过滤有问题的题目。

Result: 在保留问题上，AutoCode测试套件与官方评判的一致性达到接近99%，显著优于HardTests等现有方法（低于81%）。系统成功生成了被顶级程序员评为竞赛质量的新颖题目。

Conclusion: AutoCode能够可靠地生成竞赛级编程题目，通过多轮验证和交叉验证确保高质量，验证了大语言模型在复杂编程任务中的能力。

Abstract: Writing competitive programming problems is exacting. Authors must: set
constraints, input distributions, and edge cases that rule out shortcuts;
target specific algorithms (e.g., max-flow, dynamic programming, data
structures); and calibrate complexity beyond the reach of most competitors. We
argue that this makes for an ideal test of general large language model
capabilities and study whether they can do this reliably. We introduce
AutoCode, which uses multiple rounds of validation to yield competition-grade
problem statements and test cases. On held-out problems, AutoCode test suites
approach 99% consistency with official judgments, a significant improvement
over current state-of-the-art methods like HardTests, which achieve less than
81%. Furthermore, starting with a random seed problem, AutoCode can create
novel variants with reference and brute-force solutions. By cross-verifying
these generated solutions against test cases, we can further filter out
malformed problems. Our system ensures high correctness, as verified by human
experts. AutoCode successfully produces novel problems judged by
Grandmaster-level (top 0.3%) competitive programmers to be of contest quality.

</details>


### [2] [SpareCodeSearch: Searching for Code Context When You Have No Spare GPU](https://arxiv.org/abs/2510.12948)
*Minh Nguyen*

Main category: cs.SE

TL;DR: 本文证明在大型代码库中使用关键词搜索足以检索相关代码上下文，无需GPU资源，在代码补全任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成框架通常使用语义搜索，需要大量计算资源，难以集成到轻量级应用中如IDE代码补全。

Method: 使用关键词搜索替代语义搜索来检索相关代码上下文，构建输入提示。

Result: 在Code Context Competition基准测试中，Kotlin和Python轨道分别达到0.748和0.725的chRF分数。

Conclusion: 关键词搜索是检索代码上下文的有效方法，可在资源受限环境中替代语义搜索。

Abstract: Retrieval-Augmented Generation (RAG) frameworks aim to enhance Code Language
Models (CLMs) by including another module for retrieving relevant context to
construct the input prompt. However, these retrieval modules commonly use
semantic search, requiring substantial computational resources for training and
hosting these embedded models, making them infeasible to integrate into
lightweight applications such as in-IDE AI-based code completion. In this
solution paper, we prove that using keyword-search is sufficient to retrieve
relevant and useful code context inside large codebases, without the need for
extensive GPU resources. The usefulness of code contexts found by our solution
is demonstrated through their completion results on the Code Context
Competition's benchmark, reaching 0.748 and 0.725 chRF scores on Kotlin and
Python tracks, respectively.

</details>


### [3] [ADPerf: Investigating and Testing Performance in Autonomous Driving Systems](https://arxiv.org/abs/2510.13078)
*Tri Minh-Triet Pham,Diego Elias Costa,Weiyi Shang,Jinqiu Yang*

Main category: cs.SE

TL;DR: ADPerf工具用于测试自动驾驶系统中障碍物检测模块的性能，通过生成真实点云数据测试用例来暴露检测延迟增加的问题，评估延迟对轨迹预测模块的影响。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统依赖多传感器和深度学习模型进行障碍物检测，但检测延迟及其对点云数据变化的弹性尚未被充分理解，这关系到系统的安全性和有效性。

Method: 对Apollo和Autoware两个工业级自动驾驶系统进行障碍物检测模块性能测量和建模，开发ADPerf工具生成真实点云数据测试用例来暴露检测延迟问题。

Result: 评估显示3D障碍物检测模块可能成为自动驾驶系统延迟增加的主要瓶颈，这种不利影响会进一步传播到其他模块，降低系统整体可靠性。

Conclusion: 需要进行障碍物检测组件的性能测试，特别是3D障碍物检测，因为它们可能成为自动驾驶系统延迟增加的主要瓶颈，并影响其他模块的可靠性。

Abstract: Obstacle detection is crucial to the operation of autonomous driving systems,
which rely on multiple sensors, such as cameras and LiDARs, combined with code
logic and deep learning models to detect obstacles for time-sensitive
decisions. Consequently, obstacle detection latency is critical to the safety
and effectiveness of autonomous driving systems. However, the latency of the
obstacle detection module and its resilience to various changes in the LiDAR
point cloud data are not yet fully understood. In this work, we present the
first comprehensive investigation on measuring and modeling the performance of
the obstacle detection modules in two industry-grade autonomous driving
systems, i.e., Apollo and Autoware. Learning from this investigation, we
introduce ADPerf, a tool that aims to generate realistic point cloud data test
cases that can expose increased detection latency. Increasing latency decreases
the availability of the detected obstacles and stresses the capabilities of
subsequent modules in autonomous driving systems, i.e., the modules may be
negatively impacted by the increased latency in obstacle detection.
  We applied ADPerf to stress-test the performance of widely used 3D obstacle
detection modules in autonomous driving systems, as well as the propagation of
such tests on trajectory prediction modules. Our evaluation highlights the need
to conduct performance testing of obstacle detection components, especially 3D
obstacle detection, as they can be a major bottleneck to increased latency of
the autonomous driving system. Such an adverse outcome will also further
propagate to other modules, reducing the overall reliability of autonomous
driving systems.

</details>


### [4] [TRUSTVIS: A Multi-Dimensional Trustworthiness Evaluation Framework for Large Language Models](https://arxiv.org/abs/2510.13106)
*Ruoyu Sun,Da Song,Jiayang Song,Yuheng Huang,Lei Ma*

Main category: cs.SE

TL;DR: TRUSTVIS是一个自动化评估框架，通过交互式界面和可视化工具全面评估大语言模型的信任度，特别是安全性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在NLP应用中的普及，其信任度问题（尤其是安全性和鲁棒性）日益突出，需要系统化的评估方法。

Method: 集成AutoDAN等扰动方法，采用多数投票机制，通过交互式用户界面提供直观的可视化评估指标。

Result: 在Vicuna-7b、Llama2-7b和GPT-3.5等模型上的初步案例研究表明，该框架能有效识别安全性和鲁棒性漏洞。

Conclusion: TRUSTVIS不仅提供可靠的评估结果，还使复杂的评估过程对用户更加友好，有助于针对性的模型改进。

Abstract: As Large Language Models (LLMs) continue to revolutionize Natural Language
Processing (NLP) applications, critical concerns about their trustworthiness
persist, particularly in safety and robustness. To address these challenges, we
introduce TRUSTVIS, an automated evaluation framework that provides a
comprehensive assessment of LLM trustworthiness. A key feature of our framework
is its interactive user interface, designed to offer intuitive visualizations
of trustworthiness metrics. By integrating well-known perturbation methods like
AutoDAN and employing majority voting across various evaluation methods,
TRUSTVIS not only provides reliable results but also makes complex evaluation
processes accessible to users. Preliminary case studies on models like
Vicuna-7b, Llama2-7b, and GPT-3.5 demonstrate the effectiveness of our
framework in identifying safety and robustness vulnerabilities, while the
interactive interface allows users to explore results in detail, empowering
targeted model improvements. Video Link: https://youtu.be/k1TrBqNVg8g

</details>


### [5] [Isolating Compiler Bugs through Compilation Steps Analysis](https://arxiv.org/abs/2510.13128)
*Yujie Liu,Mingxuan Zhu,Shengyu Cheng,Dan Hao*

Main category: cs.SE

TL;DR: CompSCAN是一种新颖的编译器bug隔离技术，通过分析编译步骤序列来识别导致bug的编译步骤和代码元素，在真实LLVM和GCC bug上表现出优于现有技术的效果和效率。


<details>
  <summary>Details</summary>
Motivation: 编译器对软件系统至关重要，但其bug会传播到依赖软件中。现有技术主要通过变异编译输入生成测试用例，但缺乏对内部步骤的因果分析，限制了有效性。

Method: CompSCAN采用三阶段过程：(1)提取导致原始失败的编译步骤序列，(2)识别bug导致步骤并收集对应的编译器代码元素，(3)计算每个代码元素的可疑分数并输出可疑排名列表。

Result: 在185个真实LLVM和GCC bug上评估，CompSCAN在Top-1/3/5/10排名中分别成功隔离50、85、100和123个bug，相比ETEM和ODFL分别有44.51%/50.18%/36.24%/24.49%和31.58%/49.12%/44.93%/21.78%的相对改进，且运行速度更快。

Conclusion: CompSCAN通过分析编译步骤序列有效提升了编译器bug隔离的效果和效率，优于现有最先进技术。

Abstract: Compilers are essential to software systems, and their bugs can propagate to
dependent software. Ensuring compiler correctness is critical. However,
isolating compiler bugs remains challenging due to the internal complexity of
compiler execution. Existing techniques primarily mutate compilation inputs to
generate passing and failing tests, but often lack causal analysis of internal
steps, limiting their effectiveness.
  To address this limitation, we propose CompSCAN, a novel compiler bug
isolation technique that applies analysis over the sequence of compilation
steps. CompSCAN follows a three-stage process: (1) extracting the array of
compilation steps that leads to the original failure, (2) identifying
bug-causing steps and collecting corresponding compiler code elements, and (3)
calculating suspicious scores for each code element and outputting a suspicious
ranking list as the bug isolation result.
  We evaluate CompSCAN on 185 real-world LLVM and GCC bugs. Results show that
CompSCAN outperforms state-of-the-art techniques in both effectiveness and
efficiency. CompSCAN successfully isolates 50, 85, 100, and 123 bugs within the
Top-1/3/5/10 ranks, respectively. Compared with ETEM and ODFL, two
state-of-the-art compiler bug isolation techniques, CompSCAN achieves relative
improvements of 44.51% / 50.18% / 36.24% / 24.49% over ETEM, and 31.58% /
49.12% / 44.93% / 21.78% over ODFL on those metrics. Moreover, CompSCAN runs
faster on average per bug than both baselines.

</details>


### [6] [GRACE: Globally-Seeded Representation-Aware Cluster-Specific Evolution for Compiler Auto-Tuning](https://arxiv.org/abs/2510.13176)
*Haolin Pan,Chao Zha,Jinyuan Dong,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: GRACE是一个编译器自动调优框架，通过利用pass协同效应和对比学习来减少搜索空间，在LLVM IR指令数优化上实现了显著的性能提升，平均减少10%以上的指令数，同时保持每程序少于1秒的调优时间。


<details>
  <summary>Details</summary>
Motivation: 编译器pass选择和阶段排序对程序性能优化至关重要，但传统启发式方法效果不佳，迭代编译搜索成本过高，机器学习方法泛化能力有限，需要一种既高效又能良好泛化的自动调优方法。

Method: GRACE框架通过pass协同效应和加权评分生成高质量候选序列和pass池，使用对比学习和基于pass序列的数据增强创建程序嵌入，进行相似性感知聚类，在聚类内使用进化搜索生成核心集，最后通过轻量级技术选择和优化最佳序列。

Result: 在7个不同数据集上的实验表明，GRACE相比opt -Oz平均减少LLVM IR指令数10.09%(LLVM 10.0.0)和10.19%(LLVM 18.1.6)，平均调优时间每程序少于1秒。

Conclusion: GRACE在编译器自动调优方面实现了最先进的性能，通过有效的搜索空间缩减和相似性感知方法，在保持高效调优的同时实现了对未见程序的良好泛化能力。

Abstract: Compiler pass selection and phase ordering present a significant challenge in
achieving optimal program performance, particularly for objectives like code
size reduction. Standard compiler heuristics offer general applicability but
often yield suboptimal, program-specific results due to their one-size-fits-all
nature. While iterative compilation can find tailored solutions, its
prohibitive search cost limits practical use. Machine learning approaches
promise faster inference but frequently struggle with generalization to unseen
programs. This paper introduces GRACE, a novel framework for compiler
auto-tuning, demonstrated for LLVM IR instruction count optimization. GRACE
effectively curtails the search space by leveraging pass synergies and a
weighted scoring method to generate initial high-quality candidate sequences
and a pass pool. It then employs contrastive learning, using pass
sequence-based data augmentation, to create program embeddings that facilitate
similarity-aware clustering. Evolutionary search within these clusters yields a
coreset of $k$ specialized pass sequences designed for robust generalization to
unseen programs. At test time, GRACE efficiently selects the best coreset
sequence and refines it using lightweight techniques. Experimental results on
seven diverse datasets show that GRACE reduces LLVM IR instruction count by an
average of 10.09% on LLVM 10.0.0 and 10.19% on LLVM 18.1.6 compared to opt -Oz,
while incurring an average tuning time of less than 1s per program,
demonstrating its state-of-the-art performance and practical effectiveness.

</details>


### [7] [Synergy-Guided Compiler Auto-Tuning of Nested LLVM Pass Pipelines](https://arxiv.org/abs/2510.13184)
*Haolin Pan,Jinyuan Dong,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: 提出了一个专为LLVM新Pass管理器设计的编译器自动调优框架，使用形式化语法定义有效嵌套管道空间，通过基于森林的数据结构和结构感知遗传算法确保生成语法有效的优化管道。


<details>
  <summary>Details</summary>
Motivation: 现有编译器自动调优方法假设线性pass序列，这与LLVM新Pass管理器的分层设计不匹配，无法保证生成语法有效的优化管道。

Method: 使用形式化语法定义有效嵌套管道空间，开发基于森林的数据结构表示管道，构建结构感知遗传算法直接操作这些森林结构，确保所有候选解在构造时就是有效的。

Result: 在LLVM 18.1.6上评估7个基准数据集，发现的管道相比标准opt -Oz优化级别平均额外减少13.62%的指令计数。

Conclusion: 该框架能够在这个复杂、受限的搜索空间中导航，识别出有效且高效的pass管道。

Abstract: Compiler optimization relies on sequences of passes to improve program
performance. Selecting and ordering these passes automatically, known as
compiler auto-tuning, is challenging due to the large and complex search space.
Existing approaches generally assume a linear sequence of passes, a model
compatible with legacy compilers but fundamentally misaligned with the
hierarchical design of the LLVM New Pass Manager. This misalignment prevents
them from guaranteeing the generation of syntactically valid optimization
pipelines. In this work, we present a new auto-tuning framework built from the
ground up for the New Pass Manager. We introduce a formal grammar to define the
space of valid nested pipelines and a forest-based data structure for their
native representation. Upon this foundation, we develop a structure-aware
Genetic Algorithm whose operators manipulate these forests directly, ensuring
that all candidate solutions are valid by construction. The framework first
mines synergistic pass relationships to guide the search. An optional
refinement stage further explores subtle performance variations arising from
different valid structural arrangements.
  We evaluate our approach on seven benchmark datasets using LLVM 18.1.6. The
discovered pipelines achieve an average of 13.62% additional instruction count
reduction compared to the standard opt -Oz optimization level, showing that our
framework is capable of navigating this complex, constrained search space to
identify valid and effective pass pipelines.

</details>


### [8] [Towards Richer Challenge Problems for Scientific Computing Correctness](https://arxiv.org/abs/2510.13423)
*Matthew Sottile,Mohit Tekriwal,John Sarracino*

Main category: cs.SE

TL;DR: 本文呼吁为科学计算领域开发专门的验证挑战问题，以弥补形式化方法与科学计算社区之间的理解差距，并提出科学计算正确性的多个维度。


<details>
  <summary>Details</summary>
Motivation: 现有编程语言和形式化方法验证技术难以应对现实科学计算应用的复杂性，科学计算与形式化方法社区之间缺乏对机器可验证正确性挑战的共同理解。

Method: 提出为科学计算设计专门的挑战问题，这些挑战旨在补充形式化方法研究者研究的一般程序问题，确保满足科学计算应用的需求。

Result: 提出了科学计算相关正确性的多个维度，并讨论了设计评估科学计算正确性的挑战问题的指导原则和标准。

Conclusion: 需要专门针对科学计算的挑战问题来指导和评估形式化方法/编程语言验证技术的发展，以解决科学计算领域的正确性问题。

Abstract: Correctness in scientific computing (SC) is gaining increasing attention in
the formal methods (FM) and programming languages (PL) community. Existing
PL/FM verification techniques struggle with the complexities of realistic SC
applications. Part of the problem is a lack of a common understanding between
the SC and PL/FM communities of machine-verifiable correctness challenges and
dimensions of correctness in SC applications.
  To address this gap, we call for specialized challenge problems to inform the
development and evaluation of FM/PL verification techniques for correctness in
SC. These specialized challenges are intended to augment existing problems
studied by FM/PL researchers for general programs to ensure the needs of SC
applications can be met. We propose several dimensions of correctness relevant
to scientific computing, and discuss some guidelines and criteria for designing
challenge problems to evaluate correctness in scientific computing.

</details>


### [9] [Verifying a Sparse Matrix Algorithm Using Symbolic Execution](https://arxiv.org/abs/2510.13424)
*Alexander C. Wilton*

Main category: cs.SE

TL;DR: 使用符号执行来测试科学软件，特别是稀疏矩阵算法，提供比传统单元测试更强的验证保证


<details>
  <summary>Details</summary>
Motivation: 科学软件因其数学性和高度优化而复杂，容易产生传统测试难以检测的细微错误

Method: 采用符号执行方法编写类似传统单元测试的测试，应用于稀疏矩阵算法

Result: 符号执行能够提供更强的验证保证

Conclusion: 符号执行是测试复杂科学软件的有效方法，能够检测传统测试难以发现的错误

Abstract: Scientific software is, by its very nature, complex. It is mathematical and
highly optimized which makes it prone to subtle bugs not as easily detected by
traditional testing. We outline how symbolic execution can be used to write
tests similar to traditional unit tests while providing stronger verification
guarantees and apply this methodology to a sparse matrix algorithm.

</details>


### [10] [OpenDerisk: An Industrial Framework for AI-Driven SRE, with Design, Implementation, and Case Studies](https://arxiv.org/abs/2510.13561)
*Peng Di,Faqiang Chen,Xiao Bai,Hongjun Yang,Qingfeng Li,Ganglin Wei,Jian Mou,Feng Shi,Keting Chen,Peng Tang,Zhitao Shen,Zheng Li,Wenhui Shi,Junwei Guo,Hang Yu*

Main category: cs.SE

TL;DR: OpenDerisk是一个专为SRE设计的开源多智能体框架，通过诊断原生协作模型、可插拔推理引擎和知识引擎，显著提升了复杂软件问题的诊断准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现代软件复杂性不断增加，给SRE团队带来沉重运维负担，需要能够模拟专家诊断推理的AI驱动自动化解决方案。现有方法要么缺乏深度因果推理能力，要么不适用于SRE特有的专业化调查工作流程。

Method: 开发了OpenDerisk框架，包含诊断原生协作模型、可插拔推理引擎、知识引擎和标准化协议(MCP)，使专业智能体能够协作解决复杂的多领域问题。

Result: 综合评估显示OpenDerisk在准确性和效率上显著优于最先进的基线方法。已在蚂蚁集团大规模生产部署，服务超过3000名日常用户，验证了其工业级可扩展性和实际影响。

Conclusion: OpenDerisk成功填补了SRE领域AI自动化解决方案的空白，提供了专门设计的开源多智能体框架，在实际生产中证明了其有效性和可扩展性。

Abstract: The escalating complexity of modern software imposes an unsustainable
operational burden on Site Reliability Engineering (SRE) teams, demanding
AI-driven automation that can emulate expert diagnostic reasoning. Existing
solutions, from traditional AI methods to general-purpose multi-agent systems,
fall short: they either lack deep causal reasoning or are not tailored for the
specialized, investigative workflows unique to SRE. To address this gap, we
present OpenDerisk, a specialized, open-source multi-agent framework
architected for SRE. OpenDerisk integrates a diagnostic-native collaboration
model, a pluggable reasoning engine, a knowledge engine, and a standardized
protocol (MCP) to enable specialist agents to collectively solve complex,
multi-domain problems. Our comprehensive evaluation demonstrates that
OpenDerisk significantly outperforms state-of-the-art baselines in both
accuracy and efficiency. This effectiveness is validated by its large-scale
production deployment at Ant Group, where it serves over 3,000 daily users
across diverse scenarios, confirming its industrial-grade scalability and
practical impact. OpenDerisk is open source and available at
https://github.com/derisk-ai/OpenDerisk/

</details>


### [11] [Auto-repair without test cases: How LLMs fix compilation errors in large industrial embedded code](https://arxiv.org/abs/2510.13575)
*Han Fu,Sigrid Eldh,Kristian Wiklund,Andreas Ermedahl,Philipp Haller,Cyrille Artho*

Main category: cs.SE

TL;DR: 使用大型语言模型自动修复工业嵌入式系统中编译错误，在CI系统中可解决63%的编译错误，83%的修复方案合理，且将调试时间从数小时缩短至8分钟内。


<details>
  <summary>Details</summary>
Motivation: 工业嵌入式系统中软硬件协同开发经常在持续集成过程中出现编译错误，现有修复技术依赖测试用例，但不可编译代码没有测试用例可用。

Method: 采用基于大型语言模型的自动修复方法，收集了40000多个产品源代码提交，评估了四种最先进LLM在工业CI系统中的表现，并与人工修复进行比较。

Result: LLM增强的CI系统可解决基线数据集中63%的编译错误，其中83%的成功修复被认为是合理的。LLM显著减少调试时间，大多数成功案例在8分钟内完成。

Conclusion: 大型语言模型在自动修复编译错误方面表现优异，能够有效解决工业嵌入式系统中的编译问题，大幅提高开发效率。

Abstract: The co-development of hardware and software in industrial embedded systems
frequently leads to compilation errors during continuous integration (CI).
Automated repair of such failures is promising, but existing techniques rely on
test cases, which are not available for non-compilable code.
  We employ an automated repair approach for compilation errors driven by large
language models (LLMs). Our study encompasses the collection of more than 40000
commits from the product's source code. We assess the performance of an
industrial CI system enhanced by four state-of-the-art LLMs, comparing their
outcomes with manual corrections provided by human programmers. LLM-equipped CI
systems can resolve up to 63 % of the compilation errors in our baseline
dataset. Among the fixes associated with successful CI builds, 83 % are deemed
reasonable. Moreover, LLMs significantly reduce debugging time, with the
majority of successful cases completed within 8 minutes, compared to hours
typically required for manual debugging.

</details>


### [12] [Property Testing for Ocean Models. Can We Specify It? (Invited Talk)](https://arxiv.org/abs/2510.13692)
*Deepak A. Cherian*

Main category: cs.SE

TL;DR: 探索如何将属性测试理论应用于海洋数值模型，利用地球物理流体动力学理论作为属性测试来解决海洋模型正确性验证的oracle问题。


<details>
  <summary>Details</summary>
Motivation: 从属性测试文献中获得灵感，特别是John Hughes教授的工作，探索这些思想如何应用于海洋数值模型，解决测试海洋模型正确性时面临的oracle问题。

Method: 提出将一系列简单的理想化地球物理流体动力学问题框架化为属性测试，利用物理学自然特性来指定属性测试。

Result: 通过示例清晰地展示了物理学如何自然地适用于属性测试的规范制定，但具体哪些测试最可行和有用仍需进一步研究。

Conclusion: 地球物理流体动力学理论可以作为属性测试来验证海洋模型的正确性，但需要进一步确定哪些测试最具可行性和实用性。

Abstract: I take inspiration from the property-testing literature, particularly the
work of Prof. John Hughes, and explore how such ideas might be applied to
numerical models of the ocean. Specifically, I ask whether geophysical fluid
dynamics (GFD) theory, expressed as property tests, might be used to address
the oracle problem of testing the correctness of ocean models. I propose that a
number of simple idealized GFD problems can be framed as property tests. These
examples clearly illustrate how physics naturally lends itself to specifying
property tests. Which of these proposed tests might be most feasible and
useful, remains to be seen.

</details>


### [13] [On Pretraining for Project-Level Code Completion](https://arxiv.org/abs/2510.13697)
*Maksim Sapronov,Evgeniy Glukhov*

Main category: cs.SE

TL;DR: 论文研究了不同仓库处理策略对代码模型上下文学习的影响，通过扩展OpenCoder模型的上下文窗口并训练仓库级数据，在较小数据集下取得了与竞争模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 探索仓库级预训练中不同处理策略对代码模型上下文学习能力的影响，特别是在有限数据和计算资源下的有效性。

Method: 将OpenCoder模型的上下文窗口从4,096扩展到16,384个token，在10亿个精选仓库级token上进行训练，比较不同仓库处理技术。

Result: 在Long Code Arena基准测试中取得了与竞争模型相当的性能，发现不同仓库处理技术效果相似，主要收益来自适应新的RoPE缩放参数。

Conclusion: 更简单的文件级训练方法在原始序列长度下仍然非常有效，为资源受限环境下的仓库级代码补全研究开辟了可能性。

Abstract: Repository-level pretraining is commonly used to enable large language models
for code to leverage codebase-wide context. This enhances their ability to
generate accurate and context-aware code completions. In this work, we
investigate how different repository-processing strategies affect in-context
learning in OpenCoder, a 1.5B-parameter model. We extend its context window
from 4,096 to 16,384 tokens by training on additional 1B tokens of curated
repository-level data. Despite relying on a smaller dataset than competing
models (which often use hundreds of billions of tokens), our model achieves
comparable performance on the Long Code Arena benchmark. We find that various
repository-processing techniques yield similarly strong results, with the
primary gain coming from adapting to a new rotary positional embedding (RoPE)
scaling parameter. Finally, we show that a simpler file-level training approach
at the original sequence length remains highly effective, opening up
repository-level code completion research to settings with more constrained
data and compute resources.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [14] [Imperative Quantum Programming with Ownership and Borrowing in Guppy](https://arxiv.org/abs/2510.13082)
*Mark Koch,Agustín Borgna,Craig Roy,Alan Lawrence,Kartik Singhal,Seyon Sivarajah,Ross Duncan*

Main category: cs.PL

TL;DR: 开发结合线性类型与命令式语义的量子类型系统，已在Guppy编程语言中实现


<details>
  <summary>Details</summary>
Motivation: 线性类型在函数式量子编程中强制执行无克隆和无删除定理，但在命令式量子编程中尚未广泛应用

Method: 开发结合人体工学线性类型与命令式语义的量子类型系统，保持安全保证

Result: 所有想法已在Quantinuum的Guppy编程语言中实现

Conclusion: 成功开发了结合线性类型与命令式语义的量子类型系统

Abstract: Linear types enforce no-cloning and no-deleting theorems in functional
quantum programming. However, in imperative quantum programming, they have not
gained widespread adoption. This work aims to develop a quantum type system
that combines ergonomic linear typing with imperative semantics and maintains
safety guarantees. All ideas presented here have been implemented in
Quantinuum's Guppy programming language.

</details>


### [15] [Extensibility in Programming Languages: An overview](https://arxiv.org/abs/2510.13236)
*Sebastian mateos Nicolajsen*

Main category: cs.PL

TL;DR: 本文探讨编程语言可扩展性，通过文献综述识别宏、模块、类型和反射等关键主题，分析参数化和一等公民行为等跨主题特性，旨在启发未来语言设计者重视可扩展性。


<details>
  <summary>Details</summary>
Motivation: 作者在调查编程语言时缺乏对可扩展性组件的全面概述，因此希望为读者提供这一被忽视但重要的语言设计方面的介绍。

Method: 通过文献综述方法，识别和分析编程语言可扩展性的关键主题和跨主题特性。

Result: 识别出宏、模块、类型和反射四个关键可扩展性主题，并分析了参数化和一等公民行为等跨主题特性，强调了编程语言构造中可定制性和灵活性的重要性。

Conclusion: 本文旨在启发未来编程语言设计者批判性地评估和考虑其设计的可扩展性，认识到可定制性和灵活性在语言构造中的重要性。

Abstract: I here conduct an exploration of programming language extensibility, making
an argument for an often overlooked component of conventional language design.
Now, this is not a technical detailing of these components, rather, I attempt
to provide an overview as I myself have lacked during my time investigating
programming languages. Thus, read this as an introduction to the magical world
of extensibility. Through a literature review, I identify key extensibility
themes - Macros, Modules, Types, and Reflection - highlighting diverse
strategies for fostering extensibility. The analysis extends to cross-theme
properties such as Parametricism and First-class citizen behaviour, introducing
layers of complexity by highlighting the importance of customizability and
flexibility in programming language constructs. By outlining these facets of
existing programming languages and research, I aim to inspire future language
designers to assess and consider the extensibility of their creations
critically.

</details>


### [16] [Fast Trigonometric Functions using the RLIBM Approach](https://arxiv.org/abs/2510.13426)
*Sehyeok Park,Santosh Nagarakatte*

Main category: cs.PL

TL;DR: 开发三角函数的多项式近似方法，使用RLIBM方法为多种表示形式和舍入模式生成正确舍入的结果，重点解决基于π的范围缩减问题。


<details>
  <summary>Details</summary>
Motivation: 三角函数范围缩减过程中π值的舍入误差会被放大，导致错误结果，需要开发能够保持π值高精度的快速范围缩减技术。

Method: 使用RLIBM方法实现多项式近似，开发浮点和整数计算中保持π值多位精度的快速范围缩减技术。

Result: 实现了快速的三角函数实现，能够为所有32位及以下输入的多种表示形式生成正确舍入结果，且单一实现支持多种表示。

Conclusion: 通过保持π值高精度的范围缩减技术，成功开发出快速且正确舍入的三角函数实现，解决了传统方法中的精度问题。

Abstract: This paper describes our experience developing polynomial approximations for
trigonometric functions that produce correctly rounded results for multiple
representations and rounding modes using the RLIBM approach. A key challenge
with trigonometric functions concerns range reduction with "pi", which reduces
a given input in the domain of a 32-bit float to a small domain. Any rounding
error in the value of "pi" is amplified during range reduction, which can
result in wrong results. We describe our experience implementing fast range
reduction techniques that maintain a large number of bits of "pi" both with
floating-point and integer computations. The resulting implementations for
trigonometric functions are fast and produce correctly rounded results for all
inputs for multiple representations up to 32-bits with a single implementation.

</details>


### [17] [A Complementary Approach to Incorrectness Typing](https://arxiv.org/abs/2510.13725)
*Celia Mengyue Li,Sophie Pull,Steven Ramsay*

Main category: cs.PL

TL;DR: 提出了一种新的双面类型系统，用于验证带有原子和模式匹配的函数式程序的正确性和错误性。核心思想是类型应该覆盖范式集合而非值集合，这允许定义类型上的补集运算符作为类型公式的否定。


<details>
  <summary>Details</summary>
Motivation: 需要验证函数式程序的正确性和错误性，特别是针对Erlang类程序中的错误情况。传统类型系统难以同时处理正确性验证和错误性证明。

Method: 使用基于范式集合而非值集合的类型系统，引入补集运算符作为类型否定，通过子类型化的表达性公理化来实现。

Result: 补集运算符能够推导出广泛的否定原则，包括类型理论中的共蕴含类似物。系统被证明是可判定的，并且对于范式是完备的。

Conclusion: 该类型系统不仅能验证程序正确性，还能证明程序错误性，为函数式程序的全面验证提供了有效工具。

Abstract: We introduce a new two-sided type system for verifying the correctness and
incorrectness of functional programs with atoms and pattern matching. A key
idea in the work is that types should range over sets of normal forms, rather
than sets of values, and this allows us to define a complement operator on
types that acts as a negation on typing formulas. We show that the complement
allows us to derive a wide range of refutation principles within the system,
including the type-theoretic analogue of co-implication, and we use them to
certify that a number of Erlang-like programs go wrong. An expressive
axiomatisation of the complement operator via subtyping is shown decidable, and
the type system as a whole is shown to be not only sound, but also complete for
normal forms.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [18] [VSS Challenge Problem: Verifying the Correctness of AllReduce Algorithms in the MPICH Implementation of MPI](https://arxiv.org/abs/2510.13413)
*Paul D. Hovland*

Main category: cs.LO

TL;DR: 基于MPICH实现的MPI验证挑战问题，验证三种allreduce算法的功能等效性


<details>
  <summary>Details</summary>
Motivation: MPICH实现包含多种allreduce算法，需要验证它们与reduce+broadcast的功能等效性

Method: 创建三种算法的独立版本，使用CIVL工具验证其中两种

Result: 成功验证了三种算法中的两种

Conclusion: 验证了MPICH中部分allreduce算法的功能正确性

Abstract: We describe a challenge problem for verification based on the MPICH
implementation of MPI. The MPICH implementation includes several algorithms for
allreduce, all of which should be functionally equivalent to reduce followed by
broadcast. We created standalone versions of three algorithms and verified two
of them using CIVL.

</details>


### [19] [Specification and Verification for Climate Modeling: Formalization Leading to Impactful Tooling](https://arxiv.org/abs/2510.13425)
*Alper Altuntas,Allison H. Baker,John Baugh,Ganesh Gopalakrishnan,Stephen F. Siegel*

Main category: cs.LO

TL;DR: 本文主张在地球系统模型(ESM)开发中更广泛采用形式化方法，以解决传统验证方法的局限性，并通过案例研究展示了使用CIVL模型检查器验证海洋混合参数化方案中bug修复的有效性。


<details>
  <summary>Details</summary>
Motivation: 地球系统模型的复杂性（大型代码库、多样化开发社区和计算平台）给软件质量保证带来重大挑战。传统验证方法如比特级可重现性不总是可行，而人工专家评估主观且耗时。形式化方法提供了数学严谨的替代方案。

Method: 识别适合形式化规范的ESM关键方面，引入定制化框架的抽象方法，使用CIVL模型检查器对海洋混合参数化方案中的bug修复进行形式化验证。

Result: 成功演示了形式化方法在气候建模中的应用，通过案例研究验证了海洋混合参数化方案中bug修复的正确性。

Conclusion: 开发可访问的、领域特定的形式化工具能够增强模型置信度，支持更高效可靠的地球系统模型开发。

Abstract: Earth System Models (ESMs) are critical for understanding past climates and
projecting future scenarios. However, the complexity of these models, which
include large code bases, a wide community of developers, and diverse
computational platforms, poses significant challenges for software quality
assurance. The increasing adoption of GPUs and heterogeneous architectures
further complicates verification efforts. Traditional verification methods
often rely on bitwise reproducibility, which is not always feasible,
particularly under new compilers or hardware. Manual expert evaluation, on the
other hand, is subjective and time-consuming. Formal methods offer a
mathematically rigorous alternative, yet their application in ESM development
has been limited due to the lack of climate model-specific representations and
tools. Here, we advocate for the broader adoption of formal methods in climate
modeling. In particular, we identify key aspects of ESMs that are well suited
to formal specification and introduce abstraction approaches for a tailored
framework. To demonstrate this approach, we present a case study using CIVL
model checker to formally verify a bug fix in an ocean mixing parameterization
scheme. Our goal is to develop accessible, domain-specific formal tools that
enhance model confidence and support more efficient and reliable ESM
development.

</details>


### [20] [Verification Challenges in Sparse Matrix Vector Multiplication in High Performance Computing: Part I](https://arxiv.org/abs/2510.13427)
*Junchao Zhang*

Main category: cs.LO

TL;DR: 本文介绍了稀疏矩阵向量乘法(SpMV)的顺序和基本MPI并行实现，作为科学软件验证社区的挑战问题


<details>
  <summary>Details</summary>
Motivation: SpMV是依赖迭代求解器的科学代码中的基本内核，旨在为科学软件验证社区提供挑战问题

Method: 在PETSc库背景下实现了顺序和基本MPI并行的SpMV实现

Result: 提供了可用的SpMV实现作为验证基准

Conclusion: 这些实现可作为科学软件验证社区的挑战问题

Abstract: Sparse matrix vector multiplication (SpMV) is a fundamental kernel in
scientific codes that rely on iterative solvers. In this first part of our
work, we present both a sequential and a basic MPI parallel implementations of
SpMV, aiming to provide a challenge problem for the scientific software
verification community. The implementations are described in the context of the
PETSc library.

</details>


### [21] [Verification Challenge: Fractional Cascading for Multi-Nuclide Grid Lookup](https://arxiv.org/abs/2510.13428)
*Andrew R. Siegel*

Main category: cs.LO

TL;DR: 验证基于分数级联(FC)技术的重复搜索加速算法的正确性和结构特性，应用于核截面查找场景


<details>
  <summary>Details</summary>
Motivation: 在核模拟代码中，材料包含多个核素，每个核素都有排序的能量网格。朴素方法需要对每个数组单独进行二分搜索，效率较低。FC级联网格结构通过单次二分搜索和常数时间精化来降低搜索成本

Method: 基于分数级联(FC)技术构建级联网格结构，通过单次二分搜索和后续的常数时间精化来加速重复搜索

Result: 提出了验证FC算法相对于朴素方法的正确性及其结构特性的挑战

Conclusion: 需要验证FC算法在核截面查找应用中的正确性和结构特性

Abstract: We present a verification challenge based on the fractional cascading (FC)
technique for accelerating repeated searches across a collection of sorted
arrays. The specific context is nuclear cross section lookup in a simulation
code, where a material consists of many nuclides, each with its own sorted
energy grid. A naive search performs a binary search in each array
individually. The FC-based cascade grid structure reduces this cost by
performing a single binary search followed by constant-time refinements. The
challenge consists of verifying the correctness of the FC algorithm with
respect to the naive approach and validating its structural properties.

</details>
