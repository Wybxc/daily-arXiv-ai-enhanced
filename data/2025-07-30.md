<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.LO](#cs.LO) [Total: 4]
- [cs.SE](#cs.SE) [Total: 13]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [One Weird Trick to Untie Landin's Knot](https://arxiv.org/abs/2507.21317)
*Paulette Koronkevich,William J. Bowman*

Main category: cs.PL

TL;DR: 本文探讨了Landin's Knot，发现高阶引用本身不会导致非终止，关键在于对函数环境的无限制量化。通过闭包转换语言展示了这一点，并提出了限制环境量化的方法。


<details>
  <summary>Details</summary>
Motivation: 研究高阶引用在终止语言中的作用，澄清Landin's Knot的本质，探索在不引入复杂类型系统的情况下安全添加高阶引用的可能性。

Method: 使用闭包转换语言，显式化函数环境，并通过非直谓量化隐藏环境类型，展示如何利用高阶引用编码递归。

Result: 发现高阶引用本身不导致非终止，关键在于无限制的环境量化；提出限制量化可安全添加高阶引用。

Conclusion: 通过限制环境量化，可以在不引入复杂类型系统的情况下安全添加高阶引用，同时保留其存储函数的能力。

Abstract: In this work, we explore Landin's Knot, which is understood as a pattern for
encoding general recursion, including non-termination, that is possible after
adding higher-order references to an otherwise terminating language. We observe
that this isn't always true -- higher-order references, by themselves, don't
lead to non-termination. The key insight is that Landin's Knot relies not
primarily on references storing functions, but on unrestricted quantification
over a function's environment. We show this through a closure converted
language, in which the function's environment is made explicit and hides the
type of the environment through impredicative quantification. Once references
are added, this impredicative quantification can be exploited to encode
recursion. We conjecture that by restricting the quantification over the
environment, higher-order references can be safely added to terminating
languages, without resorting to more complex type systems such as linearity,
and without restricting references from storing functions.

</details>


### [2] [Fixed-Point-Oriented Programming: A Concise and Elegant Paradigm](https://arxiv.org/abs/2507.21439)
*Yong Qi Foo,Brian Sze-Kai Cheong,Michael D. Adams*

Main category: cs.PL

TL;DR: FPOP是一种新兴编程范式，旨在简化涉及自引用计算的实现，如图算法、静态分析等，提供高效且易维护的解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统编程范式缺乏对固定点计算的直接支持，导致实现复杂且易出错。FPOP通过高级抽象和优化，简化问题表达。

Method: 利用结构化推理规则和用户导向优化，FPOP允许开发者编写声明式规范，编译器确保高效执行。

Result: FPOP显著简化算法实现，提升可维护性，例如图距离问题仅需两行代码。

Conclusion: FPOP填补了理论与实践的鸿沟，有望推动该范式的进一步研究和应用。

Abstract: Fixed-Point-Oriented Programming (FPOP) is an emerging paradigm designed to
streamline the implementation of problems involving self-referential
computations. These include graph algorithms, static analysis, parsing, and
distributed computing-domains that traditionally require complex and
tricky-to-implement work-queue algorithms. Existing programming paradigms lack
direct support for these inherently fixed-point computations, leading to
inefficient and error-prone implementations.
  This white paper explores the potential of the FPOP paradigm, which offers a
high-level abstraction that enables concise and expressive problem
formulations. By leveraging structured inference rules and user-directed
optimizations, FPOP allows developers to write declarative specifications while
the compiler ensures efficient execution. It not only reduces implementation
complexity for programmers but also enhances adaptability, making it easier for
programmers to explore alternative solutions and optimizations without
modifying the core logic of their program.
  We demonstrate how FPOP simplifies algorithm implementation, improves
maintainability, and enables rapid prototyping by allowing problems to be
clearly and concisely expressed. For example, the graph distance problem can be
expressed in only two executable lines of code with FPOP, while it takes an
order of magnitude more code in other paradigms. By bridging the gap between
theoretical fixed-point formulations and practical implementations, we aim to
foster further research and adoption of this paradigm.

</details>


### [3] [Composable Effect Handling for Programming LLM-integrated Scripts](https://arxiv.org/abs/2507.22048)
*Di Wang*

Main category: cs.PL

TL;DR: 论文提出使用可组合效应处理来分离工作流逻辑与效应操作（如LLM调用、I/O和并发），从而在不牺牲性能优化机会的情况下实现模块化。


<details>
  <summary>Details</summary>
Motivation: 解决LLM集成脚本中模块化和性能的挑战，避免脚本与特定LLM实现耦合，并利用并行化机会。

Method: 采用可组合效应处理技术，将效应操作（如LLM调用）视为抽象接口，并通过效应处理器实现。

Result: 在Tree-of-Thoughts案例中实现了10倍的加速，同时保持模块化。

Conclusion: 可组合效应处理是一种适用于LLM脚本编程的有效风格。

Abstract: Implementing LLM-integrated scripts introduces challenges in modularity and
performance, as scripts are often coupled to specific LLM implementations and
fail to exploit parallelization opportunities. This paper proposes using
composable effect handling to separate workflow logic from effectful
operations, such as LLM calls, I/O, and concurrency, enabling modularity
without sacrificing the opportunity for performance optimization. By treating
these operations as abstract interfaces and discharging them via effect
handlers, this paper shows that scripts can achieve significant speedups (e.g.,
10$\times$ in a Tree-of-Thoughts case study) without compromising modularity.
This paper aims to promote composable effect handling as a programming style
for LLM scripting.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [4] [Semantic Numeration Systems as Dynamical Systems](https://arxiv.org/abs/2507.21295)
*Alexander Yu. Chunikhin*

Main category: cs.LO

TL;DR: 论文概述了语义计数系统理论的基础概念，提出将基数抽象对象（CAO）视为具有非线性控制的线性离散动态系统，并提供了状态方程。


<details>
  <summary>Details</summary>
Motivation: 研究基数抽象对象（CAO）的动态行为及其在语义计数系统中的应用。

Method: 将CAO建模为线性离散动态系统，并引入配置矩阵描述其结构和参数。

Result: 在理想可观测性假设下，推导了CAO的状态方程，并展示了配置矩阵的关键作用。

Conclusion: 配置矩阵在描述CAO结构和动态行为中具有基础性作用。

Abstract: The foundational concepts of semantic numeration systems theory are briefly
outlined. The action of cardinal semantic operators unfolds over a set of
cardinal abstract entities belonging to the cardinal semantic multeity. The
cardinal abstract object (CAO) formed by them in a certain connectivity
topology is proposed to be considered as a linear discrete dynamical system
with nonlinear control. Under the assumption of ideal observability, the CAO
state equations are provided for both stationary and non-stationary cases. The
fundamental role of the configuration matrix, which combines information about
the types of cardinal semantic operators in the CAO, their parameters and
topology of connectivity, is demonstrated.

</details>


### [5] [A Tree-Shaped Tableau for Checking the Satisfiability of Signal Temporal Logic with Bounded Temporal Operators](https://arxiv.org/abs/2507.21598)
*Beatrice Melani,Ezio Bartocci,Michele Chiari*

Main category: cs.LO

TL;DR: 提出一种新型树形单遍表方法，用于离散时间STL的可满足性检查，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决CPS设计中STL需求逻辑一致性的高效自动检查问题。

Method: 采用树形单遍表方法，利用STL公式中大时间间隔的冗余性加速检查。

Result: 实验表明，该方法在多数情况下优于现有编码方法。

Conclusion: 该方法不仅适用于一致性检查，还可用于信号合成和公式验证。

Abstract: Signal Temporal Logic (STL) is a widely recognized formal specification
language to express rigorous temporal requirements on mixed analog signals
produced by cyber-physical systems (CPS). A relevant problem in CPS design is
how to efficiently and automatically check whether a set of STL requirements is
logically consistent. This problem reduces to solving the STL satisfiability
problem, which is decidable when we assume that our system operates in discrete
time steps dictated by an embedded system's clock.
  This paper introduces a novel tree-shaped, one-pass tableau method for
satisfiability checking of discrete-time STL with bounded temporal operators.
Originally designed to prove the consistency of a given set of STL
requirements, this method has a wide range of applications beyond consistency
checking. These include synthesizing example signals that satisfy the given
requirements, as well as verifying or refuting the equivalence and implications
of STL formulas.
  Our tableau exploits redundancy arising from large time intervals in STL
formulas to speed up satisfiability checking, and can also be employed to check
Mission-Time Linear Temporal Logic (MLTL) satisfiability. We compare our
tableau with Satisfiability Modulo Theories (SMT) and First-Order Logic
encodings from the literature on a benchmark suite, partly collected from the
literature, and partly provided by an industrial partner. Our experiments show
that, in many cases, our tableau outperforms state-of-the-art encodings.

</details>


### [6] [The Shape of $\mathcal{EL}$ Proofs: A Tale of Three Calculi (Extended Version)](https://arxiv.org/abs/2507.21851)
*Christian Alrabbaa,Stefan Borgwardt,Philipp Herrmann,Markus Krötzsch*

Main category: cs.LO

TL;DR: 研究比较了三种基于后果的推理演算在描述逻辑（DL）中的证明形状和复杂性。


<details>
  <summary>Details</summary>
Motivation: 探讨不同推理演算在DL中生成的证明形状及其复杂性差异。

Method: 通过将演算转化为带有分层否定的存在规则，并使用NEMO规则引擎执行和追踪，最后将追踪结果转化为DL证明进行比较。

Result: 比较了三种演算生成的证明在不同复杂性指标上的表现。

Conclusion: 不同演算生成的证明在形状和复杂性上存在显著差异，为选择合适的推理方法提供了依据。

Abstract: Consequence-based reasoning can be used to construct proofs that explain
entailments of description logic (DL) ontologies. In the literature, one can
find multiple consequence-based calculi for reasoning in the $\mathcal{EL}$
family of DLs, each of which gives rise to proofs of different shapes. Here, we
study three such calculi and the proofs they produce on a benchmark based on
the OWL Reasoner Evaluation. The calculi are implemented using a translation
into existential rules with stratified negation, which had already been
demonstrated to be effective for the calculus of the ELK reasoner. We then use
the rule engine NEMO to evaluate the rules and obtain traces of the rule
execution. After translating these traces back into DL proofs, we compare them
on several metrics that reflect different aspects of their complexity.

</details>


### [7] [Why not? Developing ABox Abduction beyond Repairs](https://arxiv.org/abs/2507.21955)
*Anselm Haak,Patrick Koopmann,Yasir Mahmood,Anni-Yasmin Turhan*

Main category: cs.LO

TL;DR: 论文研究了在不一致知识库（KB）下的溯因任务，提出了修复语义下的溯因概念和最小性标准，并分析了DL-Lite和EL_bot描述逻辑下的计算复杂性。


<details>
  <summary>Details</summary>
Motivation: 探讨在数据错误导致知识库不一致时，如何定义和计算有效的溯因解释。

Method: 提出修复语义下的溯因概念和最小性标准，分析不同修复语义和描述逻辑下的复杂性。

Result: 给出了在DL-Lite和EL_bot描述逻辑下，判断溯因解存在性和验证的初始复杂性结果。

Conclusion: 修复语义下的溯因为不一致知识库提供了有效的解释方法，最小性标准有助于生成有用的假设。

Abstract: Abduction is the task of computing a sufficient extension of a knowledge base
(KB) that entails a conclusion not entailed by the original KB. It serves to
compute explanations, or hypotheses, for such missing entailments. While this
task has been intensively investigated for perfect data and under classical
semantics, less is known about abduction when erroneous data results in
inconsistent KBs. In this paper we define a suitable notion of abduction under
repair semantics, and propose a set of minimality criteria that guides
abduction towards `useful' hypotheses. We provide initial complexity results on
deciding existence of and verifying abductive solutions with these criteria,
under different repair semantics and for the description logics DL-Lite and
EL_bot.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [8] [Generating Highly Structured Test Inputs Leveraging Constraint-Guided Graph Refinement](https://arxiv.org/abs/2507.21271)
*Zhaorui Yang,Yuxin Qiu,Haichao Zhu,Qian Zhang*

Main category: cs.SE

TL;DR: 本文提出了一种基于图的测试输入生成框架GRAphRef，用于统一结构化数据的测试输入生成，并通过约束修复保证输入的有效性和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现代AI应用处理高度结构化数据时，现有工具生成的输入常无效且效率低，缺乏通用性。

Method: 开发GRAphRef框架，将结构化输入映射为图，通过邻居相似性引导的突变和约束修复生成有效输入。

Result: 在八个真实AI系统中验证，GRAphRef在结构有效性、语义保留和性能开销上优于现有工具。

Conclusion: GRAphRef提供了一种通用且高效的测试输入生成方法，适用于结构化数据领域。

Abstract: [Context] Modern AI applications increasingly process highly structured data,
such as 3D meshes and point clouds, where test input generation must preserve
both structural and semantic validity. However, existing fuzzing tools and
input generators are typically handcrafted for specific input types and often
generate invalid inputs that are subsequently discarded, leading to
inefficiency and poor generalizability. [Objective] This study investigates
whether test inputs for structured domains can be unified through a graph-based
representation, enabling general, reusable mutation strategies while enforcing
structural constraints. We will evaluate the effectiveness of this approach in
enhancing input validity and semantic preservation across eight AI systems.
[Method] We develop and evaluate GRAphRef, a graph-based test input generation
framework that supports constraint-based mutation and refinement. GRAphRef maps
structured inputs to graphs, applies neighbor-similarity-guided mutations, and
uses a constraint-refinement phase to repair invalid inputs. We will conduct a
confirmatory study across eight real-world mesh-processing AI systems,
comparing GRAphRef with AFL, MeshAttack, Saffron, and two ablated variants.
Evaluation metrics include structural validity, semantic preservation (via
prediction consistency), and performance overhead. Experimental data is derived
from ShapeNetCore mesh seeds and model outputs from systems like MeshCNN and
HodgeNet. Statistical analysis and component latency breakdowns will be used to
assess each hypothesis.

</details>


### [9] ["Maybe We Need Some More Examples:" Individual and Team Drivers of Developer GenAI Tool Use](https://arxiv.org/abs/2507.21280)
*Courtney Miller,Rudrajit Choudhuri,Mara Ulloa,Sankeerti Haniyur,Robert DeLine,Margaret-Anne Storey,Emerson Murphy-Hill,Christian Bird,Jenna L. Butler*

Main category: cs.SE

TL;DR: 研究发现，开发者对生成式AI工具的采用不均，主要源于对其的认知差异、使用方式和应对挑战的态度。组织对快速生产力提升的期望与学习支持不足形成矛盾，反而阻碍了生产力提升。


<details>
  <summary>Details</summary>
Motivation: 探讨开发者对生成式AI工具采用不均的原因及其对生产力和开发者角色的影响。

Method: 通过对27个团队的54名开发者（每团队一名高频和低频用户）进行配对访谈。

Result: 开发者对工具的认知（协作工具vs功能）、使用方式（实验性vs保守）和应对挑战的态度（坚持vs放弃）是采用差异的主要原因。

Conclusion: 组织对快速生产力提升的期望与学习支持不足形成“生产力压力悖论”，反而阻碍了生产力提升。

Abstract: Despite the widespread availability of generative AI tools in software
engineering, developer adoption remains uneven. This unevenness is problematic
because it hampers productivity efforts, frustrates management's expectations,
and creates uncertainty around the future roles of developers. Through paired
interviews with 54 developers across 27 teams -- one frequent and one
infrequent user per team -- we demonstrate that differences in usage result
primarily from how developers perceive the tool (as a collaborator vs.
feature), their engagement approach (experimental vs. conservative), and how
they respond when encountering challenges (with adaptive persistence vs. quick
abandonment). Our findings imply that widespread organizational expectations
for rapid productivity gains without sufficient investment in learning support
creates a "Productivity Pressure Paradox," undermining the very productivity
benefits that motivate adoption.

</details>


### [10] [Black-Box Bug-Amplification for Multithreaded Software](https://arxiv.org/abs/2507.21318)
*Yeshayahu Weiss,Gal Amram,Achiya Elyasaf,Eitan Farchi,Oded Margalit,Gera Weiss*

Main category: cs.SE

TL;DR: 提出了一种通过预测模型放大并发系统中罕见错误的方法，显著提高了错误重现率。


<details>
  <summary>Details</summary>
Motivation: 并发系统中的错误通常难以重现，因为它们仅在罕见条件下出现。

Method: 将系统视为黑盒，通过重复试验训练预测模型，估计输入配置触发错误的概率。

Result: 在17种并发错误上评估，模型引导的搜索显著提高了错误重现率，比随机采样高出一个数量级。

Conclusion: 该方法为并发错误测试提供了一种非侵入性且高效的框架。

Abstract: Bugs, especially those in concurrent systems, are often hard to reproduce
because they manifest only under rare conditions. Testers frequently encounter
failures that occur only under specific inputs, even when occurring with low
probability. We propose an approach to systematically amplify the occurrence of
such elusive bugs. We treat the system under test as a black-box and use
repeated trial executions to train a predictive model that estimates the
probability of a given input configuration triggering a bug. We evaluate this
approach on a dataset of 17 representative concurrency bugs spanning diverse
categories. Several model-based search techniques are compared against a
brute-force random sampling baseline. Our results show that an ensemble of
regression models can significantly increase bug occurrence rates across nearly
all scenarios, often achieving an order-of-magnitude improvement over random
sampling. The contributions of this work include: (i) a novel formulation of
bug-amplification as a rare-event regression problem; (ii) an empirical
evaluation of multiple techniques for amplifying bug occurrence, demonstrating
the effectiveness of model-guided search; and (iii) a practical, non-invasive
testing framework that helps practitioners expose hidden concurrency faults
without altering the internal system architecture.

</details>


### [11] [Does Editing Improve Answer Quality on Stack Overflow? A Data-Driven Investigation](https://arxiv.org/abs/2507.21329)
*Saikat Mondal,Chanchal K. Roy*

Main category: cs.SE

TL;DR: 研究分析了Stack Overflow上Python相关答案的编辑效果，发现编辑对答案质量的影响不一致，既有积极也有消极作用。


<details>
  <summary>Details</summary>
Motivation: 技术问答平台的高质量答案对软件开发至关重要，但编辑对答案质量的影响尚未系统评估。

Method: 分析了94,994个至少有一次被接受编辑的Python相关答案，评估编辑对六个质量维度的影响。

Result: 53.3%的编辑提高了语义相关性，但38.1%降低了相关性；9%的代码从不可执行变为可执行，14.7%的代码变为不可解析；32.3%的编辑增加了复杂性；20.5%的编辑引入了安全问题；51.0%的编辑优化了性能，但总体执行时间增加；49.7%的编辑降低了可读性。

Conclusion: 编辑效果不一致，可能影响软件的可维护性、安全性和效率，需引起用户和平台管理者的注意。

Abstract: High-quality answers in technical Q&A platforms like Stack Overflow (SO) are
crucial as they directly influence software development practices. Poor-quality
answers can introduce inefficiencies, bugs, and security vulnerabilities, and
thus increase maintenance costs and technical debt in production software. To
improve content quality, SO allows collaborative editing, where users revise
answers to enhance clarity, correctness, and formatting. Several studies have
examined rejected edits and identified the causes of rejection. However, prior
research has not systematically assessed whether accepted edits enhance key
quality dimensions. While one study investigated the impact of edits on C/C++
vulnerabilities, broader quality aspects remain unexplored. In this study, we
analyze 94,994 Python-related answers that have at least one accepted edit to
determine whether edits improve (1) semantic relevance, (2) code usability, (3)
code complexity, (4) security vulnerabilities, (5) code optimization, and (6)
readability. Our findings show both positive and negative effects of edits.
While 53.3% of edits improve how well answers match questions, 38.1% make them
less relevant. Some previously broken code (9%) becomes executable, yet working
code (14.7%) turns non-parsable after edits. Many edits increase complexity
(32.3%), making code harder to maintain. Instead of fixing security issues,
20.5% of edits introduce additional issues. Even though 51.0% of edits optimize
performance, execution time still increases overall. Readability also suffers,
as 49.7% of edits make code harder to read. This study highlights the
inconsistencies in editing outcomes and provides insights into how edits impact
software maintainability, security, and efficiency that might caution users and
moderators and help future improvements in collaborative editing systems.

</details>


### [12] [MAAD: Automate Software Architecture Design through Knowledge-Driven Multi-Agent Collaboration](https://arxiv.org/abs/2507.21382)
*Ruiyin Li,Yiran Zhang,Xiyu Zhou,Peng Liang,Weisong Sun,Jifeng Xuan,Zhi Jin,Yang Liu*

Main category: cs.SE

TL;DR: MAAD是一个基于多智能体系统的自动化软件架构设计框架，通过四个专业智能体协作生成架构蓝图和评估报告，优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统软件架构设计依赖人工，耗时且设计选择有限，而基于LLM的智能体在此领域的应用尚不充分。

Method: MAAD框架利用四个智能体（分析师、建模师、设计师和评估师）协作处理需求规格并生成架构设计。

Result: MAAD在生成全面架构组件和结构化评估报告方面优于MetaGPT，且工业界反馈证实其实用性。GPT-4o在LLM中表现最佳。

Conclusion: MAAD为自动化架构设计提供了有效解决方案，强调了LLM选择的重要性。

Abstract: Software architecture design is a critical, yet inherently complex and
knowledge-intensive phase of software development. It requires deep domain
expertise, development experience, architectural knowledge, careful trade-offs
among competing quality attributes, and the ability to adapt to evolving
requirements. Traditionally, this process is time-consuming and
labor-intensive, and relies heavily on architects, often resulting in limited
design alternatives, especially under the pressures of agile development. While
Large Language Model (LLM)-based agents have shown promising performance across
various SE tasks, their application to architecture design remains relatively
scarce and requires more exploration, particularly in light of diverse domain
knowledge and complex decision-making. To address the challenges, we proposed
MAAD (Multi-Agent Architecture Design), an automated framework that employs a
knowledge-driven Multi-Agent System (MAS) for architecture design. MAAD
orchestrates four specialized agents (i.e., Analyst, Modeler, Designer and
Evaluator) to collaboratively interpret requirements specifications and produce
architectural blueprints enriched with quality attributes-based evaluation
reports. We then evaluated MAAD through a case study and comparative
experiments against MetaGPT, a state-of-the-art MAS baseline. Our results show
that MAAD's superiority lies in generating comprehensive architectural
components and delivering insightful and structured architecture evaluation
reports. Feedback from industrial architects across 11 requirements
specifications further reinforces MAAD's practical usability. We finally
explored the performance of the MAAD framework with three LLMs (GPT-4o,
DeepSeek-R1, and Llama 3.3) and found that GPT-4o exhibits better performance
in producing architecture design, emphasizing the importance of LLM selection
in MAS-driven architecture design.

</details>


### [13] [LLM4VV: Evaluating Cutting-Edge LLMs for Generation and Evaluation of Directive-Based Parallel Programming Model Compiler Tests](https://arxiv.org/abs/2507.21447)
*Zachariah Sollenberger,Rahul Patel,Saieda Ali Zada,Sunita Chandrasekaran*

Main category: cs.SE

TL;DR: 论文提出了一种双LLM系统（生成型和判别型），用于生成大量编译器测试并验证其正确性，解决了LLM输出不可信和逻辑推理不可解释的问题。


<details>
  <summary>Details</summary>
Motivation: LLM在软件和测试开发中的应用日益增多，但缺乏全面且自主的验证方法，且LLM的幻觉问题和对输出结果的不可解释性限制了其可信度。

Method: 采用双LLM系统（生成型和判别型），实验了多种不同参数规模的LLM，并使用十种精心选择的指标进行评估。

Result: 实验表明，LLM在生成高质量编译器测试和自动验证方面具有潜力。

Conclusion: 双LLM系统能有效提升LLM在测试生成中的可信度和实用性。

Abstract: The usage of Large Language Models (LLMs) for software and test development
has continued to increase since LLMs were first introduced, but only recently
have the expectations of LLMs become more realistic. Verifying the correctness
of code generated by LLMs is key to improving their usefulness, but there have
been no comprehensive and fully autonomous solutions developed yet.
Hallucinations are a major concern when LLMs are applied blindly to problems
without taking the time and effort to verify their outputs, and an inability to
explain the logical reasoning of LLMs leads to issues with trusting their
results. To address these challenges while also aiming to effectively apply
LLMs, this paper proposes a dual-LLM system (i.e. a generative LLM and a
discriminative LLM) and experiments with the usage of LLMs for the generation
of a large volume of compiler tests. We experimented with a number of LLMs
possessing varying parameter counts and presented results using ten
carefully-chosen metrics that we describe in detail in our narrative. Through
our findings, it is evident that LLMs possess the promising potential to
generate quality compiler tests and verify them automatically.

</details>


### [14] [HLSDebugger: Identification and Correction of Logic Bugs in HLS Code with LLM Solutions](https://arxiv.org/abs/2507.21485)
*Jing Wang,Shang Liu,Yao Lu,Zhiyao Xie*

Main category: cs.SE

TL;DR: HLSDebugger提出了一种针对HLS逻辑调试的定制解决方案，通过生成大规模标注数据集和采用编码器-解码器结构，显著提升了调试效率。


<details>
  <summary>Details</summary>
Motivation: HLS调试对新手和缺乏硬件知识的软件工程师来说具有挑战性，LLMs的应用潜力巨大但面临数据稀缺、逻辑复杂和测试用例不足等问题。

Method: HLSDebugger生成300K标注数据集，采用编码器-解码器结构，同时执行错误定位、类型预测和修正。

Result: HLSDebugger在错误识别和修正上显著优于GPT-4等先进LLMs，修正效率提升3倍以上。

Conclusion: HLSDebugger为HLS代码自动化调试提供了重要进展。

Abstract: High-level synthesis (HLS) accelerates hardware design by enabling the
automatic translation of high-level descriptions into efficient hardware
implementations. However, debugging HLS code is a challenging and
labor-intensive task, especially for novice circuit designers or software
engineers without sufficient hardware domain knowledge. The recent emergence of
Large Language Models (LLMs) is promising in automating the HLS debugging
process. Despite the great potential, three key challenges persist when
applying LLMs to HLS logic debugging: 1) High-quality circuit data for training
LLMs is scarce, posing a significant challenge. 2) Debugging logic bugs in
hardware is inherently more complex than identifying software bugs with
existing golden test cases. 3) The absence of reliable test cases requires
multi-tasking solutions, performing both bug identification and correction.
complicates the multi-tasking required for effective HLS debugging. In this
work, we propose a customized solution named HLSDebugger to address the
challenges. HLSDebugger first generates and releases a large labeled dataset
with 300K data samples, targeting HLS logic bugs. The HLSDebugger model adopts
an encoder-decoder structure, performing bug location identification, bug type
prediction, and bug correction with the same model. HLSDebugger significantly
outperforms advanced LLMs like GPT-4 in bug identification and by more than 3x
in bug correction. It makes a substantial advancement in the exploration of
automated debugging of HLS code.

</details>


### [15] [Ethical Classification of Non-Coding Contributions in Open-Source Projects via Large Language Models](https://arxiv.org/abs/2507.21583)
*Sergio Cobos,Javier Luis Cánovas Izquierdo*

Main category: cs.SE

TL;DR: 该论文提出了一种基于大型语言模型（LLM）的方法，用于分类开源软件（OSS）项目中非代码贡献的伦理质量，以促进更健康和可持续的社区环境。


<details>
  <summary>Details</summary>
Motivation: 开源软件开发不仅面临技术挑战，还涉及社会挑战，如多样化的贡献者群体带来的伦理问题。当前的行为准则监控和执行方法存在局限性，需要更有效的解决方案。

Method: 研究基于贡献者契约定义了一套伦理指标，并利用大型语言模型（LLM）和提示工程开发了一种分类方法，用于评估非代码贡献的伦理行为。

Result: 该方法能够有效分类非代码贡献的伦理质量，为开源项目提供了一种监控和执行行为准则的新工具。

Conclusion: 通过LLM技术，论文为开源社区提供了一种可行的伦理行为评估方法，有助于提升社区的包容性和可持续性。

Abstract: The development of Open-Source Software (OSS) is not only a technical
challenge, but also a social one due to the diverse mixture of contributors. To
this aim, social-coding platforms, such as GitHub, provide the infrastructure
needed to host and develop the code, but also the support for enabling the
community's collaboration, which is driven by non-coding contributions, such as
issues (i.e., change proposals or bug reports) or comments to existing
contributions. As with any other social endeavor, this development process
faces ethical challenges, which may put at risk the project's sustainability.
To foster a productive and positive environment, OSS projects are increasingly
deploying codes of conduct, which define rules to ensure a respectful and
inclusive participatory environment, with the Contributor Covenant being the
main model to follow. However, monitoring and enforcing these codes of conduct
is a challenging task, due to the limitations of current approaches. In this
paper, we propose an approach to classify the ethical quality of non-coding
contributions in OSS projects by relying on Large Language Models (LLM), a
promising technology for text classification tasks. We defined a set of ethical
metrics based on the Contributor Covenant and developed a classification
approach to assess ethical behavior in OSS non-coding contributions, using
prompt engineering to guide the model's output.

</details>


### [16] [Predicting Maintenance Cessation of Open Source Software Repositories with An Integrated Feature Framework](https://arxiv.org/abs/2507.21678)
*Yiming Xu,Runzhi He,Hengzhi Ye,Minghui Zhou,Huaimin Wang*

Main category: cs.SE

TL;DR: 论文提出了一种基于明确归档状态和语义分析的OSS维护风险预测方法，通过大规模数据集和多视角特征框架，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 开源软件维护风险对软件供应链的质量、安全性和韧性构成威胁，现有方法存在定义模糊、可解释性差和数据不足的问题。

Method: 引入“维护终止”概念，构建大规模数据集（115,466个GitHub仓库），提出多视角特征框架（用户、维护者和项目演化特征），并采用AFT生存分析和SHAP分析。

Result: AFT生存分析的C-index达0.846，显著优于仅依赖表面特征的模型，特征消融和SHAP分析验证了方法的有效性和可解释性。

Conclusion: 该方法为大规模开源生态系统的维护风险预测提供了可扩展、可解释的基础，并在openEuler生态系统中展示了实际应用价值。

Abstract: The maintenance risks of open source software (OSS) projects pose significant
threats to the quality, security, and resilience of modern software supply
chains. While prior research has proposed diverse approaches for predicting OSS
maintenance risk -- leveraging signals ranging from surface features (e.g.,
stars, commits) to social network analyses and behavioral patterns -- existing
methods often suffer from ambiguous operational definitions, limited
interpretability, and datasets of insufficient scale or generalizability. In
this work, we introduce ``maintenance cessation'', grounded in both explicit
archival status and rigorous semantic analysis of project documentation.
Building on this foundation, we curate a large-scale, longitudinal dataset of
115,466 GitHub repositories -- encompassing 57,733 confirmed cessation events
-- complemented by comprehensive, timeline-based behavioral features. We
propose an integrated, multi-perspective feature framework for predicting
maintenance cessation, systematically combining user-centric features,
maintainer-centric features and project evolution features. AFT survival
analysis demonstrates a high C-index (0.846), substantially outperforming
models relying only on surface features. Feature ablation and SHAP analysis
further confirm the effectiveness and interpretability of our approach.
Finally, we demonstrate real-world applicability by deploying a GBSA classifier
in the openEuler ecosystem for proactive package risk screening. Our work
establishes a scalable, interpretable foundation for maintenance-risk
prediction, enabling reproducible risk management across large-scale open
source ecosystems.

</details>


### [17] [MultiAIGCD: A Comprehensive dataset for AI Generated Code Detection Covering Multiple Languages, Models,Prompts, and Scenarios](https://arxiv.org/abs/2507.21693)
*Basak Demirok,Mucahid Kutlu,Selin Mergen*

Main category: cs.SE

TL;DR: 论文介绍了MultiAIGCD数据集，用于检测AI生成的代码，覆盖Python、Java和Go语言，并评估了三种先进检测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）在代码生成中的广泛应用，维护学术诚信和招聘公平性需要开发可靠的AI生成代码检测系统。

Method: 从CodeNet数据集的问题定义和人类编写的代码中，使用六种LLM和三种提示生成代码样本，涵盖三种关键使用场景。构建了包含121,271个AI生成和32,148个人类编写的代码片段的数据集。

Result: 评估了三种先进检测模型在跨模型和跨语言等测试场景中的性能。

Conclusion: MultiAIGCD数据集和代码公开，以支持该领域的研究。

Abstract: As large language models (LLMs) rapidly advance, their role in code
generation has expanded significantly. While this offers streamlined
development, it also creates concerns in areas like education and job
interviews. Consequently, developing robust systems to detect AI-generated code
is imperative to maintain academic integrity and ensure fairness in hiring
processes. In this study, we introduce MultiAIGCD, a dataset for AI-generated
code detection for Python, Java, and Go. From the CodeNet dataset's problem
definitions and human-authored codes, we generate several code samples in Java,
Python, and Go with six different LLMs and three different prompts. This
generation process covered three key usage scenarios: (i) generating code from
problem descriptions, (ii) fixing runtime errors in human-written code, and
(iii) correcting incorrect outputs. Overall, MultiAIGCD consists of 121,271
AI-generated and 32,148 human-written code snippets. We also benchmark three
state-of-the-art AI-generated code detection models and assess their
performance in various test scenarios such as cross-model and cross-language.
We share our dataset and codes to support research in this field.

</details>


### [18] [Vibe Coding as a Reconfiguration of Intent Mediation in Software Development: Definition, Implications, and Research Agenda](https://arxiv.org/abs/2507.21928)
*Christian Meske,Tobias Hermanns,Esther von der Weiden,Kai-Uwe Loser,Thorsten Berger*

Main category: cs.SE

TL;DR: 论文探讨了AI生成的代码（vibe coding）如何改变软件开发范式，强调人类与生成式AI通过自然语言对话协作，重新分配认知工作，并提出了机遇与风险。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成代码的普及，快速采用与有限概念理解之间的脱节促使对这一新兴范式进行研究。

Method: 通过意图视角和历史分析，定义vibe coding为人类与生成式AI通过自然语言对话协作的软件开发范式。

Result: vibe coding重新分配认知工作，将专业知识从传统领域转向协作编排，同时揭示了机遇与风险。

Conclusion: 提出了涵盖人类、技术和组织方向的研究议程，以指导未来对这一范式的探索。

Abstract: Software development is undergoing a fundamental transformation as vibe
coding becomes widespread, with large portions of contemporary codebases now
being AI-generated. The disconnect between rapid adoption and limited
conceptual understanding highlights the need for an inquiry into this emerging
paradigm. Drawing on an intent perspective and historical analysis, we define
vibe coding as a software development paradigm where humans and generative AI
engage in collaborative flow to co-create software artifacts through natural
language dialogue, shifting the mediation of developer intent from
deterministic instruction to probabilistic inference. By intent mediation, we
refer to the fundamental process through which developers translate their
conceptual goals into representations that computational systems can execute.
Our results show that vibe coding reconfigures cognitive work by redistributing
epistemic labor between humans and machines, shifting the expertise in the
software development process away from traditional areas such as design or
technical implementation toward collaborative orchestration. We identify key
opportunities, including democratization, acceleration, and systemic leverage,
alongside risks, such as black box codebases, responsibility gaps, and
ecosystem bias. We conclude with a research agenda spanning human-,
technology-, and organization-centered directions to guide future
investigations of this paradigm.

</details>


### [19] [DeepGo: Predictive Directed Greybox Fuzzing](https://arxiv.org/abs/2507.21952)
*Peihong Lin,Pengfei Wang,Xu Zhou,Wei Xie,Gen Zhang,Kai Lu*

Main category: cs.SE

TL;DR: DeepGo是一种预测性定向灰盒模糊测试工具，结合历史和预测信息，通过最优路径引导模糊测试到达目标位置。


<details>
  <summary>Details</summary>
Motivation: 现有定向灰盒模糊测试（DGF）技术依赖启发式算法优化适应度指标，缺乏对未执行路径的预见性，导致难以处理复杂约束路径，效率低下。

Method: 提出路径转移模型，将DGF建模为通过特定路径转移序列到达目标的过程；使用深度神经网络构建虚拟集成环境（VEE）预测路径转移及其奖励；开发强化学习模型（RLF）生成最优路径转移序列；提出动作组概念优化模糊测试关键步骤。

Result: DeepGo能够高效引导模糊测试通过最优路径到达目标位置。

Conclusion: DeepGo通过结合历史和预测信息，显著提升了定向灰盒模糊测试的效率和目标达成能力。

Abstract: The state-of-the-art DGF techniques redefine and optimize the fitness metric
to reach the target sites precisely and quickly. However, optimizations for
fitness metrics are mainly based on heuristic algorithms, which usually rely on
historical execution information and lack foresight on paths that have not been
exercised yet. Thus, those hard-to-execute paths with complex constraints would
hinder DGF from reaching the targets, making DGF less efficient. In this paper,
we propose DeepGo, a predictive directed grey-box fuzzer that can combine
historical and predicted information to steer DGF to reach the target site via
an optimal path. We first propose the path transition model, which models DGF
as a process of reaching the target site through specific path transition
sequences. The new seed generated by mutation would cause the path transition,
and the path corresponding to the high-reward path transition sequence
indicates a high likelihood of reaching the target site through it. Then, to
predict the path transitions and the corresponding rewards, we use deep neural
networks to construct a Virtual Ensemble Environment (VEE), which gradually
imitates the path transition model and predicts the rewards of path transitions
that have not been taken yet. To determine the optimal path, we develop a
Reinforcement Learning for Fuzzing (RLF) model to generate the transition
sequences with the highest sequence rewards. The RLF model can combine
historical and predicted path transitions to generate the optimal path
transition sequences, along with the policy to guide the mutation strategy of
fuzzing. Finally, to exercise the high-reward path transition sequence, we
propose the concept of an action group, which comprehensively optimizes the
critical steps of fuzzing to realize the optimal path to reach the target
efficiently.

</details>


### [20] [Fine-Tuning Code Language Models to Detect Cross-Language Bugs](https://arxiv.org/abs/2507.21954)
*Zengyang Li,Yimeng Li,Binbin Huang,Peng Liang,Ran Mo,Hui Liu,Yutao Ma*

Main category: cs.SE

TL;DR: 本文研究了预训练代码语言模型（CodeLMs）在检测跨语言错误（CLBs）中的潜力，开发了工具CLCFinder并构建了一个CLB数据集。实验表明，微调后的CodeLMs性能显著提升，其中UniXcoder-base表现最佳。


<details>
  <summary>Details</summary>
Motivation: 多语言编程日益普遍，但跨语言错误（CLBs）难以通过单语言工具检测，因此探索CodeLMs在CLB检测中的潜力。

Method: 开发了CLCFinder工具，构建了涉及三种编程语言组合的CLB数据集，微调了13个CodeLMs并评估其性能。

Result: 微调后CodeLMs性能显著提升，UniXcoder-base表现最佳（F1=0.7407）。小模型表现优于大模型，数据集大小对性能有显著影响。

Conclusion: CodeLMs在CLB检测中具有潜力，但需针对CLB特性进行微调，数据集规模和模型选择是关键因素。

Abstract: Multilingual programming, which involves using multiple programming languages
(PLs) in a single project, is increasingly common due to its benefits. However,
it introduces cross-language bugs (CLBs), which arise from interactions between
different PLs and are difficult to detect by single-language bug detection
tools. This paper investigates the potential of pre-trained code language
models (CodeLMs) in CLB detection. We developed CLCFinder, a cross-language
code identification tool, and constructed a CLB dataset involving three PL
combinations (Python-C/C++, Java-C/C++, and Python-Java) with nine interaction
types. We fine-tuned 13 CodeLMs on this dataset and evaluated their
performance, analyzing the effects of dataset size, token sequence length, and
code comments. Results show that all CodeLMs performed poorly before
fine-tuning, but exhibited varying degrees of performance improvement after
fine-tuning, with UniXcoder-base achieving the best F1 score (0.7407). Notably,
small fine-tuned CodeLMs tended to performe better than large ones. CodeLMs
fine-tuned on single-language bug datasets performed poorly on CLB detection,
demonstrating the distinction between CLBs and single-language bugs.
Additionally, increasing the fine-tuning dataset size significantly improved
performance, while longer token sequences did not necessarily improve the model
performance. The impact of code comments varied across models. Some fine-tuned
CodeLMs' performance was improved, while others showed degraded performance.

</details>
