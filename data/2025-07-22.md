<div id=toc></div>

# Table of Contents

- [cs.LO](#cs.LO) [Total: 9]
- [cs.SE](#cs.SE) [Total: 36]
- [cs.PL](#cs.PL) [Total: 8]
- [cs.FL](#cs.FL) [Total: 9]


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [1] [A Proof System with Causal Labels (Part I): checking Individual Fairness and Intersectionality](https://arxiv.org/abs/2507.14650)
*Leonardo Ceragioli,Giuseppe Primiero*

Main category: cs.LO

TL;DR: 扩展TNDPQ演算以验证概率分类器中的个体公平性和交叉性，通过限制Weakening规则的应用条件实现。


<details>
  <summary>Details</summary>
Motivation: 解决概率分类器中个体公平性和交叉性的验证问题。

Method: 扩展TNDPQ演算，通过因果标签限制Weakening规则的应用条件。

Result: 提出了验证个体公平性和交叉性的新方法。

Conclusion: 扩展的TNDPQ演算能有效验证概率分类器的公平性和交叉性。

Abstract: In this article we propose an extension to the typed natural deduction
calculus TNDPQ to model verification of individual fairness and
intersectionality in probabilistic classifiers. Their interpretation is
obtained by formulating specific conditions for the application of the
structural rule of Weakening. Such restrictions are given by causal labels used
to check for conditional independence between protected and target variables.

</details>


### [2] [A Proof System with Causal Labels (Part II): checking Counterfactual Fairness](https://arxiv.org/abs/2507.14655)
*Leonardo Ceragioli,Giuseppe Primiero*

Main category: cs.LO

TL;DR: 扩展TNDPQ类型自然演绎演算，用于验证概率分类器中的反事实公平性。


<details>
  <summary>Details</summary>
Motivation: 为概率分类器中的反事实公平性提供形式化验证方法。

Method: 通过制定因果标签的结构条件，并检查其变化下的评估鲁棒性。

Result: 提出了一种验证反事实公平性的方法。

Conclusion: 扩展的TNDPQ能有效验证概率分类器的反事实公平性。

Abstract: In this article we propose an extension to the typed natural deduction
calculus TNDPQ to model verification of counterfactual fairness in
probabilistic classifiers. This is obtained formulating specific structural
conditions for causal labels and checking that evaluation is robust under their
variation.

</details>


### [3] [PSPACE-completeness of bimodal transitive weak-density logic](https://arxiv.org/abs/2507.14949)
*Philippe Balbiani,Olivier Gasquet*

Main category: cs.LO

TL;DR: 本文探讨了使用窗口方法解决双模态逻辑的满足性问题，特别是弱密度和传递性的结合，证明其满足性和有效性均为PSPACE完全问题。


<details>
  <summary>Details</summary>
Motivation: 重新审视双模态K4逻辑，并扩展窗口方法以解决弱密度与传递性结合的满足性问题。

Method: 结合双模态K算法与窗口方法，设计多项式算法。

Result: 证明这些逻辑的满足性和有效性均为PSPACE完全问题。

Conclusion: 窗口方法能有效解决复杂逻辑的满足性问题，扩展了其应用范围。

Abstract: Windows have been introduce in \cite{BalGasq25} as a tool for designing
polynomial algorithms to check satisfiability of a bimodal logic of
weak-density. In this paper, after revisiting the ``folklore'' case of bimodal
$\K4$ already treated in \cite{Halpern} but which is worth a fresh review, we
show that windows allow to polynomially solve the satisfiability problem when
adding transitivity to weak-density, by mixing algorithms for bimodal K
together with windows-approach. The conclusion is that both satisfiability and
validity are PSPACE-complete for these logics.

</details>


### [4] [PSPACE-completeness of Grammar logics of bounded density](https://arxiv.org/abs/2507.14956)
*Olivier Gasquet*

Main category: cs.LO

TL;DR: 多模态逻辑的有限密度问题通过窗口方法证明为PSPACE完全问题，同时单模态逻辑密度问题属于para-PSPACE。


<details>
  <summary>Details</summary>
Motivation: 研究多模态逻辑的有限密度问题及其可满足性，探索其计算复杂性。

Method: 使用基于有限窗口的表方法，引用先前研究中的技术。

Result: 证明了多模态逻辑的可满足性问题为PSPACE完全问题，单模态逻辑密度问题属于para-PSPACE。

Conclusion: 多模态逻辑的有限密度问题具有PSPACE完全复杂性，单模态逻辑密度问题在para-PSPACE中。

Abstract: We introduce the family of multi-modal logics of bounded density and with a
tableau-like approach using finite \emph{windows} which were introduced in
\cite{BalGasq25}, we prove that their satisfiability problem is
PSPACE-complete. As a side effect, the monomodal logic of density is shown to
be in para-PSPACE.

</details>


### [5] [A meta-modal logic for bisimulations](https://arxiv.org/abs/2507.15117)
*Alfredo Burrieza,Fernando Soler-Toscano,Antonio Yuste-Ginel*

Main category: cs.LO

TL;DR: 论文提出了一种模态研究，扩展了基本模态语言并定义了双模拟，同时提供了双模拟关联的Kripke模型的完备公理化。


<details>
  <summary>Details</summary>
Motivation: 研究双模拟的模态定义及其在Kripke模型中的表现。

Method: 扩展基本模态语言，引入新模态[b]，定义双模拟；构建双模拟关联的Kripke模型的公理化系统。

Result: 证明了双模拟在扩展语言中的可定义性，并提供了完备的公理化系统。

Conclusion: 扩展模态语言能有效定义双模拟，且公理化系统完备。

Abstract: We propose a modal study of the notion of bisimulation. Our contribution is
twofold. First, we extend the basic modal language with a new modality [b],
whose intended meaning is universal quantification over all states that are
bisimilar to the current one. We show that bisimulations are definable in this
object language. Second, we provide a sound and complete axiomatisation of the
class of all pairs of Kripke models linked by bisimulations.

</details>


### [6] [STL-GO: Spatio-Temporal Logic with Graph Operators for Distributed Systems with Multiple Network Topologies](https://arxiv.org/abs/2507.15147)
*Yiqi Zhao,Xinyi Yu,Bardh Hoxha,Georgios Fainekos,Jyotirmoy V. Deshmukh,Lars Lindemann*

Main category: cs.LO

TL;DR: 论文提出了一种新的逻辑STL-GO，用于建模和监控多智能体系统的需求，通过图操作符支持对智能体间交互的丰富推理，并设计了分布式监控条件。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在复杂任务中需要满足安全性和可靠性，现有方法难以捕捉其多样化的感知和通信需求。

Method: 引入STL-GO逻辑，利用图操作符分析智能体交互，并提出基于局部信息的分布式监控条件。

Result: STL-GO在表达能力和实用性上优于现有时空逻辑，案例研究验证了其有效性。

Conclusion: STL-GO为多智能体系统的需求建模和监控提供了新工具，具有实际应用潜力。

Abstract: Multi-agent systems (MASs) consisting of a number of autonomous agents that
communicate, coordinate, and jointly sense the environment to achieve complex
missions can be found in a variety of applications such as robotics, smart
cities, and internet-of-things applications. Modeling and monitoring MAS
requirements to guarantee overall mission objectives, safety, and reliability
is an important problem. Such requirements implicitly require reasoning about
diverse sensing and communication modalities between agents, analysis of the
dependencies between agent tasks, and the spatial or virtual distance between
agents. To capture such rich MAS requirements, we model agent interactions via
multiple directed graphs, and introduce a new logic -- Spatio-Temporal Logic
with Graph Operators (STL-GO). The key innovation in STL-GO are graph operators
that enable us to reason about the number of agents along either the incoming
or outgoing edges of the underlying interaction graph that satisfy a given
property of interest; for example, the requirement that an agent should sense
at least two neighboring agents whose task graphs indicate the ability to
collaborate. We then propose novel distributed monitoring conditions for
individual agents that use only local information to determine whether or not
an STL-GO specification is satisfied. We compare the expressivity of STL-GO
against existing spatio-temporal logic formalisms, and demonstrate the utility
of STL-GO and our distributed monitors in a bike-sharing and a multi-drone case
study.

</details>


### [7] [Quantum Programming in Polylogarithmic Time](https://arxiv.org/abs/2507.15415)
*Florent Ferrari,Emmanuel Hainry,Romain Péchoux,Mário Silva*

Main category: cs.LO

TL;DR: 论文介绍了一种量子编程语言，首次以编程语言为基础表征了FBQPOLYLOG类，证明了其完备性和正确性，并提供了编译策略。


<details>
  <summary>Details</summary>
Motivation: 研究如何在量子计算模型中实现多对数时间复杂度的可行性，并探索其编程语言表征。

Method: 引入一种支持一阶递归过程的量子编程语言，证明其能计算FBQPOLYLOG类函数，并提供编译到量子电路的策略。

Result: 证明了编程语言的完备性和正确性，并展示了FBQPOLYLOG与QNC的分离关系。

Conclusion: 该编程语言为FBQPOLYLOG提供了首个编程语言表征，并揭示了其与QNC的严格包含关系。

Abstract: Polylogarithmic time delineates a relevant notion of feasibility on several
classical computational models such as Boolean circuits or parallel random
access machines. As far as the quantum paradigm is concerned, this notion
yields the complexity class FBQPOLYLOG of functions approximable in
polylogarithmic time with a quantum random-access Turing machine. We introduce
a quantum programming language with first-order recursive procedures, which
provides the first programming-language-based characterization of FBQPOLYLOG.
Each program computes a function in FBQPOLYLOG (soundness) and, conversely,
each function of this complexity class is computed by a program (completeness).
We also provide a compilation strategy from programs to uniform families of
quantum circuits of polylogarithmic depth and polynomial size, whose set of
computed functions is known as QNC, and recover the well-known separation
result FBQPOLYLOG $\subsetneq$ QNC.

</details>


### [8] [A SHACL-based Data Consistency Solution for Contract Compliance Verification](https://arxiv.org/abs/2507.15420)
*Robert David,Albin Ahmeti,Geni Bushati,Amar Tauqeer,Anna Fensel*

Main category: cs.LO

TL;DR: 论文提出了一种基于知识图谱和SHACL的半自动化方法，改进GDPR合同合规性验证工具ACT，通过修复策略解决数据不一致问题。


<details>
  <summary>Details</summary>
Motivation: 现有工具ACT在GDPR合同合规性验证中存在局限性，缺乏语义互操作性和数据一致性支持，需要改进。

Method: 采用知识图谱和SHACL，提出半自动化修复策略，自动生成解决方案以恢复数据一致性。

Result: 方法已实现并集成到ACT中，测试验证了其正确性和性能。

Conclusion: 新方法有效解决了ACT的局限性，支持用户管理GDPR合规合同生命周期数据。

Abstract: In recent years, there have been many developments for GDPR-compliant data
access and sharing based on consent. For more complex data sharing scenarios,
where consent might not be sufficient, many parties rely on contracts. Before a
contract is signed, it must undergo the process of contract negotiation within
the contract lifecycle, which consists of negotiating the obligations
associated with the contract. Contract compliance verification (CCV) provides a
means to verify whether a contract is GDPR-compliant, i.e., adheres to legal
obligations and there are no violations. The rise of knowledge graph (KG)
adoption, enabling semantic interoperability using well-defined semantics,
allows CCV to be applied on KGs. In the scenario of different participants
negotiating obligations, there is a need for data consistency to ensure that
CCV is done correctly. Recent work introduced the automated contracting tool
(ACT), a KG-based and ODRL-employing tool for GDPR CCV, which was developed in
the Horizon 2020 project smashHit (https://smashhit.eu). Although the tool
reports violations with respect to obligations, it had limitations in verifying
and ensuring compliance, as it did not use an interoperable semantic formalism,
such as SHACL, and did not support users in resolving data inconsistencies. In
this work, we propose a novel approach to overcome these limitations of ACT. We
semi-automatically resolve CCV inconsistencies by providing repair strategies,
which automatically propose (optimal) solutions to the user to re-establish
data consistency and thereby support them in managing GDPR-compliant contract
lifecycle data. We have implemented the approach, integrated it into ACT and
tested its correctness and performance against basic CCV consistency
requirements.

</details>


### [9] [Computation of Interpolants for Description Logic Concepts in Hard Cases](https://arxiv.org/abs/2507.15689)
*Jean Christoph Jung,Jędrzej Kołodziejski,Frank Wolter*

Main category: cs.LO

TL;DR: 本文提出了首个计算ALC-插值的基本算法，适用于ALCH和ALCQ本体下的ALC和ALCQ概念。


<details>
  <summary>Details</summary>
Motivation: 研究描述逻辑（DLs）中无Craig插值性质（CIP）或弱化DL插值的计算和大小问题。

Method: 基于最近的插值存在性决策程序，开发了两种基本算法。

Result: 成功计算了ALC-插值，但观察到统一插值可能具有非基本大小。

Conclusion: 算法为弱化DL插值提供了有效工具，但统一插值的大小问题仍需进一步研究。

Abstract: While the computation of Craig interpolants for description logics (DLs) with
the Craig Interpolation Property (CIP) is well understood, very little is known
about the computation and size of interpolants for DLs without CIP or if one
aims at interpolating concepts in a weaker DL than the DL of the input ontology
and concepts. In this paper, we provide the first elementary algorithms
computing (i) ALC-interpolants between ALC-concepts under ALCH-ontologies and
(ii) ALC-interpolants between ALCQ-concepts under ALCQ-ontologies. The
algorithms are based on recent decision procedures for interpolant existence.
We also observe that, in contrast, uniform (possibly depth restricted)
interpolants might be of non-elementary size.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [10] [Impact of Code Context and Prompting Strategies on Automated Unit Test Generation with Modern General-Purpose Large Language Models](https://arxiv.org/abs/2507.14256)
*Jakub Walczak,Piotr Tomalak,Artur Laskowski*

Main category: cs.SE

TL;DR: 研究探讨了代码上下文和提示策略对LLM生成单元测试质量的影响，发现包含文档字符串显著提升代码充分性，而链式思维提示策略效果最佳。


<details>
  <summary>Details</summary>
Motivation: 提升软件工程中单元测试的自动生成效率，减少开发阶段的工作量。

Method: 评估不同LLM在多种上下文和提示策略下生成的单元测试质量，包括代码充分性、分支覆盖率和变异分数。

Result: 包含文档字符串显著提升测试质量，链式思维提示策略效果最佳，M5模型表现最优。

Conclusion: 代码上下文和提示策略对LLM生成单元测试质量有显著影响，链式思维策略和文档字符串是关键因素。

Abstract: Generative AI is gaining increasing attention in software engineering, where
testing remains an indispensable reliability mechanism. According to the widely
adopted testing pyramid, unit tests constitute the majority of test cases and
are often schematic, requiring minimal domain expertise. Automatically
generating such tests under the supervision of software engineers can
significantly enhance productivity during the development phase of the software
lifecycle.
  This paper investigates the impact of code context and prompting strategies
on the quality and adequacy of unit tests generated by various large language
models (LLMs) across several families. The results show that including
docstrings notably improves code adequacy, while further extending context to
the full implementation yields definitely smaller gains. Notably, the
chain-of-thought prompting strategy -- applied even to 'reasoning' models --
achieves the best results, with up to 96.3\% branch coverage, a 57\% average
mutation score, and near-perfect compilation success rate. Among the evaluated
models, M5 (Gemini 2.5 Pro) demonstrated superior performance in both mutation
score and branch coverage being still in top in terms of compilation success
rate.
  All the code and resulting test suites are publicly available at
https://github.com/peetery/LLM-analysis.

</details>


### [11] [Leveraging LLMs for Formal Software Requirements -- Challenges and Prospects](https://arxiv.org/abs/2507.14330)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: VERIFAI项目旨在通过NLP、本体建模、工件重用和LLMs等技术，自动化或半自动化地从非正式需求生成可验证的规范。


<details>
  <summary>Details</summary>
Motivation: 从模糊的自然语言需求生成正式规范是确保软件正确性的关键挑战。

Method: 结合NLP、本体建模、工件重用和LLMs技术。

Result: 初步文献综述识别了挑战和研究方向。

Conclusion: VERIFAI项目为填补非正式需求与可验证规范之间的差距提供了潜在解决方案。

Abstract: Software correctness is ensured mathematically through formal verification,
which involves the resources of generating formal requirement specifications
and having an implementation that must be verified. Tools such as
model-checkers and theorem provers ensure software correctness by verifying the
implementation against the specification. Formal methods deployment is
regularly enforced in the development of safety-critical systems e.g.
aerospace, medical devices and autonomous systems. Generating these
specifications from informal and ambiguous natural language requirements
remains the key challenge. Our project, VERIFAI^{1}, aims to investigate
automated and semi-automated approaches to bridge this gap, using techniques
from Natural Language Processing (NLP), ontology-based domain modelling,
artefact reuse, and large language models (LLMs). This position paper presents
a preliminary synthesis of relevant literature to identify recurring challenges
and prospective research directions in the generation of verifiable
specifications from informal requirements.

</details>


### [12] [Developing Shared Vocabulary System For Collaborative Software Engineering](https://arxiv.org/abs/2507.14396)
*Carey Lai Zheng Hui,Johnson Britto Jessia Esther Leena,Kumuthini Subramanian,Zhao Chenyu,Shubham Rajeshkumar Jariwala*

Main category: cs.SE

TL;DR: 研究探讨了软件开发中沟通障碍的技术因素，并通过共享词汇系统改善了文档和代码的清晰度与协作效率。


<details>
  <summary>Details</summary>
Motivation: 软件开发中的沟通障碍导致误解、低效和缺陷，亟需解决。

Method: 采用设计科学研究框架，分三阶段：问题识别、方法开发和实证验证。

Result: 共享词汇系统显著提高了信息密度、文档清晰度和协作效率。

Conclusion: 研究为改进软件工程沟通提供了实用建议，并指出了未来研究方向。

Abstract: Effective communication is a critical factor in successful software
engineering collaboration. However, communication gaps remain a persistent
challenge, often leading to misunderstandings, inefficiencies, and defects.
This research investigates the technical factors contributing to such
misunderstandings and explores the measurable benefits of establishing shared
vocabulary systems within software documentation and codebases. Using a Design
Science Research (DSR) framework, the study was structured into three iterative
phases: problem identification, method development, and empirical validation.
The problem identification phase involved thematic analysis of communication
data and semi-structured interviews, revealing key factors such as ambiguous
messaging, misalignment in documentation, inconsistent code review feedback,
and API integration miscommunication. Grounded Theory principles were employed
to design a structured methodology for collaborative vocabulary development.
Empirical validation through controlled experiments demonstrated that while
initial adoption introduced overhead, the shared vocabulary system
significantly improved information density, documentation clarity, and
collaboration efficiency over time. Findings offer actionable insights for
improving communication practices in software engineering, while also
identifying limitations and directions for future research.

</details>


### [13] [On the Effect of Token Merging on Pre-trained Models for Code](https://arxiv.org/abs/2507.14423)
*Mootez Saad,Hao Li,Tushar Sharma,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 研究探讨了合并代码语言模型中子标记隐藏表示的方法，以减少计算开销并提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统代码语言模型的标记化输出较长，可能导致计算开销增加，因此研究如何合并子标记表示以提高效率。

Method: 提出两种策略：基于平均表示的方法和学习型方法，可无缝集成现有代码语言模型。

Result: 实验显示，策略可减少浮点运算1%至19%，下游任务中漏洞检测F1分数下降1.82分，但代码翻译CodeBLEU提升2.47分。

Conclusion: 该研究为提升代码语言模型的计算效率和下游性能提供了有效方法。

Abstract: Tokenization is a fundamental component of language models for code. It
involves breaking down the input into units that are later passed to the
language model stack to learn high-dimensional representations used in various
contexts, from classification to generation. However, the output of these
tokenizers is often longer than that traditionally used in compilers and
interpreters. This could result in undesirable effects, such as increased
computational overhead. In this work, we investigate the effect of merging the
hidden representations of subtokens that belong to the same semantic unit, such
as subtokens that form a single identifier. We propose two strategies: one
based on averaging the representations and another that leverages a
learning-based approach. Both methods can be seamlessly integrated with
existing language models for code. We conduct experiments using six language
models for code: CodeBERT, GraphCodeBERT, UniXCoder, CdoeT5, CodeT5+ (220M),
and CodeT5+ (770M), across three software engineering tasks: vulnerability
detection, code classification, and code translation. Results show that these
strategies can reduce the number of floating-point operations by $1\%$ to
$19\%$. Regarding downstream performance, the most significant degradation was
observed in the vulnerability detection task, where the F1 score decreased by
$1.82$ points compared to the baseline. In contrast, for code translation, we
observed an improvement of $2.47$ points in CodeBLEU. This work contributes to
the broader effort of improving language models for code across multiple
dimensions, including both computational efficiency and downstream performance.

</details>


### [14] [Architectural Degradation: Definition, Motivations, Measurement and Remediation Approaches](https://arxiv.org/abs/2507.14547)
*Noman Ahmad,Ruoyu Su,Matteo Esposito,Andrea Janes,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: 该研究通过多源文献综述，统一了对架构退化的理解，包括定义、原因、指标和修复策略，揭示了技术与社会因素的结合，并指出持续修复的不足。


<details>
  <summary>Details</summary>
Motivation: 当前文献对架构退化的定义、指标和修复策略分散，研究旨在统一理解并填补研究空白。

Method: 对108项研究进行多源文献综述，提取定义、原因、指标、测量方法、工具和修复策略，并开发分类法。

Result: 架构退化从低层次问题转向社会技术问题，定义了54种指标和31种测量技术，但持续修复工具不足。

Conclusion: 研究呼吁整合指标、工具和修复逻辑，采取整体、主动的策略以实现可持续架构。

Abstract: Architectural degradation, also known as erosion, decay, or aging, impacts
system quality, maintainability, and adaptability. Although widely
acknowledged, current literature shows fragmented definitions, metrics, and
remediation strategies. Our study aims to unify understanding of architectural
degradation by identifying its definitions, causes, metrics, tools, and
remediation approaches across academic and gray literature. We conducted a
multivocal literature review of 108 studies extracting definitions, causes,
metrics, measurement approaches, tools, and remediation strategies. We
developed a taxonomy encompassing architectural, code, and process debt to
explore definition evolution, methodological trends, and research gaps.
Architectural degradation has shifted from a low-level issue to a
socio-technical concern. Definitions now address code violations, design drift,
and structural decay. Causes fall under architectural (e.g., poor
documentation), code (e.g., hasty fixes), and process debt (e.g., knowledge
loss). We identified 54 metrics and 31 measurement techniques, focused on
smells, cohesion/coupling, and evolution. Yet, most tools detect issues but
rarely support ongoing or preventive remediation. Degradation is both technical
and organizational. While detection is well-studied, continuous remediation
remains lacking. Our study reveals missed integration between metrics, tools,
and repair logic, urging holistic, proactive strategies for sustainable
architecture.

</details>


### [15] [Emerging Trends in Software Architecture from the Practitioners Perspective: A Five Year Review](https://arxiv.org/abs/2507.14554)
*Ruoyu Su,Noman ahmad,Matteo Esposito,Andrea Janes,Davide Taibi,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: 研究分析了五年内八个行业会议的软件架构趋势，发现Kubernetes、Cloud Native等技术主导实践，主要应用于DevOps后期阶段。


<details>
  <summary>Details</summary>
Motivation: 理解云计算、微服务等技术对软件架构的影响，揭示行业趋势。

Method: 分析5,677个行业会议演讲，使用大语言模型和专家验证提取技术及其用途。

Result: 发现450种技术中，Kubernetes等技术占主导，形成五个技术社区，主要支持混合部署。

Conclusion: 少数核心技术主导当前实践，研究需更全面关注架构设计与质量。

Abstract: Software architecture plays a central role in the design, development, and
maintenance of software systems. With the rise of cloud computing,
microservices, and containers, architectural practices have diversified.
Understanding these shifts is vital. This study analyzes software architecture
trends across eight leading industry conferences over five years. We
investigate the evolution of software architecture by analyzing talks from top
practitioner conferences, focusing on the motivations and contexts driving
technology adoption. We analyzed 5,677 talks from eight major industry
conferences, using large language models and expert validation to extract
technologies, their purposes, and usage contexts. We also explored how
technologies interrelate and fit within DevOps and deployment pipelines. Among
450 technologies, Kubernetes, Cloud Native, Serverless, and Containers dominate
by frequency and centrality. Practitioners present technology mainly related to
deployment, communication, AI, and observability. We identify five technology
communities covering automation, coordination, cloud AI, monitoring, and
cloud-edge. Most technologies span multiple DevOps stages and support hybrid
deployment. Our study reveals that a few core technologies, like Kubernetes and
Serverless, dominate the contemporary software architecture practice. These are
mainly applied in later DevOps stages, with limited focus on early phases like
planning and coding. We also show how practitioners frame technologies by
purpose and context, reflecting evolving industry priorities. Finally, we
observe how only research can provide a more holistic lens on architectural
design, quality, and evolution.

</details>


### [16] [Harnessing LLMs for Document-Guided Fuzzing of OpenCV Library](https://arxiv.org/abs/2507.14558)
*Bin Duan,Tarek Mahmud,Meiru Che,Yan Yan,Naipeng Dong,Dan Dongseong Kim,Guowei Yang*

Main category: cs.SE

TL;DR: VISTAFUZZ利用大型语言模型（LLMs）对OpenCV库进行文档引导的模糊测试，发现并修复了多个新bug。


<details>
  <summary>Details</summary>
Motivation: OpenCV库的bug可能影响下游计算机视觉应用，因此需要确保其可靠性。

Method: VISTAFUZZ通过LLMs解析API文档，提取参数约束和依赖关系，生成测试输入。

Result: 测试了330个API，发现17个新bug，其中10个已确认，5个已修复。

Conclusion: VISTAFUZZ是一种有效的OpenCV库测试方法，能显著提升其可靠性。

Abstract: The combination of computer vision and artificial intelligence is
fundamentally transforming a broad spectrum of industries by enabling machines
to interpret and act upon visual data with high levels of accuracy. As the
biggest and by far the most popular open-source computer vision library, OpenCV
library provides an extensive suite of programming functions supporting
real-time computer vision. Bugs in the OpenCV library can affect the downstream
computer vision applications, and it is critical to ensure the reliability of
the OpenCV library. This paper introduces VISTAFUZZ, a novel technique for
harnessing large language models (LLMs) for document-guided fuzzing of the
OpenCV library. VISTAFUZZ utilizes LLMs to parse API documentation and obtain
standardized API information. Based on this standardized information, VISTAFUZZ
extracts constraints on individual input parameters and dependencies between
these. Using these constraints and dependencies, VISTAFUZZ then generates new
input values to systematically test each target API. We evaluate the
effectiveness of VISTAFUZZ in testing 330 APIs in the OpenCV library, and the
results show that VISTAFUZZ detected 17 new bugs, where 10 bugs have been
confirmed, and 5 of these have been fixed.

</details>


### [17] [A first look at License Variants in the PyPI Ecosystem](https://arxiv.org/abs/2507.14594)
*Weiwei Xu,Hengzhi Ye,Kai Gao,Minghui Zhou*

Main category: cs.SE

TL;DR: 论文研究了开源许可证变体在PyPI生态系统中的普遍性和影响，开发了LV-Parser和LV-Compat工具以提高许可证分析的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 开源许可证变体在现代软件系统中普遍存在且影响重大，但现有工具未能有效处理这些变体，导致许可证分析的效率和效果受限。

Method: 通过实证研究分析PyPI生态系统中的许可证变体，并开发了基于差异分析和大语言模型的LV-Parser工具，以及自动化检测许可证不兼容性的LV-Compat流程。

Result: 研究发现许可证变体普遍但实质性修改较少（2%），但导致10.7%的下游依赖不兼容。LV-Parser准确率达0.936且计算成本降低30%，LV-Compat检测不兼容包数量是现有方法的5.2倍，精度达0.98。

Conclusion: 研究填补了许可证变体在软件生态中的知识空白，并为开发者提供了实用工具以应对开源许可证的复杂性。

Abstract: Open-source licenses establish the legal foundation for software reuse, yet
license variants, including both modified standard licenses and custom-created
alternatives, introduce significant compliance complexities. Despite their
prevalence and potential impact, these variants are poorly understood in modern
software systems, and existing tools do not account for their existence,
leading to significant challenges in both effectiveness and efficiency of
license analysis. To fill this knowledge gap, we conduct a comprehensive
empirical study of license variants in the PyPI ecosystem. Our findings show
that textual variations in licenses are common, yet only 2% involve substantive
modifications. However, these license variants lead to significant compliance
issues, with 10.7% of their downstream dependencies found to be
license-incompatible.
  Inspired by our findings, we introduce LV-Parser, a novel approach for
efficient license variant analysis leveraging diff-based techniques and large
language models, along with LV-Compat, an automated pipeline for detecting
license incompatibilities in software dependency networks. Our evaluation
demonstrates that LV-Parser achieves an accuracy of 0.936 while reducing
computational costs by 30%, and LV-Compat identifies 5.2 times more
incompatible packages than existing methods with a precision of 0.98.
  This work not only provides the first empirical study into license variants
in software packaging ecosystem but also equips developers and organizations
with practical tools for navigating the complex landscape of open-source
licensing.

</details>


### [18] [On the Effectiveness of Large Language Models in Writing Alloy Formulas](https://arxiv.org/abs/2502.15441)
*Yang Hong,Shan Jiang,Yulei Fu,Sarfraz Khurshid*

Main category: cs.SE

TL;DR: 论文探讨了使用大型语言模型（LLMs）生成Alloy声明式公式的实验，展示了LLMs在从自然语言描述生成完整公式、创建等价公式以及补全公式草图方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 声明式规范在开发安全可靠的软件系统中至关重要，但正确编写规范仍具挑战性。研究旨在探索LLMs在提升规范编写能力方面的作用。

Method: 实验分三部分：1）从自然语言描述生成完整Alloy公式；2）生成与给定Alloy公式等价的替代公式；3）补全Alloy公式草图并填充缺失部分。使用ChatGPT和DeepSeek两种LLMs，评估了11个已知规范。

Result: LLMs在从自然语言或Alloy输入生成完整公式方面表现良好，并能枚举多个独特解。此外，LLMs成功补全了公式草图，无需测试用例。

Conclusion: LLMs为规范编写提供了令人兴奋的进步，有望在软件开发中发挥关键作用，提升构建健壮软件的能力。

Abstract: Declarative specifications have a vital role to play in developing safe and
dependable software systems. Writing specifications correctly, however, remains
particularly challenging. This paper presents a controlled experiment on using
large language models (LLMs) to write declarative formulas in the well-known
language Alloy. Our use of LLMs is three-fold. One, we employ LLMs to write
complete Alloy formulas from given natural language descriptions (in English).
Two, we employ LLMs to create alternative but equivalent formulas in Alloy with
respect to given Alloy formulas. Three, we employ LLMs to complete sketches of
Alloy formulas and populate the holes in the sketches by synthesizing Alloy
expressions and operators so that the completed formulas accurately represent
the desired properties (that are given in natural language). We conduct the
experimental evaluation using 11 well-studied subject specifications and employ
two popular LLMs, namely ChatGPT and DeepSeek. The experimental results show
that the LLMs generally perform well in synthesizing complete Alloy formulas
from input properties given in natural language or in Alloy, and are able to
enumerate multiple unique solutions. Moreover, the LLMs are also successful at
completing given sketches of Alloy formulas with respect to natural language
descriptions of desired properties (without requiring test cases). We believe
LLMs offer a very exciting advance in our ability to write specifications, and
can help make specifications take a pivotal role in software development and
enhance our ability to build robust software.

</details>


### [19] [An Efficient Algorithm for Generating Minimal Unique-Cause MC/DC Test cases for Singular Boolean Expressions](https://arxiv.org/abs/2507.14687)
*Robin Lee,Youngho Nam*

Main category: cs.SE

TL;DR: 提出了一种名为'Robin's Rule'的确定性算法，为具有N个条件的SBEs直接构建N+1个测试用例的最小测试集，确保100% Unique-Cause MC/DC覆盖率，无需生成完整的真值表。


<details>
  <summary>Details</summary>
Motivation: Unique-Cause MC/DC是确保关键系统可靠性和安全性的严格覆盖标准，但其高效测试生成研究不足，尤其是针对SBEs的测试生成。

Method: 提出了'Robin's Rule'算法，通过直接构建最小测试集来满足Unique-Cause MC/DC覆盖率，并通过TCAS-II规范验证其有效性。

Result: 实验证明该方法能始终以理论最小测试数实现100%覆盖率，且比商业工具更高效。

Conclusion: 该研究为验证安全关键系统提供了实用且最优的解决方案，兼具严格性和效率。

Abstract: Modified Condition/Decision Coverage (MC/DC) is a mandatory structural
coverage criterion for ensuring the reliability and safety of critical systems.
While its strictest form, Unique-Cause MC/DC, offers the highest assurance,
research on its efficient test generation has been lacking. This gap is
particularly significant, as an analysis of large-scale avionics systems shows
that 99.7% of all conditional decisions are, in fact, Singular Boolean
Expressions (SBEs) the ideal structure for applying Unique-Cause MC/DC. This
paper proposes 'Robin's Rule', a deterministic algorithm that directly
constructs a minimal test set of N + 1 cases to guarantee 100% Unique-Cause
MC/DC for SBEs with N conditions, without generating a full truth table. To
validate our approach, we constructed a benchmark by reformulating the TCAS-II
specifications into SBEs and verified the results using an industry-standard,
certified commercial tool. The results confirm that our method consistently
achieves 100% coverage with the theoretical minimum number of tests and is more
efficient than the commercial tool. This work provides a practical and provably
optimal solution for verifying safety-critical systems, ensuring both rigor and
efficiency.

</details>


### [20] [HistoryFinder: Advancing Method-Level Source Code History Generation with Accurate Oracles and Enhanced Algorithm](https://arxiv.org/abs/2507.14716)
*Shahidul Islam,Ashik Aowal,Md Sharif Uddin,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 本研究提出了一种新的方法历史生成工具HistoryFinder，通过构建更准确的基准（oracle）并优化性能，显著优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 现有方法历史生成工具的评估因基准不准确而受限，影响了软件工程任务的效率和准确性。

Method: 结合自动化分析和专家验证构建了两个新基准（oracle），并开发了HistoryFinder工具，优化了准确性和运行性能。

Result: 在400个方法的评估中，HistoryFinder在精度、召回率和F1分数上优于其他工具，同时运行性能接近最快。

Conclusion: HistoryFinder在准确性和效率上均表现优异，是综合最佳选择，并提供了多种使用方式以促进采用。

Abstract: Reconstructing a method's change history efficiently and accurately is
critical for many software engineering tasks, including maintenance,
refactoring, and comprehension. Despite the availability of method history
generation tools such as CodeShovel and CodeTracker, existing evaluations of
their effectiveness are limited by inaccuracies in the ground truth oracles
used. In this study, we systematically construct two new oracles -- the
corrected CodeShovel oracle and a newly developed HistoryFinder oracle -- by
combining automated analysis with expert-guided manual validation. We also
introduce HistoryFinder, a new method history generation tool designed to
improve not only the accuracy and completeness of method change histories but
also to offer competitive runtime performance. Through extensive evaluation
across 400 methods from 40 open-source repositories, we show that HistoryFinder
consistently outperforms CodeShovel, CodeTracker, IntelliJ, and Git-based
baselines in terms of precision, recall, and F1 score. Moreover, HistoryFinder
achieves competitive runtime performance, offering the lowest mean and median
execution times among all the research-based tools.
  While Git-based tools exhibit the fastest runtimes, this efficiency comes at
the cost of significantly lower precision and recall -- leaving HistoryFinder
as the best overall choice when both accuracy and efficiency are important. To
facilitate adoption, we provide a web interface, CLI, and Java library for
flexible usage.

</details>


### [21] [Investigating the Role of LLMs Hyperparameter Tuning and Prompt Engineering to Support Domain Modeling](https://arxiv.org/abs/2507.14735)
*Vladyslav Bulhakov,Giordano d'Aloisio,Claudio Di Sipio,Antinisca Di Marco,Davide Di Ruscio*

Main category: cs.SE

TL;DR: 本文探讨了如何通过超参数调整和提示工程提升Llama 3.1模型在生成领域模型时的准确性，并在医疗数据模型中取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 尽管通用大语言模型（LLMs）在软件工程任务中表现优异，但在领域建模中存在局限性。微调模型虽可行，但计算资源消耗大且可能导致灾难性遗忘。

Method: 采用搜索方法对超参数进行调整，并结合提示工程，针对特定医疗数据模型优化Llama 3.1模型。

Result: 优化后的模型在医疗数据模型中表现显著优于基线LLM，并在十个不同应用领域中测试了其适用性。

Conclusion: 尽管解决方案并非普遍适用，但超参数调整与提示工程的结合在几乎所有测试的领域模型中均提升了效果。

Abstract: The introduction of large language models (LLMs) has enhanced automation in
software engineering tasks, including in Model Driven Engineering (MDE).
However, using general-purpose LLMs for domain modeling has its limitations.
One approach is to adopt fine-tuned models, but this requires significant
computational resources and can lead to issues like catastrophic forgetting.
  This paper explores how hyperparameter tuning and prompt engineering can
improve the accuracy of the Llama 3.1 model for generating domain models from
textual descriptions. We use search-based methods to tune hyperparameters for a
specific medical data model, resulting in a notable quality improvement over
the baseline LLM. We then test the optimized hyperparameters across ten diverse
application domains.
  While the solutions were not universally applicable, we demonstrate that
combining hyperparameter tuning with prompt engineering can enhance results
across nearly all examined domain models.

</details>


### [22] [Toward Inclusive AI-Driven Development: Exploring Gender Differences in Code Generation Tool Interactions](https://arxiv.org/abs/2507.14770)
*Manaal Basha,Ivan Beschastnikh,Gema Rodriguez-Perez,Cleidson R. B. de Souza*

Main category: cs.SE

TL;DR: 研究探讨代码生成工具（CGTs）在不同性别开发者中的使用差异及其对任务结果和认知负荷的影响。


<details>
  <summary>Details</summary>
Motivation: 随着CGTs的普及，其公平性和包容性尚未充分研究，尤其是性别差异可能影响工具的使用效果。

Method: 采用混合实验设计，54名参与者按性别均分，完成编程任务并收集认知负荷、任务表现等数据。

Result: 尚未得出结果，但预期揭示性别差异，为CGTs的公平设计提供依据。

Conclusion: 研究旨在推动CGTs的公平、透明和包容性设计，促进AI工具的普惠发展。

Abstract: Context: The increasing reliance on Code Generation Tools (CGTs), such as
Windsurf and GitHub Copilot, are revamping programming workflows and raising
critical questions about fairness and inclusivity. While CGTs offer potential
productivity enhancements, their effectiveness across diverse user groups have
not been sufficiently investigated. Objectives: We hypothesize that developers'
interactions with CGTs vary based on gender, influencing task outcomes and
cognitive load, as prior research suggests that gender differences can affect
technology use and cognitive processing. Methods: The study will employ a
mixed-subjects design with 54 participants, evenly divided by gender for a
counterbalanced design. Participants will complete two programming tasks
(medium to hard difficulty) with only CGT assistance and then with only
internet access. Task orders and conditions will be counterbalanced to mitigate
order effects. Data collection will include cognitive load surveys, screen
recordings, and task performance metrics such as completion time, code
correctness, and CGT interaction behaviors. Statistical analyses will be
conducted to identify statistically significant differences in CGT usage.
Expected Contributions: Our work can uncover gender differences in CGT
interaction and performance among developers. Our findings can inform future
CGT designs and help address usability and potential disparities in interaction
patterns across diverse user groups. Conclusion: While results are not yet
available, our proposal lays the groundwork for advancing fairness,
accountability, transparency, and ethics (FATE) in CGT design. The outcomes are
anticipated to contribute to inclusive AI practices and equitable tool
development for all users.

</details>


### [23] [VeriOpt: PPA-Aware High-Quality Verilog Generation via Multi-Role LLMs](https://arxiv.org/abs/2507.14776)
*Kimia Tasnia,Alexander Garcia,Tasnuva Farheen,Sazadur Rahman*

Main category: cs.SE

TL;DR: VeriOpt是一个新框架，通过角色提示和PPA优化，使LLM生成高质量Verilog代码，显著提升PPA指标。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在硬件设计中主要关注功能正确性，忽视了PPA指标，VeriOpt旨在填补这一空白。

Method: 采用角色提示（如规划师、程序员、评审员）和PPA感知优化，结合多模态反馈。

Result: 实验显示，相比基线，功耗降低88%，面积减少76%，时序闭合提升73%，功能正确性达86%。

Conclusion: VeriOpt在AI驱动的硬件设计中填补了功能与质量的鸿沟，推动了LLM在生产中的可靠应用。

Abstract: The rapid adoption of large language models(LLMs) in hardware design has
primarily focused on generating functionally correct Verilog code, overlooking
critical Power Performance-Area(PPA) metrics essential for industrial-grade
designs. To bridge this gap, we propose VeriOpt, a novel framework that
leverages role-based prompting and PPA-aware optimization to enable LLMs to
produce high-quality, synthesizable Verilog. VeriOpt structures LLM
interactions into specialized roles (e.g., Planner, Programmer, Reviewer,
Evaluator) to emulate human design workflows, while integrating PPA constraints
directly into the prompting pipeline. By combining multi-modal feedback (e.g.,
synthesis reports, timing diagrams) with PPA aware prompting, VeriOpt achieves
PPA-efficient code generation without sacrificing functional correctness.
Experimental results demonstrate up to 88% reduction in power, 76% reduction in
area and 73% improvement in timing closure compared to baseline LLM-generated
RTL, validated using industry standard EDA tools. At the same time achieves 86%
success rate in functionality evaluation. Our work advances the
state-of-the-art AI-driven hardware design by addressing the critical gap
between correctness and quality, paving the way for reliable LLM adoption in
production workflows.

</details>


### [24] [Enhancing Repository-Level Code Generation with Call Chain-Aware Multi-View Context](https://arxiv.org/abs/2507.14791)
*Yang Liu,Li Zhang,Fang Liu,Zhuohang Wang,Donglin Wei,Zhishuo Yang,Kechi Zhang,Jia Li,Lin Shi*

Main category: cs.SE

TL;DR: RepoScope提出了一种基于调用链感知的多视角上下文方法，用于仓库级代码生成，显著提升了生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在识别仓库的丰富语义和结构关系方面表现不足，导致上下文信息有限且不准确。

Method: RepoScope构建了仓库结构语义图（RSSG），并检索综合四视角上下文，结合结构和相似性上下文，提出了一种新颖的调用链预测方法和结构保持序列化算法。

Result: 在CoderEval和DevEval基准测试中，RepoScope在pass@1分数上相对提升了36.35%。

Conclusion: RepoScope通过静态分析提升了代码生成的准确性和效率，且无需额外训练或多轮LLM查询，具有广泛适用性。

Abstract: Repository-level code generation aims to generate code within the context of
a specified repository. Existing approaches typically employ
retrieval-augmented generation (RAG) techniques to provide LLMs with relevant
contextual information extracted from the repository. However, these approaches
often struggle with effectively identifying truly relevant contexts that
capture the rich semantics of the repository, and their contextual perspectives
remains narrow. Moreover, most approaches fail to account for the structural
relationships in the retrieved code during prompt construction, hindering the
LLM's ability to accurately interpret the context. To address these issues, we
propose RepoScope, which leverages call chain-aware multi-view context for
repository-level code generation. RepoScope constructs a Repository Structural
Semantic Graph (RSSG) and retrieves a comprehensive four-view context,
integrating both structural and similarity-based contexts. We propose a novel
call chain prediction method that utilizes the repository's structural
semantics to improve the identification of callees in the target function.
Additionally, we present a structure-preserving serialization algorithm for
prompt construction, ensuring the coherence of the context for the LLM.
Notably, RepoScope relies solely on static analysis, eliminating the need for
additional training or multiple LLM queries, thus ensuring both efficiency and
generalizability. Evaluation on widely-used repository-level code generation
benchmarks (CoderEval and DevEval) demonstrates that RepoScope outperforms
state-of-the-art methods, achieving up to a 36.35% relative improvement in
pass@1 scores. Further experiments emphasize RepoScope's potential to improve
code generation across different tasks and its ability to integrate effectively
with existing approaches.

</details>


### [25] [Think Like an Engineer: A Neuro-Symbolic Collaboration Agent for Generative Software Requirements Elicitation and Self-Review](https://arxiv.org/abs/2507.14969)
*Sai Zhang,Zhenchang Xing,Jieshan Chen,Dehai Zhao,Zizhong Zhu,Xiaowang Zhang,Zhiyong Feng,Xiaohong Li*

Main category: cs.SE

TL;DR: RequireCEG是一种需求提取和自我审查代理，通过因果效应图（CEGs）和神经符号协作架构，解决了用户需求描述模糊的问题，提升了生成式软件开发的效果。


<details>
  <summary>Details</summary>
Motivation: 非专业用户的需求描述通常模糊不清，导致生成式软件开发面临挑战。现有方法（如Gherkin）难以表达因果关系，因此需要一种新方法。

Method: RequireCEG通过特征树分层分析用户叙述，构建自修复的CEGs，捕捉因果关系，并优化Gherkin场景以确保一致性。

Result: 实验表明，RequireCEG在RGPair数据集上达到87%的覆盖率，多样性提升51.88%。

Conclusion: RequireCEG有效解决了用户需求模糊问题，提升了生成式软件开发的准确性和多样性。

Abstract: The vision of End-User Software Engineering (EUSE) is to empower
non-professional users with full control over the software development
lifecycle. It aims to enable users to drive generative software development
using only natural language requirements. However, since end-users often lack
knowledge of software engineering, their requirement descriptions are
frequently ambiguous, raising significant challenges to generative software
development. Although existing approaches utilize structured languages like
Gherkin to clarify user narratives, they still struggle to express the causal
logic between preconditions and behavior actions. This paper introduces
RequireCEG, a requirement elicitation and self-review agent that embeds
causal-effect graphs (CEGs) in a neuro-symbolic collaboration architecture.
RequireCEG first uses a feature tree to analyze user narratives hierarchically,
clearly defining the scope of software components and their system behavior
requirements. Next, it constructs the self-healing CEGs based on the elicited
requirements, capturing the causal relationships between atomic preconditions
and behavioral actions. Finally, the constructed CEGs are used to review and
optimize Gherkin scenarios, ensuring consistency between the generated Gherkin
requirements and the system behavior requirements elicited from user
narratives. To evaluate our method, we created the RGPair benchmark dataset and
conducted extensive experiments. It achieves an 87% coverage rate and raises
diversity by 51.88%.

</details>


### [26] [The Rise of AI Teammates in Software Engineering (SE) 3.0: How Autonomous Coding Agents Are Reshaping Software Engineering](https://arxiv.org/abs/2507.15003)
*Hao Li,Haoxiang Zhang,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 论文介绍了AIDev数据集，这是首个大规模记录AI编码代理在真实环境中操作的数据集，为研究AI在软件开发中的协作提供了实证基础。


<details>
  <summary>Details</summary>
Motivation: 随着AI队友（如自主编码代理）在软件开发中的兴起，需要实证数据来研究其实际表现和协作模式，填补理论与实践的差距。

Method: 通过收集456,000个由五个领先AI代理（如GitHub Copilot）提交的拉取请求数据，覆盖61,000个仓库和47,000名开发者，构建AIDev数据集。

Result: AI代理在提交速度上优于人类，但拉取请求的接受率较低，且代码结构更简单，揭示了信任和效用差距。

Conclusion: AIDev为研究AI原生工作流和人类-AI协作提供了开放、可扩展的资源，支持未来软件开发中AI的优化与治理。

Abstract: The future of software engineering--SE 3.0--is unfolding with the rise of AI
teammates: autonomous, goal-driven systems collaborating with human developers.
Among these, autonomous coding agents are especially transformative, now
actively initiating, reviewing, and evolving code at scale. This paper
introduces AIDev, the first large-scale dataset capturing how such agents
operate in the wild. Spanning over 456,000 pull requests by five leading
agents--OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code--across
61,000 repositories and 47,000 developers, AIDev provides an unprecedented
empirical foundation for studying autonomous teammates in software development.
  Unlike prior work that has largely theorized the rise of AI-native software
engineering, AIDev offers structured, open data to support research in
benchmarking, agent readiness, optimization, collaboration modeling, and AI
governance. The dataset includes rich metadata on PRs, authorship, review
timelines, code changes, and integration outcomes--enabling exploration beyond
synthetic benchmarks like SWE-bench. For instance, although agents often
outperform humans in speed, their PRs are accepted less frequently, revealing a
trust and utility gap. Furthermore, while agents accelerate code
submission--one developer submitted as many PRs in three days as they had in
three years--these are structurally simpler (via code complexity metrics).
  We envision AIDev as a living resource: extensible, analyzable, and ready for
the SE and AI communities. Grounding SE 3.0 in real-world evidence, AIDev
enables a new generation of research into AI-native workflows and supports
building the next wave of symbiotic human-AI collaboration. The dataset is
publicly available at https://github.com/SAILResearch/AI_Teammates_in_SE3.
  > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Software Engineering
Agent

</details>


### [27] [Survey of GenAI for Automotive Software Development: From Requirements to Executable Code](https://arxiv.org/abs/2507.15025)
*Nenad Petrovic,Vahid Zolfaghari,Andre Schamschurko,Sven Kirchner,Fengjunjie Pan,Chengdng Wu,Nils Purschke,Aleksei Velsh,Krzysztof Lebioda,Yinglei Song,Yi Zhang,Lukasz Mazur,Alois Knoll*

Main category: cs.SE

TL;DR: 本文探讨了生成式人工智能（GenAI）在汽车软件开发中的应用，重点关注需求处理、合规性和代码生成，并提出了一个通用的GenAI辅助工作流程。


<details>
  <summary>Details</summary>
Motivation: 汽车软件开发过程冗长且昂贵，GenAI有望减少人工干预和复杂流程的处理成本。

Method: 研究了三种GenAI技术（LLMs、RAG、VLMs）及其在代码生成中的提示技术，并基于文献综述提出了通用工作流程。

Result: 总结了行业合作伙伴对GenAI工具使用情况的调查结果。

Conclusion: GenAI在汽车软件开发中具有潜力，尤其是在需求处理和代码生成方面。

Abstract: Adoption of state-of-art Generative Artificial Intelligence (GenAI) aims to
revolutionize many industrial areas by reducing the amount of human
intervention needed and effort for handling complex underlying processes.
Automotive software development is considered to be a significant area for
GenAI adoption, taking into account lengthy and expensive procedures, resulting
from the amount of requirements and strict standardization. In this paper, we
explore the adoption of GenAI for various steps of automotive software
development, mainly focusing on requirements handling, compliance aspects and
code generation. Three GenAI-related technologies are covered within the
state-of-art: Large Language Models (LLMs), Retrieval Augmented Generation
(RAG), Vision Language Models (VLMs), as well as overview of adopted prompting
techniques in case of code generation. Additionally, we also derive a
generalized GenAI-aided automotive software development workflow based on our
findings from this literature review. Finally, we include a summary of a survey
outcome, which was conducted among our automotive industry partners regarding
the type of GenAI tools used for their daily work activities.

</details>


### [28] [Can LLMs Generate User Stories and Assess Their Quality?](https://arxiv.org/abs/2507.15157)
*Giovanni Quattrocchi,Liliana Pasquale,Paola Spoletini,Luciano Baresi*

Main category: cs.SE

TL;DR: 论文探讨了如何利用LLMs在敏捷框架中自动化需求获取，并评估了LLMs生成用户故事（US）的质量及其语义评估能力。


<details>
  <summary>Details</summary>
Motivation: 需求获取是需求工程中最具挑战性的活动之一，传统方法在语义质量评估上耗时且手动。研究旨在探索LLMs在自动化和评估需求质量方面的潜力。

Method: 使用10种先进的LLMs模拟客户访谈生成US，并与人类生成的US进行质量对比，同时测试LLMs自动评估US语义质量的能力。

Result: LLMs生成的US在覆盖率和风格质量上与人类相似，但多样性和创造性较低，且满足验收标准的频率较低。LLMs在明确评估标准下能可靠评估语义质量。

Conclusion: LLMs可辅助需求获取和语义质量评估，减少人工工作量，但在多样性和创造性方面仍需改进。

Abstract: Requirements elicitation is still one of the most challenging activities of
the requirements engineering process due to the difficulty requirements
analysts face in understanding and translating complex needs into concrete
requirements. In addition, specifying high-quality requirements is crucial, as
it can directly impact the quality of the software to be developed. Although
automated tools allow for assessing the syntactic quality of requirements,
evaluating semantic metrics (e.g., language clarity, internal consistency)
remains a manual and time-consuming activity. This paper explores how LLMs can
help automate requirements elicitation within agile frameworks, where
requirements are defined as user stories (US). We used 10 state-of-the-art LLMs
to investigate their ability to generate US automatically by emulating customer
interviews. We evaluated the quality of US generated by LLMs, comparing it with
the quality of US generated by humans (domain experts and students). We also
explored whether and how LLMs can be used to automatically evaluate the
semantic quality of US. Our results indicate that LLMs can generate US similar
to humans in terms of coverage and stylistic quality, but exhibit lower
diversity and creativity. Although LLM-generated US are generally comparable in
quality to those created by humans, they tend to meet the acceptance quality
criteria less frequently, regardless of the scale of the LLM model. Finally,
LLMs can reliably assess the semantic quality of US when provided with clear
evaluation criteria and have the potential to reduce human effort in
large-scale assessments.

</details>


### [29] [Deep Learning Framework Testing via Heuristic Guidance Based on Multiple Model Measurements](https://arxiv.org/abs/2507.15181)
*Yinglong Zou,Juan Zhai,Chunrong Fang,Yanzhou Mu,Jiawei Liu,Zhenyu Chen*

Main category: cs.SE

TL;DR: DLMMM是一种新的深度学习框架测试方法，通过融合多种模型测量指标来提升测试效果。


<details>
  <summary>Details</summary>
Motivation: 现有测试方法在检测框架缺陷时存在三个关键局限性：未能定量测量算子组合多样性、忽略模型执行时间、未考虑不同测量指标间的相关性。

Method: DLMMM定量测量模型的缺陷检测性能、算子组合多样性和执行时间，并基于相关性融合这些指标以实现权衡。此外，设计了多级启发式指导生成测试输入模型。

Result: DLMMM通过融合多指标和设计多级启发式指导，提升了测试效果。

Conclusion: DLMMM克服了现有方法的局限性，为深度学习框架测试提供了更全面的解决方案。

Abstract: Deep learning frameworks serve as the foundation for developing and deploying
deep learning applications. To enhance the quality of deep learning frameworks,
researchers have proposed numerous testing methods using deep learning models
as test inputs. However, existing methods predominantly measure model bug
detection effectiveness as heuristic indicators, presenting three critical
limitations: Firstly, existing methods fail to quantitatively measure model's
operator combination variety, potentially missing critical operator
combinations that could trigger framework bugs. Secondly, existing methods
neglect measuring model execution time, resulting in the omission of numerous
models potential for detecting more framework bugs within limited testing time.
Thirdly, existing methods overlook correlation between different model
measurements, relying simply on single-indicator heuristic guidance without
considering their trade-offs. To overcome these limitations, we propose DLMMM,
the first deep learning framework testing method to include multiple model
measurements into heuristic guidance and fuse these measurements to achieve
their trade-off. DLMMM firstly quantitatively measures model's bug detection
performance, operator combination variety, and model execution time. After
that, DLMMM fuses the above measurements based on their correlation to achieve
their trade-off. To further enhance testing effectiveness, DLMMM designs
multi-level heuristic guidance for test input model generation.

</details>


### [30] [Cultural Impact on Requirements Engineering Activities: Bangladeshi Practitioners' View](https://arxiv.org/abs/2507.15188)
*Chowdhury Shahriar Muzammel,Maria Spichkova,James Harland*

Main category: cs.SE

TL;DR: 研究探讨了孟加拉国文化背景下需求工程（RE）的采纳情况及其文化影响。


<details>
  <summary>Details</summary>
Motivation: 需求工程是软件开发中互动密集的阶段，可能受利益相关者国家文化影响。孟加拉国IT行业增长迅速但相关研究较少，需关注文化差异以避免误解和冲突。

Method: 研究旨在调查孟加拉国文化背景下RE的采纳情况及文化对RE活动的影响。

Result: 未明确提及具体结果，但强调文化影响对RE活动的重要性。

Conclusion: 了解文化影响有助于提升RE实践的多样性、包容性及避免冲突。

Abstract: Requirements Engineering (RE) is one of the most interaction-intensive phases
of software development. This means that RE activities might be especially
impacted by stakeholders' national culture. Software development projects
increasingly have a very diverse range of stakeholders. To future-proof RE
activities, we need to help RE practitioners avoid misunderstandings and
conflicts that might arise from not understanding potential Cultural Influences
(CIs). Moreover, an awareness of CIs supports diversity and inclusion in the IT
profession. Bangladesh has a growing IT sector with some unique socio-cultural
characteristics, and has been largely overlooked in this research field. In
this study, we aim to investigate how the RE process is adopted in the context
of Bangladeshi culture and what cultural influences impact overall RE
activities.

</details>


### [31] [Towards Using Personas in Requirements Engineering: What Has Been Changed Recently?](https://arxiv.org/abs/2507.15197)
*Chowdhury Shahriar Muzammel,Maria Spichkova,James Harland*

Main category: cs.SE

TL;DR: 本文通过系统映射研究（SMS）探讨了2023年至2025年间需求工程（RE）中人物角色的最新应用趋势，尤其是生成式AI方法的影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机是了解人物角色在需求工程中的最新应用，特别是生成式AI方法对其构建和验证的影响。

Method: 采用系统映射研究（SMS）方法，分析了22篇相关文献，涵盖人物角色的表示、构建、验证及其在RE活动中的应用。

Result: 研究发现AI方法在人物角色构建和验证中的应用增多，模板化人物角色更受欢迎，且验证相关研究比例上升。

Conclusion: 结论指出生成式AI对人物角色在需求工程中的应用产生了显著影响，未来研究可进一步探索AI在此领域的作用。

Abstract: In requirements engineering (RE), personas are now being used to represent
user expectations and needs. This systematic mapping study (SMS) aims to
explore the most recent studies and to cover recent changes in trends,
especially related to the recent evolution of Generative AI approaches. Our SMS
covers the period between April 2023 and April 2025. We identified 22 relevant
publications and analysed persona representation, construction, validation, as
well as RE activities covered by personas. We identified that a number of
studies applied AI-based solutions for persona construction and validation. We
observed that template-based personas are becoming more popular nowadays. We
also observed an increase in the proportion of studies covering validation
aspects.

</details>


### [32] [SimdBench: Benchmarking Large Language Models for SIMD-Intrinsic Code Generation](https://arxiv.org/abs/2507.15224)
*Yibo He,Shuoran Zhao,Jiaming Huang,Yingjie Fu,Hao Yu,Cunjian Huang,Tao Xie*

Main category: cs.SE

TL;DR: SimdBench是首个专门为SIMD指令代码生成设计的基准测试，包含136个任务，评估了18种LLM在生成SIMD代码时的表现，发现其性能普遍低于标量代码生成。


<details>
  <summary>Details</summary>
Motivation: SIMD指令编程在性能关键任务中广泛应用，但现有代码生成基准仅关注标量代码，缺乏对SIMD代码生成能力的评估。

Method: 提出SimdBench基准测试，包含136个任务，针对五种代表性SIMD指令集（SSE、AVX、Neon、SVE、RVV），系统评估18种LLM的代码生成能力。

Result: LLM在生成SIMD代码时的正确率（pass@k）普遍低于标量代码生成。

Conclusion: SimdBench填补了SIMD代码生成评估的空白，为LLM在该领域的进一步改进提供了方向。

Abstract: SIMD (Single Instruction Multiple Data) instructions and their compiler
intrinsics are widely supported by modern processors to accelerate
performance-critical tasks. SIMD intrinsic programming, a trade-off between
coding productivity and high performance, is widely used in the development of
mainstream performance-critical libraries and daily computing tasks. Large
Language Models (LLMs), which have demonstrated strong and comprehensive
capabilities in code generation, show promise in assisting programmers with the
challenges of SIMD intrinsic programming. However, existing code-generation
benchmarks focus on only scalar code, and it is unclear how LLMs perform in
generating vectorized code using SIMD intrinsics. To fill this gap, we propose
SimdBench, the first code benchmark specifically designed for SIMD-intrinsic
code generation, comprising 136 carefully crafted tasks and targeting five
representative SIMD intrinsics: SSE (x86 Streaming SIMD Extension), AVX (x86
Advanced Vector Extension), Neon (ARM Advanced SIMD Extension), SVE (ARM
Scalable Vector Extension), and RVV (RISC-V Vector Extension). We conduct a
systematic evaluation (measuring both correctness and performance) of 18
representative LLMs on SimdBench, resulting in a series of novel and insightful
findings. Our evaluation results demonstrate that LLMs exhibit a universal
decrease in pass@k during SIMD-intrinsic code generation compared to
scalar-code generation. Our in-depth analysis highlights promising directions
for the further advancement of LLMs in the challenging domain of SIMD-intrinsic
code generation. SimdBench is fully open source at
https://anonymous.4open.science/r/SimdBench-1B3F/ to benefit the broader
research community.

</details>


### [33] [Code Clone Detection via an AlphaFold-Inspired Framework](https://arxiv.org/abs/2507.15226)
*Changguo Jia,Yi Zhan,Tianqi Zhao,Hengzhi Ye,Minghui Zhou*

Main category: cs.SE

TL;DR: AlphaCC利用AlphaFold的序列到结构建模能力，通过多语言适用的token序列表示代码片段，实现跨语言的代码克隆检测。


<details>
  <summary>Details</summary>
Motivation: 现有代码克隆检测方法难以捕捉代码语义或依赖语言特定分析器，AlphaCC受AlphaFold启发，利用序列相似性解决这一问题。

Method: AlphaCC将代码转为token序列，构建多序列对齐增强上下文理解，采用改进的注意力编码器建模依赖关系，并通过相似度评分和分类确定克隆对。

Result: 在多语言数据集上，AlphaCC表现优于基线方法，展示出强大的语义理解能力，同时保持高效性。

Conclusion: AlphaCC为跨语言代码克隆检测提供了高效且语义敏感的解决方案。

Abstract: Code clone detection, which aims to identify functionally equivalent code
fragments, plays a critical role in software maintenance and vulnerability
analysis. Substantial methods have been proposed to detect code clones, but
they fall short in capturing code semantics or relying on language-specific
analyzers. Inspired by the remarkable success of AlphaFold in predicting
three-dimensional protein structures from protein sequences, in this paper, we
leverage AlphaFold for code clone detection based on the insight that protein
sequences and token sequences share a common linear sequential structure. In
particular, we propose AlphaCC, which represents code fragments as token
sequences to ensure multi-language applicability and adapts AlphaFold's
sequence-to-structure modeling capability to infer code semantics. The pipeline
of AlphaCC goes through three steps. First, AlphaCC transforms each input code
fragment into a token sequence and, motivated by AlphaFold's use of multiple
sequence alignment (MSA) to enhance contextual understanding, constructs an MSA
from lexically similar token sequences. Second, AlphaCC adopts a modified
attention-based encoder based on AlphaFold to model dependencies within and
across token sequences. Finally, unlike AlphaFold's protein structure
prediction task, AlphaCC computes similarity scores between token sequences
through a late interaction strategy and performs binary classification to
determine code clone pairs. Comprehensive evaluations on three language-diverse
datasets demonstrate AlphaCC's applicability across multiple programming
languages. On two semantic clone detection datasets, it consistently
outperforms all baselines, showing strong semantic understanding. Moreover,
AlphaCC maintains competitive efficiency, enabling practical usage in
large-scale clone detection tasks.

</details>


### [34] [FaultLine: Automated Proof-of-Vulnerability Generation Using LLM Agents](https://arxiv.org/abs/2507.15241)
*Vikram Nitin,Baishakhi Ray,Roshanak Zilouchian Moghaddam*

Main category: cs.SE

TL;DR: FaultLine是一种基于LLM代理的工作流，用于自动生成漏洞验证测试（PoV），在跨语言环境中表现优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 软件安全漏洞报告常缺少验证测试（PoV），导致修复验证困难。生成PoV测试需要复杂的程序分析，传统方法难以实现。

Method: FaultLine通过追踪输入流（从API到漏洞点）、分析分支条件，并基于反馈循环生成PoV测试，无需语言特定的分析工具。

Result: 在多语言数据集上，FaultLine生成PoV测试的成功率比现有技术（CodeAct 2.1）高77%（16 vs. 9个项目）。

Conclusion: 分层推理可提升LLM代理在PoV测试生成中的表现，但问题仍具挑战性。代码和数据集已公开以促进研究。

Abstract: Despite the critical threat posed by software security vulnerabilities,
reports are often incomplete, lacking the proof-of-vulnerability (PoV) tests
needed to validate fixes and prevent regressions. These tests are crucial not
only for ensuring patches work, but also for helping developers understand how
vulnerabilities can be exploited. Generating PoV tests is a challenging
problem, requiring reasoning about the flow of control and data through deeply
nested levels of a program.
  We present FaultLine, an LLM agent workflow that uses a set of carefully
designed reasoning steps, inspired by aspects of traditional static and dynamic
program analysis, to automatically generate PoV test cases. Given a software
project with an accompanying vulnerability report, FaultLine 1) traces the flow
of an input from an externally accessible API ("source") to the "sink"
corresponding to the vulnerability, 2) reasons about the conditions that an
input must satisfy in order to traverse the branch conditions encountered along
the flow, and 3) uses this reasoning to generate a PoV test case in a
feedback-driven loop. FaultLine does not use language-specific static or
dynamic analysis components, which enables it to be used across programming
languages.
  To evaluate FaultLine, we collate a challenging multi-lingual dataset of 100
known vulnerabilities in Java, C and C++ projects. On this dataset, FaultLine
is able to generate PoV tests for 16 projects, compared to just 9 for CodeAct
2.1, a popular state-of-the-art open-source agentic framework. Thus, FaultLine
represents a 77% relative improvement over the state of the art. Our findings
suggest that hierarchical reasoning can enhance the performance of LLM agents
on PoV test generation, but the problem in general remains challenging. We make
our code and dataset publicly available in the hope that it will spur further
research in this area.

</details>


### [35] [Input Reduction Enhanced LLM-based Program Repair](https://arxiv.org/abs/2507.15251)
*Boyang Yang,Luyao Ren,Xin Yin,Jiadong Ren,Haoye Tian,Shunfu Jin*

Main category: cs.SE

TL;DR: ReduceFix是一种基于LLM的自动程序修复方法，通过自动减少测试输入来避免长提示中的关键信息丢失问题，显著提高了修复性能。


<details>
  <summary>Details</summary>
Motivation: LLM在长提示中难以保留关键信息，导致修复性能下降。ReduceFix旨在通过减少测试输入来解决这一问题。

Method: ReduceFix自动生成一个缩减器，最小化失败诱导的测试输入，并将缩减后的输入用于指导补丁生成。

Result: 在LFTBench基准测试中，ReduceFix平均缩减输入89.1%，修复性能提升高达53.8%。

Conclusion: 自动减少失败输入是LLM-based APR的有效补充，显著提升了其可扩展性和效果。

Abstract: Large Language Models (LLMs) have shown great potential in Automated Program
Repair (APR). Test inputs, being crucial for reasoning the root cause of
failures, are always included in the prompt for LLM-based APR. Unfortunately,
LLMs struggle to retain key information in long prompts. When the test inputs
are extensive in the prompt, this may trigger the "lost-in-the-middle" issue,
compromising repair performance. To address this, we propose ReduceFix, an
LLM-based APR approach with a built-in component that automatically reduces
test inputs while retaining their failure-inducing behavior. ReduceFix prompts
an LLM to generate a reducer that minimizes failure-inducing test inputs
without human effort, and then feeds the reduced failure-inducing inputs to
guide patch generation.
  For targeted evaluation, we constructed LFTBench, the first long-input APR
benchmark with 200 real bugs from 20 programming tasks, each paired with a
failure-inducing input whose median size is 1 MB. On this benchmark, ReduceFix
shrinks inputs by 89.1% on average and improves overall pass@10 by up to 53.8%
relative to a prompt that includes the original test, and by 17.6% compared
with omitting the test entirely. Adding the same reduction step to ChatRepair
increases its fix rate by 21.3% without other changes. Ablation studies further
highlight the impact of input length and compressed failure information on
repair success. These results underscore that automatically reducing failing
inputs is a practical and powerful complement to LLM-based APR, significantly
improving its scalability and effectiveness.

</details>


### [36] [Butterfly Effects in Toolchains: A Comprehensive Analysis of Failed Parameter Filling in LLM Tool-Agent Systems](https://arxiv.org/abs/2507.15296)
*Qian Xiong,Yuekai Huang,Ziyou Jiang,Zhiyuan Chang,Yujia Zheng,Tianhao Li,Mingyang Li*

Main category: cs.SE

TL;DR: 论文研究了工具代理范式中参数失败的问题，提出了分类和改进建议。


<details>
  <summary>Details</summary>
Motivation: 探索工具代理范式中参数失败的现象及其原因，以提高LLM的可靠性和有效性。

Method: 构建参数失败分类法，通过输入扰动方法分析失败类别与输入源的关系。

Result: 实验表明参数名称幻觉失败主要源于LLM固有局限，其他失败模式与输入源问题相关。

Conclusion: 建议标准化工具返回格式、改进错误反馈机制和确保参数一致性以提升工具代理交互效果。

Abstract: The emergence of the tool agent paradigm has broadened the capability
boundaries of the Large Language Model (LLM), enabling it to complete more
complex tasks. However, the effectiveness of this paradigm is limited due to
the issue of parameter failure during its execution. To explore this phenomenon
and propose corresponding suggestions, we first construct a parameter failure
taxonomy in this paper. We derive five failure categories from the invocation
chain of a mainstream tool agent. Then, we explore the correlation between
three different input sources and failure categories by applying 15 input
perturbation methods to the input. Experimental results show that parameter
name hallucination failure primarily stems from inherent LLM limitations, while
issues with input sources mainly cause other failure patterns. To improve the
reliability and effectiveness of tool-agent interactions, we propose
corresponding improvement suggestions, including standardizing tool return
formats, improving error feedback mechanisms, and ensuring parameter
consistency.

</details>


### [37] [StackTrans: From Large Language Model to Large Pushdown Automata Model](https://arxiv.org/abs/2507.15343)
*Kechi Zhang,Ge Li,Jia Li,Huangzhao Zhang,Yihong Dong,Jia Li,Jingjing Xu,Zhi Jin*

Main category: cs.SE

TL;DR: StackTrans通过引入隐藏状态栈改进Transformer架构，解决了其无法有效捕捉Chomsky层次结构的问题，并在多个任务中表现优于标准Transformer和其他基线模型。


<details>
  <summary>Details</summary>
Motivation: Transformer架构虽然强大，但在处理Chomsky层次结构（如正则表达式或确定性上下文无关文法）时存在局限性。受下推自动机的启发，研究者提出了StackTrans来解决这一问题。

Method: StackTrans在Transformer层之间显式引入隐藏状态栈，支持可微的栈操作（如压入和弹出），并与现有框架（如flash-attention）兼容。

Result: StackTrans在Chomsky层次结构和自然语言任务中均优于标准Transformer及其他基线模型，其360M参数的模型甚至优于参数多2-3倍的开源LLM。

Conclusion: StackTrans通过引入栈机制显著提升了Transformer的能力，尤其在处理复杂语法结构时表现出色，同时保持了高效性和可扩展性。

Abstract: The Transformer architecture has emerged as a landmark advancement within the
broad field of artificial intelligence, effectively catalyzing the advent of
large language models (LLMs). However, despite its remarkable capabilities and
the substantial progress it has facilitated, the Transformer architecture still
has some limitations. One such intrinsic limitation is its inability to
effectively capture the Chomsky hierarchy, such as regular expressions or
deterministic context-free grammars. Drawing inspiration from pushdown
automata, which efficiently resolve deterministic context-free grammars using
stacks, we propose StackTrans to address the aforementioned issue within LLMs.
Unlike previous approaches that modify the attention computation, StackTrans
explicitly incorporates hidden state stacks between Transformer layers. This
design maintains compatibility with existing frameworks like flash-attention.
Specifically, our design features stack operations -- such as pushing and
popping hidden states -- that are differentiable and can be learned in an
end-to-end manner. Our comprehensive evaluation spans benchmarks for both
Chomsky hierarchies and large-scale natural languages. Across these diverse
tasks, StackTrans consistently outperforms standard Transformer models and
other baselines. We have successfully scaled StackTrans up from 360M to 7B
parameters. In particular, our from-scratch pretrained model StackTrans-360M
outperforms several larger open-source LLMs with 2-3x more parameters,
showcasing its superior efficiency and reasoning capability.

</details>


### [38] [Applying the Chinese Wall Reverse Engineering Technique to Large Language Model Code Editing](https://arxiv.org/abs/2507.15599)
*Manatsawin Hanmongkolchai*

Main category: cs.SE

TL;DR: 论文提出了一种“中国墙”技术，通过强模型指导弱模型，提升其性能，但实际应用受限于缺乏无版权限制的公开领域模型。


<details>
  <summary>Details</summary>
Motivation: 尽管代码大语言模型（Code LLM）在编程环境中广泛应用，但其训练数据集未公开，引发版权担忧。现有模型因数据有限而性能不足，仅作为概念验证。

Method: 应用“中国墙”技术，利用高质量模型生成详细指令指导弱模型，提升其复杂任务处理能力。

Result: 实验显示，该技术使Comma v0.1 1T在CanItEdit基准上性能提升66%，Starcoder2 Instruct提升约20%。

Conclusion: 该技术能显著提升弱模型性能，但因缺乏无版权限制的公开领域模型，当前实际应用受限。

Abstract: Large language models for code (Code LLM) are increasingly utilized in
programming environments. Despite their utility, the training datasets for top
LLM remain undisclosed, raising concerns about potential copyright violations.
Some models, such as Pleias and Comma put emphasis on data curation and
licenses, however, with limited training data these models are not competitive
and only serve as proof of concepts. To improve the utility of these models, we
propose an application of the "Chinese Wall" technique, inspired by the reverse
engineering technique of the same name -- a high quality model is used to
generate detailed instructions for a weaker model. By doing so, a weaker but
ethically aligned model may be used to perform complicated tasks that,
otherwise, can only be completed by more powerful models. In our evaluation,
we've found that this technique improves Comma v0.1 1T's performance in
CanItEdit benchmark by over 66%, and Starcoder2 Instruct by roughly 20%
compared to when running the same model on the benchmark alone. The practical
application of this technique today, however, may be limited due to the lack of
models trained on public domain content without copyright restrictions.

</details>


### [39] [Hot Topics and Common Challenges: an Empirical Study of React Discussions on Stack Overflow](https://arxiv.org/abs/2507.15624)
*Yusuf Sulistyo Nugroho,Ganno Tribuana Kurniaji,Syful Islam,Mohammed Humayun Kabir,Vanesya Aura Ardity,Md. Kamal Uddin*

Main category: cs.SE

TL;DR: 该研究分析了Stack Overflow上React相关的问题，发现算法错误是最常见的问题，中声誉用户贡献最多。


<details>
  <summary>Details</summary>
Motivation: 尽管React在Web开发中很受欢迎，但用户面临的具体挑战尚不清楚，因此研究旨在填补这一空白。

Method: 采用探索性数据分析方法，研究关键词、错误分类和用户声誉相关的错误。

Result: 结果显示最常用的关键词包括code、link等，算法错误是最常见问题，中声誉用户贡献占55.77%。

Conclusion: 研究结果为React社区提供了早期实施阶段的指导建议，有助于克服采用挑战。

Abstract: React is a JavaScript library used to build user interfaces for single-page
applications. Although recent studies have shown the popularity and advantages
of React in web development, the specific challenges users face remain unknown.
Thus, this study aims to analyse the React-related questions shared on Stack
Overflow. The study utilizes an exploratory data analysis to investigate the
most frequently discussed keywords, error classification, and user
reputation-based errors, which is the novelty of this work. The results show
the top eight most frequently used keywords on React-related questions, namely,
code, link, vir, href, connect, azure, windows, and website. The error
classification of questions from the sample shows that algorithmic error is the
most frequent issue faced by all groups of users, where mid-reputation users
contribute the most, accounting for 55.77%. This suggests the need for the
community to provide guidance materials in solving algorithm-related problems.
We expect that the results of this study will provide valuable insight into
future research to support the React community during the early stages of
implementation, facilitating their ability to effectively overcome challenges
to adoption.

</details>


### [40] [SustainDiffusion: Optimising the Social and Environmental Sustainability of Stable Diffusion Models](https://arxiv.org/abs/2507.15663)
*Giordano d'Aloisio,Tosin Fadahunsi,Jay Choy,Rebecca Moussa,Federica Sarro*

Main category: cs.SE

TL;DR: SustainDiffusion通过优化超参数和提示结构，减少Stable Diffusion模型的性别和种族偏见，同时降低能耗，保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 解决Stable Diffusion模型在社会和环境可持续性方面的负面影响，如性别和种族偏见及高能耗问题。

Method: SustainDiffusion搜索最优超参数和提示结构组合，以减少偏见和能耗，同时保持图像质量。

Result: 实验表明，SustainDiffusion将性别偏见减少68%，种族偏见减少59%，能耗降低48%，且结果稳定且泛化性强。

Conclusion: SustainDiffusion证明无需微调或改变模型架构，即可提升文本到图像生成模型的社会和环境可持续性。

Abstract: Background: Text-to-image generation models are widely used across numerous
domains. Among these models, Stable Diffusion (SD) - an open-source
text-to-image generation model - has become the most popular, producing over 12
billion images annually. However, the widespread use of these models raises
concerns regarding their social and environmental sustainability.
  Aims: To reduce the harm that SD models may have on society and the
environment, we introduce SustainDiffusion, a search-based approach designed to
enhance the social and environmental sustainability of SD models.
  Method: SustainDiffusion searches the optimal combination of hyperparameters
and prompt structures that can reduce gender and ethnic bias in generated
images while also lowering the energy consumption required for image
generation. Importantly, SustainDiffusion maintains image quality comparable to
that of the original SD model.
  Results: We conduct a comprehensive empirical evaluation of SustainDiffusion,
testing it against six different baselines using 56 different prompts. Our
results demonstrate that SustainDiffusion can reduce gender bias in SD3 by 68%,
ethnic bias by 59%, and energy consumption (calculated as the sum of CPU and
GPU energy) by 48%. Additionally, the outcomes produced by SustainDiffusion are
consistent across multiple runs and can be generalised to various prompts.
  Conclusions: With SustainDiffusion, we demonstrate how enhancing the social
and environmental sustainability of text-to-image generation models is possible
without fine-tuning or changing the model's architecture.

</details>


### [41] [Modeling CubeSat Storage Battery Discharge: Equivalent Circuit Versus Machine Learning Approaches](https://arxiv.org/abs/2507.15666)
*Igor Turkin,Lina Volobuieva,Andriy Chukhray,Oleksandr Liubimov*

Main category: cs.SE

TL;DR: 文章比较了两种建模CubeSat卫星电池放电的方法：等效电路分析和机器学习，旨在为卫星电池放电建模提供合理选择。


<details>
  <summary>Details</summary>
Motivation: 研究卫星电池放电建模有助于预测自主电源系统断开的后果，并确保轨道设备的容错性。

Method: 基于CubeSat卫星的轨道数据样本，采用等效电路分析和机器学习两种方法进行建模。

Result: 等效电路方法透明但灵活性不足；机器学习模型更准确且能适应复杂条件。

Conclusion: 机器学习在建模CubeSat电池放电方面表现更优，适合实际应用。

Abstract: The subject of the article is the study and comparison of two approaches to
modelling the battery discharge of a CubeSat satellite: analytical using
equivalent circuit and machine learning. The article aims to make a reasoned
choice of the approach to modelling the battery discharge of a CubeSat
satellite. Modelling the battery discharge of a satellite will enable the
prediction of the consequences of disconnecting the autonomous power system and
ensure the fault tolerance of equipment in orbit. Therefore, the selected study
is relevant and promising. This study focuses on the analysis of CubeSat
satellite data, based explicitly on orbital data samples of the power system,
which include data available at the time of the article publication. The
dataset contains data on the voltage, current, and temperature of the battery
and solar panels attached to the five sides of the satellite. In this context,
two approaches are considered: analytical modelling based on physical laws and
machine learning, which uses empirical data to create a predictive model.
Results: A comparative analysis of the modeling results reveals that the
equivalent circuit approach has the advantage of transparency, as it identifies
possible parameters that facilitate understanding of the relationships.
However, the model is less flexible to environmental changes or non-standard
satellite behavior. The machine learning model demonstrated more accurate
results, as it can account for complex dependencies and adapt to actual
conditions, even when they deviate from theoretical assumptions.

</details>


### [42] [BugScope: Learn to Find Bugs Like Human](https://arxiv.org/abs/2507.15671)
*Jinyao Guo,Chengpeng Wang,Dominic Deluca,Jinjie Liu,Zhuo Zhang,Xiangyu Zhang*

Main category: cs.SE

TL;DR: BugScope是一个基于LLM的多智能体系统，通过模拟人类审计员学习新错误模式的方式，显著提高了软件错误检测的精度和召回率。


<details>
  <summary>Details</summary>
Motivation: 传统静态分析工具在覆盖范围和适应性上存在局限，而现有基于LLM的方法难以应对复杂错误。BugScope旨在解决这些问题。

Method: BugScope通过程序切片提取相关检测上下文，并构建定制化的检测提示，指导LLM进行准确推理。

Result: 在40个真实错误的数据集上，BugScope达到87.04%的精度和90.00%的召回率，F1分数超过现有工具0.44。在Linux内核等大规模系统中发现141个未知错误。

Conclusion: BugScope在错误检测领域具有显著的实际影响，展示了其在复杂场景下的高效性和实用性。

Abstract: Detecting software bugs remains a fundamental challenge due to the extensive
diversity of real-world defects. Traditional static analysis tools often rely
on symbolic workflows, which restrict their coverage and hinder adaptability to
customized bugs with diverse anti-patterns. While recent advances incorporate
large language models (LLMs) to enhance bug detection, these methods continue
to struggle with sophisticated bugs and typically operate within limited
analysis contexts. To address these challenges, we propose BugScope, an
LLM-driven multi-agent system that emulates how human auditors learn new bug
patterns from representative examples and apply that knowledge during code
auditing. Given a set of examples illustrating both buggy and non-buggy
behaviors, BugScope synthesizes a retrieval strategy to extract relevant
detection contexts via program slicing and then constructs a tailored detection
prompt to guide accurate reasoning by the LLM. Our evaluation on a curated
dataset of 40 real-world bugs drawn from 21 widely-used open-source projects
demonstrates that BugScope achieves 87.04% precision and 90.00% recall,
surpassing state-of-the-art industrial tools by 0.44 in F1 score. Further
testing on large-scale open-source systems, including the Linux kernel,
uncovered 141 previously unknown bugs, of which 78 have been fixed and 7
confirmed by developers, highlighting BugScope's substantial practical impact.

</details>


### [43] [Do AI models help produce verified bug fixes?](https://arxiv.org/abs/2507.15822)
*Li Huang,Ilgiz Mustafin,Marco Piccioni,Alessandro Schena,Reto Weber,Bertrand Meyer*

Main category: cs.SE

TL;DR: 论文探讨了AI（特别是大语言模型）在自动程序修复（APR）中的应用，通过实验比较了程序员使用和不使用LLM的调试效果，并提出了相关方法论和建议。


<details>
  <summary>Details</summary>
Motivation: 研究AI（尤其是大语言模型）是否能在实践中显著提升自动程序修复的效果，并探索程序员如何利用LLM辅助调试。

Method: 采用随机分组实验，一组程序员使用LLM，另一组不使用，通过程序验证工具验证修复的正确性，并采用Goal-Query-Metric方法分析数据。

Result: 实验结果与预期有差异，提出了LLM在调试中的7种使用模式，并提供了优化LLM使用的建议。

Conclusion: 研究为AI和LLM在程序修复中的合理应用提供了初步依据，并提出了可复用的实验方法和实用建议。

Abstract: Among areas of software engineering where AI techniques -- particularly,
Large Language Models -- seem poised to yield dramatic improvements, an
attractive candidate is Automatic Program Repair (APR), the production of
satisfactory corrections to software bugs. Does this expectation materialize in
practice? How do we find out, making sure that proposed corrections actually
work? If programmers have access to LLMs, how do they actually use them to
complement their own skills?
  To answer these questions, we took advantage of the availability of a
program-proving environment, which formally determines the correctness of
proposed fixes, to conduct a study of program debugging with two randomly
assigned groups of programmers, one with access to LLMs and the other without,
both validating their answers through the proof tools. The methodology relied
on a division into general research questions (Goals in the Goal-Query-Metric
approach), specific elements admitting specific answers (Queries), and
measurements supporting these answers (Metrics). While applied so far to a
limited sample size, the results are a first step towards delineating a proper
role for AI and LLMs in providing guaranteed-correct fixes to program bugs.
  These results caused surprise as compared to what one might expect from the
use of AI for debugging and APR. The contributions also include: a detailed
methodology for experiments in the use of LLMs for debugging, which other
projects can reuse; a fine-grain analysis of programmer behavior, made possible
by the use of full-session recording; a definition of patterns of use of LLMs,
with 7 distinct categories; and validated advice for getting the best of LLMs
for debugging and Automatic Program Repair.

</details>


### [44] [Investigating the Use of LLMs for Evidence Briefings Generation in Software Engineering](https://arxiv.org/abs/2507.15828)
*Mauro Marcelino,Marcos Alves,Bianca Trinkenreich,Bruno Cartaxo,Sérgio Soares,Simone D. J. Barbosa,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 该论文提出了一种基于RAG的LLM工具，用于自动生成证据简报，并通过实验比较其与人工简报在内容保真度、易理解性和实用性上的表现。


<details>
  <summary>Details</summary>
Motivation: 证据简报对软件工程行业有用，但人工制作成本高，阻碍其广泛应用。研究目标是评估LLM生成简报的可行性。

Method: 开发RAG-based LLM工具，生成两份简报，设计对照实验比较LLM与人工简报的效果。

Result: 实验结果待报告。

Conclusion: 结论将根据实验结果得出。

Abstract: [Context] An evidence briefing is a concise and objective transfer medium
that can present the main findings of a study to software engineers in the
industry. Although practitioners and researchers have deemed Evidence Briefings
useful, their production requires manual labor, which may be a significant
challenge to their broad adoption. [Goal] The goal of this registered report is
to describe an experimental protocol for evaluating LLM-generated evidence
briefings for secondary studies in terms of content fidelity, ease of
understanding, and usefulness, as perceived by researchers and practitioners,
compared to human-made briefings. [Method] We developed an RAG-based LLM tool
to generate evidence briefings. We used the tool to automatically generate two
evidence briefings that had been manually generated in previous research
efforts. We designed a controlled experiment to evaluate how the LLM-generated
briefings compare to the human-made ones regarding perceived content fidelity,
ease of understanding, and usefulness. [Results] To be reported after the
experimental trials. [Conclusion] Depending on the experiment results.

</details>


### [45] [Observing Fine-Grained Changes in Jupyter Notebooks During Development Time](https://arxiv.org/abs/2507.15831)
*Sergey Titov,Konstantin Grotov,Cristina Sarasua,Yaroslav Golubev,Dhivyabharathi Ramasamy,Alberto Bacchelli,Abraham Bernstein,Timofey Bryksin*

Main category: cs.SE

TL;DR: 该论文填补了数据科学中计算笔记本动态开发过程的研究空白，通过工具集收集开发时的代码变更，分析笔记本的动态特性和变更类型。


<details>
  <summary>Details</summary>
Motivation: 现有研究集中于细粒度日志分析，但缺乏对数据科学中计算笔记本开发过程的研究。

Method: 引入工具集收集Jupyter笔记本的代码变更，收集20名开发者的工作数据（2,655个单元格和9,207次执行），分析变更类型。

Result: 发现笔记本变更多为小修复和迭代修改，表明笔记本不仅是开发和探索工具，也是调试工具。

Conclusion: 提出了未来研究方向，并提供了新的数据集和分析方法。

Abstract: In software engineering, numerous studies have focused on the analysis of
fine-grained logs, leading to significant innovations in areas such as
refactoring, security, and code completion. However, no similar studies have
been conducted for computational notebooks in the context of data science.
  To help bridge this research gap, we make three scientific contributions: we
(1) introduce a toolset for collecting code changes in Jupyter notebooks during
development time; (2) use it to collect more than 100 hours of work related to
a data analysis task and a machine learning task (carried out by 20 developers
with different levels of expertise), resulting in a dataset containing 2,655
cells and 9,207 cell executions; and (3) use this dataset to investigate the
dynamic nature of the notebook development process and the changes that take
place in the notebooks.
  In our analysis of the collected data, we classified the changes made to the
cells between executions and found that a significant number of these changes
were relatively small fixes and code iteration modifications. This suggests
that notebooks are used not only as a development and exploration tool but also
as a debugging tool. We report a number of other insights and propose potential
future research directions on the novel data.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [46] [NPUEval: Optimizing NPU Kernels with LLMs and Open Source Compilers](https://arxiv.org/abs/2507.14403)
*Sarunas Kalade,Graham Schelle*

Main category: cs.PL

TL;DR: 论文介绍了NPUEval基准测试，用于评估LLM生成的NPU内核代码的功能正确性和向量化效率，结果显示前沿模型表现有限，平均得分仅10%。


<details>
  <summary>Details</summary>
Motivation: NPU编程较新且开发者社区分散，LLM预训练数据中缺乏领域优化代码示例，需要基准测试推动研究。

Method: 提出NPUEval基准，包含102个常见机器学习算子，利用开源编译器工具评估LLM生成代码的功能和效率。

Result: 前沿模型如DeepSeek R1在部分内核上表现良好（50%+向量化），但整体平均得分仅10%。

Conclusion: NPUEval为代码生成和NPU内核优化研究提供了重要基准，未来需进一步改进模型表现。

Abstract: Neural processing units (NPUs) are gaining prominence in power-sensitive
devices like client devices, with AI PCs being defined by their inclusion of
these specialized processors. Running AI workloads efficiently on these devices
requires libraries of optimized kernels. Creating efficient kernels demands
expertise in domain-specific C++ with vector intrinsics and in-depth knowledge
of the target architecture. Unlike GPU programming, which has had years to
mature, NPU programming is new, with smaller and more fragmented developer
communities across hardware platforms. This fragmentation poses a challenge
when utilizing LLMs to assist in writing NPU kernels, as domain-specific
optimized code examples are underrepresented in LLM pre-training data.
  In this paper we introduce NPUEval -- a benchmark for writing and evaluating
NPU kernels, consisting of 102 common operators for machine learning workloads.
We evaluate LLM generated code on actual hardware based on both functional
correctness and vectorization efficiency using open source compiler tools
targeting the AMD NPU. We evaluate a range of state-of-the-art LLMs with a mix
of proprietary and open-weight models. Latest reasoning models like DeepSeek
R1, show promising results achieving out-of-the-box 50%+ vectorization on
select kernels. However, the average score across the entire dataset remains
roughly 10% even with compiler feedback and vectorized kernel examples --
showing that this is a challenging dataset even for frontier models. The
dataset and evaluation code will be released with a permissive open source
license, providing an essential benchmark for advancing research in code
generation and NPU kernel optimization.

</details>


### [47] [Timetide: A programming model for logically synchronous distributed systems](https://arxiv.org/abs/2507.14471)
*Logan Kenwright,Partha Roop,Nathan Allen,Călin Caşcaval,Avinash Malik*

Main category: cs.PL

TL;DR: Timetide是一种新型多时钟同步语言，解决了分布式系统中确定性编程的挑战，无需物理时钟同步。


<details>
  <summary>Details</summary>
Motivation: 传统同步语言依赖昂贵的物理时钟同步，难以扩展，确定性分布式编程仍具挑战性。

Method: 开发了多时钟语义的同步程序，基于逻辑同步模型，无需物理时钟同步。

Result: Timetide是首个支持分布式和形式化验证的多时钟同步语言。

Conclusion: Timetide为确定性分布式编程提供了高效、可扩展的解决方案。

Abstract: Massive strides in deterministic models have been made using synchronous
languages. They are mainly focused on centralised applications, as the
traditional approach is to compile away the concurrency. Time triggered
languages such as Giotto and Lingua Franca are suitable for distribution albeit
that they rely on expensive physical clock synchronisation, which is both
expensive and may suffer from scalability. Hence, deterministic programming of
distributed systems remains challenging. We address the challenges of
deterministic distribution by developing a novel multiclock semantics of
synchronous programs. The developed semantics is amenable to seamless
distribution. Moreover, our programming model, Timetide, alleviates the need
for physical clock synchronisation by building on the recently proposed logical
synchrony model for distributed systems. We discuss the important aspects of
distributing computation, such as network communication delays, and explore the
formal verification of Timetide programs. To the best of our knowledge,
Timetide is the first multiclock synchronous language that is both amenable to
distribution and formal verification without the need for physical clock
synchronisation or clock gating.

</details>


### [48] [Hear Your Code Fail, Voice-Assisted Debugging for Python](https://arxiv.org/abs/2507.15007)
*Sayed Mahbub Hasan Amiri,Md. Mainul Islam,Mohammad Shakhawat Hossen,Sayed Majhab Hasan Amiri,Mohammad Shawkat Ali Mamun,Sk. Humaun Kabir,Naznin Akter*

Main category: cs.PL

TL;DR: 论文介绍了一款创新的语音辅助调试插件，通过听觉和视觉反馈显著降低认知负荷并加快错误识别。


<details>
  <summary>Details</summary>
Motivation: 解决传统调试方式对视觉依赖性强的问题，提升编程可访问性和认知效率。

Method: 采用全局异常钩子架构，结合pyttsx3语音转换和Tkinter GUI可视化，实现多模态错误反馈。

Result: 实验显示认知负荷降低37%，错误识别速度提升78%，兼容Python 3.7+环境。

Conclusion: 插件显著提升编程可访问性，未来将整合GPT修复建议和多语言翻译。

Abstract: This research introduces an innovative voice-assisted debugging plugin for
Python that transforms silent runtime errors into actionable audible
diagnostics. By implementing a global exception hook architecture with pyttsx3
text-to-speech conversion and Tkinter-based GUI visualization, the solution
delivers multimodal error feedback through parallel auditory and visual
channels. Empirical evaluation demonstrates 37% reduced cognitive load (p<0.01,
n=50) compared to traditional stack-trace debugging, while enabling 78% faster
error identification through vocalized exception classification and
contextualization. The system achieves sub-1.2 second voice latency with under
18% CPU overhead during exception handling, vocalizing error types and
consequences while displaying interactive tracebacks with documentation deep
links. Criteria validate compatibility across Python 3.7+ environments on
Windows, macOS, and Linux platforms. Needing only two lines of integration
code, the plugin significantly boosts availability for aesthetically impaired
designers and supports multitasking workflows through hands-free error medical
diagnosis. Educational applications show particular promise, with pilot studies
indicating 45% faster debugging skill acquisition among novice programmers.
Future development will incorporate GPT-based repair suggestions and real-time
multilingual translation to further advance auditory debugging paradigms. The
solution represents a fundamental shift toward human-centric error diagnostics,
bridging critical gaps in programming accessibility while establishing new
standards for cognitive efficiency in software development workflows.

</details>


### [49] [Invariant Generation for Floating-Point Programs via Constraint Solving](https://arxiv.org/abs/2507.15017)
*Xuran Cai,Liqian Chen,Hongfei Fu*

Main category: cs.PL

TL;DR: 该论文提出了一种理论框架，结合FPTaylor的一阶微分特征和约束求解方法，用于生成浮点程序的不变量，并设计了两种多项式不变量生成算法。实验表明，该方法在时间和精度上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 浮点运算中的舍入误差累积可能导致程序失败，因此需要生成考虑浮点误差的紧致不变量以确保程序正确性。

Method: 提出理论框架，结合FPTaylor的一阶微分特征和约束求解方法，设计两种多项式不变量生成算法：一种需要初始不变量输入，另一种适用于多项式程序。

Result: 实验结果显示，该方法在时间和精度上优于现有技术。

Conclusion: 该框架有效解决了浮点程序不变量生成问题，并提升了计算效率和精度。

Abstract: In numeric-intensive computations, it is well known that the execution of
floating-point programs is imprecise as floating point arithmetics (e.g.,
addition, subtraction, multiplication, division, etc.) incurs rounding errors.
Albeit the rounding error is small for every single floating-point operation,
the aggregation of such error in multiple operations may be dramatic and cause
catastrophic program failures. Therefore, to ensure the correctness of
floating-point programs, the effect of floating point error needs to be
carefully taken into account. In this work, we consider the invariant
generation for floating point programs, whose aim is to generate tight
invariants under the perturbation of floating point errors. Our main
contribution is a theoretical framework on how to apply constraint solving
methods to address the invariant generation problem. In our framework, we
propose a novel combination between the first-order differential
characterization by FPTaylor (TOPLAS 2018) and constraint solving methods,
aiming to reduce the computational burden of constraint solving. Moreover, we
devise two polynomial invariant generation algorithms to instantiate the
framework. The first algorithm is applicable to a wide range of floating-point
operations but requires an initial (coarse) invariant as external input, while
the second does not require an initial invariant but is limited to polynomial
programs. Furthermore, we show how conditional branches, a difficult issue in
floating-point analysis, can be handled in our framework. Experimental results
show that our algorithms outperform SOTA approaches in both the time efficiency
and the precision of the generated invariants over a variety of benchmarks.

</details>


### [50] [A Few Fit Most: Improving Performance Portability of SGEMM on GPUs using Multi-Versioning](https://arxiv.org/abs/2507.15277)
*Robert Hochgraf,Sreepathi Pai*

Main category: cs.PL

TL;DR: 论文提出了一种多版本代码生成方法（portability tuning），用于解决自动调优（autotuning）在GPU环境中因环境变化导致的性能下降问题，并验证了其性能优于传统自动调优方法。


<details>
  <summary>Details</summary>
Motivation: 手动优化线性代数内核以适应不同GPU设备和应用复杂且耗时，而自动调优方法容易因环境变化（如设备或输入特性变化）而失效，需要重新调优。

Method: 采用多版本代码生成技术（multi-versioning），开发了一个称为portability tuning的框架，自动生成无需重新调优的性能可移植代码。

Result: 在CLBlast线性代数库的GEMM内核上验证，portability tuning的性能优于默认内核，接近理论最大性能的90%，且能泛化到新设备。

Conclusion: 多版本代码生成方法在性能可移植性上优于传统自动调优，且无需针对新设备重新调优。

Abstract: Hand-optimizing linear algebra kernels for different GPU devices and
applications is complex and labor-intensive. Instead, many developers use
automatic performance tuning (autotuning) to achieve high performance on a
variety of devices. However, autotuning "overfits", and must be redone if any
part of the environment changes, such as if the device or input characteristics
change.
  In most non-trivial cases, a single compute kernel cannot maintain
near-optimal performance across all environments. Changing the kernel to
specialize it to the current execution environment is possible, but on GPUs,
runtime tuning and compilation can be expensive.
  In this work, we use multi-versioning -- producing several variants of the
same code -- as a way to generate performance portable code. We describe a
framework called portability tuning that can automatically generate
multi-versioned code whose performance is portable, requiring no retuning.
  We evaluate our framework on a dataset of execution times for GEMM kernels
from the CLBlast linear algebra library. We find our portability tuning
techniques outperform CLBlast's default kernels -- often approaching within 10%
of the theoretical maximum performance -- despite CLBlast using autotuning
techniques. Further, we find that our generated programs generalize well to new
and unseen devices, matching the performance of autotuning without ever
portability tuning for those devices.

</details>


### [51] [Bayesian Separation Logic](https://arxiv.org/abs/2507.15530)
*Shing Hin Ho,Nicolas Wu,Azalea Raad*

Main category: cs.PL

TL;DR: 本文介绍了贝叶斯分离逻辑（BaSL），填补了现有分离逻辑无法处理贝叶斯更新的空白，并证明了其能建模贝叶斯编程语言的关键特性。


<details>
  <summary>Details</summary>
Motivation: 现有分离逻辑无法处理贝叶斯编程语言（BPPLs）的核心特性——贝叶斯更新，因此需要一种新的逻辑来填补这一空白。

Method: 提出贝叶斯分离逻辑（BaSL），基于Rokhlin-Simmons分解定理证明内部贝叶斯定理，并通过Kripke资源幺半群和σ-有限测度空间建模。

Result: BaSL成功建模了贝叶斯更新、非归一化分布、条件分布等概念，并验证了统计模型的属性，如贝叶斯硬币翻转的期望值。

Conclusion: BaSL为贝叶斯编程语言提供了语义支持，填补了现有分离逻辑的不足，并展示了其在实际统计模型中的应用潜力。

Abstract: Bayesian probabilistic programming languages (BPPLs) let users denote
statistical models as code while the interpreter infers the posterior
distribution. The semantics of BPPLs are usually mathematically complex and
unable to reason about desirable properties such as expected values and
independence of random variables. To reason about these properties in a
non-Bayesian setting, probabilistic separation logics such as PSL and Lilac
interpret separating conjunction as probabilistic independence of random
variables. However, no existing separation logic can handle Bayesian updating,
which is the key distinguishing feature of BPPLs.
  To close this gap, we introduce Bayesian separation logic (BaSL), a
probabilistic separation logic that gives semantics to BPPL. We prove an
internal version of Bayes' theorem using a result in measure theory known as
the Rokhlin-Simmons disintegration theorem. Consequently, BaSL can model
probabilistic programming concepts such as Bayesian updating, unnormalised
distribution, conditional distribution, soft constraint, conjugate prior and
improper prior while maintaining modularity via the frame rule. The model of
BaSL is based on a novel instantiation of Kripke resource monoid via
$\sigma$-finite measure spaces over the Hilbert cube, and the semantics of
Hoare triple is compatible with an existing denotational semantics of BPPL
based on the category of $s$-finite kernels. Using BaSL, we then prove
properties of statistical models such as the expected value of Bayesian coin
flip, correlation of random variables in the collider Bayesian network, and the
posterior distributions of the burglar alarm model, a parameter estimation
algorithm, and the Gaussian mixture model.

</details>


### [52] [Formal Analysis of Networked PLC Controllers Interacting with Physical Environments](https://arxiv.org/abs/2507.15596)
*Jaeseo Lee,Kyungmin Bae*

Main category: cs.PL

TL;DR: 提出了一种统一的形式化框架，用于分析PLC驱动的系统，整合了离散PLC语义、网络通信和连续物理行为，并通过偏序减少状态爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 现有形式化验证技术通常忽略PLC与物理环境及网络通信的交互，难以分析真实工业系统中的连续动态和通信延迟。

Method: 开发了一个统一框架，结合离散PLC语义、网络通信和连续物理行为，并应用偏序减少技术以降低状态爆炸。

Result: 框架能够精确分析具有连续动态和网络通信的PLC驱动系统，同时通过偏序减少显著减少状态数量。

Conclusion: 该框架为复杂PLC系统的形式化验证提供了有效工具，解决了现有技术忽略环境交互的问题。

Abstract: Programmable Logic Controllers (PLCs) are widely used in industrial
automation to control physical systems. As PLC applications become increasingly
complex, ensuring their correctness is crucial. Existing formal verification
techniques focus on individual PLC programs in isolation, often neglecting
interactions with physical environments and network communication between
controllers. This limitation poses significant challenges in analyzing
real-world industrial systems, where continuous dynamics and communication
delays play a critical role. In this paper, we present a unified formal
framework that integrates discrete PLC semantics, networked communication, and
continuous physical behaviors. To mitigate state explosion, we apply partial
order reduction, significantly reducing the number of explored states while
maintaining correctness. Our framework enables precise analysis of PLC-driven
systems with continuous dynamics and networked communication.

</details>


### [53] [Closure Conversion, Flat Environments, and the Complexity of Abstract Machines](https://arxiv.org/abs/2507.15843)
*Beniamino Accattoli,Dan Ghica,Giulio Guerrieri,Cláudio Belo Lourenço,Claudio Sacerdoti Coen*

Main category: cs.PL

TL;DR: 本文研究了闭包转换与抽象机器之间的关系，提出了新的证明技术、环境处理方法和时间复杂性分析。


<details>
  <summary>Details</summary>
Motivation: 探讨闭包转换与抽象机器中闭包和环境概念的异同，并研究其对程序正确性和效率的影响。

Method: 采用简单的λ演算和元组作为源语言，分析源语言和目标语言的抽象机器，专注于扁平闭包/环境的情况。

Result: 提出了新的闭包转换正确性证明技术，改进了抽象机器中环境处理方式，并证明闭包转换不影响整体时间复杂度。

Conclusion: 闭包转换虽增加代码大小但降低动态成本，且不改变整体复杂性，为编译器优化提供了新思路。

Abstract: Closure conversion is a program transformation at work in compilers for
functional languages to turn inner functions into global ones, by building
closures pairing the transformed functions with the environment of their free
variables. Abstract machines rely on similar and yet different concepts of
closures and environments.
  In this paper, we study the relationship between the two approaches. We adopt
a very simple {\lambda}-calculus with tuples as source language and study
abstract machines for both the source language and the target of closure
conversion. Moreover, we focus on the simple case of flat
closures/environments, that is, with no sharing of environments. We provide
three contributions.
  Firstly, a new simple proof technique for the correctness of closure
conversion, inspired by abstract machines.
  Secondly, we show how the closure invariants of the target language allow us
to design a new way of handling environments in abstract machines, not
suffering the shortcomings of other styles.
  Thirdly, we study the machines from the point of view of time complexity,
adapting analyses by Accattoli and co-authors. We show that closure conversion
decreases various dynamic costs while increasing the size of the initial code.
Despite these changes, the overall complexity of the machines before and after
closure conversion turns out to be the same.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [54] [Studying homing and synchronizing sequences for Timed Finite State Machines with output delays](https://arxiv.org/abs/2507.14526)
*Evgenii Vinarskii,Jakub Ruszil,Adam Roman,Natalia Kushik*

Main category: cs.FL

TL;DR: 论文研究了带输出延迟的定时有限状态机（TFSM）的最终状态识别序列（同步和归位序列），并探讨了其性质。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解定时有限状态机中同步和归位序列的特性，以及这些特性与无定时状态机的差异。

Method: 方法包括形式化定义同步和归位序列，并探讨了截断后继树和FSM抽象等方法在TFSM中的适用性。

Result: 结果表明，某些无定时状态机的性质在定时状态机中不成立，并确定了适用特定方法的TFSM子类。

Conclusion: 结论指出需要针对不同子类采用不同方法，并评估了生成最短序列的复杂度。

Abstract: The paper introduces final state identification (synchronizing and homing)
sequences for Timed Finite State Machines (TFSMs) with output delays and
investigates their properties. We formally define the notions of homing
sequences (HSs) and synchronizing sequences (SSs) for these TFSMs and
demonstrate that several properties that hold for untimed machines do not
necessarily apply to timed ones. Furthermore, we explore the applicability of
various approaches for deriving SSs and HSs for Timed FSMs with output delays,
such as truncated successor tree-based and FSM abstraction-based methods.
Correspondingly, we identify the subclasses of TFSMs for which these approaches
can be directly applied and those for which other methods are required.
Additionally, we evaluate the complexity of existence check and derivation of
(shortest) HSs / SSs for TFSMs with output delays.

</details>


### [55] [Input-Driven Pushdown Automata with Translucent Input Letters](https://arxiv.org/abs/2507.15310)
*Martin Kutrib,Andreas Malcher,Matthias Wendlandt*

Main category: cs.FL

TL;DR: 研究了具有透明输入字母的输入驱动下推自动机，比较了确定性与非确定性模型的差异，分析了返回与非返回模式的计算能力，并探讨了语言族的闭包性质及可判定性问题。


<details>
  <summary>Details</summary>
Motivation: 探讨输入驱动下推自动机在透明输入字母下的行为差异，特别是在不同模式（返回与非返回）下的计算能力，以及确定性与非确定性模型的表现。

Method: 通过分析输入驱动下推自动机的透明输入字母处理机制，比较确定性与非确定性模型在不同模式下的计算能力，并研究语言族的闭包性质及可判定性问题。

Result: 非确定性模型在返回和非返回模式下均比确定性模型更强；非返回模式在确定性和非确定性情况下均优于返回模式；确定性语言族在布尔运算下具有完整闭包性质，而非确定性语言族不封闭于补集；非确定性情况下的普遍性、包含性、等价性和正则性问题不可半判定。

Conclusion: 透明输入字母显著影响输入驱动下推自动机的计算能力，非确定性模型和非返回模式表现更强，但非确定性语言族的闭包性质较弱，且某些问题不可判定。

Abstract: Input-driven pushdown automata with translucent input letters are
investigated. Here, the use of translucent input letters means that the input
is processed in several sweeps and that, depending on the current state of the
automaton, some input symbols are visible and can be processed, whereas some
other symbols are invisible, and may be processed in another sweep.
Additionally, the returning mode as well as the non-returning mode are
considered, where in the former mode a new sweep must start after processing a
visible input symbol. Input-driven pushdown automata differ from traditional
pushdown automata by the fact that the actions on the pushdown store (push,
pop, nothing) are dictated by the input symbols. We obtain the result that the
input-driven nondeterministic model is computationally stronger than the
deterministic model both in the returning mode and in the non-returning mode,
whereas it is known that the deterministic and the nondeterministic model are
equivalent for input-driven pushdown automata without translucency. It also
turns out that the non-returning model is computationally stronger than the
returning model both in the deterministic and nondeterministic case.
Furthermore, we investigate the closure properties of the language families
introduced under the Boolean operations. We obtain a complete picture in the
deterministic case, whereas in the nondeterministic case the language families
are shown to be not closed under complementation. Finally, we look at
decidability questions and obtain the non-semidecidability of the questions of
universality, inclusion, equivalence, and regularity in the nondeterministic
case.

</details>


### [56] [Idefix-Closed Languages and Their Application in Contextual Grammars](https://arxiv.org/abs/2507.15312)
*Marvin Ködding,Bianca Truthe*

Main category: cs.FL

TL;DR: 本文研究了上下文文法中基于子正则语言族的选择语言的表达能力，重点分析了中缀、前缀和后缀闭包语言（统称为idefix闭包语言），并与其他子正则语言族进行了比较。此外，还扩展了现有层次结构，并解决了一个关于后缀闭包选择语言的开放性问题。


<details>
  <summary>Details</summary>
Motivation: 探索上下文文法中不同子正则语言族的选择能力，以扩展现有理论并解决未解决的问题。

Method: 通过比较中缀、前缀和后缀闭包语言与其他子正则语言族（如有限、幂分离、交换等），分析其在外部和内部上下文文法中的层次结构。

Result: 扩展了现有语言族层次结构，并解决了内部上下文文法中后缀闭包选择语言的开放性问题。

Conclusion: 研究为上下文文法的理论框架提供了新的语言族分类，并填补了相关领域的空白。

Abstract: In this paper, we continue the research on the power of contextual grammars
with selection languages from subfamilies of the family of regular languages.
We investigate infix-, prefix-, and suffix-closed languages (referred to as
idefix-closed languages) and compare such language families to some other
subregular families of languages (finite, monoidal, nilpotent, combinational,
(symmetric) definite, ordered, non-counting, power-separating, commutative,
circular, union-free, star, and comet languages). Further, we compare the
families of the hierarchies obtained for external and internal contextual
grammars with the language families defined by these new types for the
selection. In this way, we extend the existing hierarchies by new language
families. Moreover, we solve an open problem regarding internal contextual
grammars with suffix-closed selection languages.

</details>


### [57] [On a Generalization of the Christoffel Tree: Epichristoffel Trees](https://arxiv.org/abs/2507.15313)
*Abhishek Krishnamoorthy,Robinson Thamburaj,Durairaj Gnanaraj Thomas*

Main category: cs.FL

TL;DR: 本文介绍了epichristoffel树的概念，用于确定一类具有重要性质的epichristoffel词，这些词可以分解为更小的epichristoffel词。


<details>
  <summary>Details</summary>
Motivation: 研究epichristoffel词的性质，尤其是与Christoffel词的相似性和差异性。

Method: 引入epichristoffel树作为工具，分析epichristoffel词的子类及其分解性质。

Result: 确定了epichristoffel词的一个子类，这些词可以分解为更小的epichristoffel词，并展示了相关结果。

Conclusion: epichristoffel树是研究epichristoffel词的有力工具，有助于理解其性质。

Abstract: Sturmian words form a family of one-sided infinite words over a binary
alphabet that are obtained as a discretization of a line with an irrational
slope starting from the origin. A finite version of this class of words called
Christoffel words has been extensively studied for their interesting
properties. It is a class of words that has a geometric and an algebraic
definition, making it an intriguing topic of study for many mathematicians.
Recently, a generalization of Christoffel words for an alphabet with 3 letters
or more, called epichristoffel words, using episturmian morphisms has been
studied, and many of the properties of Christoffel words have been shown to
carry over to epichristoffel words; however, many properties are not shared by
them as well. In this paper, we introduce the notion of an epichristoffel tree,
which proves to be a useful tool in determining a subclass of epichristoffel
words that share an important property of Christoffel words, which is the
ability to factorize an epichristoffel word as a product of smaller
epichristoffel words. We also use the epichristoffel tree to present some
interesting results that help to better understand epichristoffel words.

</details>


### [58] [Orchestration of Music by Grammar Systems](https://arxiv.org/abs/2507.15314)
*Jozef Makiš,Alexander Meduna,Zbyněk Křivka*

Main category: cs.FL

TL;DR: 该研究定义了多生成规则同步的分散上下文语法系统，用于为整个乐团的音乐编排提供计算支持，并以古典音乐和爵士乐为例进行说明。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用语法系统解决乐团音乐编排的计算问题。

Method: 定义多生成规则同步的分散上下文语法系统（无擦除规则），并应用于古典和爵士乐编排。

Result: 展示了如何为乐团编排音乐，并提供了古典和爵士乐的实例。

Conclusion: 提出了五个与这种编排方式相关的开放问题。

Abstract: This application-oriented study concerns computational musicology, which
makes use of grammar systems. We define multi-generative rule-synchronized
scattered-context grammar systems (without erasing rules) and demonstrates how
to simultaneously make the arrangement of a musical composition for performance
by a whole orchestra, consisting of several instruments. Primarily, an
orchestration like this is illustrated by examples in terms of classical music.
In addition, the orchestration of jazz compositions is sketched as well. The
study concludes its discussion by suggesting five open problem areas related to
this way of orchestration.

</details>


### [59] [On Repetitive Finite Automata with Translucent Words](https://arxiv.org/abs/2507.15315)
*František Mráz,Friedrich Otto*

Main category: cs.FL

TL;DR: 研究了确定性（DFAwtw）和非确定性（NFAwtw）有限自动机的重复变体，发现其表达能力显著增强。


<details>
  <summary>Details</summary>
Motivation: 探索重复行为对有限自动机表达能力的影响。

Method: 引入重复变体的DFAwtw和NFAwtw，分析其计算行为。

Result: 重复DFAwtw接受的语言甚至不是半线性的，表明重复性显著增强表达能力。

Conclusion: 重复性显著提升了DFAwtw和NFAwtw的表达能力。

Abstract: We introduce and study the repetitive variants of the deterministic and the
nondeterministic finite automaton with translucent words (DFAwtw and NFAwtw).
On seeing the right sentinel, a repetitive NFAwtw need not halt immediately,
accepting or rejecting, but it may change into another state and continue with
its computation. We establish that a repetitive DFAwtw already accepts a
language that is not even semi-linear, which shows that the property of being
repetitive increases the expressive capacity of the DFAwtw and the NFAwtw
considerably.

</details>


### [60] [A Myhill-Nerode Type Characterization of 2detLIN Languages](https://arxiv.org/abs/2507.15316)
*Benedek Nagy*

Main category: cs.FL

TL;DR: 线性自动机与5' -> 3' Watson-Crick有限自动机等价，接受线性语言类LIN。确定性模型仅接受子集2detLIN，并基于Myhill-Nerode类等价性进行表征。


<details>
  <summary>Details</summary>
Motivation: 研究线性自动机及其确定性模型的表达能力，并探索其语言类的等价性表征。

Method: 使用双头线性自动机模型，基于前缀-后缀对的Myhill-Nerode类等价性进行表征。

Result: 确定性模型仅接受2detLIN语言类，且表征需满足完整性和无交叉对的条件。

Conclusion: 通过前缀-后缀对的等价类，成功表征了2detLIN语言类，并明确了其约束条件。

Abstract: Linear automata are automata with two reading heads starting from the two
extremes of the input, are equivalent to 5' -> 3' Watson-Crick (WK) finite
automata. The heads read the input in opposite directions and the computation
finishes when the heads meet. These automata accept the class LIN of linear
languages. The deterministic counterpart of these models, on the one hand, is
less expressive, as only a proper subset of LIN, the class 2detLIN is accepted;
and on the other hand, they are also equivalent in the sense of the class of
the accepted languages. Now, based on these automata models, we characterize
the class of 2detLIN languages with a Myhill-Nerode type of equivalence
classes. However, as these automata may do the computation of both the prefix
and the suffix of the input, we use prefix-suffix pairs in our classes.
Additionally, it is proven that finitely many classes in the characterization
match with the 2detLIN languages, but we have some constraints on the used
prefix-suffix pairs, i.e., the characterization should have the property to be
complete and it must not have any crossing pairs.

</details>


### [61] [On some Classes of Reversible 2-head Automata](https://arxiv.org/abs/2507.15317)
*Benedek Nagy,Walaa Yasin*

Main category: cs.FL

TL;DR: 分析了确定性双头有限自动机（2-head finite automata）在可逆计算中的能力，发现其能接受某些线性语言（如回文），但无法接受所有正则语言。同时，证明了受限变体（如1-limited和complete reversible 2-head finite automata）的计算能力较弱，并形成严格层次结构。


<details>
  <summary>Details</summary>
Motivation: 研究双头有限自动机在可逆计算中的表现，探索其计算能力及受限变体的特性。

Method: 分析确定性双头有限自动机的可逆性，定义受限变体（1-limited和complete reversible），并通过状态分类和输入完全读取要求进行特征化。

Result: 发现双头有限自动机能接受某些线性语言（如回文），但无法处理所有正则语言；受限变体形成严格层次结构。

Conclusion: 双头有限自动机在可逆计算中具有独特能力，受限变体的研究揭示了计算能力的层次性。

Abstract: Deterministic 2-head finite automata which are machines that process an input
word from both ends are analyzed for their ability to perform reversible
computations. This implies that the automata are backward deterministic,
enabling unique forward and backward computation. We explore the computational
power of such automata, discovering that, while some regular languages cannot
be accepted by these machines, they are capable of accepting some
characteristic linear languages, e.g., the language of palindromes.
Additionally, we prove that restricted variants, i.e., both 1-limited
reversible 2-head finite automata and complete reversible 2-head finite
automata are less powerful and they form a proper hierarchy. In the former, in
each computation step exactly one input letter is being processed, i.e., only
one of the heads can read a letter. These automata are also characterized by
putting their states to classes based on the head(s) used to reach and to leave
the state. In the complete reversible 2-head finite automata, it is required
that any input can be fully read by the automaton. The accepted families are
also compared to the classes generated by left deterministic linear grammars.

</details>


### [62] [The theory of reachability in trace-pushdown systems](https://arxiv.org/abs/2507.15733)
*Dietrich Kuske*

Main category: cs.FL

TL;DR: 研究了存储Mazurkiewicz迹而非单个字的下推系统，识别了一类可判定其一阶理论的可达性配置图的系统。


<details>
  <summary>Details</summary>
Motivation: 扩展下推系统的表达能力，研究更复杂的存储结构（如Mazurkiewicz迹和多栈系统）的配置图理论。

Method: 将下推系统与Mazurkiewicz迹结合，分析其作为图幺半群的价自动机特性，并识别特定类别的系统。

Result: 发现一类系统的一阶理论可达性可判定，补充了D'Osualdo等人的结果。

Conclusion: 该研究为复杂存储结构的自动机理论提供了新的可判定性结果。

Abstract: We consider pushdown systems that store, instead of a single word, a
Mazurkiewicz trace on its stack. These systems are special cases of valence
automata over graph monoids and subsume multi-stack systems. We identify a
class of such systems that allow to decide the first-order theory of their
configuration graph with reachability.
  This result complements results by D'Osualdo, Meyer, and Zetzsche (namely the
decidability for arbitrary pushdown systems under a severe restriction on the
dependence alphabet).

</details>
