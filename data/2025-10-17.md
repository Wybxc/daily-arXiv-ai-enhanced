<div id=toc></div>

# Table of Contents

- [cs.LO](#cs.LO) [Total: 4]
- [cs.FL](#cs.FL) [Total: 3]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 18]


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [1] [T-BAT semantics and its logics](https://arxiv.org/abs/2510.14361)
*Pawel Pawlowski*

Main category: cs.LO

TL;DR: T-BAT逻辑是一个四值非确定性逻辑系统，用于表达非正式可证性概念。本文研究了该语义的各种弱化和强化变体，证明了所有可定义逻辑的完备性，并提供了直观的公理化。


<details>
  <summary>Details</summary>
Motivation: 研究T-BAT逻辑的四值非确定性语义，探索不同语义变体与所诱导公理之间的复杂相互作用，并为该逻辑提供直观的公理化。

Method: 通过将真值转换为语义对象语言中的特定表达式来证明所有可定义逻辑的完备性，同时使用克里普克语义从模态角度分析公理，提供它们诱导的框架条件。

Result: 证明了所有在T-BAT语义中可定义逻辑的完备性，并通过克里普克语义为这些公理提供了相应的框架条件。

Conclusion: 成功研究了T-BAT逻辑的四值非确定性语义，建立了完备性结果，并为该逻辑提供了直观的公理化系统。

Abstract: \textbf{T-BAT} logic is a formal system designed to express the notion of
informal provability. This type of provability is closely related to
mathematical practice and is quite often contrasted with formal provability,
understood as a formal derivation in an appropriate formal system.
\textbf{T-BAT} is a non-deterministic four-valued logic. The logical values in
\textbf{T-BAT} semantics convey not only the information whether a given
formula is true but also about its provability status.
  The primary aim of our paper is to study the proposed four-valued
non-deterministic semantics. We look into the intricacies of the interactions
between various weakenings and strengthenings of the semantics with axioms that
they induce. We prove the completeness of all the logics that are definable in
this semantics by transforming truth values into specific expressions
formulated within the object language of the semantics. Additionally, we
utilize Kripke semantics to examine these axioms from a modal perspective by
providing a frame condition that they induce. The secondary aim of this paper
is to provide an intuitive axiomatization of \textbf{T-BAT} logic.

</details>


### [2] [Optimization Modulo Integer Linear-Exponential Programs](https://arxiv.org/abs/2510.14550)
*S Hitarth,Alessio Mansutti,Guruprerana Shabadi*

Main category: cs.LO

TL;DR: 该论文研究了整数线性指数规划的优化问题复杂性，建立了最优解可以用整数线性指数直线程序(ILESLP)简洁表示，并给出了在整数分解预言机下的多项式时间算法，将该问题置于NPO扩展类中。


<details>
  <summary>Details</summary>
Motivation: 研究整数线性指数规划的优化问题复杂性，这类问题扩展了经典整数线性规划，加入了指数函数和取模运算，其判定问题已被证明是NP完全的。

Method: 1. 证明最优解可以用ILESLP简洁表示；2. 设计在整数分解预言机下的多项式时间算法来验证ILESLP编码的解；3. 将该优化问题置于NPO的扩展类中。

Result: 1. 最优解可以用ILESLP表示；2. 存在多项式时间算法验证ILESLP解；3. 该优化问题属于FNP^NP类中的NPO扩展。

Conclusion: 整数线性指数规划的优化问题具有可简洁表示的最优解，在适当计算模型下可高效处理，并被置于合适的复杂性类中。

Abstract: This paper presents the first study of the complexity of the optimization
problem for integer linear-exponential programs which extend classical integer
linear programs with the exponential function $x \mapsto 2^x$ and the remainder
function ${(x,y) \mapsto (x \bmod 2^y)}$. The problem of deciding if such a
program has a solution was recently shown to be NP-complete in [Chistikov et
al., ICALP'24]. The optimization problem instead asks for a solution that
maximizes (or minimizes) a linear-exponential objective function, subject to
the constraints of an integer linear-exponential program. We establish the
following results:
  1. If an optimal solution exists, then one of them can be succinctly
represented as an integer linear-exponential straight-line program (ILESLP): an
arithmetic circuit whose gates always output an integer value (by construction)
and implement the operations of addition, exponentiation, and multiplication by
rational numbers.
  2. There is an algorithm that runs in polynomial time, given access to an
integer factoring oracle, which determines whether an ILESLP encodes a solution
to an integer linear-exponential program. This algorithm can also be used to
compare the values taken by the objective function on two given solutions.
  Building on these results, we place the optimization problem for integer
linear-exponential programs within an extension of the optimization class
$\text{NPO}$ that lies within $\text{FNP}^{\text{NP}}$. In essence, this
extension forgoes determining the optimal solution via binary search.

</details>


### [3] [Problems and Consequences of Bilateral Notions of (Meta-)Derivability](https://arxiv.org/abs/2510.14619)
*Sara Ayhan*

Main category: cs.LO

TL;DR: 本文探讨了双边主义证明论语义学在序列演算中的实现问题，指出需要同时双化序列符号和水平线，这揭示了证明与反驳概念之间的不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 双边主义要求证明系统不仅要显示连接词的可证性条件，还要显示其可反驳性条件。在序列演算中实现这一点面临特殊挑战，因为序列演算本身包含两种可推导关系。

Method: 分析序列演算中序列符号和水平线这两种可推导关系的双化过程，探讨双化水平线时出现的问题及其概念根源。

Result: 发现双化序列符号相对直接，但双化水平线会揭示证明与反驳概念之间的深层不平衡问题。

Conclusion: 本文分析了问题的根源，并提出了在系统中保持双边主义所需平衡的可能解决方案。

Abstract: A bilateralist take on proof-theoretic semantics can be understood as
demanding of a proof system to display not only rules giving the connectives'
provability conditions but also their refutability conditions. On such a view,
then, a system with two derivability relations is obtained, which can be quite
naturally expressed in a proof system of natural deduction but which faces
obstacles in a sequent calculus representation. Since in a sequent calculus
there are two derivability relations inherent, one expressed by the sequent
sign and one by the horizontal lines holding between sequents, in a truly
bilateral calculus both need to be dualized. While dualizing the sequent sign
is rather straightforwardly corresponding to dualizing the horizontal lines in
natural deduction, dualizing the horizontal lines in sequent calculus, uncovers
problems that, as will be argued in this paper, shed light on deeper conceptual
issues concerning an imbalance between the notions of proof vs. refutation. The
roots of this problem will be further analyzed and possible solutions on how to
retain a bilaterally desired balance in our system are presented.

</details>


### [4] [Admissibility of Substitution Rule in Cyclic-Proof Systems](https://arxiv.org/abs/2510.14749)
*Kenji Saotome,Koji Nakazawa*

Main category: cs.LO

TL;DR: 本文证明了在循环证明系统CLKID^ω中，假设存在切割规则的情况下，替换规则是可采纳的。通过将循环证明展开为无穷形式，提升替换规则，并重新放置回边来构造不含替换规则的循环证明。


<details>
  <summary>Details</summary>
Motivation: 替换规则在循环证明系统中使理论案例分析复杂化并增加证明搜索的计算成本，因为每个序列都可以是替换规则的实例的结论。因此，在理论和计算层面都希望证明替换规则的可采纳性。

Method: 将循环证明展开为无穷形式，提升替换规则，然后重新放置回边来构造不含替换规则的循环证明。如果限制替换中不包含函数符号，该方法还可扩展到更广泛的系统。

Result: 证明了在CLKID^ω系统中（假设存在切割规则）替换规则是可采纳的。当限制替换不包含函数符号时，该结果可扩展到无切割CLKID^ω和分离逻辑的循环证明系统。

Conclusion: 通过将循环证明展开为无穷形式并重新构造回边的方法，成功证明了循环证明系统中替换规则的可采纳性，这一结果对理论和计算层面都具有重要意义。

Abstract: This paper investigates the admissibility of the substitution rule in
cyclic-proof systems. The substitution rule complicates theoretical case
analysis and increases computational cost in proof search since every sequent
can be a conclusion of an instance of the substitution rule; hence,
admissibility is desirable on both fronts. While admissibility is often shown
by local proof transformations in non-cyclic systems, such transformations may
disrupt cyclic structure and do not readily apply. Prior remarks suggested that
the substitution rule is likely nonadmissible in the cyclic-proof system
CLKID^omega for first-order logic with inductive predicates. In this paper, we
prove admissibility in CLKID^omega, assuming the presence of the cut rule. Our
approach unfolds a cyclic proof into an infinitary form, lifts the substitution
rules, and places back edges to construct a cyclic proof without the
substitution rule. If we restrict substitutions to exclude function symbols,
the result extends to a broader class of systems, including cut-free
CLKID^omega and cyclic-proof systems for the separation logic.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [5] [Decidability and Characterization of Expansivity for Group Cellular Automata](https://arxiv.org/abs/2510.14568)
*Niccolo' Castronuovo,Alberto Dennunzio,Luciano Margara*

Main category: cs.FL

TL;DR: 本文研究了群元胞自动机的扩展性，为阿贝尔群提供了易于检查的扩展性特征，证明了扩展性对于一般（非阿贝尔）群是可判定性质，并证明扩展群元胞自动机严格包含在拓扑传递的单射群元胞自动机中。


<details>
  <summary>Details</summary>
Motivation: 研究群元胞自动机的扩展性特征和可判定性，特别关注阿贝尔群和一般群的区别，以及扩展性与拓扑传递性之间的关系。

Method: 使用群论和动力系统理论的方法，分析群元胞自动机的代数结构，通过构造性证明来建立扩展性的特征和可判定性。

Result: 为阿贝尔群提供了易于检查的扩展性特征，证明了扩展性对于一般群是可判定的，并建立了扩展群元胞自动机与拓扑传递单射群元胞自动机之间的严格包含关系。

Conclusion: 群元胞自动机的扩展性在阿贝尔群中具有简洁的特征化，在一般群中是可判定的，且扩展群元胞自动机是拓扑传递单射群元胞自动机的真子类。

Abstract: Group cellular automata are continuous, shift-commuting endomorphisms of
$G^\mathbb{Z}$, where $G$ is a finite group. We provide an easy-to-check
characterization of expansivity for group cellular automata on abelian groups
and we prove that expansivity is a decidable property for general (non-abelian)
groups. Moreover, we show that the class of expansive group cellular automata
is strictly contained in that of topologically transitive injective group
cellular automata.

</details>


### [6] [Efficient Verification of Metric Temporal Properties with Past in Pointwise Semantics](https://arxiv.org/abs/2510.14699)
*S. Akshay,Prerak Contractor,Paul Gastin,R. Govind,B. Srivathsan*

Main category: cs.FL

TL;DR: 提出了一种新的MITL模型检查方法，专注于在点语义下整合过去时态并最大化自动机确定性，通过将MITL转换为同步网络定时自动机，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: MITL是表达定时属性的强大逻辑，但现有方法在处理过去和未来时态组合时效率不高，需要开发更有效的模型检查技术。

Method: 定义同步网络定时自动机，将MITL过去片段线性时间转换为确定性定时自动机，扩展到完整MITL时使用广义定时自动机，并采用SCC-based活性算法分析。

Result: 原型工具在72个公式基准测试中显著优于现有技术，在点语义下MITL可满足性检查性能大幅提升，并在两个知名基准上验证了端到端模型检查算法的有效性。

Conclusion: 该方法成功整合了MITL的过去和未来时态，实现了高效的确定性自动机转换，为实时系统模型检查提供了更强大的工具。

Abstract: Model checking for real-timed systems is a rich and diverse topic. Among the
different logics considered, Metric Interval Temporal Logic (MITL) is a
powerful and commonly used logic, which can succinctly encode many interesting
timed properties especially when past and future modalities are used together.
In this work, we develop a new approach for MITL model checking in the
pointwise semantics, where our focus is on integrating past and maximizing
determinism in the translated automata.
  Towards this goal, we define synchronous networks of timed automata with
shared variables and show that the past fragment of MITL can be translated in
linear time to synchronous networks of deterministic timed automata. Moreover
determinism can be preserved even when the logic is extended with future
modalities at the top-level of the formula. We further extend this approach to
the full MITL with past, translating it into networks of generalized timed
automata (GTA) with future clocks (which extend timed automata and event clock
automata). We present an SCC-based liveness algorithm to analyse GTA. We
implement our translation in a prototype tool which handles both finite and
infinite timed words and supports past modalities. Our experimental evaluation
demonstrates that our approach significantly outperforms the state-of-the-art
in MITL satisfiability checking in pointwise semantics on a benchmark suite of
72 formulas. Finally, we implement an end-to-end model checking algorithm for
pointwise semantics and demonstrate its effectiveness on two well-known
benchmarks.

</details>


### [7] [On the order of lazy cellular automata](https://arxiv.org/abs/2510.14841)
*Edgar Alcalá-Arroyo,Alonso Castillo-Ramirez*

Main category: cs.FL

TL;DR: 研究在任意群G和字母表A上定义的懒惰元胞自动机，分析其阶数（即变换幂次的基数），建立了阶数关于活跃转移模式p和固定符号a的一般上界，并证明当p是拟常数模式时该上界可达。


<details>
  <summary>Details</summary>
Motivation: 研究最简单的一类元胞自动机——懒惰元胞自动机的动力学行为，特别是其阶数性质，因为虽然这类自动机行为相对简单，但其阶数完全依赖于活跃转移模式p和固定符号a的选择，存在微妙的问题。

Method: 通过分析懒惰元胞自动机的定义特性，建立阶数的一般上界，并针对拟常数模式这一特殊情况证明上界的可达性。

Result: 建立了懒惰元胞自动机阶数关于p和a的一般上界，并证明了当p是拟常数模式时该上界可以达到。

Conclusion: 懒惰元胞自动机的阶数性质可以通过活跃转移模式p和固定符号a来刻画，在拟常数模式情况下可以达到理论上的最大阶数。

Abstract: We study the most elementary family of cellular automata defined over an
arbitrary group universe $G$ and an alphabet $A$: the lazy cellular automata,
which act as the identity on configurations in $A^G$, except when they read a
unique active transition $p \in A^S$, in which case they write a fixed symbol
$a \in A$. As expected, the dynamical behavior of lazy cellular automata is
relatively simple, yet subtle questions arise since they completely depend on
the choice of $p$ and $a$. In this paper, we investigate the order of a lazy
cellular automaton $\tau : A^G \to A^G$, defined as the cardinality of the set
$\{ \tau^k : k \in \mathbb{N} \}$. In particular, we establish a general upper
bound for the order of $\tau$ in terms of $p$ and $a$, and we prove that this
bound is attained when $p$ is a quasi-constant pattern.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [8] [HITrees: Higher-Order Interaction Trees](https://arxiv.org/abs/2510.14558)
*Amir Mohammad Fadaei Ayyam,Michael Sammler*

Main category: cs.PL

TL;DR: 提出了高阶交互树(HITrees)，这是第一个在非守卫类型理论中支持高阶效应的交互树变体，能够建模并行组合和call/cc等复杂语义特性。


<details>
  <summary>Details</summary>
Motivation: 传统交互树虽然提供了可重用的效应库，但其效应概念不支持高阶效应（即接受或返回单子计算的效应），而这些效应对于建模并行组合和call/cc等复杂语义特性至关重要。

Method: 采用两种关键技术：1）设计效应概念使得具有高阶输入的效应的不动点可以在类型理论中表示为归纳类型；2）使用去函数化将高阶输出编码为一阶表示。在Lean证明助手中实现HITrees，并提供了包括并发、递归和call/cc在内的全面效应库。

Result: 成功实现了HITrees，提供了两种解释：作为状态转移系统和作为单子程序。通过定义具有并行组合和call/cc的语言语义，展示了HITrees的表达能力。

Conclusion: HITrees是第一个在非守卫类型理论中支持高阶效应的交互树变体，为复杂系统的形式验证提供了更强大的组合语义基础。

Abstract: Recent years have witnessed the rise of compositional semantics as a
foundation for formal verification of complex systems. In particular,
interaction trees have emerged as a popular denotational semantics. Interaction
trees achieve compositionality by providing a reusable library of effects.
However, their notion of effects does not support higher-order effects, i.e.,
effects that take or return monadic computations. Such effects are essential to
model complex semantic features like parallel composition and call/cc.
  We introduce Higher-Order Interaction Trees (HITrees), the first variant of
interaction trees to support higher-order effects in a non-guarded type theory.
HITrees accomplish this through two key techniques: first, by designing the
notion of effects such that the fixpoints of effects with higher-order input
can be expressed as inductive types inside the type theory; and second, using
defunctionalization to encode higher-order outputs into a first-order
representation. We implement HITrees in the Lean proof assistant, accompanied
by a comprehensive library of effects including concurrency, recursion, and
call/cc. Furthermore, we provide two interpretations of HITrees, as state
transition systems and as monadic programs. To demonstrate the expressiveness
of HITrees, we apply them to define the semantics of a language with parallel
composition and call/cc.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [9] [Caruca: Effective and Efficient Specification Mining for Opaque Software Components](https://arxiv.org/abs/2510.14279)
*Evangelos Lamprou,Seong-Heon Jung,Mayank Keoliya,Lukas Lazarek,Konstantinos Kallas,Michael Greenberg,Nikos Vasilakis*

Main category: cs.SE

TL;DR: Caruca是一个用于自动挖掘不透明命令规范的系统，通过LLM翻译文档、探索有效调用空间、系统调用拦截等方式，为现有系统生成可直接使用的命令规范。


<details>
  <summary>Details</summary>
Motivation: 现有系统需要手动创建命令规范，这个过程繁琐、易错且耗时，限制了这些系统的实用性。

Method: 使用大语言模型将命令文档翻译为结构化调用语法，探索语法有效的命令调用和执行环境空间，通过系统调用和文件系统拦截提取关键命令属性。

Result: 在60个GNU Coreutils、POSIX和第三方命令上测试，除一个案例外都能生成正确规范，完全消除了手动工作，目前为最先进的静态分析工具提供完整规范。

Conclusion: Caruca能够自动生成准确且可用的命令规范，显著提高了依赖规范系统的实用性。

Abstract: A wealth of state-of-the-art systems demonstrate impressive improvements in
performance, security, and reliability on programs composed of opaque
components, such as Unix shell commands. To reason about commands, these
systems require partial specifications. However, creating such specifications
is a manual, laborious, and error-prone process, limiting the practicality of
these systems. This paper presents Caruca, a system for automatic specification
mining for opaque commands. To overcome the challenge of language diversity
across commands, Caruca first instruments a large language model to translate a
command's user-facing documentation into a structured invocation syntax. Using
this representation, Caruca explores the space of syntactically valid command
invocations and execution environments. Caruca concretely executes each
command-environment pair, interposing at the system-call and filesystem level
to extract key command properties such as parallelizability and filesystem pre-
and post-conditions. These properties can be exported in multiple specification
formats and are immediately usable by existing systems. Applying Caruca across
60 GNU Coreutils, POSIX, and third-party commands across several
specification-dependent systems shows that Caruca generates correct
specifications for all but one case, completely eliminating manual effort from
the process and currently powering the full specifications for a
state-of-the-art static analysis tool.

</details>


### [10] [From Craft to Constitution: A Governance-First Paradigm for Principled Agent Engineering](https://arxiv.org/abs/2510.13857)
*Qiang Xu,Xiangyu Wen,Changran Xu,Zeju Li,Jianyuan Zhong*

Main category: cs.SE

TL;DR: 提出了ArbiterOS架构，旨在解决LLM代理从原型到生产部署中存在的"工艺危机"问题，通过治理优先的范式来构建可信赖的自主系统。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推动了"代理时代"的到来，但现有代理系统在关键任务应用中表现出脆弱性、不可预测性和不可信赖性，存在从原型到生产的"工艺危机"。

Method: 引入治理优先的代理工程范式，提出了名为ArbiterOS的正式架构，旨在解决概率性处理器与确定性软件工程思维之间的范式不匹配问题。

Result: 提出了解决代理系统"工艺危机"的理论框架和架构设计，但未提供具体实验结果。

Conclusion: 需要采用治理优先的工程范式来构建可信赖的自主代理系统，ArbiterOS架构为此提供了理论基础。

Abstract: The advent of powerful Large Language Models (LLMs) has ushered in an ``Age
of the Agent,'' enabling autonomous systems to tackle complex goals. However,
the transition from prototype to production is hindered by a pervasive ``crisis
of craft,'' resulting in agents that are brittle, unpredictable, and ultimately
untrustworthy in mission-critical applications. This paper argues this crisis
stems from a fundamental paradigm mismatch -- attempting to command inherently
probabilistic processors with the deterministic mental models of traditional
software engineering. To solve this crisis, we introduce a governance-first
paradigm for principled agent engineering, embodied in a formal architecture we
call ArbiterOS.

</details>


### [11] [Benchmarking Correctness and Security in Multi-Turn Code Generation](https://arxiv.org/abs/2510.13859)
*Ruchit Rawal,Jeffrey Yang Fan Chiang,Chihao Shen,Jeffery Siyuan Tian,Aastha Mahajan,Tom Goldstein,Yizheng Chen*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: AI coding assistants powered by large language models (LLMs) have transformed
software development, significantly boosting productivity. While existing
benchmarks evaluate the correctness and security of LLM-generated code, they
are typically limited to single-turn tasks that do not reflect the iterative
nature of real-world development. We introduce MT-Sec, the first benchmark to
systematically evaluate both correctness and security in multi-turn coding
scenarios. We construct this using a synthetic data pipeline that transforms
existing single-turn tasks into semantically aligned multi-turn interaction
sequences, allowing reuse of original test suites while modeling the complexity
of real-world coding processes. We evaluate 32 open- and closed-source models,
and three agent-scaffolding on MT-Sec and observe a consistent 20-27% drop in
"correct and secure" outputs from single-turn to multi-turn settings -- even
among state-of-the-art models. Beyond full-program generation, we also evaluate
models on multi-turn code-diff generation -- an unexplored yet practically
relevant setting -- and find that models perform worse here, with increased
rates of functionally incorrect and insecure outputs. Finally, we find that
while agent scaffoldings boost single-turn code generation performance, they
are not quite as effective in multi-turn evaluations. Together, these findings
highlight the need for benchmarks that jointly evaluate correctness and
security in multi-turn, real-world coding workflows.

</details>


### [12] [A11YN: aligning LLMs for accessible web UI code generation](https://arxiv.org/abs/2510.13914)
*Janghan Yoon,Jaegwan Cho,Junhyeok Kim,Jiwan Chung,Jaehyun Jeon,Youngjae Yu*

Main category: cs.SE

TL;DR: A11yn方法通过优化奖励函数来对齐代码生成LLM，使其可靠地生成符合可访问性标准的网页UI，将不可访问率降低60%。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在生成网页界面时经常复制训练数据中的可访问性缺陷，导致界面无法满足多样化用户的需求和情境。

Method: A11yn优化了一个新颖的奖励函数，根据可访问性测试引擎识别的违规严重程度来惩罚违反Web内容可访问性指南(WCAG)的行为，并构建了UIReq-6.8K数据集用于训练。

Result: A11yn显著优于强基线方法，将不可访问率比基础模型降低了60%，同时保持了生成UI的语义保真度和视觉质量。

Conclusion: 研究表明可访问性可以在LLM中系统优化，证明了为可访问性对齐代码生成的可行性。

Abstract: Large language models (LLMs) have recently demonstrated strong capabilities
in generating functional and aesthetic web interfaces directly from
instructions. However, these models often replicate accessibility flaws from
their training data, resulting in interfaces that exclude users with diverse
needs and contexts. To address this gap, we introduce A11yn, the first method
that aligns code-generating LLMs to reliably produce accessibility-compliant
web UIs. A11yn optimizes a novel reward function that penalizes violations of
the Web Content Accessibility Guidelines (WCAG), with penalties scaled to the
severity of each violation as identified by an accessibility testing engine. To
support training, we construct UIReq-6.8K, a dataset of 6,800 diverse
instructions for web UI generation. For evaluation, we introduce RealUIReq-300,
a benchmark of 300 real-world web UI requests grounded and manually curated
from public web pages, spanning a broad range of use cases. Empirical results
show that A11yn significantly outperforms strong baselines, lowering the
Inaccessibility Rate by 60% over the base model while preserving semantic
fidelity and visual quality of generated UIs. These findings demonstrate that
accessibility can be systematically optimized within LLMs, showing the
feasibility of aligning code generation for accessibility.

</details>


### [13] [Signature in Code Backdoor Detection, how far are we?](https://arxiv.org/abs/2510.13992)
*Quoc Hung Le,Thanh Le-Cong,Bach Le,Bowen Xu*

Main category: cs.SE

TL;DR: 重新评估谱签名防御方法在代码模型后门攻击检测中的适用性，发现传统设置效果不佳，提出新的代理指标来更准确评估防御性能


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在软件开发中的广泛应用，后门攻击成为重大威胁，而现有的谱签名防御方法在代码模型中的效果可能不理想

Method: 系统评估谱签名防御在不同攻击场景和防御配置下的有效性，分析关键因素的不同设置对检测效果的影响

Result: 发现代码后门检测中广泛使用的谱签名设置通常不是最优的，探索了关键因素的不同设置影响，并发现了一个新的代理指标

Conclusion: 谱签名防御在代码模型后门检测中需要优化配置，新提出的代理指标能够更准确地评估防御性能而无需模型重新训练

Abstract: As Large Language Models (LLMs) become increasingly integrated into software
development workflows, they also become prime targets for adversarial attacks.
Among these, backdoor attacks are a significant threat, allowing attackers to
manipulate model outputs through hidden triggers embedded in training data.
Detecting such backdoors remains a challenge, and one promising approach is the
use of Spectral Signature defense methods that identify poisoned data by
analyzing feature representations through eigenvectors. While some prior works
have explored Spectral Signatures for backdoor detection in neural networks,
recent studies suggest that these methods may not be optimally effective for
code models. In this paper, we revisit the applicability of Spectral
Signature-based defenses in the context of backdoor attacks on code models. We
systematically evaluate their effectiveness under various attack scenarios and
defense configurations, analyzing their strengths and limitations. We found
that the widely used setting of Spectral Signature in code backdoor detection
is often suboptimal. Hence, we explored the impact of different settings of the
key factors. We discovered a new proxy metric that can more accurately estimate
the actual performance of Spectral Signature without model retraining after the
defense.

</details>


### [14] [One Bug, Hundreds Behind: LLMs for Large-Scale Bug Discovery](https://arxiv.org/abs/2510.14036)
*Qiushi Wu,Yue Xiao,Dhilung Kirat,Kevin Eykholt,Jiyong Jang,Douglas Lee Schales*

Main category: cs.SE

TL;DR: BugStone是一个基于LLVM和大型语言模型的程序分析系统，用于检测软件中重复出现的模式错误(RPBs)。该系统利用已修复的错误实例来识别一致错误模式，然后在整个程序中查找相似代码段。在Linux内核中发现了22K+潜在问题，验证了246个有效错误。


<details>
  <summary>Details</summary>
Motivation: 大型程序中的错误修复耗时耗力，但修复一个错误后，程序中可能还存在类似的未修复错误实例。这些重复模式错误(RPBs)会扩大攻击面，因为攻击者可以利用已报告但未在其他地方修复的错误模式。

Method: BugStone结合LLVM程序分析和大型语言模型，通过分析已修复的错误实例来识别一致错误模式(如特定API误用)，然后在全程序中搜索相似代码段。系统从135个独特RPBs开始，扩展到大规模检测。

Result: 在Linux内核中识别了超过22,000个潜在问题，手动分析400个发现中有246个有效。在包含1.9K安全错误的数据集上，BugStone达到92.2%的精确度和79.1%的成对准确率。

Conclusion: RPBs在软件中普遍存在且严重影响安全性。BugStone证明了利用已修复错误实例来检测类似漏洞的有效性，为大规模软件安全分析提供了实用工具。

Abstract: Fixing bugs in large programs is a challenging task that demands substantial
time and effort. Once a bug is found, it is reported to the project
maintainers, who work with the reporter to fix it and eventually close the
issue. However, across the program, there are often similar code segments,
which may also contain the bug, but were missed during discovery. Finding and
fixing each recurring bug instance individually is labor intensive. Even more
concerning, bug reports can inadvertently widen the attack surface as they
provide attackers with an exploitable pattern that may be unresolved in other
parts of the program.
  In this paper, we explore these Recurring Pattern Bugs (RPBs) that appear
repeatedly across various code segments of a program or even in different
programs, stemming from a same root cause, but are unresolved. Our
investigation reveals that RPBs are widespread and can significantly compromise
the security of software programs. This paper introduces BugStone, a program
analysis system empowered by LLVM and a Large Language Model (LLM). The key
observation is that many RPBs have one patched instance, which can be leveraged
to identify a consistent error pattern, such as a specific API misuse. By
examining the entire program for this pattern, it is possible to identify
similar sections of code that may be vulnerable. Starting with 135 unique RPBs,
BugStone identified more than 22K new potential issues in the Linux kernel.
Manual analysis of 400 of these findings confirmed that 246 were valid. We also
created a dataset from over 1.9K security bugs reported by 23 recent top-tier
conference works. We manually annotate the dataset, identify 80 recurring
patterns and 850 corresponding fixes. Even with a cost-efficient model choice,
BugStone achieved 92.2% precision and 79.1% pairwise accuracy on the dataset.

</details>


### [15] [David vs. Goliath: A comparative study of different-sized LLMs for code generation in the domain of automotive scenario generation](https://arxiv.org/abs/2510.14115)
*Philipp Bauerfeind,Amir Salarpour,David Fernandez,Pedram MohajerAnsari,Johannes Reschke,Mert D. Pesé*

Main category: cs.SE

TL;DR: NL2Scenic是一个用于评估从自然语言生成Scenic自动驾驶场景代码的数据集和框架，包含146个NL/Scenic对和30个测试用例，评估了13个模型并提出了EDIT-COMP作为与人类判断相关性最好的评估指标。


<details>
  <summary>Details</summary>
Motivation: 解决现有NL-to-Scenic生成中数据稀缺、可复现性差和评估指标不一致的问题，为自动驾驶场景编程提供标准化评估基准。

Method: 构建包含146个NL/Scenic对的数据集，设计30个难度分层的测试用例，使用Example Retriever和14种提示变体，评估13个模型（4个专有模型和9个开源代码模型），使用文本指标和执行指标，并与专家研究比较。

Result: EDIT-SIM与人类判断相关性最好，提出的EDIT-COMP指标提高了排名保真度；GPT-4o表现最佳，Qwen2.5Coder-14B达到其88%的专家分数；检索增强提示能提升小模型性能；中等规模开源模型是实用且经济的选择。

Conclusion: NL2Scenic和EDIT-COMP为Scenic代码生成提供了标准化、可复现的评估基础，表明中等规模开源模型是自动驾驶场景编程的实用且经济有效的选择。

Abstract: Scenario simulation is central to testing autonomous driving systems. Scenic,
a domain-specific language (DSL) for CARLA, enables precise and reproducible
scenarios, but NL-to-Scenic generation with large language models (LLMs)
suffers from scarce data, limited reproducibility, and inconsistent metrics. We
introduce NL2Scenic, an open dataset and framework with 146 NL/Scenic pairs, a
difficulty-stratified 30-case test split, an Example Retriever, and 14
prompting variants (ZS, FS, CoT, SP, MoT). We evaluate 13 models: four
proprietary (GPT-4o, GPT-5, Claude-Sonnet-4, Gemini-2.5-pro) and nine
open-source code models (Qwen2.5Coder 0.5B-32B; CodeLlama 7B/13B/34B), using
text metrics (BLEU, ChrF, EDIT-SIM, CrystalBLEU) and execution metrics
(compilation and generation), and compare them with an expert study (n=11).
EDIT-SIM correlates best with human judgments; we also propose EDIT-COMP (F1 of
EDIT-SIM and compilation) as a robust dataset-level proxy that improves ranking
fidelity. GPT-4o performs best overall, while Qwen2.5Coder-14B reaches about 88
percent of its expert score on local hardware. Retrieval-augmented prompting,
Few-Shot with Example Retriever (FSER), consistently boosts smaller models, and
scaling shows diminishing returns beyond mid-size, with Qwen2.5Coder
outperforming CodeLlama at comparable scales. NL2Scenic and EDIT-COMP offer a
standardized, reproducible basis for evaluating Scenic code generation and
indicate that mid-size open-source models are practical, cost-effective options
for autonomous-driving scenario programming.

</details>


### [16] [A Hybrid, Knowledge-Guided Evolutionary Framework for Personalized Compiler Auto-Tuning](https://arxiv.org/abs/2510.14292)
*Haolin Pan,Hongbin Zhang,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: 提出了一种混合知识引导进化框架，通过离线构建编译知识库和在线使用知识增强的遗传算法，自动优化编译器pass序列，相比-Oz基准平均减少11.0%的LLVM IR指令。


<details>
  <summary>Details</summary>
Motivation: 传统编译器优化标志（如-O3、-Oz）采用一刀切方法，无法充分发挥程序性能潜力。编译器pass序列优化是NP难问题，需要更智能的个性化优化方法。

Method: 混合知识引导进化框架：离线阶段构建包含pass行为向量、pass分组、协同pass图和原型pass序列的知识库；在线阶段使用知识增强的遗传算法进行语义感知重组和定向修复突变。

Result: 在7个公开数据集上，相比高度优化的opt -Oz基准，平均额外减少11.0%的LLVM IR指令，展现了发现个性化高性能优化序列的最先进能力。

Conclusion: 该框架通过结合离线知识提取和在线进化搜索，有效解决了编译器pass序列优化的NP难问题，实现了显著的性能提升。

Abstract: Compiler pass auto-tuning is critical for enhancing software performance, yet
finding the optimal pass sequence for a specific program is an NP-hard problem.
Traditional, general-purpose optimization flags like -O3 and -Oz adopt a
one-size-fits-all approach, often failing to unlock a program's full
performance potential. To address this challenge, we propose a novel Hybrid,
Knowledge-Guided Evolutionary Framework. This framework intelligently guides
online, personalized optimization using knowledge extracted from a large-scale
offline analysis phase. During the offline stage, we construct a comprehensive
compilation knowledge base composed of four key components: (1) Pass Behavioral
Vectors to quantitatively capture the effectiveness of each optimization; (2)
Pass Groups derived from clustering these vectors based on behavior similarity;
(3) a Synergy Pass Graph to model beneficial sequential interactions; and (4) a
library of Prototype Pass Sequences evolved for distinct program types. In the
online stage, a bespoke genetic algorithm leverages this rich knowledge base
through specially designed, knowledge-infused genetic operators. These
operators transform the search by performing semantically-aware recombination
and targeted, restorative mutations. On a suite of seven public datasets, our
framework achieves an average of 11.0% additional LLVM IR instruction reduction
over the highly-optimized opt -Oz baseline, demonstrating its state-of-the-art
capability in discovering personalized, high-performance optimization
sequences.

</details>


### [17] [A Systematic Study of Time Limit Exceeded Errors in Online Programming Assignments](https://arxiv.org/abs/2510.14339)
*Jialu Zhang,Jialiang Gu,Wangmeiyu Zhang,José Pablo Cambronero,John Kolesar,Ruzica Piskac,Daming Li,Hanyuan Shi*

Main category: cs.SE

TL;DR: 本文提出了首个针对在线编程中TLE错误的大规模实证研究，开发了自动化修复工具Nettle和评估框架Nettle-Eval，显著提升了TLE错误的修复成功率。


<details>
  <summary>Details</summary>
Motivation: 在线编程平台上的时间限制超出(TLE)错误难以诊断和修复，错误信息缺乏诊断洞察，平台支持有限，导致许多用户放弃提交。

Method: 手动分析1000个Codeforces的TLE提交，分类根本原因；开发Nettle工具，结合LLM、编译器反馈和测试用例生成自动化修复；创建Nettle-Eval评估框架。

Result: TLE错误不仅源于低效算法，还包括无限循环、数据结构使用不当和I/O低效；Nettle在1000个真实案例中达到98.5%的修复率，远超LLM基线。

Conclusion: TLE错误具有多样性，Nettle工具能有效自动化修复TLE错误，其修复均通过官方检查器验证，证明了框架的可靠性。

Abstract: Online programming platforms such as Codeforces and LeetCode attract millions
of users seeking to learn to program or refine their skills for industry
interviews. A major challenge for these users is the Time Limit Exceeded (TLE)
error, triggered when a program exceeds the execution time bound. Although
designed as a performance safeguard, TLE errors are difficult to resolve: error
messages provide no diagnostic insight, platform support is minimal, and
existing debugging tools offer little help. As a result, many users abandon
their submissions after repeated TLE failures.
  This paper presents the first large-scale empirical study of TLE errors in
online programming. We manually analyzed 1000 Codeforces submissions with TLE
errors, classified their root causes, and traced how users attempted to fix
them. Our analysis shows that TLE errors often arise not only from inefficient
algorithms but also from infinite loops, improper data structure use, and
inefficient I/O, challenging the conventional view that TLEs are purely
performance issues.
  Guided by these findings, we introduce Nettle, the first automated repair
tool specifically designed for TLE errors, and Nettle-Eval, the first framework
for evaluating TLE repairs. Integrating LLMs with targeted automated feedback
generated by the compiler and test cases, Nettle produces small, correct code
edits that eliminate TLEs while preserving functionality. Evaluated on the same
1000 real-world cases, Nettle achieves a 98.5% fix rate, far exceeding the
strongest LLM baseline, and all of its repairs pass both Nettle-Eval and the
platform's official checker, confirming the reliability of our framework.

</details>


### [18] [PathFix: Automated Program Repair with Expected Path](https://arxiv.org/abs/2510.14341)
*Xu He,Shu Wang,Kun Sun*

Main category: cs.SE

TL;DR: PathFix是一种新的自动程序修复方法，通过从正确执行路径中提取路径敏感约束来生成补丁，解决了现有方法生成过多候选补丁和过拟合测试用例的问题。


<details>
  <summary>Details</summary>
Motivation: 现有自动程序修复方法由于难以生成精确规范，面临生成过多合理补丁候选和过拟合部分测试用例的挑战。

Method: PathFix采用四步法：追踪错误路径、推导期望路径、通过解决状态约束生成和评估补丁、验证补丁正确性。还集成大语言模型提升修复性能。

Result: 实验结果显示PathFix优于现有解决方案，特别是在处理循环和递归等复杂程序结构方面表现突出。

Conclusion: PathFix通过路径敏感约束和LLM集成，有效提升了自动程序修复的性能和可扩展性。

Abstract: Automated program repair (APR) techniques are effective in fixing inevitable
defects in software, enhancing development efficiency and software robustness.
However, due to the difficulty of generating precise specifications, existing
APR methods face two main challenges: generating too many plausible patch
candidates and overfitting them to partial test cases. To tackle these
challenges, we introduce a new APR method named PathFix, which leverages
path-sensitive constraints extracted from correct execution paths to generate
patches for repairing buggy code. It is based on one observation: if a buggy
program is repairable, at least one expected path is supposed to replace the
fault path in the patched program. PathFix operates in four main steps. First,
it traces fault paths reaching the fault output in the buggy program. Second,
it derives expected paths by analyzing the desired correct output on the
control flow graph, where an expected path defines how a feasible patch leads
to the correct execution. Third, PathFix generates and evaluates patches by
solving state constraints along the expected path. Fourth, we validate the
correctness of the generated patch. To further enhance repair performance and
mitigate scalability issues introduced by path-sensitive analysis, we integrate
a large language model (LLM) into our framework. Experimental results show that
PathFix outperforms existing solutions, particularly in handling complex
program structures such as loops and recursion.

</details>


### [19] [Towards Automated Governance: A DSL for Human-Agent Collaboration in Software Projects](https://arxiv.org/abs/2510.14465)
*Adem Ait,Gwendal Jouneaux,Javier Luis Cánovas Izquierdo,Jordi Cabot*

Main category: cs.SE

TL;DR: 提出一种新的领域特定语言(DSL)，用于在涉及多样化利益相关者（包括AI代理）的软件项目中定义和执行治理政策。


<details>
  <summary>Details</summary>
Motivation: 软件开发中的利益相关者日益多样化，包括来自不同背景的人类贡献者和AI代理，这在开源软件项目中带来了独特的治理挑战，因为明确的政策往往缺乏或不清晰。

Method: 设计一种领域特定语言(DSL)，能够定义和执行丰富的治理政策，支持在多样化利益相关者系统中的治理需求。

Result: 该DSL为实现更强大、适应性更强且最终自动化的治理提供了途径。

Conclusion: 这种DSL为软件项目（特别是开源项目）中更有效的协作铺平了道路。

Abstract: The stakeholders involved in software development are becoming increasingly
diverse, with both human contributors from varied backgrounds and AI-powered
agents collaborating together in the process. This situation presents unique
governance challenges, particularly in Open-Source Software (OSS) projects,
where explicit policies are often lacking or unclear. This paper presents the
vision and foundational concepts for a novel Domain-Specific Language (DSL)
designed to define and enforce rich governance policies in systems involving
diverse stakeholders, including agents. This DSL offers a pathway towards more
robust, adaptable, and ultimately automated governance, paving the way for more
effective collaboration in software projects, especially OSS ones.

</details>


### [20] [E2Edev: Benchmarking Large Language Models in End-to-End Software Development Task](https://arxiv.org/abs/2510.14509)
*Jingyao Liu,Chen Huang,Zhizhao Guan,Wenqiang Lei,Yang Deng*

Main category: cs.SE

TL;DR: E2EDev是一个端到端软件开发基准，包含细粒度用户需求、BDD测试场景和自动化测试流水线，通过人机协同多智能体标注框架减少标注工作量，评估显示现有E2ESD框架在解决这些任务时仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 解决端到端软件开发(E2ESD)中缺乏高质量基准的问题，同时减少人工标注工作量，为评估和改进E2ESD解决方案提供可靠工具。

Method: 提出E2EDev基准，包含细粒度需求、BDD测试场景和自动化测试流水线；使用人机协同多智能体标注框架(HITL-MAA)确保质量并减少标注工作量；评估多种E2ESD框架和LLM骨干模型。

Result: 评估显示现有E2ESD框架在解决这些任务时持续面临困难，突显了对更有效和成本效益的E2ESD解决方案的迫切需求。

Conclusion: E2EDev为端到端软件开发提供了高质量基准，揭示了当前方法的局限性，强调了开发更有效E2ESD解决方案的重要性。

Abstract: E2EDev comprises (i) a fine-grained set of user requirements, (ii) {multiple
BDD test scenarios with corresponding Python step implementations for each
requirement}, and (iii) a fully automated testing pipeline built on the Behave
framework. To ensure its quality while reducing the annotation effort, E2EDev
leverages our proposed Human-in-the-Loop Multi-Agent Annotation Framework
(HITL-MAA). {By evaluating various E2ESD frameworks and LLM backbones with
E2EDev}, our analysis reveals a persistent struggle to effectively solve these
tasks, underscoring the critical need for more effective and cost-efficient
E2ESD solutions. Our codebase and benchmark are publicly available at
https://github.com/SCUNLP/E2EDev.

</details>


### [21] [Software Testing Education and Industry Needs - Report from the ENACTEST EU Project](https://arxiv.org/abs/2510.14625)
*Mehrdad Saadatmand,Abbas Khan,Beatriz Marin,Ana C. R Paiva,Nele Van Asch,Graham Moran,Felix Cammaerts,Monique Snoeck,Alexandra Mendes*

Main category: cs.SE

TL;DR: 该研究调查了软件测试行业的能力需求，识别了当前测试教育的知识差距，并通过焦点小组、访谈和范围综述揭示了学术文献中未涉及的能力和差距。


<details>
  <summary>Details</summary>
Motivation: 软件开发的不断演变要求测试人员持续适应新工具和实践，需要研究行业中的测试能力需求以及当前教育存在的知识差距。

Method: 通过两个焦点小组会议和对多个领域专业人士的访谈，结合精心策划的小规模范围综述，使用主题定性分析方法进行研究。

Result: 研究发现专业培训方法、行业培训挑战、培训质量评估方式、学术教育与行业需求之间的知识差距、测试教育的未来需求和趋势，以及公司内部知识转移方法等方面的问题。范围综述证实了AI测试、安全测试和软技能等领域的知识差距。

Conclusion: 软件测试教育需要更好地与行业需求对接，特别是在新兴技术领域如AI测试、安全测试和软技能培养方面存在显著的知识差距，需要改进培训方法和知识转移机制。

Abstract: The evolving landscape of software development demands that software testers
continuously adapt to new tools, practices, and acquire new skills. This study
investigates software testing competency needs in industry, identifies
knowledge gaps in current testing education, and highlights competencies and
gaps not addressed in academic literature. This is done by conducting two focus
group sessions and interviews with professionals across diverse domains,
including railway industry, healthcare, and software consulting and performing
a curated small-scale scoping review. The study instrument, co-designed by
members of the ENACTEST project consortium, was developed collaboratively and
refined through multiple iterations to ensure comprehensive coverage of
industry needs and educational gaps. In particular, by performing a thematic
qualitative analysis, we report our findings and observations regarding:
professional training methods, challenges in offering training in industry,
different ways of evaluating the quality of training, identified knowledge gaps
with respect to academic education and industry needs, future needs and trends
in testing education, and knowledge transfer methods within companies. Finally,
the scoping review results confirm knowledge gaps in areas such as AI testing,
security testing and soft skills.

</details>


### [22] [ATGen: Adversarial Reinforcement Learning for Test Case Generation](https://arxiv.org/abs/2510.14635)
*Qingyao Li,Xinyi Dai,Weiwen Liu,Xiangyang Li,Yasheng Wang,Ruiming Tang,Yong Yu,Weinan Zhang*

Main category: cs.SE

TL;DR: ATGen是一个通过对抗性强化学习训练测试用例生成的框架，突破静态数据集的固定难度限制，持续生成更复杂的bug来提升测试效果。


<details>
  <summary>Details</summary>
Motivation: 现有测试生成方法依赖静态数据集，存在"固定难度上限"，无法发现超出训练范围的新颖或复杂bug，限制了LLM生成代码的可靠性提升。

Method: 采用对抗性强化学习框架，让测试生成器与对抗性代码生成器相互博弈，动态创建难度递增的课程，通过RL优化同时最大化"输出准确性"和"攻击成功率"。

Result: 实验表明ATGen显著优于最先进基线方法，既能作为更有效的Best-of-N推理过滤器，也能作为训练代码生成模型的更高质量奖励源。

Conclusion: ATGen建立了一种新的动态范式，通过持续提升测试难度来改进LLM生成代码的可靠性，突破了静态训练的局限性。

Abstract: Large Language Models (LLMs) excel at code generation, yet their outputs
often contain subtle bugs, for which effective test cases are a critical
bottleneck. Existing test generation methods, whether based on prompting or
supervised fine-tuning, rely on static datasets. This imposes a
``fixed-difficulty ceiling'', fundamentally limiting their ability to uncover
novel or more complex bugs beyond their training scope. To overcome this, we
introduce ATGen, a framework that trains a test case generator via adversarial
reinforcement learning. ATGen pits a test generator against an adversarial code
generator that continuously crafts harder bugs to evade the current policy.
This dynamic loop creates a curriculum of increasing difficulty challenging
current policy. The test generator is optimized via Reinforcement Learning (RL)
to jointly maximize ``Output Accuracy'' and ``Attack Success'', enabling it to
learn a progressively stronger policy that breaks the fixed-difficulty ceiling
of static training. Extensive experiments demonstrate that ATGen significantly
outperforms state-of-the-art baselines. We further validate its practical
utility, showing it serves as both a more effective filter for Best-of-N
inference and a higher-quality reward source for training code generation
models. Our work establishes a new, dynamic paradigm for improving the
reliability of LLM-generated code.

</details>


### [23] [Requirement Identification for Traffic Simulations in Driving Simulators](https://arxiv.org/abs/2510.14653)
*Sven Tarlowski,Lutz Eckstein*

Main category: cs.SE

TL;DR: 提出了一种系统识别交通仿真需求的方法论，通过结构化方法确保交通仿真的真实性和有效性。


<details>
  <summary>Details</summary>
Motivation: 解决确保交通仿真真实性的挑战，为汽车开发和测试提供可靠的仿真环境。

Method: 基于各研究阶段的子目标，采用结构化方法推导微观层面、智能体模型和视觉表示的具体技术需求。

Result: 该方法能够保持高保真度，提高实验结果的效度和参与者参与度。

Conclusion: 通过明确研究目标与交通仿真设计之间的联系，该方法支持稳健的汽车开发和测试。

Abstract: This paper addresses the challenge of ensuring realistic traffic conditions
by proposing a methodology that systematically identifies traffic simulation
requirements. Using a structured approach based on sub-goals in each study
phase, specific technical needs are derived for microscopic levels, agent
models, and visual representation. The methodology aims to maintain a high
degree of fidelity, enhancing both the validity of experimental outcomes and
participant engagement. By providing a clear link between study objectives and
traffic simulation design, this approach supports robust automotive development
and testing.

</details>


### [24] [LLM Agents for Automated Web Vulnerability Reproduction: Are We There Yet?](https://arxiv.org/abs/2510.14700)
*Bin Liu,Yanjie Zhao,Guoai Xu,Haoyu Wang*

Main category: cs.SE

TL;DR: 本文首次全面评估了最先进的LLM代理在自动化Web漏洞复现中的表现，发现虽然LLM代理在简单漏洞上表现良好，但在需要复杂环境配置和多组件交互的复杂漏洞上存在显著差距。


<details>
  <summary>Details</summary>
Motivation: LLM代理在软件工程和网络安全任务中展现出强大能力，但自动化Web漏洞复现这一关键应用尚未得到充分探索。本文旨在填补这一空白，评估LLM代理在真实世界Web漏洞复现场景中的实际表现。

Method: 系统评估了20个来自不同领域的LLM代理在16个维度上的表现，包括技术能力、环境适应性和用户体验因素，并在3个代表性Web漏洞上进行测试。随后选择3个表现最佳的代理在包含80个真实世界CVE的基准数据集上进行深入评估。

Result: LLM代理在简单的基于库的漏洞上取得了合理成功，但在需要多组件环境的复杂基于服务的漏洞上持续失败。复杂的环境配置和认证障碍导致代理能够执行利用代码但无法触发实际漏洞。在认证信息不完整时，性能下降超过33%。

Conclusion: 当前LLM代理能力与可靠自动化漏洞复现需求之间存在显著差距，需要在环境适应性和自主问题解决能力方面取得进展。

Abstract: Large language model (LLM) agents have demonstrated remarkable capabilities
in software engineering and cybersecurity tasks, including code generation,
vulnerability discovery, and automated testing. One critical but underexplored
application is automated web vulnerability reproduction, which transforms
vulnerability reports into working exploits. Although recent advances suggest
promising potential, challenges remain in applying LLM agents to real-world web
vulnerability reproduction scenarios. In this paper, we present the first
comprehensive evaluation of state-of-the-art LLM agents for automated web
vulnerability reproduction. We systematically assess 20 agents from software
engineering, cybersecurity, and general domains across 16 dimensions, including
technical capabilities, environment adaptability, and user experience factors,
on 3 representative web vulnerabilities. Based on the results, we select three
top-performing agents (OpenHands, SWE-agent, and CAI) for in-depth evaluation
on our benchmark dataset of 80 real-world CVEs spanning 7 vulnerability types
and 6 web technologies. Our results reveal that while LLM agents achieve
reasonable success on simple library-based vulnerabilities, they consistently
fail on complex service-based vulnerabilities requiring multi-component
environments. Complex environment configurations and authentication barriers
create a gap where agents can execute exploit code but fail to trigger actual
vulnerabilities. We observe high sensitivity to input guidance, with
performance degrading by over 33% under incomplete authentication information.
Our findings highlight the significant gap between current LLM agent
capabilities and the demands of reliable automated vulnerability reproduction,
emphasizing the need for advances in environmental adaptation and autonomous
problem-solving capabilities.

</details>


### [25] [Leveraging Code Cohesion Analysis to Identify Source Code Supply Chain Attacks](https://arxiv.org/abs/2510.14778)
*Maor Reuben,Ido Mendel,Or Feldman,Moshe Kravchik,Mordehai Guri,Rami Puzis*

Main category: cs.SE

TL;DR: 提出一种基于命名预测的内聚度（NPC）指标的无监督方法，通过量化源代码中的内聚度破坏来检测恶意代码注入。


<details>
  <summary>Details</summary>
Motivation: 供应链攻击通过合法项目中注入恶意代码严重威胁软件安全，这类攻击虽然罕见但破坏性极大，需要自动化工具来检测意图模糊的代码注入。

Method: 使用基于命名预测的内聚度指标分析函数内聚度变化，比较恶意代码注入与自然代码更新时的内聚度波动差异。

Result: 在54,707个函数和369个开源C++仓库的分析中，代码注入会降低内聚度并使命名模式转向更短、描述性更差的名称。在极端不平衡测试集下，NPC方法在1:1000比例下达到36.41%的Precision@100，在1:10000比例下达到12.47%。

Conclusion: 自动化的内聚度测量，特别是基于命名预测的内聚度，有助于识别供应链攻击，提高源代码完整性。

Abstract: Supply chain attacks significantly threaten software security with malicious
code injections within legitimate projects. Such attacks are very rare but may
have a devastating impact. Detecting spurious code injections using automated
tools is further complicated as it often requires deciphering the intention of
both the inserted code and its context. In this study, we propose an
unsupervised approach for highlighting spurious code injections by quantifying
cohesion disruptions in the source code. Using a name-prediction-based cohesion
(NPC) metric, we analyze how function cohesion changes when malicious code is
introduced compared to natural cohesion fluctuations. An analysis of 54,707
functions over 369 open-source C++ repositories reveals that code injection
reduces cohesion and shifts naming patterns toward shorter, less descriptive
names compared to genuine function updates. Considering the sporadic nature of
real supply-chain attacks, we evaluate the proposed method with extreme
test-set imbalance and show that monitoring high-cohesion functions with NPC
can effectively detect functions with injected code, achieving a Precision@100
of 36.41% at a 1:1,000 ratio and 12.47% at 1:10,000. These results suggest that
automated cohesion measurements, in general, and name-prediction-based
cohesion, in particular, may help identify supply chain attacks, improving
source code integrity.

</details>


### [26] [Instruction Set Migration at Warehouse Scale](https://arxiv.org/abs/2510.14928)
*Eric Christopher,Kevin Crossan,Wolff Dobson,Chris Kennelly,Drew Lewis,Kun Lin,Martin Maas,Parthasarathy Ranganathan,Emma Rapati,Brian Yang*

Main category: cs.SE

TL;DR: 本文分析了从x86到Arm架构的大规模代码迁移挑战，发现现代ISA迁移主要依赖重新编译而非二进制翻译，并展示了AI在自动化迁移过程中的重要作用。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为ISA迁移的主要挑战是二进制翻译，但现代开源生态系统使得重新编译成为可能，这带来了新的多方面挑战。

Method: 通过分析Google从x86到Arm的大规模迁移（涵盖近40,000个代码提交），推导出ISA迁移任务分类，并展示自动化方法和AI的应用。

Result: 识别了ISA迁移中的关键任务，证明了AI在自动解决这些任务中的重要作用，并展示了Google的自动化迁移实践。

Conclusion: 现代ISA迁移面临与二进制翻译不同的新挑战，AI技术在其中发挥关键作用，但仍有一些挑战性任务需要进一步研究。

Abstract: Migrating codebases from one instruction set architecture (ISA) to another is
a major engineering challenge. A recent example is the adoption of Arm (in
addition to x86) across the major Cloud hyperscalers. Yet, this problem has
seen limited attention by the academic community. Most work has focused on
static and dynamic binary translation, and the traditional conventional wisdom
has been that this is the primary challenge.
  In this paper, we show that this is no longer the case. Modern ISA migrations
can often build on a robust open-source ecosystem, making it possible to
recompile all relevant software from scratch. This introduces a new and
multifaceted set of challenges, which are different from binary translation.
  By analyzing a large-scale migration from x86 to Arm at Google, spanning
almost 40,000 code commits, we derive a taxonomy of tasks involved in ISA
migration. We show how Google automated many of the steps involved, and
demonstrate how AI can play a major role in automatically addressing these
tasks. We identify tasks that remain challenging and highlight research
challenges that warrant further attention.

</details>
