<div id=toc></div>

# Table of Contents

- [cs.FL](#cs.FL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 21]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.LO](#cs.LO) [Total: 4]


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [1] [Unambiguous Acceptance of Thin Coalgebras](https://arxiv.org/abs/2510.26371)
*Anton Chernev,Corina Cîrstea,Helle Hvid Hansen,Clemens Kupke*

Main category: cs.FL

TL;DR: 该论文将自动机理论中的单义自动机构造从薄树推广到解析函子的薄余代数，扩展了构造范围并提供了概念清晰度。


<details>
  <summary>Details</summary>
Motivation: 单义自动机在反应式系统验证中有应用价值，它们扩展了确定性自动机类别同时保持了某些理想特性。现有构造仅限于薄树，需要推广到更广泛的代数结构。

Method: 通过余代数框架将单义自动机构造从薄树推广到解析函子的薄余代数，并将自动机接受与相干代数的语言识别联系起来。

Result: 成功构建了适用于解析函子薄余代数的单义自动机构造，并建立了有限相干代数识别语言的自动机理论特征。

Conclusion: 该工作扩展了单义自动机的应用范围，为薄余代数提供了统一的构造框架，并建立了自动机理论与相干代数之间的联系。

Abstract: Automata admitting at most one accepting run per structure, known as
unambiguous automata, find applications in verification of reactive systems as
they extend the class of deterministic automata whilst maintaining some of
their desirable properties. In this paper, we generalise a classical
construction of unambiguous automata from thin trees to thin coalgebras for
analytic functors. This achieves two goals: extending the existing construction
to a larger class of structures, and providing conceptual clarity and
parametricity to the construction by formalising it in the coalgebraic
framework. As part of the construction, we link automaton acceptance of
languages of thin coalgebras to language recognition via so-called coherent
algebras, which were previously introduced for studying thin coalgebras. This
link also allows us to establish an automata-theoretic characterisation of
languages recognised by finite coherent algebras.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [Internal Vulnerabilities, External Threats: A Grounded Framework for Enterprise Open Source Risk Governance](https://arxiv.org/abs/2510.25882)
*Wenhao Yang,Minghui Zhou,Daniel Izquierdo Cortázar,Yehui Wang*

Main category: cs.SE

TL;DR: 提出了一个企业开源风险治理框架OTVM，通过"目标->威胁->漏洞->缓解"的逻辑链，帮助企业从战术风险管理转向整体风险治理。


<details>
  <summary>Details</summary>
Motivation: 传统风险管理仅关注技术工具，无法应对系统性威胁如上游"静默修复"、社区冲突或许可证变更，存在治理盲区。

Method: 基于扎根理论研究，与15位从业者合作开发整体风险治理框架，并通过三位行业专家的回顾性案例研究验证框架效用。

Result: 建立了包含战略目标矩阵、外部威胁与内部漏洞双重分类法、以及可操作的缓解框架的完整风险治理体系。

Conclusion: 该工作为企业提供了从被动"救火"转向主动构建组织"免疫系统"的诊断工具和系统路径。

Abstract: Enterprise engagement with open source has evolved from tactical adoption to
strategic deep integration, exposing them to a complex risk landscape far
beyond mere code. However, traditional risk management, narrowly focused on
technical tools, is structurally inadequate for systemic threats like upstream
"silent fixes", community conflicts, or sudden license changes, creating a
dangerous governance blind spot. To address this governance vacuum and enable
the necessary shift from tactical risk management to holistic risk governance,
we conducted a grounded theory study with 15 practitioners to develop a
holistic risk governance framework. Our study formalizes an analytical
framework built on a foundational risk principle: an uncontrollable External
Threat (e.g., a sudden license change in a key dependency) only becomes a
critical risk when it exploits a controllable Internal Vulnerability (e.g., an
undefined risk appetite for single-vendor projects), which then amplifies the
impact.The framework operationalizes this principle through a clear logical
chain: "Objectives -> Threats -> Vulnerabilities -> Mitigation" (OTVM). This
provides a holistic decision model that transcends mere technical checklists.
Based on this logic, our contributions are: (1) a "Strategic Objectives Matrix"
to clarify goals; (2) a systematic dual taxonomy of External Threats (Ex-Tech,
Ex-Comm, Ex-Eco) and Internal Vulnerabilities (In-Strat, In-Ops, In-Tech); and
(3) an actionable mitigation framework mapping capability-building to these
vulnerabilities. The framework's analytical utility was validated by three
industry experts through retrospective case studies on real-world incidents.
This work provides a novel diagnostic lens and a systematic path for
enterprises to shift from reactive "firefighting" to proactively building an
organizational "immune system".

</details>


### [3] [CHCVerif: A Portfolio-Based Solver for Constrained Horn Clauses](https://arxiv.org/abs/2510.26431)
*Mihály Dobos-Kovács,Levente Bajczi,András Vörös*

Main category: cs.SE

TL;DR: CHCVERIF是一个基于组合策略的CHC求解器，采用软件验证方法解决约束Horn子句问题，能够重用成熟的软件验证工具处理涉及位向量和低级语义的基准测试。


<details>
  <summary>Details</summary>
Motivation: 约束Horn子句（CHCs）被广泛用作各种验证任务的中间表示，但现有求解器在处理位向量和低级语义时面临挑战。作者希望利用成熟的软件验证工具来解决这些问题。

Method: 开发了CHCVERIF，这是一个基于组合策略的CHC求解器，采用软件验证方法，重用现有的软件验证工具作为后端来处理CHC基准测试。

Result: 在线性整数算术基准测试中表现一般，但在位向量基准测试中取得了适度的成功。结果表明使用软件验证工具作为CHC求解后端的可行性和潜力。

Conclusion: 使用软件验证工具作为CHC求解的后端是可行且有潜力的，特别是在精心构建的组合策略支持下，特别适用于处理位向量和低级语义的基准测试。

Abstract: Constrained Horn Clauses (CHCs) are widely adopted as intermediate
representations for a variety of verification tasks, including safety checking,
invariant synthesis, and interprocedural analysis. This paper introduces
CHCVERIF, a portfolio-based CHC solver that adopts a software verification
approach for solving CHCs. This approach enables us to reuse mature software
verification tools to tackle CHC benchmarks, particularly those involving
bitvectors and low-level semantics. Our evaluation shows that while the method
enjoys only moderate success with linear integer arithmetic, it achieves modest
success on bitvector benchmarks. Moreover, our results demonstrate the
viability and potential of using software verification tools as backends for
CHC solving, particularly when supported by a carefully constructed portfolio.

</details>


### [4] [PRISM: Proof-Carrying Artifact Generation through LLM x MDE Synergy and Stratified Constraints](https://arxiv.org/abs/2510.25890)
*Tong Ma,Hui Lai,Hui Wang,Zhenhu Tian,Jizhou Wang,Haichao Wu,Yongfan Gao,Chaochao Li,Fengjie Xu,Ling Fang*

Main category: cs.SE

TL;DR: PRISM将大语言模型与模型驱动工程相结合，为安全和合规关键领域生成符合监管要求的工件和机器可验证证据。


<details>
  <summary>Details</summary>
Motivation: 解决在安全和合规关键领域中，传统方法难以生成符合监管要求的可验证工件的问题，减少人工修复工作量。

Method: 采用三支柱方法：统一元模型整合异构模式，集成约束模型编译结构语义要求，约束引导可验证生成通过两层强制执行。

Result: 在汽车软件工程和跨境法律管辖场景中，PRISM能生成结构有效、可审计的工件，显著减少人工修复工作量。

Conclusion: PRISM为自动化工件生成提供了实用路径，具有内置保证机制，能够与现有工具链集成。

Abstract: PRISM unifies Large Language Models with Model-Driven Engineering to generate
regulator-ready artifacts and machine-checkable evidence for safety- and
compliance-critical domains. PRISM integrates three pillars: a Unified
Meta-Model (UMM) reconciles heterogeneous schemas and regulatory text into a
single semantic space; an Integrated Constraint Model (ICM) compiles structural
and semantic requirements into enforcement artifacts including generation-time
automata (GBNF, DFA) and post-generation validators (e.g., SHACL, SMT); and
Constraint-Guided Verifiable Generation (CVG) applies these through two-layer
enforcement - structural constraints drive prefix-safe decoding while
semantic/logical validation produces machine-checkable certificates. When
violations occur, PRISM performs audit-guided repair and records generation
traces for compliance review. We evaluate PRISM in automotive software
engineering (AUTOSAR) and cross-border legal jurisdiction (Brussels I bis).
PRISM produces structurally valid, auditable artifacts that integrate with
existing tooling and substantially reduce manual remediation effort, providing
a practical path toward automated artifact generation with built-in assurance.

</details>


### [5] [A Process Mining-Based System For The Analysis and Prediction of Software Development Workflows](https://arxiv.org/abs/2510.25935)
*Antía Dorado,Iván Folgueira,Sofía Martín,Gonzalo Martín,Álvaro Porto,Alejandro Ramos,John Wallace*

Main category: cs.SE

TL;DR: CodeSight是一个端到端系统，通过整合过程挖掘和机器学习来预测软件开发中的截止期限合规性，使用LSTM模型基于PR活动序列预测剩余解决时间。


<details>
  <summary>Details</summary>
Motivation: 在软件开发工作流中，需要一种能够主动预测截止期限合规性的系统，以便及早识别潜在的截止期限违规风险，实现更主动的软件项目管理。

Method: 系统从GitHub捕获开发和部署数据，转换为过程挖掘日志进行分析，生成指标和仪表板，然后使用LSTM模型基于序列活动轨迹和静态特征预测PR剩余解决时间。

Result: 在测试中，系统在预测截止期限合规性方面表现出高精度和高F1分数，证明了过程挖掘与机器学习整合的有效性。

Conclusion: CodeSight展示了将过程挖掘与机器学习相结合在主动软件项目管理中的价值，能够有效预测截止期限合规性并提供可操作的洞察。

Abstract: CodeSight is an end-to-end system designed to anticipate deadline compliance
in software development workflows. It captures development and deployment data
directly from GitHub, transforming it into process mining logs for detailed
analysis. From these logs, the system generates metrics and dashboards that
provide actionable insights into PR activity patterns and workflow efficiency.
Building on this structured representation, CodeSight employs an LSTM model
that predicts remaining PR resolution times based on sequential activity traces
and static features, enabling early identification of potential deadline
breaches. In tests, the system demonstrates high precision and F1 scores in
predicting deadline compliance, illustrating the value of integrating process
mining with machine learning for proactive software project management.

</details>


### [6] [Beyond Synthetic Benchmarks: Evaluating LLM Performance on Real-World Class-Level Code Generation](https://arxiv.org/abs/2510.26130)
*Musfiqur Rahman,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: LLMs在函数级代码生成表现出色，但在真实软件项目的类级实现中正确率显著下降，从合成基准的84-89%降至真实世界任务的25-34%。检索增强生成在部分文档情况下最有效，错误分析显示类型和属性错误是主要失败模式。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在真实软件项目中生成类级代码的能力，现有研究主要关注函数级代码生成，而类级实现在实际开发中更为复杂且重要。

Method: 从开源仓库构建新基准，包含真实世界类，分为已见和未见分区，评估多种LLM在不同输入规范、检索增强配置和文档完整性下的表现。

Result: LLMs在真实世界类任务中正确率仅为25-34%，远低于合成基准的84-89%。检索增强生成在部分文档时能提升4-7%正确率，但可能引入依赖冲突。

Conclusion: 当前LLMs在类级工程能力存在严重局限，需要改进上下文建模、文档策略和检索集成来提升生产代码辅助工具的效果。

Abstract: Large language models (LLMs) have advanced code generation at the function
level, yet their ability to produce correct class-level implementations in
authentic software projects remains poorly understood. This work introduces a
novel benchmark derived from open-source repositories, comprising real-world
classes divided into seen and unseen partitions to evaluate generalization
under practical conditions. The evaluation examines multiple LLMs under varied
input specifications, retrieval-augmented configurations, and documentation
completeness levels.
  Results reveal a stark performance disparity: LLMs achieve 84% to 89%
correctness on established synthetic benchmarks but only 25% to 34% on
real-world class tasks, with negligible differences between familiar and novel
codebases. Comprehensive docstrings yield modest gains of 1% to 3% in
functional accuracy, though statistical significance is rare.
Retrieval-augmented generation proves most effective with partial
documentation, improving correctness by 4% to 7% by supplying concrete
implementation patterns absent from specifications. Error profiling identifies
AttributeError, TypeError, and AssertionError as dominant failure modes (84% of
cases), with synthetic tests overemphasizing assertion issues and real-world
scenarios highlighting type and attribute mismatches. Retrieval augmentation
reduces logical flaws but can introduce dependency conflicts.
  The benchmark and analysis expose critical limitations in current LLM
capabilities for class-level engineering, offering actionable insights for
enhancing context modelling, documentation strategies, and retrieval
integration in production code assistance tools.

</details>


### [7] [Reduction of Test Re-runs by Prioritizing Potential Order Dependent Flaky Tests](https://arxiv.org/abs/2510.26171)
*Hasnain Iqbal,Zerina Begum,Kazi Sakib*

Main category: cs.SE

TL;DR: 提出了一种基于共享静态字段分析的方法来优先排序潜在的顺序依赖测试，显著提高了OD测试检测效率


<details>
  <summary>Details</summary>
Motivation: 顺序依赖测试会导致持续集成管道失败，现有检测方法需要多次重复运行测试，成本高昂，需要优先识别潜在OD测试以减少重复运行

Method: 通过分析测试类中的共享静态字段来识别更可能具有顺序依赖性的测试

Result: 在27个项目模块的实验中，成功在23个案例中优先排序所有OD测试，平均减少65.92%的测试执行和72.19%的不必要重复运行

Conclusion: 该方法通过降低执行成本，显著提高了OD测试检测的效率

Abstract: Flaky tests can make automated software testing unreliable due to their
unpredictable behavior. These tests can pass or fail on the same code base on
multiple runs. However, flaky tests often do not refer to any fault, even
though they can cause the continuous integration (CI) pipeline to fail. A
common type of flaky test is the order-dependent (OD) test. The outcome of an
OD test depends on the order in which it is run with respect to other test
cases. Several studies have explored the detection and repair of OD tests.
However, their methods require re-runs of tests multiple times, that are not
related to the order dependence. Hence, prioritizing potential OD tests is
necessary to reduce the re-runs. In this paper, we propose a method to
prioritize potential order-dependent tests. By analyzing shared static fields
in test classes, we identify tests that are more likely to be order-dependent.
In our experiment on 27 project modules, our method successfully prioritized
all OD tests in 23 cases, reducing test executions by an average of 65.92% and
unnecessary re-runs by 72.19%. These results demonstrate that our approach
significantly improves the efficiency of OD test detection by lowering
execution costs.

</details>


### [8] [The "4W+1H" of Software Supply Chain Security Checklist for Critical Infrastructure](https://arxiv.org/abs/2510.26174)
*Liming Dong,Sung Une Lee,Zhenchang Xing,Muhammad Ejaz Ahmed,Stefan Avgoustakis*

Main category: cs.SE

TL;DR: 本文通过多源文献综述分析了软件供应链安全实践，发现现有框架对关键基础设施领域针对性不足，提出了包含80个问题的结构化多层级检查表来评估和增强软件供应链安全。


<details>
  <summary>Details</summary>
Motivation: 软件供应链攻击日益频繁和复杂，对关键基础设施构成严重威胁，而现有安全实践分散且不足，缺乏针对关键基础设施领域的专门框架。

Method: 采用多源文献综述方法，分析国际框架、澳大利亚监管文件和学术研究，使用"4W+1H"分析方法，综合10个核心类别的软件供应链安全实践。

Result: 发现现有框架很少专门针对关键基础设施领域，识别了框架指导与行业特定需求之间的差距，开发了包含80个问题的结构化检查表。

Conclusion: 需要采用整合的、情境感知的方法来保护关键基础设施免受不断演变的软件供应链风险威胁。

Abstract: The increasing frequency and sophistication of software supply chain attacks
pose severe risks to critical infrastructure sectors, threatening national
security, economic stability, and public safety. Despite growing awareness,
existing security practices remain fragmented and insufficient, with most
frameworks narrowly focused on isolated life cycle stages or lacking alignment
with the specific needs of critical infrastructure (CI) sectors. In this paper,
we conducted a multivocal literature review across international frameworks,
Australian regulatory sources, and academic studies to identify and analyze
security practices across the software supply chain, especially specific CI
sector. Our analysis found that few existing frameworks are explicitly tailored
to CI domains. We systematically leveraged identified software supply chain
security frameworks, using a "4W+1H" analytical approach, we synthesized ten
core categories (what) of software supply chain security practices, mapped them
across life-cycle phases (when), stakeholder roles (who), and implementation
levels (how), and examined their coverage across existing frameworks (where).
Building on these insights, the paper culminates in structured, multi-layered
checklist of 80 questions designed to relevant stakeholders evaluate and
enhance their software supply chain security. Our findings reveal gaps between
framework guidance and sector-specific needs, highlight the need for
integrated, context-aware approaches to safeguard critical infrastructure from
evolving software supply chain risks.

</details>


### [9] [A Research Roadmap for Augmenting Software Engineering Processes and Software Products with Generative AI](https://arxiv.org/abs/2510.26275)
*Domenico Amalfitano,Andreas Metzger,Marco Autili,Tommaso Fulcini,Tobias Hey,Jan Keim,Patrizio Pelliccione,Vincenzo Scotti,Anne Koziolek,Raffaela Mirandola,Andreas Vogelsang*

Main category: cs.SE

TL;DR: 本文应用设计科学研究方法构建了GenAI增强软件工程的路线图，通过三轮循环整合多方证据，使用麦克卢汉四元法系统分析GenAI对SE的影响，识别了四种基本增强形式及其研究挑战与机遇，并提出了2030年软件工程的十个预测。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正在快速改变软件工程实践，影响SE流程的执行方式以及软件系统的开发、运营和演进过程。需要构建系统化的路线图来指导GenAI在SE领域的应用和发展。

Method: 采用设计科学研究方法，通过三轮循环逐步整合证据：FSE 2025研讨会讨论、快速文献综述和同行反馈会议。使用麦克卢汉四元法作为概念工具系统分析GenAI对SE的影响。

Result: 识别了GenAI在SE中的四种基本增强形式，系统描述了相关研究挑战和机遇，并整合为未来研究方向。通过多轮验证过程提供了透明可复现的分析基础。

Conclusion: 研究为分析GenAI如何影响SE流程、方法和工具提供了坚实基础，并为这一快速发展领域的未来研究构建了框架，最终提出了2030年软件工程的十个预测。

Abstract: Generative AI (GenAI) is rapidly transforming software engineering (SE)
practices, influencing how SE processes are executed, as well as how software
systems are developed, operated, and evolved. This paper applies design science
research to build a roadmap for GenAI-augmented SE. The process consists of
three cycles that incrementally integrate multiple sources of evidence,
including collaborative discussions from the FSE 2025 "Software Engineering
2030" workshop, rapid literature reviews, and external feedback sessions
involving peers. McLuhan's tetrads were used as a conceptual instrument to
systematically capture the transforming effects of GenAI on SE processes and
software products.The resulting roadmap identifies four fundamental forms of
GenAI augmentation in SE and systematically characterizes their related
research challenges and opportunities. These insights are then consolidated
into a set of future research directions. By grounding the roadmap in a
rigorous multi-cycle process and cross-validating it among independent author
teams and peers, the study provides a transparent and reproducible foundation
for analyzing how GenAI affects SE processes, methods and tools, and for
framing future research within this rapidly evolving area. Based on these
findings, the article finally makes ten predictions for SE in the year 2030.

</details>


### [10] [Empowering RepoQA-Agent based on Reinforcement Learning Driven by Monte-carlo Tree Search](https://arxiv.org/abs/2510.26287)
*Guochang Li,Yuchen Liu,Zhen Qin,Yunkun Wang,Jianping Zhong,Chen Zhi,Binhua Li,Fei Huang,Yongbin Li,Shuiguang Deng*

Main category: cs.SE

TL;DR: RepoSearch-R1是一个基于蒙特卡洛树搜索的强化学习框架，用于解决仓库级软件工程任务中的代码库导航和信息提取问题，无需模型蒸馏或外部监督。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在显著限制：无训练方法难以有效指导代理工具使用和决策，基于训练的方法依赖大模型蒸馏，存在企业环境中的数据合规问题。

Method: 引入RepoSearch-R1框架，基于蒙特卡洛树搜索的强化学习，通过自训练生成多样化高质量推理轨迹，构建专门用于仓库问答任务的RepoQA-Agent。

Result: 在仓库问答任务上，RepoSearch-R1相比无检索方法提升16.0%答案完整性，比迭代检索方法提升19.5%，训练效率比通用强化学习方法提高33%。

Conclusion: 冷启动训练方法消除了数据合规问题，同时在仓库级推理任务中保持了强大的探索多样性和答案完整性。

Abstract: Repository-level software engineering tasks require large language models
(LLMs) to efficiently navigate and extract information from complex codebases
through multi-turn tool interactions. Existing approaches face significant
limitations: training-free, in-context learning methods struggle to guide
agents effectively in tool utilization and decision-making based on
environmental feedback, while training-based approaches typically rely on
costly distillation from larger LLMs, introducing data compliance concerns in
enterprise environments. To address these challenges, we introduce
RepoSearch-R1, a novel agentic reinforcement learning framework driven by
Monte-carlo Tree Search (MCTS). This approach allows agents to generate
diverse, high-quality reasoning trajectories via self-training without
requiring model distillation or external supervision. Based on RepoSearch-R1,
we construct a RepoQA-Agent specifically designed for repository
question-answering tasks. Comprehensive evaluation on repository
question-answering tasks demonstrates that RepoSearch-R1 achieves substantial
improvements of answer completeness: 16.0% enhancement over no-retrieval
methods, 19.5% improvement over iterative retrieval methods, and 33% increase
in training efficiency compared to general agentic reinforcement learning
approaches. Our cold-start training methodology eliminates data compliance
concerns while maintaining robust exploration diversity and answer completeness
across repository-level reasoning tasks.

</details>


### [11] [Environmental Impact of CI/CD Pipelines](https://arxiv.org/abs/2510.26413)
*Nuno Saavedra,Alexandra Mendes,João F. Ferreira*

Main category: cs.SE

TL;DR: 研究GitHub Actions CI/CD管道的碳足迹和水足迹，发现其环境影响显著，并提出通过优化资源配置、选择环保区域等策略来减少浪费。


<details>
  <summary>Details</summary>
Motivation: CI/CD管道在软件开发中广泛使用，但其碳足迹和水足迹等环境影响通常不为开发者所知，随着云计算环境影响的增长，了解这些足迹变得日益重要。

Method: 基于Cloud Carbon Footprint框架的方法论，使用文献中最大的工作流运行数据集，包含超过220万次工作流运行和18,000多个仓库。

Result: GitHub Actions生态系统产生显著的碳足迹和水足迹：最可能情景下碳足迹为456.9 MTCO2e（相当于7,615棵城市树木一年的碳捕获量），水足迹为5,738.2千升（相当于美国家庭5,053年的用水量）。

Conclusion: 建议通过减少计算资源浪费来减轻环境影响，包括在法国和英国等低环境影响区域部署运行器、优化调度执行时间以及减小仓库大小。

Abstract: CI/CD pipelines are widely used in software development, yet their
environmental impact, particularly carbon and water footprints (CWF), remains
largely unknown to developers, as CI service providers typically do not
disclose such information. With the growing environmental impact of cloud
computing, understanding the CWF of CI/CD services has become increasingly
important.
  This work investigates the CWF of using GitHub Actions, focusing on
open-source repositories where usage is free and unlimited for standard
runners. We build upon a methodology from the Cloud Carbon Footprint framework
and we use the largest dataset of workflow runs reported in the literature to
date, comprising over 2.2 million workflow runs from more than 18,000
repositories.
  Our analysis reveals that the GitHub Actions ecosystem results in a
substantial CWF. Our estimates for the carbon footprint in 2024 range from
150.5 MTCO2e in the most optimistic scenario to 994.9 MTCO2e in the most
pessimistic scenario, while the water footprint ranges from 1,989.6 to 37,664.5
kiloliters. The most likely scenario estimates are 456.9 MTCO2e for carbon
footprint and 5,738.2 kiloliters for water footprint. To provide perspective,
the carbon footprint in the most likely scenario is equivalent to the carbon
captured by 7,615 urban trees in a year, and the water footprint is comparable
to the water consumed by an average American family over 5,053 years.
  We explore strategies to mitigate this impact, primarily by reducing wasted
computational resources. Key recommendations include deploying runners in
regions whose energy production has a low environmental impact such as France
and the United Kingdom, implementing stricter deactivation policies for
scheduled runs and aligning their execution with periods when the regional
energy mix is more environmentally favorable, and reducing the size of
repositories.

</details>


### [12] [Nexus: Execution-Grounded Multi-Agent Test Oracle Synthesis](https://arxiv.org/abs/2510.26423)
*Dong Huang,Mingzhe Du,Jie M. Zhang,Zheng Lin,Meng Luo,Qianru Zhang,See-Kiong Ng*

Main category: cs.SE

TL;DR: Nexus是一个新颖的多智能体框架，用于生成测试预言，通过专门化智能体的结构化审议、验证和迭代自优化过程，显著提升了测试预言准确率和下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决非回归测试中测试预言生成的长期挑战，即如何为被测函数产生能够准确判断其行为是否符合预期的测试预言。

Method: 采用多智能体框架，包含四个体现不同测试理念的专门化智能体进行协作审议，生成候选FUT实现并在安全沙箱中执行验证，对失败的预言进行自动化调试和修正的迭代自优化循环。

Result: 在七个不同基准测试中，Nexus始终显著优于最先进的基线方法。例如，在LiveCodeBench上将GPT-4.1-Mini的测试级预言准确率从46.30%提升至57.73%，在HumanEval上将错误检测率从90.91%提升至95.45%，自动程序修复成功率从35.23%提升至69.32%。

Conclusion: Nexus框架通过多智能体协作和迭代自优化机制，有效解决了测试预言生成问题，显著提升了预言准确率和下游任务的性能表现。

Abstract: Test oracle generation in non-regression testing is a longstanding challenge
in software engineering, where the goal is to produce oracles that can
accurately determine whether a function under test (FUT) behaves as intended
for a given input. In this paper, we introduce Nexus, a novel multi-agent
framework to address this challenge. Nexus generates test oracles by leveraging
a diverse set of specialized agents that synthesize test oracles through a
structured process of deliberation, validation, and iterative self-refinement.
During the deliberation phase, a panel of four specialist agents, each
embodying a distinct testing philosophy, collaboratively critiques and refines
an initial set of test oracles. Then, in the validation phase, Nexus generates
a plausible candidate implementation of the FUT and executes the proposed
oracles against it in a secure sandbox. For any oracle that fails this
execution-based check, Nexus activates an automated selfrefinement loop, using
the specific runtime error to debug and correct the oracle before
re-validation. Our extensive evaluation on seven diverse benchmarks
demonstrates that Nexus consistently and substantially outperforms
state-of-theart baselines. For instance, Nexus improves the test-level oracle
accuracy on the LiveCodeBench from 46.30% to 57.73% for GPT-4.1-Mini. The
improved accuracy also significantly enhances downstream tasks: the bug
detection rate of GPT4.1-Mini generated test oracles on HumanEval increases
from 90.91% to 95.45% for Nexus compared to baselines, and the success rate of
automated program repair improves from 35.23% to 69.32%.

</details>


### [13] [SecureReviewer: Enhancing Large Language Models for Secure Code Review through Secure-aware Fine-tuning](https://arxiv.org/abs/2510.26457)
*Fang Liu,Simiao Liu,Yinghao Zhu,Xiaoli Lian,Li Zhang*

Main category: cs.SE

TL;DR: SecureReviewer是一个针对安全代码审查的LLM增强方法，通过构建专用数据集、安全感知微调和RAG技术，提升LLM在识别和解决安全问题方面的能力，并引入SecureBLEU评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有代码审查方法主要关注通用目的审查，在识别和解决安全相关问题方面效果有限，且面临数据稀缺和评估指标不足的挑战。

Method: 构建安全代码审查专用数据集，采用安全感知微调策略训练LLM，集成RAG技术减少幻觉并增强输出可靠性，引入SecureBLEU评估指标。

Result: 实验结果表明SecureReviewer在安全漏洞检测准确性和生成审查评论的整体质量及实用性方面优于现有最优基线方法。

Conclusion: SecureReviewer通过专门的数据集、微调策略和评估指标，有效提升了LLM在安全代码审查中的能力，为软件安全开发提供了有力支持。

Abstract: Identifying and addressing security issues during the early phase of the
development lifecycle is critical for mitigating the long-term negative impacts
on software systems. Code review serves as an effective practice that enables
developers to check their teammates' code before integration into the codebase.
To streamline the generation of review comments, various automated code review
approaches have been proposed, where LLM-based methods have significantly
advanced the capabilities of automated review generation. However, existing
models primarily focus on general-purpose code review, their effectiveness in
identifying and addressing security-related issues remains underexplored.
Moreover, adapting existing code review approaches to target security issues
faces substantial challenges, including data scarcity and inadequate evaluation
metrics. To address these limitations, we propose SecureReviewer, a new
approach designed for enhancing LLMs' ability to identify and resolve
security-related issues during code review. Specifically, we first construct a
dataset tailored for training and evaluating secure code review capabilities.
Leveraging this dataset, we fine-tune LLMs to generate code review comments
that can effectively identify security issues and provide fix suggestions with
our proposed secure-aware fine-tuning strategy. To mitigate hallucination in
LLMs and enhance the reliability of their outputs, we integrate the RAG
technique, which grounds the generated comments in domain-specific security
knowledge. Additionally, we introduce SecureBLEU, a new evaluation metric
designed to assess the effectiveness of review comments in addressing security
issues. Experimental results demonstrate that SecureReviewer outperforms
state-of-the-art baselines in both security issue detection accuracy and the
overall quality and practical utility of generated review comments.

</details>


### [14] [Automated Extract Method Refactoring with Open-Source LLMs: A Comparative Study](https://arxiv.org/abs/2510.26480)
*Sivajeet Chand,Melih Kilic,Roland Würsching,Sushant Kumar Pandey,Alexander Pretschner*

Main category: cs.SE

TL;DR: 评估5个开源LLM在Python代码Extract Method重构任务上的表现，发现RCI提示策略优于单次提示，最佳模型测试通过率达82.9%，显著降低代码行数和复杂度，开发者调查显示70%以上接受度。


<details>
  <summary>Details</summary>
Motivation: 自动化Extract Method重构仍然具有挑战性，开源高效LLM为自动化此类高级任务提供了新方法。

Method: 系统评估5个3B-8B参数的开源LLM，使用自动指标评估功能正确性和代码质量，比较单次提示与RCI提示策略的效果。

Result: RCI提示在测试通过率和重构质量上表现更好，最佳模型测试通过率82.9%，代码行数从12.1降至5.6，复杂度从4.6降至3.3，开发者调查70%以上接受。

Conclusion: RCI提示策略能有效提升自动化重构质量，传统指标与人工评估存在差异，需要人机协同评估，开源基准为未来LLM自动化重构研究提供基础。

Abstract: Automating the Extract Method refactoring (EMR) remains challenging and
largely manual despite its importance in improving code readability and
maintainability. Recent advances in open-source, resource-efficient Large
Language Models (LLMs) offer promising new approaches for automating such
high-level tasks. In this work, we critically evaluate five state-of-the-art
open-source LLMs, spanning 3B to 8B parameter sizes, on the EMR task for Python
code. We systematically assess functional correctness and code quality using
automated metrics and investigate the impact of prompting strategies by
comparing one-shot prompting to a Recursive criticism and improvement (RCI)
approach. RCI-based prompting consistently outperforms one-shot prompting in
test pass rates and refactoring quality. The best-performing models,
Deepseek-Coder-RCI and Qwen2.5-Coder-RCI, achieve test pass percentage (TPP)
scores of 0.829 and 0.808, while reducing lines of code (LOC) per method from
12.103 to 6.192 and 5.577, and cyclomatic complexity (CC) from 4.602 to 3.453
and 3.294, respectively. A developer survey on RCI-generated refactorings shows
over 70% acceptance, with Qwen2.5-Coder rated highest across all evaluation
criteria. In contrast, the original code scored below neutral, particularly in
readability and maintainability, underscoring the benefits of automated
refactoring guided by quality prompts. While traditional metrics like CC and
LOC provide useful signals, they often diverge from human judgments,
emphasizing the need for human-in-the-loop evaluation. Our open-source
benchmark offers a foundation for future research on automated refactoring with
LLMs.

</details>


### [15] [Envisioning Future Interactive Web Development: Editing Webpage with Natural Language](https://arxiv.org/abs/2510.26516)
*Truong Hai Dang,Jingyu Xiao,Yintong Huo*

Main category: cs.SE

TL;DR: 提出Instruct4Edit自动数据生成管道，使用LLM合成高质量网页编辑微调数据集，通过微调使模型能更好理解人类意图并生成精确的代码修改


<details>
  <summary>Details</summary>
Motivation: 网页应用开发依赖迭代式代码修改，传统方法耗时且手动。LLM能生成UI代码，但根据新设计需求编辑现有代码仍具挑战，主要缺乏大规模高质量微调数据

Method: 构建自动化数据生成管道：生成多样化指令、应用相应代码修改、进行视觉验证确保正确性，创建Instruct4Edit数据集用于微调

Result: 在Instruct4Edit上微调的模型在将人类意图转化为精确、结构连贯且视觉准确的代码修改方面表现一致提升

Conclusion: 为基于自然语言的网页编辑提供了可扩展且透明的基础，证明微调较小的开源模型能达到与专有系统竞争的性能

Abstract: The evolution of web applications relies on iterative code modifications, a
process that is traditionally manual and time-consuming. While Large Language
Models (LLMs) can generate UI code, their ability to edit existing code from
new design requirements (e.g., "center the logo") remains a challenge. This is
largely due to the absence of large-scale, high-quality tuning data to align
model performance with human expectations. In this paper, we introduce a novel,
automated data generation pipeline that uses LLMs to synthesize a high-quality
fine-tuning dataset for web editing, named Instruct4Edit. Our approach
generates diverse instructions, applies the corresponding code modifications,
and performs visual verification to ensure correctness. By fine-tuning models
on Instruct4Edit, we demonstrate consistent improvement in translating human
intent into precise, structurally coherent, and visually accurate code changes.
This work provides a scalable and transparent foundation for natural language
based web editing, demonstrating that fine-tuning smaller open-source models
can achieve competitive performance with proprietary systems. We release all
data, code implementations, and model checkpoints for reproduction.

</details>


### [16] [Reflecting on Empirical and Sustainability Aspects of Software Engineering Research in the Era of Large Language Models](https://arxiv.org/abs/2510.26538)
*David Williams,Max Hort,Maria Kechagia,Aldeida Aleti,Justyna Petke,Federica Sarro*

Main category: cs.SE

TL;DR: 本文分析了软件工程研究中使用大型语言模型带来的挑战，包括基准测试严谨性、数据污染、可复现性和可持续性问题，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 软件工程研究中使用大型语言模型引入了基准测试严谨性、数据污染、可复现性和可持续性等新挑战，需要研究社区反思这些问题。

Method: 通过结构化分析当前ICSE会议上的LLM-based软件工程研究，识别良好实践和持续存在的缺陷。

Result: 研究结果提供了当前LLM-based软件工程研究的结构化概览，突出了令人鼓舞的实践和持续存在的不足。

Conclusion: 建议加强基准测试严谨性、提高可复现性，并解决LLM-based软件工程的财务和环境成本问题。

Abstract: Software Engineering (SE) research involving the use of Large Language Models
(LLMs) has introduced several new challenges related to rigour in benchmarking,
contamination, replicability, and sustainability. In this paper, we invite the
research community to reflect on how these challenges are addressed in SE. Our
results provide a structured overview of current LLM-based SE research at ICSE,
highlighting both encouraging practices and persistent shortcomings. We
conclude with recommendations to strengthen benchmarking rigour, improve
replicability, and address the financial and environmental costs of LLM-based
SE.

</details>


### [17] ["Show Me You Comply... Without Showing Me Anything": Zero-Knowledge Software Auditing for AI-Enabled Systems](https://arxiv.org/abs/2510.26576)
*Filippo Scaramuzza,Renato Cordeiro Ferreira,Tomaz Maia Suller,Giovanni Quattrocchi,Damian Andrew Tamburri,Willem-Jan van den Heuvel*

Main category: cs.SE

TL;DR: ZKMLOps是一个基于零知识证明的MLOps验证框架，通过密码学协议在不泄露敏感信息的情况下验证AI系统的合规性，解决了AI审计中的透明度与隐私保护冲突。


<details>
  <summary>Details</summary>
Motivation: AI系统在关键领域的应用面临可信度挑战，传统验证方法成本高且不适合AI黑盒特性，同时透明度要求与数据/模型保护之间存在冲突。

Method: 将零知识证明集成到机器学习运维生命周期中，结合软件工程模式，提供模块化、可重复的合规性密码证明生成流程。

Result: 通过金融风险审计案例验证了框架实用性，并对主流ZKP协议进行了实证评估，分析了不同复杂度ML模型的性能权衡。

Conclusion: ZKMLOps框架为AI系统提供了可验证的问责机制，在保护敏感资产的同时满足监管合规要求。

Abstract: The increasing exploitation of Artificial Intelligence (AI) enabled systems
in critical domains has made trustworthiness concerns a paramount showstopper,
requiring verifiable accountability, often by regulation (e.g., the EU AI Act).
Classical software verification and validation techniques, such as procedural
audits, formal methods, or model documentation, are the mechanisms used to
achieve this. However, these methods are either expensive or heavily manual and
ill-suited for the opaque, "black box" nature of most AI models. An intractable
conflict emerges: high auditability and verifiability are required by law, but
such transparency conflicts with the need to protect assets being audited-e.g.,
confidential data and proprietary models-leading to weakened accountability. To
address this challenge, this paper introduces ZKMLOps, a novel MLOps
verification framework that operationalizes Zero-Knowledge Proofs
(ZKPs)-cryptographic protocols allowing a prover to convince a verifier that a
statement is true without revealing additional information-within
Machine-Learning Operations lifecycles. By integrating ZKPs with established
software engineering patterns, ZKMLOps provides a modular and repeatable
process for generating verifiable cryptographic proof of compliance. We
evaluate the framework's practicality through a study of regulatory compliance
in financial risk auditing and assess feasibility through an empirical
evaluation of top ZKP protocols, analyzing performance trade-offs for ML models
of increasing complexity.

</details>


### [18] [Online and Interactive Bayesian Inference Debugging](https://arxiv.org/abs/2510.26579)
*Nathanael Nussbaumer,Markus Böck,Jürgen Cito*

Main category: cs.SE

TL;DR: 提出了一种新颖的贝叶斯推断调试方法，通过在开发环境中直接集成调试工具，显著减少了调试时间和所需专业知识。


<details>
  <summary>Details</summary>
Motivation: 概率编程虽然便于构建贝叶斯模型和进行后验推断，但推断调试过程非常困难，需要大量时间和深厚知识。

Method: 提出了满足贝叶斯推断调试框架要求的在线交互式调试工具，直接在开发环境中实现。

Result: 在18位有经验参与者的研究中，该方法显著减少了推断调试任务的时间和难度。

Conclusion: 这种在线交互式贝叶斯推断调试方法能够有效降低调试门槛，提高效率。

Abstract: Probabilistic programming is a rapidly developing programming paradigm which
enables the formulation of Bayesian models as programs and the automation of
posterior inference. It facilitates the development of models and conducting
Bayesian inference, which makes these techniques available to practitioners
from multiple fields. Nevertheless, probabilistic programming is notoriously
difficult as identifying and repairing issues with inference requires a lot of
time and deep knowledge. Through this work, we introduce a novel approach to
debugging Bayesian inference that reduces time and required knowledge
significantly. We discuss several requirements a Bayesian inference debugging
framework has to fulfill, and propose a new tool that meets these key
requirements directly within the development environment. We evaluate our
results in a study with 18 experienced participants and show that our approach
to online and interactive debugging of Bayesian inference significantly reduces
time and difficulty on inference debugging tasks.

</details>


### [19] [Stitch: Step-by-step LLM Guided Tutoring for Scratch](https://arxiv.org/abs/2510.26634)
*Yuan Si,Kyle Qi,Daming Li,Hanyuan Shi,Jialu Zhang*

Main category: cs.SE

TL;DR: Stitch是一个交互式编程教学系统，通过逐步引导而非直接展示正确答案来帮助初学者调试Scratch程序中的语义错误。


<details>
  <summary>Details</summary>
Motivation: 现有的调试方法通常直接向学习者展示正确程序，这种方法虽然能修复错误，但削弱了问题解决能力的培养。需要一种更有效的教学反馈方式。

Method: 系统通过Diff-Analyze模块对比学生项目与参考实现，识别关键差异，使用大语言模型解释这些变化的重要性。学习者通过自定义渲染引擎检查高亮块，理解解释并选择性应用部分修复。

Result: 与最先进的自动反馈生成工具相比，Stitch的逐步引导系统提供了更有效的学习体验，显著提升了学习效果。

Conclusion: 在基于块的编程环境中，逐步辅导比直接展示答案和现有自动反馈工具更能有效促进学习，为编程教育反馈的有效性提供了新证据。

Abstract: Block-based environments such as Scratch are increasingly popular in
programming education. While block syntax reduces surface errors, semantic bugs
remain common and challenging for novices to resolve. Existing debugging
workflows typically show the correct program directly to learners, a strategy
that may fix errors but undermines the development of problem-solving skills.
  We present Stitch, an interactive tutoring system that replaces "showing the
answer" with step-by-step scaffolding. The system's Diff-Analyze module
contrasts a student's project with a reference implementation, identifies the
most critical differences, and uses a large language model to explain why these
changes matter. Learners inspect highlighted blocks through a custom rendering
engine, understand the explanations, and selectively apply partial fixes. This
iterative process continues until the intended functionality is achieved.
  We evaluate Stitch in an empirical study, comparing it against a
state-of-the-art automated feedback generation tool for Scratch. Our key
insight is that simply presenting the correct program is pedagogically
ineffective. In contrast, our interactive, step-by-step guided system promotes
a more effective learning experience. More broadly, what constitutes effective
feedback in block-based programming remains an open question. Our evaluation
provides new evidence that step-by-step tutoring significantly enhances
learning outcomes, outperforming both direct-answer approaches and current
automated feedback generation tools.

</details>


### [20] [Process-based Indicators of Vulnerability Re-Introducing Code Changes: An Exploratory Case Study](https://arxiv.org/abs/2510.26676)
*Samiha Shimmi,Nicholas M. Synovic,Mona Rahimi,George K. Thiruvathukal*

Main category: cs.SE

TL;DR: 该研究通过分析ImageMagick项目中76个漏洞重新引入案例，发现过程指标（如问题腐败度和问题密度）与漏洞重新引入密切相关，强调漏洞重新引入是累积开发活动和社会技术条件的结果。


<details>
  <summary>Details</summary>
Motivation: 软件漏洞即使在修复后仍会持续存在或重新出现，现有研究很少探索过程指标是否能揭示随时间推移的风险开发活动，这对于预测和缓解软件漏洞至关重要。

Method: 在ImageMagick项目中进行案例研究，将纵向过程指标（如巴士因子、问题密度、问题腐败度）与漏洞重新引入活动相关联，重点分析提交级别的安全修复，而不仅仅是文件级预测。

Result: 研究发现重新引入通常与增加的问题腐败度和波动的问题密度相一致，反映了问题管理和团队响应能力的短期低效。

Conclusion: 过程指标与代码指标相结合可以为预测风险修复和加强软件安全提供基础，漏洞重新引入很少是孤立行动的结果，而是源于累积开发活动和社会技术条件。

Abstract: Software vulnerabilities often persist or re-emerge even after being fixed,
revealing the complex interplay between code evolution and socio-technical
factors. While source code metrics provide useful indicators of
vulnerabilities, software engineering process metrics can uncover patterns that
lead to their introduction. Yet few studies have explored whether process
metrics can reveal risky development activities over time -- insights that are
essential for anticipating and mitigating software vulnerabilities. This work
highlights the critical role of process metrics along with code changes in
understanding and mitigating vulnerability reintroduction. We move beyond
file-level prediction and instead analyze security fixes at the commit level,
focusing not only on whether a single fix introduces a vulnerability but also
on the longer sequences of changes through which vulnerabilities evolve and
re-emerge. Our approach emphasizes that reintroduction is rarely the result of
one isolated action, but emerges from cumulative development activities and
socio-technical conditions. To support this analysis, we conducted a case study
on the ImageMagick project by correlating longitudinal process metrics such as
bus factor, issue density, and issue spoilage with vulnerability reintroduction
activities, encompassing 76 instances of reintroduced vulnerabilities. Our
findings show that reintroductions often align with increased issue spoilage
and fluctuating issue density, reflecting short-term inefficiencies in issue
management and team responsiveness. These observations provide a foundation for
broader studies that combine process and code metrics to predict risky fixes
and strengthen software security.

</details>


### [21] [Using Copilot Agent Mode to Automate Library Migration: A Quantitative Assessment](https://arxiv.org/abs/2510.26699)
*Aylton Almeida,Laerte Xavier,Marco Tulio Valente*

Main category: cs.SE

TL;DR: 评估使用GitHub Copilot Agent Mode自动迁移SQLAlchemy库版本的效果，该代理在API迁移方面表现良好（迁移覆盖率100%），但在保持应用功能方面表现不佳（测试通过率仅39.75%）。


<details>
  <summary>Details</summary>
Motivation: 软件系统更新对于避免技术债务、安全漏洞和遗留系统僵化至关重要，但手动更新库和框架耗时且容易出错。LLM和代理编码系统为自动化此类维护任务提供了新机会。

Method: 使用GitHub的Copilot Agent Mode（一个能够规划和执行多步迁移工作流的自主AI系统），在10个客户端应用的数据集上评估SQLAlchemy库的更新。引入迁移覆盖率指标来量化正确迁移的API使用点比例。

Result: LLM代理能够成功迁移SQLAlchemy版本间的功能和API使用（迁移覆盖率中位数100%），但未能保持应用程序功能，导致测试通过率较低（中位数39.75%）。

Conclusion: 虽然LLM代理在API迁移方面表现优异，但在保持应用整体功能方面仍有不足，表明自动化迁移工具在功能完整性方面需要进一步改进。

Abstract: Keeping software systems up to date is essential to avoid technical debt,
security vulnerabilities, and the rigidity typical of legacy systems. However,
updating libraries and frameworks remains a time consuming and error-prone
process. Recent advances in Large Language Models (LLMs) and agentic coding
systems offer new opportunities for automating such maintenance tasks. In this
paper, we evaluate the update of a well-known Python library, SQLAlchemy,
across a dataset of ten client applications. For this task, we use the Github's
Copilot Agent Mode, an autonomous AI systema capable of planning and executing
multi-step migration workflows. To assess the effectiveness of the automated
migration, we also introduce Migration Coverage, a metric that quantifies the
proportion of API usage points correctly migrated. The results of our study
show that the LLM agent was capable of migrating functionalities and API usages
between SQLAlchemy versions (migration coverage: 100%, median), but failed to
maintain the application functionality, leading to a low test-pass rate
(39.75%, median).

</details>


### [22] [Optimized Log Parsing with Syntactic Modifications](https://arxiv.org/abs/2510.26793)
*Nafid Enan,Gias Uddin*

Main category: cs.SE

TL;DR: 该论文对基于语法和语义的日志解析器进行了全面的实证研究，发现语义方法在模板识别方面表现更好，而语法方法效率更高且分组准确率更好。基于研究结果，作者提出了SynLog+作为两阶段日志解析架构的第二阶段，显著提高了解析准确率。


<details>
  <summary>Details</summary>
Motivation: 日志解析是自动化日志分析的第一步，但现有日志解析器采用多种不同技术，需要评估它们的特性和性能，以帮助理解各种方法的优缺点。

Method: 通过全面的实证研究比较语法和语义日志解析器，以及单阶段和两阶段解析架构。基于研究发现，提出了SynLog+模板识别模块作为两阶段架构的第二阶段。

Result: 语义方法在正确识别模板方面表现更好，语法日志解析器效率高10到1000倍且分组准确率更好。两阶段架构相比单阶段架构持续提高准确率。SynLog+使语法和语义日志解析器的解析准确率分别平均提高236%和20%，且几乎没有额外运行时成本。

Conclusion: 两阶段日志解析架构能显著提高解析准确率，SynLog+作为第二阶段模块能有效提升现有解析器的性能，为日志分析提供了更优的解决方案。

Abstract: Logs provide valuable insights into system runtime and assist in software
development and maintenance. Log parsing, which converts semi-structured log
data into structured log data, is often the first step in automated log
analysis. Given the wide range of log parsers utilizing diverse techniques, it
is essential to evaluate them to understand their characteristics and
performance. In this paper, we conduct a comprehensive empirical study
comparing syntax- and semantic-based log parsers, as well as single-phase and
two-phase parsing architectures. Our experiments reveal that semantic-based
methods perform better at identifying the correct templates and syntax-based
log parsers are 10 to 1,000 times more efficient and provide better grouping
accuracy although they fall short in accurate template identification.
Moreover, two-phase architecture consistently improves accuracy compared to
single-phase architecture. Based on the findings of this study, we propose
SynLog+, a template identification module that acts as the second phase in a
two-phase log parsing architecture. SynLog+ improves the parsing accuracy of
syntax-based and semantic-based log parsers by 236\% and 20\% on average,
respectively, with virtually no additional runtime cost.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [23] [Fair intersection of seekable iterators](https://arxiv.org/abs/2510.26016)
*Michael Arntzenius*

Main category: cs.PL

TL;DR: 该论文展示了通过有界工作实现公平性的思想不仅适用于miniKanren的搜索策略，也适用于实现最坏情况最优连接


<details>
  <summary>Details</summary>
Motivation: 探索miniKanren中通过有界工作实现公平性的思想在其他领域的应用潜力

Method: 使用可搜索迭代器接口实现最坏情况最优连接，适合在函数式语言中进行浅层嵌入

Result: 证明了公平性通过有界工作的思想可以优雅地组合实现最坏情况最优连接

Conclusion: 公平性通过有界工作的思想具有广泛适用性，可以统一miniKanren的搜索策略和数据库连接算法

Abstract: miniKanren's key semantic advance over Prolog is to implement a complete yet
efficient search strategy, fairly interleaving execution between disjuncts.
This fairness is accomplished by bounding how much work is done exploring one
disjunct before switching to the next. We show that the same idea -- fairness
via bounded work -- underlies an elegant compositional approach to implementing
worst-case optimal joins using a seekable iterator interface, suitable for
shallow embedding in functional languages.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [24] [Finding Regular Herbrand Models for CHCs using Answer Set Programming](https://arxiv.org/abs/2510.26428)
*Gregoire Maire,Thomas Genet*

Main category: cs.LO

TL;DR: 提出使用Clingo将带代数数据类型的约束Horn子句编码为SAT问题，构建树自动机来识别Herbrand模型，实现半完备的可满足性检查


<details>
  <summary>Details</summary>
Motivation: 证明带代数数据类型的约束Horn子句的可满足性，通过构建识别Herbrand模型的树自动机

Method: 将CHCs与ADTs编码为ASP问题，使用Clingo求解SAT问题来构建树自动机

Result: 实现了半完备的可满足性检查器，能找到正则Herbrand模型或反例

Conclusion: 该方法能有效检查CHCs的可满足性，当存在正则Herbrand模型时能找到自动机，问题不可满足时能找到反例

Abstract: We are interested in proving satisfiability of Constrained Horn Clauses
(CHCs) over Algebraic Data Types (ADTs). We propose to prove satisfiability by
building a tree automaton recognizing the Herbrand model of the CHCs. If such
an automaton exists then the model is said to be regular, i.e., the Herbrand
model is a regular set of atoms. Kostyukov et al. have shown how to derive an
automaton when CVC4 finds a finite model of the CHCs. We propose an alternative
way to build the automaton using an encoding into a SAT problem using Clingo,
an Answer Set Programming (ASP) tool. We implemented a translation of CHCs with
ADTs into an ASP problem. Combined with Clingo, we obtain a semi-complete
satisfiability checker: it finds a tree automaton if a regular Herbrand model
exists or finds a counter-example if the problem is unsatisfiable.

</details>


### [25] [Semantic Properties of Computations Defined by Elementary Inference Systems](https://arxiv.org/abs/2510.26429)
*Salvador Lucas*

Main category: cs.LO

TL;DR: 该论文提出了使用基本推理系统定义集合/关系/计算的方法，通过一阶理论Th(I)和规范模型来分析语义属性，并展示了如何通过任意模型来证明或反驳这些属性。


<details>
  <summary>Details</summary>
Motivation: 研究如何分析由基本推理系统定义的编程语言和系统的属性，特别是那些可以通过重写系统描述的计算。

Method: 使用斯穆利安的基本形式系统构建基本推理系统I，定义一阶理论Th(I)，通过规范模型或任意模型来验证语义属性F的满足性。

Result: 展示了如何通过任意模型A来证明或反驳语义属性F，即使规范模型M是不可计算的。

Conclusion: 该方法适用于分析编程语言和系统的属性，特别是重写系统，提供了一种处理不可计算规范模型的实用方法。

Abstract: We consider sets/relations/computations defined by *Elementary Inference
Systems* I, which are obtained from Smullyan's *elementary formal systems*
using Gentzen's notation for inference rules, and proof trees for atoms
P(t_1,...,t_n), where predicate P represents the considered
set/relation/computation. A first-order theory Th(I), actually a set of
definite Horn clauses, is given to I. Properties of objects defined by I are
expressed as first-order sentences F, which are proved true or false by
*satisfaction* M |= F of F in a *canonical* model M of Th(I). For this reason,
we call F a *semantic property* of I. Since canonical models are, in general,
incomputable, we show how to (dis)prove semantic properties by satisfiability
in an *arbitrary* model A of Th(I). We apply these ideas to the analysis of
properties of programming languages and systems whose computations can be
described by means of an elementary inference system. In particular,
rewriting-based systems.

</details>


### [26] [Theta as a Horn Solver](https://arxiv.org/abs/2510.26430)
*Levente Bajczi,Milán Mondok,Vince Molnár*

Main category: cs.LO

TL;DR: Theta是一个CHC验证框架，本文详细描述了其算法特点、在CHC-COMP竞赛中的表现，并纠正了配置问题后的重新评估结果。


<details>
  <summary>Details</summary>
Motivation: 填补Theta验证框架在CHC求解方面的技术细节、设计权衡和局限性方面的研究空白，特别是在CHC-COMP竞赛背景下的表现分析。

Method: 将约束Horn子句转换为控制流自动机进行分析，详细描述Theta采用的算法及其区别于其他CHC求解器的独特特性。

Result: 在2025年CHC-COMP竞赛中，Theta因配置问题表现不佳；重新在修正设置下执行竞赛基准测试，展示了工具的实际能力。

Conclusion: Theta在正确配置下具有实际验证能力，本文提供了对其算法特点和性能的全面分析，有助于理解该工具在CHC求解领域的定位。

Abstract: Theta is a verification framework that has participated in the CHC-COMP
competition since 2023. While its core approach -- based on transforming
constrained Horn clauses (CHCs) into control-flow automata (CFAs) for analysis
-- has remained mostly unchanged, Theta's verification techniques, design
trade-offs, and limitations have remained mostly unexplored in the context of
CHCs. This paper fills that gap: we provide a detailed description of the
algorithms employed by Theta, highlighting the unique features that distinguish
it from other CHC solvers. We also analyze the strengths and weaknesses of the
tool in the context of CHC-COMP benchmarks. Notably, in the 2025 edition of the
competition, Theta's performance was impacted by a configuration issue, leading
to suboptimal results. To provide a clearer picture of Theta's actual
capabilities, we re-execute the tool on the competition benchmarks under
corrected settings and report on the resulting performance.

</details>


### [27] [Bridge and Bound: A Logic-Based Framework for Abstracting (Preliminary Report)](https://arxiv.org/abs/2510.26654)
*Andrzej Szalas*

Main category: cs.LO

TL;DR: 本文提出了一个新的基于逻辑的抽象建模框架，超越了传统的必要条件蕴含，同时涵盖充分条件，定义了近似抽象并研究了其最紧形式和精确形式。


<details>
  <summary>Details</summary>
Motivation: 抽象是减少复杂性同时保留关键属性的基本过程，但在现实世界建模和推理中处理不完美或不完整信息时，传统方法存在局限，需要更全面的抽象框架。

Method: 在经典逻辑中开发了一个逻辑框架，定义近似抽象、研究其最紧形式和精确形式，并扩展到分层抽象，实现复杂系统的层次简化。

Result: 提出了一个超越传统必要条件蕴含的抽象建模框架，能够处理充分条件，支持分层抽象和复杂系统简化。

Conclusion: 该逻辑框架为抽象过程提供了更全面的建模方法，特别适用于处理不完整信息和近似表示的场景，在复杂系统建模中具有重要价值。

Abstract: At its core, abstraction is the process of generalizing from specific
instances to broader concepts or models, with the primary objective of reducing
complexity while preserving properties essential to the intended purpose. It is
a fundamental, often implicit, principle that structures the understanding,
communication, and development of both scientific knowledge and everyday
beliefs. Studies on abstraction have evolved from its origins in Ancient Greek
philosophy through methodological approaches in psychological and philosophical
theories to computational frameworks.
  Formally, abstraction can be understood as the transformation of a source
representation into an abstract representation that discards certain details
while retaining desirable features. In real-world modeling and reasoning,
abstraction is crucial, particularly when managing imperfect or incomplete
information that calls for approximate representations. This paper introduces a
novel logic-based framework for modeling abstraction processes that goes beyond
the traditional entailment of necessary conditions to encompass sufficient
conditions as well. We define approximate abstractions, study their tightest
and exact forms, and extend the approach to layered abstractions, enabling
hierarchical simplification of complex systems and models. The computational
complexity of the related reasoning tasks is also discussed.
  For clarity, our framework is developed within classical logic, chosen for
its simplicity, expressiveness, and computational friendliness.

</details>
