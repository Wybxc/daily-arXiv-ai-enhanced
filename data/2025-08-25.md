<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 17]
- [cs.FL](#cs.FL) [Total: 2]
- [cs.LO](#cs.LO) [Total: 5]
- [cs.PL](#cs.PL) [Total: 5]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [A Systematic Literature Review of Machine Learning Approaches for Migrating Monolithic Systems to Microservices](https://arxiv.org/abs/2508.15941)
*Imen Trabelsi,Brahim Mahmoudi,Jean Baptiste Minani,Naouel Moha,Yann-Gaël Guéhéneuc*

Main category: cs.SE

TL;DR: 本文通过系统文献综述分析了81项关于使用机器学习技术将单体系统迁移到微服务的研究，总结了自动化迁移阶段、输入数据、ML技术应用、评估方法和面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 单体系统面临可扩展性和可维护性挑战，微服务架构成为解决方案，但迁移过程复杂且资源密集，需要机器学习技术来自动化部分迁移阶段。目前缺乏对现有ML迁移方法的系统研究。

Method: 采用系统文献综述(SLR)方法，遵循PRISMA指南，分析了2015-2024年间发表的81项主要研究，提取和分析数据以回答研究问题。

Result: 研究发现监控和服务识别等迁移阶段研究较为充分，而打包微服务等阶段尚未探索。主要挑战包括数据可用性有限、可扩展性和复杂性约束、工具支持不足以及缺乏标准化基准测试。

Conclusion: 需要更全面的解决方案来解决迁移过程中的关键挑战，包括改善数据可用性、开发更好的工具支持以及建立标准化基准测试框架。

Abstract: Scalability and maintainability challenges in monolithic systems have led to
the adoption of microservices, which divide systems into smaller, independent
services. However, migrating existing monolithic systems to microservices is a
complex and resource-intensive task, which can benefit from machine learning
(ML) to automate some of its phases. Choosing the right ML approach for
migration remains challenging for practitioners. Previous works studied
separately the objectives, artifacts, techniques, tools, and benefits and
challenges of migrating monolithic systems to microservices. No work has yet
investigated systematically existing ML approaches for this migration to
understand the \revised{automated migration phases}, inputs used, ML techniques
applied, evaluation processes followed, and challenges encountered. We present
a systematic literature review (SLR) that aggregates, synthesises, and
discusses the approaches and results of 81 primary studies (PSs) published
between 2015 and 2024. We followed the Preferred Reporting Items for Systematic
Review and Meta-Analysis (PRISMA) statement to report our findings and answer
our research questions (RQs). We extract and analyse data from these PSs to
answer our RQs. We synthesise the findings in the form of a classification that
shows the usage of ML techniques in migrating monolithic systems to
microservices. The findings reveal that some phases of the migration process,
such as monitoring and service identification, are well-studied, while others,
like packaging microservices, remain unexplored. Additionally, the findings
highlight key challenges, including limited data availability, scalability and
complexity constraints, insufficient tool support, and the absence of
standardized benchmarking, emphasizing the need for more holistic solutions.

</details>


### [2] [Breaking Barriers in Software Testing: The Power of AI-Driven Automation](https://arxiv.org/abs/2508.16025)
*Saba Naqvi,Mohammad Baqar*

Main category: cs.SE

TL;DR: AI驱动的测试框架，使用NLP、强化学习和预测模型自动生成和验证测试用例，提高测试效率和可靠性


<details>
  <summary>Details</summary>
Motivation: 传统软件测试方法速度慢、成本高且覆盖率不足，需要更智能的自动化解决方案来应对日益复杂的软件环境

Method: 结合自然语言处理、强化学习和预测模型，将自然语言需求转换为可执行测试，通过持续学习和实时分析优化测试过程，并采用策略驱动的信任和公平性模型减轻偏差

Result: 案例研究表明在缺陷检测、测试工作量减少和发布周期加速方面取得了可衡量的改进

Conclusion: AI增强的测试框架能够将测试从被动的手动过程转变为主动的自适应系统，在复杂环境中显著提升软件质量

Abstract: Software testing remains critical for ensuring reliability, yet traditional
approaches are slow, costly, and prone to gaps in coverage. This paper presents
an AI-driven framework that automates test case generation and validation using
natural language processing (NLP), reinforcement learning (RL), and predictive
models, embedded within a policy-driven trust and fairness model. The approach
translates natural language requirements into executable tests, continuously
optimizes them through learning, and validates outcomes with real-time analysis
while mitigating bias. Case studies demonstrate measurable gains in defect
detection, reduced testing effort, and faster release cycles, showing that
AI-enhanced testing improves both efficiency and reliability. By addressing
integration and scalability challenges, the framework illustrates how AI can
shift testing from a reactive, manual process to a proactive, adaptive system
that strengthens software quality in increasingly complex environments.

</details>


### [3] [Measuring the effectiveness of code review comments in GitHub repositories: A machine learning approach](https://arxiv.org/abs/2508.16053)
*Shadikur Rahman,Umme Ayman Koana,Hasibul Karim Shanto,Mahmuda Akter,Chitra Roy,Aras M. Ismael*

Main category: cs.SE

TL;DR: 本文通过实证研究比较了7种机器学习算法在代码审查文本情感分类中的效果，发现线性支持向量机分类器(SVC)准确率最高。


<details>
  <summary>Details</summary>
Motivation: 程序员需要了解代码审查评论的情感极性以避免错误，但目前缺乏对机器学习技术在此任务上效率的系统研究。

Method: 从GitHub三个开源项目中提取13,557条代码审查评论并手动标注，使用7种机器学习算法进行情感极性分类比较。

Result: 线性支持向量机分类器(SVC)在所有测试算法中取得了最高的分类准确率。

Conclusion: 该研究为基于代码评论构建解决方案提供了有效方法，有助于避免误解和提高开发效率。

Abstract: This paper illustrates an empirical study of the working efficiency of
machine learning techniques in classifying code review text by semantic
meaning. The code review comments from the source control repository in GitHub
were extracted for development activity from the existing year for three
open-source projects. Apart from that, programmers need to be aware of their
code and point out their errors. In that case, it is a must to classify the
sentiment polarity of the code review comments to avoid an error. We manually
labelled 13557 code review comments generated by three open source projects in
GitHub during the existing year. In order to recognize the sentiment polarity
(or sentiment orientation) of code reviews, we use seven machine learning
algorithms and compare those results to find the better ones. Among those
Linear Support Vector Classifier(SVC) classifier technique achieves higher
accuracy than others. This study will help programmers to make any solution
based on code reviews by avoiding misconceptions.

</details>


### [4] [From Benchmark Data To Applicable Program Repair: An Experience Report](https://arxiv.org/abs/2508.16071)
*Mahinthan Chandramohan,Jovan Jancic,Yuntong Zhang,Padmanabhan Krishnan*

Main category: cs.SE

TL;DR: 论文结合多种技术实现自动程序修复，在标准基准测试中表现优于其他方法，但无法处理工业环境中的真实缺陷。研究发现形式化规范能提升LLM生成单元测试的质量，但对常见错误帮助有限。当前挑战包括规范语言表达能力不足和测试通过不能保证补丁正确性。


<details>
  <summary>Details</summary>
Motivation: 解决自动程序修复技术在学术基准测试表现良好但无法处理工业环境中真实缺陷的问题，探索如何通过形式化规范等技术提升修复质量。

Method: 结合文献中的多种技术，使用形式化规范（JML）增强代码，利用LLM生成更高质量的单元测试，并探索契约自动机、示例编程和测试用例修复等方法。

Result: 在标准基准测试中表现优于其他技术，形式化规范能提升复杂生产代码的测试覆盖率和异常处理，但对空指针、越界等常见错误帮助有限。

Conclusion: 学术基准测试与工业实际需求存在差距，当前技术无法保证补丁正确性，需要更强大的验证工具和丰富谓词，未来工作将整合人工反馈并关注生产力提升。

Abstract: This paper describes our approach to automated program repair. We combine
various techniques from the literature to achieve this. Our experiments show
that our approach performs better than other techniques on standard benchmarks.
However, on closer inspection, none of these techniques work on realistic
defects that we see in industry.
  We find that augmenting code with formal specifications enables LLMs to
generate higher-quality unit tests, especially for complex production code with
improved coverage of edge cases and exception handling. However, specifications
add little value for well-understood errors (e.g., null pointer, index out of
bounds), but are beneficial for logic and string manipulation errors. Despite
encouraging benchmark results, real-world adoption is limited since passing
tests do not guarantee correct patches. Current challenges include insufficient
expressiveness of the JML specification language, necessitating advanced
verification tools and richer predicates. Our ongoing work is exploring
contract automata, programming by example, and testcase repair, with a focus on
integrating human feedback and measuring productivity gains - highlighting the
gap between academic benchmarks and practical industry needs

</details>


### [5] [Validating Terrain Models in Digital Twins for Trustworthy sUAS Operations](https://arxiv.org/abs/2508.16104)
*Arturo Miguel Russell Bernal,Maureen Petterson,Pedro Antonio Alarcon Granadeno,Michael Murphy,James Mason,Jane Cleland-Huang*

Main category: cs.SE

TL;DR: 本文提出了一种基于软件工程原则的三维验证流程，用于验证环境数字孪生中的地形模型，以支持小型无人机系统在复杂环境中的安全飞行和任务执行。


<details>
  <summary>Details</summary>
Motivation: 随着小型无人机系统在陌生复杂环境中的部署增加，准确的环境数字孪生（包括天气、空域和地形数据）对于安全飞行规划和搜索监视操作至关重要。但实际部署中存在显著的不确定性，需要强大的验证过程。

Method: 提出基于软件工程原则的三维验证流程，涵盖测试粒度、从仿真到真实世界的流程以及简单到边缘条件的分析。使用配备地形感知数字阴影的多无人机平台进行验证。

Result: 通过融合美国地质调查局数据集和卫星影像构建高分辨率地形模型，支持任务执行。验证过程解决了数据粒度有限、地形不连续性、GPS和传感器误差等挑战。

Conclusion: 该方法为环境数字孪生地形模型的验证提供了系统化的解决方案，有助于提高小型无人机系统在复杂环境中的操作安全性和可靠性。

Abstract: With the increasing deployment of small Unmanned Aircraft Systems (sUAS) in
unfamiliar and complex environments, Environmental Digital Twins (EDT) that
comprise weather, airspace, and terrain data are critical for safe flight
planning and for maintaining appropriate altitudes during search and
surveillance operations. With the expansion of sUAS capabilities through edge
and cloud computing, accurate EDT are also vital for advanced sUAS
capabilities, like geolocation. However, real-world sUAS deployment introduces
significant sources of uncertainty, necessitating a robust validation process
for EDT components. This paper focuses on the validation of terrain models, one
of the key components of an EDT, for real-world sUAS tasks. These models are
constructed by fusing U.S. Geological Survey (USGS) datasets and satellite
imagery, incorporating high-resolution environmental data to support mission
tasks. Validating both the terrain models and their operational use by sUAS
under real-world conditions presents significant challenges, including limited
data granularity, terrain discontinuities, GPS and sensor inaccuracies, visual
detection uncertainties, as well as onboard resources and timing constraints.
We propose a 3-Dimensions validation process grounded in software engineering
principles, following a workflow across granularity of tests, simulation to
real world, and the analysis of simple to edge conditions. We demonstrate our
approach using a multi-sUAS platform equipped with a Terrain-Aware Digital
Shadow.

</details>


### [6] [ARSP: Automated Repair of Verilog Designs via Semantic Partitioning](https://arxiv.org/abs/2508.16517)
*Bingkun Yao,Ning Wang,Xiangfeng Liu,Yuxin Du,Yuchen Hu,Hong Gao,Zhe Jiang,Nan Guan*

Main category: cs.SE

TL;DR: ARSP是一个两阶段系统，通过语义引导的分段方法解决Verilog调试中bug信号稀释问题，在工业规模模块上显著优于主流商业LLM和现有调试工具。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自动化调试方法在工业规模模块上表现不佳，主要原因是长上下文中bug信号被大量无关代码稀释，分散了模型的注意力。

Method: 两阶段系统：分区LLM将模块分割成语义紧密的片段；修复LLM修补每个片段；编辑合并时不改变无关逻辑。使用合成数据框架生成片段级训练对来监督两个模型。

Result: ARSP达到77.92% pass@1和83.88% pass@5，优于Claude-3.7、Strider和MEIC。语义分区相比整模块调试将pass@1提高11.6%，pass@5提高10.2%。

Conclusion: 片段级范围缩减在基于LLM的Verilog调试中非常有效，语义分区方法成功解决了bug信号稀释问题，显著提升了调试性能。

Abstract: Debugging functional Verilog bugs consumes a significant portion of front-end
design time. While Large Language Models (LLMs) have demonstrated great
potential in mitigating this effort, existing LLM-based automated debugging
methods underperform on industrial-scale modules. A major reason for this is
bug signal dilution in long contexts, where a few bug-relevant tokens are
overwhelmed by hundreds of unrelated lines, diffusing the model's attention. To
address this issue, we introduce ARSP, a two-stage system that mitigates
dilution via semantics-guided fragmentation. A Partition LLM splits a module
into semantically tight fragments; a Repair LLM patches each fragment; edits
are merged without altering unrelated logic. A synthetic data framework
generates fragment-level training pairs spanning bug types, design styles, and
scales to supervise both models. Experiments show that ARSP achieves 77.92%
pass@1 and 83.88% pass@5, outperforming mainstream commercial LLMs including
Claude-3.7 and SOTA automated Verilog debugging tools Strider and MEIC. Also,
semantic partitioning improves pass@1 by 11.6% and pass@5 by 10.2% over
whole-module debugging, validating the effectiveness of fragment-level scope
reduction in LLM-based Verilog debugging.

</details>


### [7] [The Fools are Certain; the Wise are Doubtful: Exploring LLM Confidence in Code Completion](https://arxiv.org/abs/2508.16131)
*Zoe Kotti,Konstantina Dritsa,Diomidis Spinellis,Panos Louridas*

Main category: cs.SE

TL;DR: 本文通过测量代码困惑度来评估LLM在代码生成时的置信度，发现强类型语言比动态类型语言困惑度更低，不同编程语言和模型选择对困惑度有显著影响。


<details>
  <summary>Details</summary>
Motivation: 下游评估指标复杂且不可靠，而内在指标如困惑度可以简单、通用地衡量LLM置信度，作为代码功能正确性和幻觉风险的代理指标。

Method: 使用多种LLM模型，对来自657个GitHub项目的1008个文件样本，跨编程语言、模型和数据集测量代码困惑度。

Result: 强类型语言困惑度低于动态类型语言；脚本语言困惑度较高；Perl困惑度普遍较高，Java较低；困惑度取决于LLM模型而非代码数据集；代码注释通常增加困惑度但不影响语言排名。

Conclusion: 研究结果可帮助LLM研究者、开发者和用户基于语言特性、模型选择和代码特征来评估LLM代码补全在特定软件项目中的适用性和收益。

Abstract: Code completion entails the task of providing missing tokens given a
surrounding context. It can boost developer productivity while providing a
powerful code discovery tool. Following the Large Language Model (LLM) wave,
code completion has been approached with diverse LLMs fine-tuned on code (code
LLMs). The performance of code LLMs can be assessed with downstream and
intrinsic metrics. Downstream metrics are usually employed to evaluate the
practical utility of a model, but can be unreliable and require complex
calculations and domain-specific knowledge. In contrast, intrinsic metrics such
as perplexity, entropy, and mutual information, which measure model confidence
or uncertainty, are simple, versatile, and universal across LLMs and tasks, and
can serve as proxies for functional correctness and hallucination risk in
LLM-generated code. Motivated by this, we evaluate the confidence of LLMs when
generating code by measuring code perplexity across programming languages,
models, and datasets using various LLMs, and a sample of 1008 files from 657
GitHub projects. We find that strongly-typed languages exhibit lower perplexity
than dynamically typed languages. Scripting languages also demonstrate higher
perplexity. Perl appears universally high in perplexity, whereas Java appears
low. Code perplexity depends on the employed LLM, but not on the code dataset.
Although code comments often increase perplexity, the language ranking based on
perplexity is barely affected by their presence. LLM researchers, developers,
and users can employ our findings to assess the benefits and suitability of
LLM-based code completion in specific software projects based on how language,
model choice, and code characteristics impact model confidence.

</details>


### [8] [Towards Recommending Usability Improvements with Multimodal Large Language Models](https://arxiv.org/abs/2508.16165)
*Sebastian Lubos,Alexander Felfernig,Gerhard Leitner,Julian Schwazer*

Main category: cs.SE

TL;DR: 多模态LLM在可用性评估中的应用研究，通过将可用性问题按严重程度排序，实现部分自动化评估流程


<details>
  <summary>Details</summary>
Motivation: 传统可用性评估方法（如可用性测试和检查）虽然有效但资源密集且需要专家参与，小型组织难以承担。多模态LLM的发展为自动化可用性评估提供了新机会

Method: 将可用性评估构建为推荐任务，多模态LLM通过分析软件界面的文本、视觉和结构方面，对可用性问题按严重程度进行排序。进行了概念验证研究，比较LLM生成的改进建议与专家评估

Result: 研究发现LLM能够实现更快、更经济高效的可用性评估，在专家资源有限的情况下可作为实用替代方案

Conclusion: 多模态LLM在可用性评估方面具有巨大潜力，能够降低评估成本和提高可及性，特别适合资源有限的组织环境

Abstract: Usability describes a set of essential quality attributes of user interfaces
(UI) that influence human-computer interaction. Common evaluation methods, such
as usability testing and inspection, are effective but resource-intensive and
require expert involvement. This makes them less accessible for smaller
organizations. Recent advances in multimodal LLMs offer promising opportunities
to automate usability evaluation processes partly by analyzing textual, visual,
and structural aspects of software interfaces. To investigate this possibility,
we formulate usability evaluation as a recommendation task, where multimodal
LLMs rank usability issues by severity. We conducted an initial
proof-of-concept study to compare LLM-generated usability improvement
recommendations with usability expert assessments. Our findings indicate the
potential of LLMs to enable faster and more cost-effective usability
evaluation, which makes it a practical alternative in contexts with limited
expert resources.

</details>


### [9] [LLM-Assisted Semantic Alignment and Integration in Collaborative Model-Based Systems Engineering Using SysML v2](https://arxiv.org/abs/2508.16181)
*Zirui Li,Stephan Husung,Haoze Wang*

Main category: cs.SE

TL;DR: 基于GPT的大语言模型辅助SysML v2模型语义对齐的结构化提示驱动方法


<details>
  <summary>Details</summary>
Motivation: 解决模型基于系统工程中跨组织协作时独立开发系统模型的语义对齐挑战

Method: 迭代开发包括模型提取、语义匹配和验证的对齐方法和交互提示，利用SysML v2的别名、导入和元数据扩展构造

Result: 通过测量系统示例进行了实验展示，讨论了方法的优势和局限性

Conclusion: 结构化的LLM辅助语义对齐方法为跨组织MBSE协作提供了可追踪的软对齐集成解决方案

Abstract: Cross-organizational collaboration in Model-Based Systems Engineering (MBSE)
faces many challenges in achieving semantic alignment across independently
developed system models. SysML v2 introduces enhanced structural modularity and
formal semantics, offering a stronger foundation for interoperable modeling.
Meanwhile, GPT-based Large Language Models (LLMs) provide new capabilities for
assisting model understanding and integration. This paper proposes a
structured, prompt-driven approach for LLM-assisted semantic alignment of SysML
v2 models. The core contribution lies in the iterative development of an
alignment approach and interaction prompts, incorporating model extraction,
semantic matching, and verification. The approach leverages SysML v2 constructs
such as alias, import, and metadata extensions to support traceable, soft
alignment integration. It is demonstrated with a GPT-based LLM through an
example of a measurement system. Benefits and limitations are discussed.

</details>


### [10] [A Systematic Mapping Study on Smart Cities Modeling Approaches](https://arxiv.org/abs/2508.16273)
*Maria Teresa Rossi,Martina De Sanctis,Ludovico Iovino,Manuel Wimmer*

Main category: cs.SE

TL;DR: 这是一个关于智慧城市建模方法的系统性地图研究，分析了现有研究趋势和建模方法，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 智慧城市是一个复杂的多学科领域，需要了解各种建模方法在不同应用领域的使用情况，以获得研究概览、识别出版趋势和未来研究方向。

Method: 采用Petersen等人的系统性地图研究指南，对智慧城市建模方法的文献进行系统分析。

Result: 主要发现：(1)智慧治理是最受关注的建模维度；(2)最常用的建模方法是业务、架构和本体论建模；(3)大部分技术尚未在实际环境中得到验证；(4)不同研究社群在多种发表场合分享成果。

Conclusion: 研究结果为理解智慧城市建模的现状提供了基础，并对模型驱动工程社区有重要意义。

Abstract: The Smart City concept was introduced to define an idealized city
characterized by automation and connection. It then evolved rapidly by
including further aspects, such as economy, environment. Since then, many
publications have explored various aspects of Smart Cities across different
application domains and research communities, acknowledging the
interdisciplinary nature of this subject. In particular, our interest focuses
on how smart cities are designed and modeled, as a whole or as regards with
their subsystems, when dealing with the accomplishment of the research goals in
this complex and heterogeneous domain. To this aim, we performed a systematic
mapping study on smart cities modeling approaches identifying the relevant
contributions (i) to get an overview of existing research approaches, (ii) to
identify whether there are any publication trends, and (iii) to identify
possible future research directions. We followed the guidelines for conducting
systematic mapping studies by Petersen et al. to analyze smart cities modeling
publications. Our analysis revealed the following main findings: (i) smart
governance is the most investigated and modeled smart city dimension; (ii) the
most used modeling approaches are business, architectural, and ontological
modeling approaches, spanning multiple application fields; (iii) the great
majority of existing technologies for modeling smart cities are not yet proven
in operational environments; (iv) diverse research communities publish their
results in a multitude of different venues which further motivates the
presented literature study. Researchers can use our results for better
understanding the state-of-the-art in modeling smart cities, and as a
foundation for further analysis of specific approaches about smart cities
modeling. Lastly, we also discuss the impact of our analysis for the
Model-Driven Engineering community.

</details>


### [11] [Metamorphic Coverage](https://arxiv.org/abs/2508.16307)
*Jinsheng Ba,Yuancheng Jiang,Manuel Rigger*

Main category: cs.SE

TL;DR: 提出了Metamorphic Coverage (MC)这一新的覆盖率指标，用于评估蜕变测试方法的有效性，相比传统代码覆盖率和变异测试具有更好的敏感性和效率


<details>
  <summary>Details</summary>
Motivation: 传统代码覆盖率无法准确衡量代码验证程度，而变异测试计算成本过高，需要一种新的评估蜕变测试方法有效性的指标

Method: 定义并系统评估Metamorphic Coverage (MC)，该指标检查蜕变测试中测试输入对所执行的差异代码的覆盖情况，基于代码在测试输入对中差异执行可能暴露bug的直觉

Result: MC与64个bug中的50个bug修复位置重叠，比行覆盖率与bug数量有更强的正相关性，敏感性是行覆盖率的4倍，平均值为行覆盖率的1/6，计算时间比变异测试少359倍，在数据库系统测试中比代码覆盖率多发现41%的bug

Conclusion: MC在评估蜕变测试方法和改进测试用例生成方面具有广泛应用前景，能够更有效地指导测试并发现更多bug

Abstract: Metamorphic testing is a widely used methodology that examines an expected
relation between pairs of executions to automatically find bugs, such as
correctness bugs. We found that code coverage cannot accurately measure the
extent to which code is validated and mutation testing is computationally
expensive for evaluating metamorphic testing methods. In this work, we propose
Metamorphic Coverage (MC), a coverage metric that examines the distinct code
executed by pairs of test inputs within metamorphic testing. Our intuition is
that, typically, a bug can be observed if the corresponding code is executed
when executing either test input but not the other one, so covering more
differential code covered by pairs of test inputs might be more likely to
expose bugs. While most metamorphic testing methods have been based on this
general intuition, our work defines and systematically evaluates MC on five
widely used metamorphic testing methods for testing database engines,
compilers, and constraint solvers. The code measured by MC overlaps with the
bug-fix locations of 50 of 64 bugs found by metamorphic testing methods, and MC
has a stronger positive correlation with bug numbers than line coverage. MC is
4x more sensitive than line coverage in distinguishing testing methods'
effectiveness, and the average value of MC is 6x smaller than line coverage
while still capturing the part of the program that is being tested. MC required
359x less time than mutation testing. Based on a case study for an automated
database system testing approach, we demonstrate that when used for feedback
guidance, MC significantly outperforms code coverage, by finding 41\% more
bugs. Consequently, this work might have broad applications for assessing
metamorphic testing methods and improving test-case generation.

</details>


### [12] [SATORI: Static Test Oracle Generation for REST APIs](https://arxiv.org/abs/2508.16318)
*Juan C. Alonso,Alberto Martin-Lopez,Sergio Segura,Gabriele Bavota,Antonio Ruiz-Cortés*

Main category: cs.SE

TL;DR: SATORI是一种基于静态分析的REST API测试预言生成方法，使用大语言模型分析OpenAPI规范来自动推断API的预期行为，能够生成大量有效的测试预言，并在工业API评估中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的REST API测试生成工具在测试数据生成方面很强，但支持的测试预言类型有限，通常仅限于崩溃、回归和API规范合规性检查。需要一种能够自动生成更丰富测试预言的方法。

Method: SATORI采用黑盒方法，通过分析OpenAPI规范，使用大语言模型来推断API操作的响应字段属性（如名称和描述），从而生成测试预言。该方法还扩展了PostmanAssertify工具来自动将生成的预言转换为可执行断言。

Result: 在12个工业API的17个操作上评估，SATORI能够为每个操作自动生成多达数百个有效的测试预言，F1分数达到74.3%，优于需要执行API的动态方法AGORA+（69.3%）。两种方法结合使用可以找到90%的标注真实数据集中的预言。

Conclusion: SATORI成功证明了静态方法在测试预言生成方面的有效性，发现了18个流行API中的bug并促使文档更新。静态和动态预言推断方法是互补的，结合使用可以获得最佳效果。

Abstract: REST API test case generation tools are evolving rapidly, with growing
capabilities for the automated generation of complex tests. However, despite
their strengths in test data generation, these tools are constrained by the
types of test oracles they support, often limited to crashes, regressions, and
noncompliance with API specifications or design standards. This paper
introduces SATORI (Static API Test ORacle Inference), a black-box approach for
generating test oracles for REST APIs by analyzing their OpenAPI Specification.
SATORI uses large language models to infer the expected behavior of an API by
analyzing the properties of the response fields of its operations, such as
their name and descriptions. To foster its adoption, we extended the
PostmanAssertify tool to automatically convert the test oracles reported by
SATORI into executable assertions. Evaluation results on 17 operations from 12
industrial APIs show that SATORI can automatically generate up to hundreds of
valid test oracles per operation. SATORI achieved an F1-score of 74.3%,
outperforming the state-of-the-art dynamic approach AGORA+ (69.3%)-which
requires executing the API-when generating comparable oracle types. Moreover,
our findings show that static and dynamic oracle inference methods are
complementary: together, SATORI and AGORA+ found 90% of the oracles in our
annotated ground-truth dataset. Notably, SATORI uncovered 18 bugs in popular
APIs (Amadeus Hotel, Deutschebahn, FDIC, GitLab, Marvel, OMDb and Vimeo)
leading to documentation updates by the API maintainers.

</details>


### [13] [The (C)omprehensive (A)rchitecture (P)attern (I)ntegration method: Navigating the sea of technology](https://arxiv.org/abs/2508.16341)
*Sebastian Copei,Oliver Hohlfeld,Jens Kosiol*

Main category: cs.SE

TL;DR: CAPI方法通过诊断决策树推荐架构模式，降低技术选型复杂度，有效帮助软件项目选择合适的技术架构


<details>
  <summary>Details</summary>
Motivation: 技术日新月异，单个开发者难以了解所有趋势和工具，使得工具选择和架构设计决策变得复杂，尤其对于大型软件系统

Method: 提出CAPI（全面架构模式集成）方法，使用诊断决策树根据用户需求推荐架构模式，通过模式抽象减少决策复杂度

Result: 研究发现技术选择主要通过试错进行，CAPI被一致认为有帮助，能够复现参与者的生产架构环境

Conclusion: CAPI方法通过模式推荐有效简化技术选型过程，为软件架构决策提供了系统化的解决方案

Abstract: The technological landscape changes daily, making it nearly impossible for a
single person to be aware of all trends or available tools that may or may not
be suitable for their software project. This makes tool selection and
architectural design decisions a complex problem, especially for large-scale
software systems. To tackle this issue, we introduce CAPI, the Comprehensive
Architecture Pattern Integration method that uses a diagnostic decision tree to
suggest architectural patterns depending on user needs. By suggesting patterns
instead of tools, the overall complexity for further decisions is lower as
there are fewer architectural patterns than tools due to the abstract nature of
patterns. Moreover, since tools implement patterns, each non-proposed pattern
reduces the number of tools to choose from, reducing complexity. We iteratively
developed CAPI, evaluating its understandability and usability in small studies
with academic participants. When satisfied with the outcome, we performed a
user-study with industry representatives to investigate the state-of-the-art in
technology selection and the effectiveness of our proposed method. We find that
technology selection is largely performed via trial and error, that CAPI is
uniformly perceived as helpful, and that CAPI is able to reproduce the
productive architectural environments of our participants.

</details>


### [14] [AetherCode: Evaluating LLMs' Ability to Win In Premier Programming Competitions](https://arxiv.org/abs/2508.16402)
*Zihan Wang,Jiaze Chen,Zhicheng Liu,Markus Mak,Yidi Du,Geonsik Moon,Luoqi Xu,Aaron Tua,Kunshuo Peng,Jiayi Lu,Mingfei Xia,Boqian Zou,Chenyang Ran,Guang Tian,Shoutai Zhu,Yeheng Duan,Zhenghui Kang,Zhenxing Lin,Shangshu Li,Qiang Luo,Qingshen Long,Zhiyong Chen,Yihan Xiao,Yurong Wu,Daoguang Zan,Yuyi Fu,Mingxuan Wang,Ming Ding*

Main category: cs.SE

TL;DR: AetherCode是一个新的编程基准测试，从IOI和ICPC等顶级编程竞赛中选取问题，提供更广的覆盖范围和更高难度，通过自动生成和人工验证相结合的方式构建全面的测试套件，旨在更准确地评估大型语言模型的编程能力。


<details>
  <summary>Details</summary>
Motivation: 当前编程能力评估基准存在两个关键局限：问题难度和范围不足，以及低质量测试用例导致的评估偏差，这夸大了模型的实际能力，掩盖了LLM与精英人类程序员之间的巨大差距。

Method: 从IOI和ICPC等顶级编程竞赛中选取问题构建AetherCode基准，采用自动生成和人工验证相结合的混合方法构建全面、专家验证的测试套件。

Result: AetherCode提供了更严格可靠的评估标准，能够更真实地衡量LLM的代码推理能力。

Conclusion: AetherCode通过结合具有挑战性的问题设计和稳健的评估，为代码推理领域的未来研究设立了新标准，提供了更准确的LLM能力测量工具。

Abstract: Competitive programming has emerged as a critical benchmark for evaluating
the reasoning and coding capabilities of Large Language Models (LLMs). Despite
impressive progress on existing benchmarks, we argue that current evaluations
overstate model proficiency, masking a substantial gap between LLMs and elite
human programmers. This gap arises from two key limitations: insufficient
difficulty and scope of benchmark problems, and evaluation bias from
low-quality test cases. To address these shortcomings, we present AetherCode, a
new benchmark that draws problems from premier programming competitions such as
IOI and ICPC, offering broader coverage and higher difficulty. AetherCode
further incorporates comprehensive, expert-validated test suites built through
a hybrid of automated generation and human curation, ensuring rigorous and
reliable assessment. By combining challenging problem design with robust
evaluation, AetherCode provides a more faithful measure of LLM capabilities and
sets a new standard for future research in code reasoning.

</details>


### [15] [LLM-GUARD: Large Language Model-Based Detection and Repair of Bugs and Security Vulnerabilities in C++ and Python](https://arxiv.org/abs/2508.16419)
*Akshay Mhatre,Noujoud Nader,Patrick Diehl,Deepti Gupta*

Main category: cs.SE

TL;DR: 本研究系统评估了ChatGPT-4、Claude 3和LLaMA 4在检测软件bug方面的表现，发现它们在简单代码问题上表现优异，但在复杂安全漏洞和大规模生产代码中效果下降。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地应用于软件开发，但其在检测复杂安全漏洞方面的实际效果尚未得到充分探索，需要进行系统性实证评估。

Method: 使用包含基础编程错误、经典安全漏洞和高级生产级bug的基准数据集，采用新颖的多阶段上下文感知提示协议模拟真实调试场景，并通过分级评分标准衡量检测准确性、推理深度和修复质量。

Result: 所有模型在识别语法和语义问题上表现出色，适合教育用途和自动化代码审计的初步审查。但在复杂安全漏洞和大规模生产代码场景中性能下降，ChatGPT-4和Claude 3比LLaMA 4提供更细致的上下文分析。

Conclusion: LLMs作为可靠代码分析工具既有潜力也存在当前限制，需要进一步改进以处理复杂安全漏洞。

Abstract: Large Language Models (LLMs) such as ChatGPT-4, Claude 3, and LLaMA 4 are
increasingly embedded in software/application development, supporting tasks
from code generation to debugging. Yet, their real-world effectiveness in
detecting diverse software bugs, particularly complex, security-relevant
vulnerabilities, remains underexplored. This study presents a systematic,
empirical evaluation of these three leading LLMs using a benchmark of
foundational programming errors, classic security flaws, and advanced,
production-grade bugs in C++ and Python. The dataset integrates real code from
SEED Labs, OpenSSL (via the Suresoft GLaDOS database), and PyBugHive, validated
through local compilation and testing pipelines. A novel multi-stage,
context-aware prompting protocol simulates realistic debugging scenarios, while
a graded rubric measures detection accuracy, reasoning depth, and remediation
quality. Our results show that all models excel at identifying syntactic and
semantic issues in well-scoped code, making them promising for educational use
and as first-pass reviewers in automated code auditing. Performance diminishes
in scenarios involving complex security vulnerabilities and large-scale
production code, with ChatGPT-4 and Claude 3 generally providing more nuanced
contextual analyses than LLaMA 4. This highlights both the promise and the
present constraints of LLMs in serving as reliable code analysis tools.

</details>


### [16] [Using LLMs and Essence to Support Software Practice Adoption](https://arxiv.org/abs/2508.16445)
*Sonia Nicoletti,Paolo Ciancarini*

Main category: cs.SE

TL;DR: 这篇论文探索了将Essence软件工程标准框架与大语言模型集成，通过RAG系统开发专业聊天机器人来支持软件开发最佳实践的应用和过程管理。


<details>
  <summary>Details</summary>
Motivation: 虽然NLP和AI研究已深入研究代码生成任务，但对于支持最佳实践采用、工作方式演进和过程健康监控的自动化工具关注不够，需要填补这一空白。

Method: 开发了一个专业聊天机器人，采用检索增强生成(RAG)系统从经过精选的知识库中检索相关上下文信息。使用四种不同的LLM创建多个聊天机器器配置，并评估基础模型和RAG增强版本。

Result: 通过检索上下文的相关性和生成响应的质量进行评估，对比分析显示该系统在领域特定任务中一贯表现优于基础模型。

Conclusion: 这项工作通过促进结构化软件工程知识的访问，有助于缩小理论框架与实际应用之间的差距，有可能改善过程管理和软件开发实践的采用。LLM基础的自动化在提升软件工程学习和决策方面具有潜力。

Abstract: Recent advancements in natural language processing (NLP) have enabled the
development of automated tools that support various domains, including software
engineering. However, while NLP and artificial intelligence (AI) research has
extensively focused on tasks such as code generation, less attention has been
given to automating support for the adoption of best practices, the evolution
of ways of working, and the monitoring of process health. This study addresses
this gap by exploring the integration of Essence, a standard and thinking
framework for managing software engineering practices, with large language
models (LLMs). To this end, a specialised chatbot was developed to assist
students and professionals in understanding and applying Essence. The chatbot
employs a retrieval-augmented generation (RAG) system to retrieve relevant
contextual information from a curated knowledge base. Four different LLMs were
used to create multiple chatbot configurations, each evaluated both as a base
model and augmented with the RAG system. The system performance was evaluated
through both the relevance of retrieved context and the quality of generated
responses. Comparative analysis against the general-purpose LLMs demonstrated
that the proposed system consistently outperforms its baseline counterpart in
domain-specific tasks. By facilitating access to structured software
engineering knowledge, this work contributes to bridging the gap between
theoretical frameworks and practical application, potentially improving process
management and the adoption of software development practices. While further
validation through user studies is required, these findings highlight the
potential of LLM-based automation to enhance learning and decision-making in
software engineering.

</details>


### [17] [How Small is Enough? Empirical Evidence of Quantized Small Language Models for Automated Program Repair](https://arxiv.org/abs/2508.16499)
*Kazuki Kusama,Honglin Shu,Masanari Kondo,Yasutaka Kamei*

Main category: cs.SE

TL;DR: 小型语言模型(SLMs)在程序自动修复任务中可以达到与大型语言模型(LLMs)相当甚至更好的准确率，同时显著降低计算资源需求，int8量化技术进一步提升了效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然提高了程序自动修复的准确性，但计算资源需求过高，因此研究计算资源需求较低的小型语言模型在程序修复任务中的表现。

Method: 在QuixBugs基准测试上进行实验，比较SLMs和LLMs的错误修复准确率，并分析int8量化对APR性能的影响。

Result: 最新的小型语言模型修复错误的准确率与大型语言模型相当甚至更高，int8量化对APR准确率影响极小但显著降低了内存需求。

Conclusion: 小型语言模型是程序自动修复任务中大型语言模型的可行替代方案，在保持竞争力的准确率同时降低了计算成本，量化技术能进一步提升效率而不影响效果。

Abstract: Background: Large language models (LLMs) have greatly improved the accuracy
of automated program repair (APR) methods. However, LLMs are constrained by
high computational resource requirements. Aims: We focus on small language
models (SLMs), which perform well even with limited computational resources
compared to LLMs. We aim to evaluate whether SLMs can achieve competitive
performance in APR tasks. Method: We conducted experiments on the QuixBugs
benchmark to compare the bug-fixing accuracy of SLMs and LLMs. We also analyzed
the impact of int8 quantization on APR performance. Results: The latest SLMs
can fix bugs as accurately as--or even more accurately than--LLMs. Also, int8
quantization had minimal effect on APR accuracy while significantly reducing
memory requirements. Conclusions: SLMs present a viable alternative to LLMs for
APR, offering competitive accuracy with lower computational costs, and
quantization can further enhance their efficiency without compromising
effectiveness.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [18] [Passive Model Learning of Visibly Deterministic Context-free Grammars](https://arxiv.org/abs/2508.16305)
*Edi Muškardin,Tamim Burgstaller*

Main category: cs.FL

TL;DR: PAPNI是一种被动自动机学习算法，能够从正负样本中学习确定性上下文无关文法，通过扩展RPNI算法并利用可见确定性输入字母表的先验知识来实现。


<details>
  <summary>Details</summary>
Motivation: 现有的RPNI算法只能学习正则语言，无法处理更复杂的确定性上下文无关文法。需要开发能够学习确定性下推自动机的方法，扩展被动学习的能力范围。

Method: 将RPNI算法作为基础，通过预处理步骤扩展其能力，利用输入字母表分解为推入、弹出和不影响堆栈的符号这一先验知识，实现确定性下推自动机的学习。

Result: 在文献中的各种确定性上下文无关文法上进行了评估，与标准RPNI相比，学习模型的预测准确性有所提升。

Conclusion: PAPNI成功地将被动学习扩展到确定性上下文无关文法，为学习更复杂的语言类提供了有效方法，在保持RPNI简洁性的同时增强了表达能力。

Abstract: We present PAPNI, a passive automata learning algorithm capable of learning
deterministic context-free grammars, which are modeled with visibly
deterministic pushdown automata. PAPNI is a generalization of RPNI, a passive
automata learning algorithm capable of learning regular languages from positive
and negative samples. PAPNI uses RPNI as its underlying learning algorithm
while assuming a priori knowledge of the visibly deterministic input alphabet,
that is, the alphabet decomposition into symbols that push to the stack, pop
from the stack, or do not affect the stack.
  In this paper, we show how passive learning of deterministic pushdown
automata can be viewed as a preprocessing step of standard RPNI
implementations. We evaluate the proposed approach on various deterministic
context-free grammars found in the literature and compare the predictive
accuracy of learned models with RPNI.

</details>


### [19] [Automata Learning -- Expect Delays!](https://arxiv.org/abs/2508.16384)
*Gabriel Dengler,Sven Apel,Holger Hermanns*

Main category: cs.FL

TL;DR: 本文研究在存在随机延迟情况下的主动自动机学习，提出了一种将行为学习和延迟采样分离的方法，以避免根状态附近的过度采样问题。


<details>
  <summary>Details</summary>
Motivation: 在随机延迟环境下学习Mealy机时，传统方法会在状态空间根部进行大量重复采样，导致效率低下。需要一种更高效的方法来准确估计具有不同延迟特性的状态转换。

Method: 将行为学习和延迟采样概念上分离，利用学习逻辑行为时获得的信息来生成高效的输入序列，从而收集所需的延迟样本。特别处理相同输入/输出行为可能来自不同延迟特性的情况。

Result: 实证研究表明，该方法在广泛的基准测试中优于朴素基线方法，并在关系数据库连接顺序等实际场景中验证了其适用性。

Conclusion: 提出的分离学习方法有效解决了随机延迟环境下的主动自动机学习问题，显著减少了采样开销，同时保持了学习准确性，具有实际应用价值。

Abstract: This paper studies active automata learning (AAL) in the presence of
stochastic delays. We consider Mealy machines that have stochastic delays
associated with each transition and explore how the learner can efficiently
arrive at faithful estimates of those machines, the precision of which
crucially relies on repetitive sampling of transition delays. While it is
possible to na\"ively integrate the delay sampling into AAL algorithms such as
$L^*$, this leads to considerable oversampling near the root of the state
space. We address this problem by separating conceptually the learning of
behavior and delays such that the learner uses the information gained while
learning the logical behavior to arrive at efficient input sequences for
collecting the needed delay samples. We put emphasis on treating cases in which
identical input/output behaviors might stem from distinct delay
characteristics. Finally, we provide empirical evidence that our method
outperforms the na\"ive baseline across a wide range of benchmarks and
investigate its applicability in a realistic setting by studying the join order
in a relational database.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [20] [Experimental Results for Vampire on the Equational Theories Project](https://arxiv.org/abs/2508.15856)
*Mikoláš Janota*

Main category: cs.LO

TL;DR: Vampire定理证明器在Equational Theories项目中表现出色，能够自动证明所有成立的一阶逻辑蕴含式，并反驳绝大多数不成立的蕴含式。


<details>
  <summary>Details</summary>
Motivation: Equational Theories项目完成后引发了进一步研究，需要探索自动化定理证明器在处理特定一阶逻辑蕴含式方面的能力。

Method: 使用Vampire自动化定理证明器来验证特定类型的一阶逻辑蕴含式的有效性和无效性。

Result: Vampire能够证明所有考虑的成立蕴含式，并且能够反驳绝大多数不成立的蕴含式。

Conclusion: Vampire定理证明器在自动化证明和反驳一阶逻辑蕴含式方面表现出强大的能力，为相关领域的自动化推理提供了有力工具。

Abstract: Equational Theories Project is a collaborative effort, which explores the
validity of certain first-order logic implications of certain kind. The project
has been completed but triggered further research. This report investigates how
much can be automatically proven and disproven by the automated theorem prover
Vampire. An interesting conclusion is that Vampire can prove all the considered
implications that hold and also is able to refute a vast majority of those that
do not hold.

</details>


### [21] [Lean Meets Theoretical Computer Science: Scalable Synthesis of Theorem Proving Challenges in Formal-Informal Pairs](https://arxiv.org/abs/2508.15878)
*Terry Jingchen Zhang,Wenyuan Jiang,Rongchuan Liu,Yisong Wang,Junran Yang,Ning Wang,Nicole Ni,Yinya Huang,Mrinmaya Sachan*

Main category: cs.LO

TL;DR: 利用理论计算机科学自动生成任意多的具有验证式非正式对应关系的定理证明问题，解决传统数据集刻缺问题，通过Busy Beaver和混合布尔算术问题验证了模型在长篇幅证明生成上的显著缺口


<details>
  <summary>Details</summary>
Motivation: 传统形式定理证明数据集因手工编译成本高和具有验证式非正式对应关系的挑战性问题稀缺，限制了大语言模型推理能力的评估和进步

Method: 提出使用理论计算机科学作为可扩展的严格证明问题来源，通过算法定义自动生成任意多的定理-证明对。在Busy Beaver和混合布尔算术两个领域实现框架，自动合成具有并行形式(Lean4)和非形式(Markdown)规范的问题

Result: 在前沿模型上评估显示自动定理证明存在显著缺口：DeepSeekProver-V2-671B在Busy Beaver问题上达到57.5%成功率，但在混合布尔算术问题上仅获12%。这些问题计算验证容易但长篇幅证明生成极其困难

Conclusion: 理论计算机科学领域提供了价值较高的可扩展证明挑战来源，能够推动自动推理研究的进步，尤其在测试模型在计算验证容易但证明生成复杂的问题上的表现

Abstract: Formal theorem proving (FTP) has emerged as a critical foundation for
evaluating the reasoning capabilities of large language models, enabling
automated verification of mathematical proofs at scale. However, progress has
been constrained by limited datasets due to the high cost of manual curation
and the scarcity of challenging problems with verified formal-informal
correspondences. We propose leveraging theoretical computer science (TCS) as a
scalable source of rigorous proof problems, where algorithmic definitions
enable automated generation of arbitrarily many challenging theorem-proof
pairs. We demonstrate this approach on two TCS domains: Busy Beaver problems,
which involve proving bounds on Turing machine halting behavior, and Mixed
Boolean Arithmetic problems, which combine logical and arithmetic reasoning.
Our framework automatically synthesizes problems with parallel formal (Lean4)
and informal (Markdown) specifications, creating a scalable pipeline for
generating verified proof challenges. Evaluation on frontier models reveals
substantial gaps in automated theorem proving: while DeepSeekProver-V2-671B
achieves 57.5\% success on Busy Beaver problems, it manages only 12\% on Mixed
Boolean Arithmetic problems. These results highlight the difficulty of
long-form proof generation even for problems that are computationally easy to
verify, demonstrating the value of TCS domains for advancing automated
reasoning research.

</details>


### [22] [Disjunctions of Two Dependence Atoms](https://arxiv.org/abs/2508.16146)
*Nicolas Fröhlich,Phokion G. Kolaitis,Arne Meier*

Main category: cs.LO

TL;DR: 该论文研究了依赖逻辑中两个一元依赖原子析取式的模型检测问题复杂度，建立了三分法定理，并分类了任意两个依赖原子析取式的复杂度。


<details>
  <summary>Details</summary>
Motivation: 从数据库理论的角度出发，研究依赖逻辑中功能依赖表达式的计算复杂度，特别是两个依赖原子析取式的模型检测问题。

Method: 通过理论分析和复杂度分类，建立了三分法定理，对两个一元依赖原子析取式的模型检测问题进行完全分类，并扩展到任意两个依赖原子的情况。

Result: 证明了对于每个这样的公式，模型检测问题要么是NL完全的，要么是LOGSPACE完全的，要么是一阶可定义的（即在AC[0]中）。同时还识别了一类新的2CNF公式，其可满足性问题是LOGSPACE完全的。

Conclusion: 该研究为依赖逻辑中简单依赖表达式的计算复杂度提供了完整的分类，对数据库理论和计算复杂性理论都有重要意义。

Abstract: Dependence logic is a formalism that augments the syntax of first-order logic
with dependence atoms asserting that the value of a variable is determined by
the values of some other variables, i.e., dependence atoms express functional
dependencies in relational databases. On finite structures, dependence logic
captures NP, hence there are sentences of dependence logic whose model-checking
problem is NP-complete. In fact, it is known that there are disjunctions of
three dependence atoms whose model-checking problem is NP-complete. Motivated
from considerations in database theory, we study the model-checking problem for
disjunctions of two unary dependence atoms and establish a trichotomy theorem,
namely, for every such formula, one of the following is true for the
model-checking problem: (i) it is NL-complete; (ii) it is LOGSPACE-complete;
(iii) it is first-order definable (hence, in AC[0]). Furthermore, we classify
the complexity of the model-checking problem for disjunctions of two arbitrary
dependence atoms, and also characterize when such a disjunction is coherent,
i.e., when it satisfies a certain small-model property. Along the way, we
identify a new class of 2CNF-formulas whose satisfiability problem is
LOGSPACE-complete.

</details>


### [23] [A Reduction of Input/Output Logics to SAT](https://arxiv.org/abs/2508.16242)
*Alexander Steen*

Main category: cs.LO

TL;DR: 本文提出了I/O逻辑的自动化方法，通过将条件规范推理问题转化为命题可满足性问题序列，并开发了原型工具rio进行实现和验证。


<details>
  <summary>Details</summary>
Motivation: I/O逻辑作为基于规范的义务逻辑家族，用于推理规范、义务、许可和禁令，但缺乏有效的自动化推理工具。本文旨在填补这一空白，为I/O逻辑提供实用的自动化推理方法。

Method: 采用将I/O逻辑推理问题归约为命题可满足性问题序列的方法，开发了名为rio的原型推理器来实现所提出的过程。

Result: 成功实现了I/O逻辑的自动化推理，通过rio工具在示例案例上进行了验证，展示了方法的可行性和有效性。

Conclusion: 提出的归约方法和rio工具为I/O逻辑提供了有效的自动化推理解决方案，为规范推理的实际应用奠定了基础。

Abstract: Deontic logics are formalisms for reasoning over norms, obligations,
permissions and prohibitions. Input/Output (I/O) Logics are a particular family
of so-called norm-based deontic logics that formalize conditional norms outside
of the underlying object logic language, where conditional norms do not carry a
truth-value themselves. In this paper, an automation approach for I/O logics is
presented that makes use of suitable reductions to (sequences of) propositional
satisfiability problems. A prototypical implementation, named rio (reasoner for
input/output logics), of the proposed procedures is presented and applied to
illustrative examples.

</details>


### [24] [Uppaal Coshy: Automatic Synthesis of Compact Shields for Hybrid Systems](https://arxiv.org/abs/2508.16345)
*Asger Horn Brorholt,Andreas Holck Høeg-Petersen,Peter Gjøl Jensen,Kim Guldstrand Larsen,Marius Mikučionis,Christian Schilling,Andrzej Wąsowski*

Main category: cs.LO

TL;DR: Uppaal Coshy是一个自动合成安全策略的工具，用于连续状态空间的马尔可夫决策过程，通过状态空间分区和双人安全博弈来近似求解，并采用Caap算法压缩表示。


<details>
  <summary>Details</summary>
Motivation: 针对连续状态空间和复杂混合动态的马尔可夫决策过程，需要自动合成安全策略（shield），但传统方法面临混合系统可达性等算法难题。

Method: 采用状态空间分区方法，通过模拟近似求解双人安全博弈问题，支持随机混合自动机的Uppaal模型，并使用Caap算法生成紧凑的决策树表示。

Result: 实现了完全自动化的工具，能够处理表达力强的Uppaal模型，通过精细网格提高精度，同时通过压缩表示显著减少存储需求。

Conclusion: Uppaal Coshy提供了一种有效的近似方法来合成安全策略，Caap算法成功解决了精细网格存储效率低的问题，为混合系统的安全验证提供了实用工具。

Abstract: We present Uppaal Coshy, a tool for automatic synthesis of a safety strategy
-- or shield -- for Markov decision processes over continuous state spaces and
complex hybrid dynamics. The general methodology is to partition the state
space and then solve a two-player safety game, which entails a number of
algorithmically hard problems such as reachability for hybrid systems. The
general philosophy of Uppaal Coshy is to approximate hard-to-obtain solutions
using simulations. Our implementation is fully automatic and supports the
expressive formalism of Uppaal models, which encompass stochastic hybrid
automata. The precision of our partition-based approach benefits from using
finer grids, which however are not efficient to store. We include an algorithm
called Caap to efficiently compute a compact representation of a shield in the
form of a decision tree, which yields significant reductions.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [25] [Synthesizing DSLs for Few-Shot Learning](https://arxiv.org/abs/2508.16063)
*Paul Krogmeier,P. Madhusudan*

Main category: cs.PL

TL;DR: 本文研究在符号领域中为少样本学习合成领域特定语言(DSL)的问题，证明了在特定条件下该问题的可判定性。


<details>
  <summary>Details</summary>
Motivation: 为了解决符号领域中少样本学习的问题，需要能够自动合成适合特定任务的领域特定语言，以提高学习效率和泛化能力。

Method: 提出基于基础语言和少样本学习问题实例的DSL合成方法，使用树自动机评估语义，并分析语法树深度与表达式大小的对应关系。

Result: 证明了在语义可由树自动机评估且表达式大小对应语法树深度的条件下，DSL合成问题是可判定的，并确定了解决问题的语法对应正则树集。

Conclusion: 该研究为符号领域少样本学习的DSL自动合成提供了理论基础和可判定性保证，对宏语法定义的DSL变体问题也证明了可判定性。

Abstract: We study the problem of synthesizing domain-specific languages (DSLs) for
few-shot learning in symbolic domains. Given a base language and instances of
few-shot learning problems, where each instance is split into training and
testing samples, the DSL synthesis problem asks for a grammar over the base
language that guarantees that small expressions solving training samples also
solve corresponding testing samples. We prove that the problem is decidable for
a class of languages whose semantics over fixed structures can be evaluated by
tree automata and when expression size corresponds to parse tree depth in the
grammar, and, furthermore, the grammars solving the problem correspond to a
regular set of trees. We also prove decidability results for variants of the
problem where DSLs are only required to express solutions for input learning
problems and where DSLs are defined using macro grammars.

</details>


### [26] [Correctness-Guaranteed Code Generation via Constrained Decoding](https://arxiv.org/abs/2508.15866)
*Lingxiao Li,Salar Rahili,Yiwei Zhao*

Main category: cs.PL

TL;DR: 通过上下文敏感解析器和约束解码算法，生成语义正确的程序，确保运行时正确性


<details>
  <summary>Details</summary>
Motivation: 解决代码生成模型在视频游戏和机器人等领域中需要一次性正确程序的挑战

Method: 构建动态解析器树(ToP)，结合模块化上下文无关语法和上下文信息，通过正则表达式约束指导下一个标记生成

Result: 在sLua语言中验证了方法的有效性，能够生成符合任何脚本API的语义正确程序

Conclusion: 该方法能够为运行时关键组件提供语义正确性保证，并在游戏机制生成中实现了运行时正确性

Abstract: Language Models (LMs) are increasingly being used for code generation, but
ensuring the correctness of generated programs remains a significant challenge.
Although imperfect code may be acceptable during software development with
human oversight, domains such as video games and robotics require one-shot
correctness for runtime-critical components. We present a constrained decoding
algorithm for generating semantically correct programs that incorporates a
context-sensitive parser, which, at each step, outputs a regular expression
that satisfies a critical non-extensible property to guide the generation of
the next token sequence that can continue to a correct program. To build such a
context-sensitive parser, we propose a framework of a dynamic tree of parsers
(ToP) during parsing, where each parser corresponds to a modular context-free
grammar enriched with contextual information such as variable scopes and type
constraints, with tree branches representing ambiguity in the future code
segment. We demonstrate our approach through sLua, a strongly typed variant of
Lua, showing that our method can generate semantically correct programs
conforming to any prescribed scripting API. We further show that, with careful
design, our semantic guarantees extend to runtime correctness, as validated in
the application of generating game mechanics for a roguelike video game.

</details>


### [27] [Automated Formal Verification of a Software Fault Isolation System](https://arxiv.org/abs/2508.15898)
*Matthew Sotoudeh,Zachary Yedidia*

Main category: cs.PL

TL;DR: \u901a\u8fc7\u81ea\u52a8\u5316\u5f62\u5f0f\u9a8c\u8bc1\u786e\u4fddLFI\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u5b66\u9690\u79d1\u极简总结：通过自动化形式验证确保LFI验证器的正确性，防止沙盒逃逸


<details>
  <summary>Details</summary>
Motivation: \u8f6f\u4ef6\u6545\u969c\u9694\u79bb\uff08SFI\uff09\u9a8c\u8bc1\u5668\u7684\u58f0\u97f3\u6027\u7f3a\u9677\u4f1a\u7834\u574f\u5b89\u5168\u6a21\u578b\uff0c\u5141\u8bb8\u4e0d\u53ef\u4fe1\u4ee3\u7801\u8bfb\u53d6\u4fdd\u62a4\u5185\u5b58\uff0c\u9700\u8981\u89e3\u51b3\u9a8c\u8bc1\u5668\u7f3a\u9677\u7684\u5b89\u5168\u98ce\u9669

Method: \u91c7\u7528\u81ea\u52a8\u5316\u5f62\u5f0f\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u5bf9\u8f7b\u91cf\u7ea7\u6545\u969c\u9694\u79bb\uff08LFI\uff09\u7cfb\u7edf\u8fdb\u884c\u4e25\u683c\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1

Result: \u5f62\u5f0f\u9a8c\u8bc1\u786e\u8ba4LFI\u9a8c\u8bc1\u5668\u901a\u8fc7\u7684\u7a0b\u5e8f\u4ece\u4e0d\u4f1a\u8bfb\u5199\u6307\u5b9a\u6c99\u76d2\u533a\u57df\u4ee5\u5916\u7684\u5185\u5b58

Conclusion: \u901a\u8fc7\u5f62\u5f0f\u9a8c\u8bc1\u53ef\u4ee5\u6709\u6548\u89e3\u51b3SFI\u9a8c\u8bc1\u5668\u7684\u58f0\u97f3\u6027\u7f3a\u9677\u95ee\u9898\uff0c\u63d0\u9ad8\u6c99\u76d2\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027

Abstract: Software fault isolation (SFI) is a popular way to sandbox untrusted
software. A key component of SFI is the verifier that checks the untrusted code
is written in a subset of the machine language that guarantees it never reads
or writes outside of a region of memory dedicated to the sandbox. Soundness
bugs in the SFI verifier would break the SFI security model and allow the
supposedly sandboxed code to read protected memory. In this paper, we address
the concern of SFI verifier bugs by performing an automated formal verification
of a recent SFI system called Lightweight Fault Isolation (LFI). In particular,
we formally verify that programs accepted by the LFI verifier never read or
write to memory outside of a designated sandbox region.

</details>


### [28] [Leveraging Large Language Models to Detect Missed Peephole Optimizations](https://arxiv.org/abs/2508.16125)
*Zhenyang Xu,Hongxu Xu,Yongqiang Tian,Xintong Zhou,Chengnian Sun*

Main category: cs.PL

TL;DR: Lampo是一个利用大型语言模型检测遗漏窥孔优化的自动化框架，通过结合LLMs的创造性代码优化能力和严格的正确性验证，在LLVM生态系统中成功发现了多个先前遗漏的优化机会。


<details>
  <summary>Details</summary>
Motivation: 窥孔优化是编译器优化中的关键类别，但由于指令集极其复杂多样，发现新的有效窥孔优化具有挑战性。现有方法要么扩展性差，要么只能捕获有限的优化子集。

Method: 提出Lampo框架，协同结合大型语言模型的创造性代码优化能力（虽然不可靠）和翻译验证工具执行的严格正确性验证，采用反馈驱动的迭代过程。

Result: 在LLVM生态系统中，Lampo平均能成功检测出25个先前报告的遗漏优化中的17个，其中22个可以通过不同LLM找到。相比之下，最先进的超级优化器Souper只识别了15个。在7个月开发中，Lampo发现了26个遗漏优化，15个已确认，6个已修复。

Conclusion: Lampo在持续检测遗漏窥孔优化方面展现出强大潜力，证明了LLMs与形式化验证工具结合在编译器优化领域的有效性。

Abstract: By replacing small, suboptimal instruction sequences within programs with a
more efficient equivalent, peephole optimization can not only directly optimize
code size and performance, but also potentially enables further transformations
in the subsequent optimization pipeline. Although peephole optimization is a
critical class of compiler optimizations, discovering new and effective
peephole optimizations is challenging as the instruction sets can be extremely
complex and diverse. Previous methods either do not scale well or can only
capture a limited subset of peephole optimizations. In this work, we leverage
Large Language Models (LLMs) to detect missed peephole optimizations. We
propose Lampo, a novel automated framework that synergistically combines the
creative but unreliable code optimization ability of LLMs with rigorous
correctness verification performed by translation validation tools, integrated
in a feedback-driven iterative process. Through a comprehensive evaluation
within LLVM ecosystems, we show that Lampo can successfully detect up to 17 out
of 25 previously reported missed optimizations in LLVM on average, and that 22
out of 25 can potentially be found by Lampo with different LLMs. For
comparison, the state-of-the-art superoptimizer for LLVM, Souper, identified 15
of them. Moreover, within seven months of development and intermittent
experiments, Lampo found 26 missed peephole optimizations, 15 of which have
been confirmed and 6 already fixed. These results demonstrate Lampo's strong
potential in continuously detecting missed peephole optimizations.

</details>


### [29] [On the Duality of Task and Actor Programming Models](https://arxiv.org/abs/2508.16522)
*Rohan Yadav,Joseph Guman,Sean Treichler,Michael Garland,Alex Aiken,Fredrik Kjolstad,Michael Bauer*

Main category: cs.PL

TL;DR: 这篇论文探讨了任务基和演员基编程模型的对偶性，并提出技术使任务基系统在保持生产力的同时达到类似演员基系统的性能水平


<details>
  <summary>Details</summary>
Motivation: 分布式和异构机器的编程模型需要在开发生产力和性能之间找到平衡，任务基模型生产力高但性能较低，演员基模型性能高但生产力较低

Method: 证明任务基和演员基编程模型的对偶性，并将这种对偶性扩展到性能层面，提出技术使任务基系统达到类似演员基系统的性能

Result: 在Realm运行时中减少了1.7-5.3倍的开销，性能接近Charm++和MPI等演员基系统的2倍以内；在Legion中实现了1.3-5.0倍的强缩放性能提升

Conclusion: 通过对偶性分析和相应技术，任务基系统可以在保持高生产力的同时达到与演员基系统相类似的性能水平，解决了两者之间的传统投资组合问题

Abstract: Programming models for distributed and heterogeneous machines are rapidly
growing in popularity to meet the demands of modern workloads. Task and actor
models are common choices that offer different trade-offs between development
productivity and achieved performance. Task-based models offer better
productivity and composition of software, whereas actor-based models routinely
deliver better peak performance due to lower overheads. While task-based and
actor-based models appear to be different superficially, we demonstrate these
programming models are duals of each other. Importantly, we show that this
duality extends beyond functionality to performance, and elucidate techniques
that let task-based systems deliver performance competitive with actor-based
systems without compromising productivity. We apply these techniques to both
Realm, an explicitly parallel task-based runtime, as well as Legion, an
implicitly parallel task-based runtime. We show these techniques reduce Realm's
overheads by between 1.7-5.3x, coming within a factor of two of the overheads
imposed by heavily optimized actor-based systems like Charm++ and MPI. We
further show that our techniques enable between 1.3-5.0x improved strong
scaling of unmodified Legion applications.

</details>
