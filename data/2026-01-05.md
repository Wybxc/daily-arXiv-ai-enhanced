<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 10]
- [cs.LO](#cs.LO) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Understanding Security Risks of AI Agents' Dependency Updates](https://arxiv.org/abs/2601.00205)
*Tanmay Singla,Berk Çakar,Paschal C. Amusuo,James C. Davis*

Main category: cs.SE

TL;DR: AI编码代理在依赖更新中比人类更频繁选择已知漏洞版本，且修复更困难，导致净漏洞增加而非减少


<details>
  <summary>Details</summary>
Motivation: 随着AI编码代理通过PR修改软件，需要了解其在依赖决策中是否引入独特的安全风险，因为依赖是现代软件供应链的关键控制点

Method: 研究7个生态系统中117,062个依赖变更，对比AI代理和人类作者PR中的依赖决策，分析漏洞版本选择频率和修复难度

Result: AI代理选择已知漏洞版本频率更高（2.46% vs 1.64%），修复更困难（36.8%需主版本升级 vs 12.9%），AI驱动依赖工作净增98个漏洞，人类驱动净减1,316个漏洞

Conclusion: AI编码代理的依赖决策存在安全风险，需要PR时漏洞筛查和注册表感知的防护机制来提高AI驱动依赖更新的安全性

Abstract: Package dependencies are a critical control point in modern software supply chains. Dependency changes can substantially alter a project's security posture. As AI coding agents increasingly modify software via pull requests, it is unclear whether their dependency decisions introduce distinct security risks.
  We study 117,062 dependency changes from agent- and human-authored pull requests across seven ecosystems. Agents select known-vulnerable versions more often than humans (2.46% vs. 1.64%), and their vulnerable selections are more disruptive to remediate, with 36.8% requiring major-version upgrades compared to 12.9% for humans, despite patched alternatives existing in most cases. At the aggregate level, agent-driven dependency work yields a net vulnerability increase of 98, whereas human-authored work yields a net reduction of 1,316. These findings motivate pull-request-time vulnerability screening and registry-aware guardrails to make agent-driven dependency updates safer.

</details>


### [2] [Advanced Vulnerability Scanning for Open Source Software: Detection and Mitigation of Log4j Vulnerabilities](https://arxiv.org/abs/2601.00235)
*Victor Wen,Zedong Peng*

Main category: cs.SE

TL;DR: 开发了一个先进的Log4j扫描工具，通过评估软件的实际可利用性来减少误报，集成到GitHub Actions中实现自动化持续扫描。


<details>
  <summary>Details</summary>
Motivation: Log4Shell漏洞披露后，尽管有补丁版本，但仍有大量下载包含易受攻击的包。现有检测工具主要关注Log4j版本识别，导致大量误报，因为它们不检查软件是否真的可被恶意利用。

Method: 开发先进的Log4j扫描工具，首先识别漏洞，然后提供针对性的缓解建议和即时反馈。通过GitHub Actions集成，提供自动化持续扫描能力，确保在代码变更时及时识别漏洞。

Result: 评估了28个开源软件项目的不同版本，从140个扫描样本中实现了91.4%的准确率。GitHub Action实现已在GitHub市场可用。

Conclusion: 该工具提供了一种可靠的方法来检测和缓解开源项目中的漏洞，通过集成到现有开发工作流程中实现实时监控和快速响应潜在威胁。

Abstract: Automated detection of software vulnerabilities remains a critical challenge in software security. Log4j is an industrial-grade Java logging framework listed as one of the top 100 critical open source projects. On Dec. 10, 2021 a severe vulnerability Log4Shell was disclosed before being fully patched with Log4j2 version 2.17.0 on Dec. 18, 2021. However, to this day about 4.1 million, or 33 percent of all Log4j downloads in the last 7 days contain vulnerable packages. Many Log4Shell scanners have since been created to detect if a user's installed Log4j version is vulnerable. Current detection tools primarily focus on identifying the version of Log4j installed, leading to numerous false positives, as they do not check if the software scanned is really vulnerable to malicious actors. This research aims to develop an advanced Log4j scanning tool that can evaluate the real-world exploitability of the software, thereby reducing false positives. Our approach first identifies vulnerabilities and then provides targeted recommendations for mitigating these detected vulnerabilities, along with instant feedback to users. By leveraging GitHub Actions, our tool offers automated and continuous scanning capabilities, ensuring timely identification of vulnerabilities as code changes occur. This integration into existing development workflows enables real-time monitoring and quicker responses to potential threats. We demonstrate the effectiveness of our approach by evaluating 28 open-source software projects across different releases, achieving an accuracy rate of 91.4% from a sample of 140 scans. Our GitHub action implementation is available at the GitHub marketplace and can be accessed by anyone interested in improving their software security and for future studies. This tool provides a dependable way to detect and mitigate vulnerabilities in open-source projects.

</details>


### [3] [An Empirical Evaluation of LLM-Based Approaches for Code Vulnerability Detection: RAG, SFT, and Dual-Agent Systems](https://arxiv.org/abs/2601.00254)
*Md Hasan Saju,Maher Muhtadi,Akramul Azim*

Main category: cs.SE

TL;DR: 该研究比较了三种LLM方法（RAG、SFT、双代理框架）在软件漏洞检测中的效果，发现RAG方法在准确率和F1分数上表现最佳，强调了领域知识增强对实际应用的重要性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的快速发展为自动化软件漏洞检测提供了新机遇，这是保护现代代码库的关键任务。研究旨在评估不同LLM方法在漏洞检测中的有效性。

Method: 研究比较了三种方法：1）检索增强生成（RAG），整合互联网和MITRE CWE数据库的外部领域知识；2）监督微调（SFT），使用参数高效的QLoRA适配器；3）双代理LLM框架，其中第二个代理审计和优化第一个代理的输出。评估基于从Big-Vul和GitHub真实代码库中整理的数据集，重点关注五个关键CWE类别。

Result: RAG方法取得了最高的整体准确率（0.86）和F1分数（0.85），显示了上下文增强的价值。SFT方法也表现出色。双代理系统在提高推理透明度和错误缓解方面显示出潜力，同时减少了资源开销。

Conclusion: 研究结果表明，整合领域专业知识机制显著增强了LLM在实际漏洞检测任务中的适用性，RAG方法因其结合外部知识的能力而表现最佳。

Abstract: The rapid advancement of Large Language Models (LLMs) presents new opportunities for automated software vulnerability detection, a crucial task in securing modern codebases. This paper presents a comparative study on the effectiveness of LLM-based techniques for detecting software vulnerabilities. The study evaluates three approaches, Retrieval-Augmented Generation (RAG), Supervised Fine-Tuning (SFT), and a Dual-Agent LLM framework, against a baseline LLM model. A curated dataset was compiled from Big-Vul and real-world code repositories from GitHub, focusing on five critical Common Weakness Enumeration (CWE) categories: CWE-119, CWE-399, CWE-264, CWE-20, and CWE-200. Our RAG approach, which integrated external domain knowledge from the internet and the MITRE CWE database, achieved the highest overall accuracy (0.86) and F1 score (0.85), highlighting the value of contextual augmentation. Our SFT approach, implemented using parameter-efficient QLoRA adapters, also demonstrated strong performance. Our Dual-Agent system, an architecture in which a secondary agent audits and refines the output of the first, showed promise in improving reasoning transparency and error mitigation, with reduced resource overhead. These results emphasize that incorporating a domain expertise mechanism significantly strengthens the practical applicability of LLMs in real-world vulnerability detection tasks.

</details>


### [4] [In Line with Context: Repository-Level Code Generation via Context Inlining](https://arxiv.org/abs/2601.00376)
*Chao Hu,Wenhao Zeng,Yuling Shi,Beijun Shen,Xiaodong Gu*

Main category: cs.SE

TL;DR: InlineCoder通过将未完成函数内联到其调用图中，将仓库级代码生成转化为函数级任务，利用锚点生成和双向内联（上游内联和下游检索）来增强仓库上下文理解。


<details>
  <summary>Details</summary>
Motivation: 现有仓库级代码生成方法（如RAG或基于上下文的函数选择）主要依赖表面相似性，难以捕捉仓库中复杂的跨函数、类和模块的依赖关系，导致对仓库级语义理解不足。

Method: 1. 给定函数签名，首先生成草稿完成（锚点），近似下游依赖并支持基于困惑度的置信度估计；2. 双向内联过程：上游内联将锚点嵌入调用者以捕捉多样化使用场景；下游检索将锚点的被调用者集成到提示中以提供精确依赖上下文；3. 结合草稿完成、上游和下游视角的丰富上下文为LLM提供全面的仓库视图。

Result: 论文未提供具体实验结果，但方法理论上通过将仓库理解转化为更简单的函数级编码任务，解决了现有方法在捕捉复杂仓库依赖方面的不足。

Conclusion: InlineCoder通过创新的内联方法，将仓库级代码生成重新构建为函数级任务，有效增强了LLM对仓库上下文的理解能力，为处理复杂依赖关系提供了新思路。

Abstract: Repository-level code generation has attracted growing attention in recent years. Unlike function-level code generation, it requires the model to understand the entire repository, reasoning over complex dependencies across functions, classes, and modules. However, existing approaches such as retrieval-augmented generation (RAG) or context-based function selection often fall short: they primarily rely on surface-level similarity and struggle to capture the rich dependencies that govern repository-level semantics. In this paper, we introduce InlineCoder, a novel framework for repository-level code generation. InlineCoder enhances the understanding of repository context by inlining the unfinished function into its call graph, thereby reframing the challenging repository understanding as an easier function-level coding task. Given a function signature, InlineCoder first generates a draft completion, termed an anchor, which approximates downstream dependencies and enables perplexity-based confidence estimation. This anchor drives a bidirectional inlining process: (i) Upstream Inlining, which embeds the anchor into its callers to capture diverse usage scenarios; and (ii) Downstream Retrieval, which integrates the anchor's callees into the prompt to provide precise dependency context. The enriched context, combining draft completion with upstream and downstream perspectives, equips the LLM with a comprehensive repository view.

</details>


### [5] [On Plagiarism and Software Plagiarism](https://arxiv.org/abs/2601.00429)
*Rares Folea,Emil Slusanschi*

Main category: cs.SE

TL;DR: 本文探讨软件相似性自动检测的复杂性，介绍开源软件解决方案Project Martial，并综述现有反软件抄袭方法。


<details>
  <summary>Details</summary>
Motivation: 软件相似性检测面临数字工件的独特挑战，需要有效工具来应对软件抄袭问题，特别是在学术和商业应用中。

Method: 通过分析学术界和法律领域的现有方法，包括重要诉讼和法院裁决；基于可用工件分类检测挑战；综述指纹识别、软件水印、代码嵌入等技术；在Project Martial中应用部分技术。

Result: 介绍了Project Martial这一开源软件相似性检测解决方案，并提供了对现有技术的系统分类和综述。

Conclusion: 软件相似性检测是一个复杂但重要的问题，Project Martial为这一问题提供了实用的开源解决方案，同时系统梳理了该领域的技术和法律背景。

Abstract: This paper explores the complexities of automatic detection of software similarities, in relation to the unique challenges of digital artifacts, and introduces Project Martial, an open-source software solution for detecting code similarity. This research enumerates some of the existing approaches to counter software plagiarism by examining both the academia and legal landscape, including notable lawsuits and court rulings that have shaped the understanding of software copyright infringements in commercial applications. Furthermore, we categorize the classes of detection challenges based on the available artifacts, and we provide a survey of the previously studied techniques in the literature, including solutions based on fingerprinting, software birthmarks, or code embeddings, and exemplify how a subset of them can be applied in the context of Project Martial.

</details>


### [6] [DSL or Code? Evaluating the Quality of LLM-Generated Algebraic Specifications: A Case Study in Optimization at Kinaxis](https://arxiv.org/abs/2601.00469)
*Negin Ayoughi,David Dewar,Shiva Nejati,Mehrdad Sabetzadeh*

Main category: cs.SE

TL;DR: EXEOS：基于LLM的方法，从自然语言描述生成AMPL模型和Python代码，并通过求解器反馈迭代优化，在数学优化领域评估AMPL与Python的生成质量。


<details>
  <summary>Details</summary>
Motivation: 模型驱动工程（MDE）虽然提供抽象和分析严谨性，但开发和维护模型的高成本限制了其在许多领域的工业采用。大型语言模型（LLMs）可以通过从自然语言描述直接生成模型来帮助改变这种成本平衡。然而，对于领域特定语言（DSLs），LLM生成的模型可能不如主流语言（如Python）生成的代码准确，因为后者在LLM训练语料库中占主导地位。

Method: 提出EXEOS方法：基于LLM从自然语言问题描述生成AMPL模型和Python代码，并通过求解器反馈进行迭代优化。使用公开优化数据集和工业合作伙伴Kinaxis的真实供应链案例进行评估，比较生成的AMPL模型与Python代码在可执行性和正确性方面的表现。

Result: 消融研究表明：AMPL在可执行性和正确性方面与Python竞争，有时甚至更好；EXEOS中的设计选择提高了生成规范的质量。

Conclusion: LLM可以有效地从自然语言生成领域特定语言（如AMPL）的模型，通过适当的反馈机制，DSL生成的模型质量可以与主流语言相媲美甚至更好，这为降低模型驱动工程的采用成本提供了有前景的途径。

Abstract: Model-driven engineering (MDE) provides abstraction and analytical rigour, but industrial adoption in many domains has been limited by the cost of developing and maintaining models. Large language models (LLMs) can help shift this cost balance by supporting direct generation of models from natural-language (NL) descriptions. For domain-specific languages (DSLs), however, LLM-generated models may be less accurate than LLM-generated code in mainstream languages such as Python, due to the latter's dominance in LLM training corpora. We investigate this issue in mathematical optimization, with AMPL, a DSL with established industrial use. We introduce EXEOS, an LLM-based approach that derives AMPL models and Python code from NL problem descriptions and iteratively refines them with solver feedback. Using a public optimization dataset and real-world supply-chain cases from our industrial partner Kinaxis, we evaluate generated AMPL models against Python code in terms of executability and correctness. An ablation study with two LLM families shows that AMPL is competitive with, and sometimes better than, Python, and that our design choices in EXEOS improve the quality of generated specifications.

</details>


### [7] [Multi-Agent Coordinated Rename Refactoring](https://arxiv.org/abs/2601.00482)
*Abhiram Bellur,Mohammed Raihan Ullah,Fraol Batole,Mohit Kansara,Masaharu Morimoto,Kai Ishikawa,Haifeng Chen,Yaroslav Zharov,Timofey Bryksin,Tien N. Nguyen,Hridesh Rajan,Danny Dig*

Main category: cs.SE

TL;DR: 论文提出首个多智能体框架，用于自动化协调重命名重构任务，通过智能体与开发者协作，减少人工负担并保持开发者主导地位。


<details>
  <summary>Details</summary>
Motivation: 协调重命名是软件开发中频繁但具有挑战性的任务，开发者需要手动在多文件和上下文中传播重命名重构，过程繁琐且易错。现有启发式方法产生过多误报，而普通大语言模型因上下文限制和无法与重构工具交互而提供不完整建议。

Method: 设计了三智能体框架：1) 范围推断智能体将开发者初始重构线索转化为明确的自然语言声明范围；2) 计划执行智能体使用该计划识别需要重构的程序元素，并通过调用IDE可信重构API安全执行更改；3) 复制智能体指导项目范围内的搜索。

Result: 通过对100个开源项目的609K提交进行形成性研究，并调查205名开发者，验证了框架的有效性。该框架能显著减少开发者负担，同时保持开发者在重构过程中的主导地位。

Conclusion: AI智能体在软件开发中的主要价值在于扩展开发者的推理和行动能力，而非取代人类参与。多智能体框架成功展示了智能体如何与开发者协同工作，自动化协调重命名这一重复性任务，有效解决了现有方法的局限性。

Abstract: The primary value of AI agents in software development lies in their ability to extend the developer's capacity for reasoning and action, not to supplant human involvement. To showcase how to use agents working in tandem with developers, we designed a novel approach for carrying out coordinated renaming. Coordinated renaming, where a single rename refactoring triggers refactorings in multiple, related identifiers, is a frequent yet challenging task. Developers must manually propagate these rename refactorings across numerous files and contexts, a process that is both tedious and highly error-prone. State-of-the-art heuristic-based approaches produce an overwhelming number of false positives, while vanilla Large Language Models (LLMs) provide incomplete suggestions due to their limited context and inability to interact with refactoring tools. This leaves developers with incomplete refactorings or burdens them with filtering too many false positives. Coordinated renaming is exactly the kind of repetitive task that agents can significantly reduce the developers' burden while keeping them in the driver's seat.
  We designed, implemented, and evaluated the first multi-agent framework that automates coordinated renaming. It operates on a key insight: a developer's initial refactoring is a clue to infer the scope of related refactorings. Our Scope Inference Agent first transforms this clue into an explicit, natural-language Declared Scope. The Planned Execution Agent then uses this as a strict plan to identify program elements that should undergo refactoring and safely executes the changes by invoking the IDE's own trusted refactoring APIs. Finally, the Replication Agent uses it to guide the project-wide search. We first conducted a formative study on the practice of coordinated renaming in 609K commits in 100 open-source projects and surveyed 205 developers ...

</details>


### [8] [STELLAR: A Search-Based Testing Framework for Large Language Model Applications](https://arxiv.org/abs/2601.00497)
*Lev Sorokin,Ivan Vasilev,Ken E. Friedl,Andrea Stocco*

Main category: cs.SE

TL;DR: STELLAR是一个基于搜索的自动化测试框架，用于发现LLM应用中的不当响应，通过进化优化探索特征组合，比基线方法多发现2.5-4.3倍故障。


<details>
  <summary>Details</summary>
Motivation: LLM应用在各个领域广泛部署，但容易产生不准确、虚构或有害的响应，且其高维输入空间使得系统化测试非常困难。

Method: 将测试生成建模为优化问题，将输入空间离散化为风格、内容和扰动特征，采用进化优化动态探索更可能暴露故障的特征组合。

Result: 在三个LLM对话问答系统上评估：安全基准测试和车载导航推荐系统。STELLAR比现有基线方法多发现2.5-4.3倍故障。

Conclusion: STELLAR框架能有效系统化地发现LLM应用中的不当响应，为LLM测试提供了新的自动化方法。

Abstract: Large Language Model (LLM)-based applications are increasingly deployed across various domains, including customer service, education, and mobility. However, these systems are prone to inaccurate, fictitious, or harmful responses, and their vast, high-dimensional input space makes systematic testing particularly challenging. To address this, we present STELLAR, an automated search-based testing framework for LLM-based applications that systematically uncovers text inputs leading to inappropriate system responses. Our framework models test generation as an optimization problem and discretizes the input space into stylistic, content-related, and perturbation features. Unlike prior work that focuses on prompt optimization or coverage heuristics, our work employs evolutionary optimization to dynamically explore feature combinations that are more likely to expose failures. We evaluate STELLAR on three LLM-based conversational question-answering systems. The first focuses on safety, benchmarking both public and proprietary LLMs against malicious or unsafe prompts. The second and third target navigation, using an open-source and an industrial retrieval-augmented system for in-vehicle venue recommendations. Overall, STELLAR exposes up to 4.3 times (average 2.5 times) more failures than the existing baseline approaches.

</details>


### [9] [SEMODS: A Validated Dataset of Open-Source Software Engineering Models](https://arxiv.org/abs/2601.00635)
*Alexandra González,Xavier Franch,Silverio Martínez-Fernández*

Main category: cs.SE

TL;DR: SEMODS是一个包含3,427个Hugging Face模型的软件工程专用数据集，通过自动化收集和人工标注结合LLM辅助验证，将模型与软件开发周期任务关联，支持数据分析、模型发现、基准测试和模型适配等应用。


<details>
  <summary>Details</summary>
Motivation: 将人工智能集成到软件工程中需要一个专门针对SE任务的模型集合。由于Hugging Face上有数百万个模型且新模型不断涌现，没有专门的目录难以识别适合SE任务的模型。

Method: 从Hugging Face提取3,427个模型，结合自动化收集与严格验证（包括人工标注和大型语言模型辅助），将模型链接到软件开发周期中的SE任务和活动。

Result: 创建了SEMODS数据集，提供模型评估结果的标准化表示，支持多种应用场景。

Conclusion: SEMODS填补了软件工程领域专用模型目录的空白，为AI在SE中的集成提供了重要资源。

Abstract: Integrating Artificial Intelligence into Software Engineering (SE) requires having a curated collection of models suited to SE tasks. With millions of models hosted on Hugging Face (HF) and new ones continuously being created, it is infeasible to identify SE models without a dedicated catalogue. To address this gap, we present SEMODS: an SE-focused dataset of 3,427 models extracted from HF, combining automated collection with rigorous validation through manual annotation and large language model assistance. Our dataset links models to SE tasks and activities from the software development lifecycle, offering a standardized representation of their evaluation results, and supporting multiple applications such as data analysis, model discovery, benchmarking, and model adaptation.

</details>


### [10] [Early-Stage Prediction of Review Effort in AI-Generated Pull Requests](https://arxiv.org/abs/2601.00753)
*Dao Sy Duy Minh,Huynh Trung Kiet,Tran Chi Nguyen,Nguyen Lam Phu Quy,Phu Hoa Pham,Nguyen Dinh Ha Duong,Truong Bao Tran*

Main category: cs.SE

TL;DR: 该研究分析了33,707个AI代理生成的PR，发现其呈现两极分化的行为模式：28.3%为即时合并，其余则陷入迭代审查循环。研究者提出了基于静态结构特征的Circuit Breaker模型，能有效预测高审查成本的PR，挑战了AI辅助代码审查的现有假设。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理从代码补全工具转变为能大规模创建PR的完整团队成员，软件维护者面临新挑战：不仅要审查代码，还要管理与非人类贡献者的复杂交互循环。核心问题是：能否在人类交互开始前就预测哪些AI生成的PR会消耗过多审查精力？

Method: 分析AIDev数据集中33,707个AI代理生成的PR（来自2,807个仓库），发现两极分化的行为模式。提出Circuit Breaker分流模型，仅使用静态结构特征（如文件修改模式、代码结构变化）来预测高审查成本的PR。使用LightGBM模型，并与语义文本特征（TF-IDF、CodeBERT）进行对比。

Result: 发现AI代理PR呈现两极分化：28.3%在1分钟内合并（窄自动化任务成功），其余则陷入迭代审查循环。LightGBM模型在时间分割上达到AUC 0.957，语义特征预测价值可忽略。在20%审查预算下，模型能拦截69%的总审查工作量。

Conclusion: AI代理的审查负担由其修改的内容结构决定，而非其语义内容，这挑战了当前AI辅助代码审查的假设。研究强调了在人机协作中需要结构性治理机制，而非依赖语义分析。

Abstract: As autonomous AI agents transition from code completion tools to full-fledged teammates capable of opening pull requests (PRs) at scale, software maintainers face a new challenge: not just reviewing code, but managing complex interaction loops with non-human contributors. This paradigm shift raises a critical question: can we predict which agent-generated PRs will consume excessive review effort before any human interaction begins?
  Analyzing 33,707 agent-authored PRs from the AIDev dataset across 2,807 repositories, we uncover a striking two-regime behavioral pattern that fundamentally distinguishes autonomous agents from human developers. The first regime, representing 28.3 percent of all PRs, consists of instant merges (less than 1 minute), reflecting success on narrow automation tasks. The second regime involves iterative review cycles where agents frequently stall or abandon refinement (ghosting).
  We propose a Circuit Breaker triage model that predicts high-review-effort PRs (top 20 percent) at creation time using only static structural features. A LightGBM model achieves AUC 0.957 on a temporal split, while semantic text features (TF-IDF, CodeBERT) provide negligible predictive value. At a 20 percent review budget, the model intercepts 69 percent of total review effort, enabling zero-latency governance.
  Our findings challenge prevailing assumptions in AI-assisted code review: review burden is dictated by what agents touch, not what they say, highlighting the need for structural governance mechanisms in human-AI collaboration.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [11] [Proceedings 41st International Conference on Logic Programming](https://arxiv.org/abs/2601.00047)
*Martin Gebser,Daniela Inclezan,Francesco Ricca,Manuel Carro,Miroslaw Truszczynski*

Main category: cs.LO

TL;DR: ICLP 2025会议论文集，包含逻辑编程领域的技术通讯，涵盖理论、语言设计、程序分析、应用等多个主题


<details>
  <summary>Details</summary>
Motivation: 作为逻辑编程领域的顶级国际会议，ICLP自1982年以来一直是展示该领域研究成果的重要平台。本次会议旨在汇集逻辑编程领域的最新研究进展，促进学术交流与合作。

Method: 通过三个投稿轨道收集论文：主轨道、IJCAI快速轨道和近期发表研究轨道。论文涵盖理论基础、语言设计、编程方法论、程序分析、优化、应用和实现方法论等多个领域。

Result: 出版了包含ICLP 2025技术通讯的论文集，收录了逻辑编程领域的最新研究成果，反映了该领域的当前研究热点和发展趋势。

Conclusion: ICLP 2025成功举办了第41届会议，继续作为逻辑编程领域的重要学术交流平台，推动了该领域的研究发展和知识传播。

Abstract: Since the first conference in Marseille in 1982, the International Conference on Logic Programming (ICLP) has been the premier international event for presenting research in logic programming. These proceedings include the Technical Communications of the 41st ICLP, held on 12-19 September 2025 at the University of Calabria in Rende, Italy. The papers and extended abstracts in this volume address the following areas and topics: theoretical foundations, language design and programming methodologies, program analysis and optimization, applications and implementation methodologies. This volume features contributions to three submission tracks of ICLP 2025: the Main track, IJCAI fast track, and Recently Published Research track.

</details>


### [12] [Quantifier Elimination Meets Treewidth](https://arxiv.org/abs/2601.00312)
*Hao Wu,Jiyu Zhu,Amir Kafshdar Goharshady,Jie An,Bican Xia,Naijun Zhan*

Main category: cs.LO

TL;DR: 提出基于变量依赖图树宽度的量化消除新框架，利用结构稀疏性将FME和CAD的最坏复杂度从双重指数降为单指数


<details>
  <summary>Details</summary>
Motivation: 解决Fourier-Motzkin消除和圆柱代数分解在消除量化块时面临的双重指数复杂度障碍，利用量化公式变量依赖图的结构稀疏性来降低复杂度

Method: 提出基于参数化算法的新框架：利用变量依赖图的树宽度度量图的树状特性，在依赖图的树分解上构建动态规划框架，应用于FME和CAD量化消除过程

Result: 当树宽度为常数时，框架显著改进FME和CAD的复杂度，从双重指数降为单指数；在稀疏线性实算术和非线性实算术基准测试中，算法在低树宽度实例上优于现有启发式方法

Conclusion: 通过利用量化公式的结构稀疏性和树宽度参数，成功开发出高效的量化消除框架，为处理大规模稀疏量化问题提供了理论保证和实用算法

Abstract: In this paper, we address the complexity barrier inherent in Fourier-Motzkin elimination (FME) and cylindrical algebraic decomposition (CAD) when eliminating a block of (existential) quantifiers. To mitigate this, we propose exploiting structural sparsity in the variable dependency graph of quantified formulas. Utilizing tools from parameterized algorithms, we investigate the role of treewidth, a parameter that measures the graph's tree-likeness, in the process of quantifier elimination. A novel dynamic programming framework, structured over a tree decomposition of the dependency graph, is developed for applying FME and CAD, and is also extensible to general quantifier elimination procedures. Crucially, we prove that when the treewidth is a constant, the framework achieves a significant exponential complexity improvement for both FME and CAD, reducing the worst-case complexity bound from doubly exponential to single exponential. Preliminary experiments on sparse linear real arithmetic (LRA) and nonlinear real arithmetic (NRA) benchmarks confirm that our algorithm outperforms the existing popular heuristic-based approaches on instances exhibiting low treewidth.

</details>
