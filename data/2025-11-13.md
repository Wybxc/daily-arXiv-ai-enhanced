<div id=toc></div>

# Table of Contents

- [cs.FL](#cs.FL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 10]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.LO](#cs.LO) [Total: 1]


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [1] [Good-for-MDP State Reduction for Stochastic LTL Planning](https://arxiv.org/abs/2511.09073)
*Christoph Weinhuber,Giuseppe De Giacomo,Yong Li,Sven Schewe,Qiyi Tang*

Main category: cs.FL

TL;DR: 提出了一种新的GFM自动机状态空间缩减技术，显著减少自动机状态数量，并针对特定形式的LTL公式提供了直接构造方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法将LTL公式转换为GFM自动机，但自动机规模是影响可扩展性的主要因素，需要减少状态数量以提高效率。

Method: 采用复杂的转换链，利用对抗环境中good-for-games最小化的最新进展，并针对G F φ形式的公式提供直接构造方法。

Result: 实验证明状态缩减技术具有实际有效性，专门构造方法在可扩展性方面具有优势。

Conclusion: 提出的状态缩减技术和专门构造方法能够显著提高基于LTL的MDP规划的可扩展性。

Abstract: We study stochastic planning problems in Markov Decision Processes (MDPs) with goals specified in Linear Temporal Logic (LTL). The state-of-the-art approach transforms LTL formulas into good-for-MDP (GFM) automata, which feature a restricted form of nondeterminism. These automata are then composed with the MDP, allowing the agent to resolve the nondeterminism during policy synthesis. A major factor affecting the scalability of this approach is the size of the generated automata. In this paper, we propose a novel GFM state-space reduction technique that significantly reduces the number of automata states. Our method employs a sophisticated chain of transformations, leveraging recent advances in good-for-games minimisation developed for adversarial settings. In addition to our theoretical contributions, we present empirical results demonstrating the practical effectiveness of our state-reduction technique. Furthermore, we introduce a direct construction method for formulas of the form $\mathsf{G}\mathsf{F}\varphi$, where $\varphi$ is a co-safety formula. This construction is provably single-exponential in the worst case, in contrast to the general doubly-exponential complexity. Our experiments confirm the scalability advantages of this specialised construction.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [Triage in Software Engineering: A Systematic Review of Research and Practice](https://arxiv.org/abs/2511.08607)
*Yongxin Zhao,Shenglin Zhang,Yujia Wu,Yuxin Sun,Yongqian Sun,Dan Pei,Chetan Bansal,Minghua Ma*

Main category: cs.SE

TL;DR: 本文对2004年至今的234篇论文进行了全面综述，深入分析了软件系统故障分诊的基本概念、系统架构和问题陈述，总结了开源数据集和评估指标，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统复杂性不断增加，故障分诊已成为系统运维中的基本流程。海量异构数据使得有效的故障分诊对于维护系统可靠性、促进可维护性和实现快速问题响应变得不可或缺。

Method: 通过比较学术和工业研究的不同目标，分析工业实践的实证研究，识别限制故障分诊系统实际部署的主要障碍，并总结广泛采用的开源数据集和评估指标。

Result: 提供了故障分诊领域的统一视角，识别了实际部署中的主要障碍，并为从业者提供了方法选择和性能评估的指导。

Conclusion: 本文概述了潜在未来方向和新兴机遇，以促进学术创新与工业应用之间更紧密的整合。

Abstract: As modern software systems continue to grow in complexity, triage has become a fundamental process in system operations and maintenance. Triage aims to efficiently prioritize, assign, and assess issues to ensure the reliability of complex environments. The vast amount of heterogeneous data generated by software systems has made effective triage indispensable for maintaining reliability, facilitating maintainability, and enabling rapid issue response. Motivated by these challenges, researchers have devoted extensive effort to advancing triage automation and have achieved significant progress over the past two decades. This survey provides a comprehensive review of 234 papers from 2004 to the present, offering an in-depth examination of the fundamental concepts, system architecture, and problem statement. By comparing the distinct goals of academic and industrial research and by analyzing empirical studies of industrial practices, we identify the major obstacles that limit the practical deployment of triage systems. To assist practitioners in method selection and performance evaluation, we summarize widely adopted open-source datasets and evaluation metrics, providing a unified perspective on the measurement of triage effectiveness. Finally, we outline potential future directions and emerging opportunities to foster a closer integration between academic innovation and industrial application. All reviewed papers and projects are available at https://github.com/AIOps-Lab-NKU/TriageSurvey.

</details>


### [3] [Energy Consumption of Dataframe Libraries for End-to-End Deep Learning Pipelines:A Comparative Analysis](https://arxiv.org/abs/2511.08644)
*Punit Kumar,Asif Imran,Tevfik Kosar*

Main category: cs.SE

TL;DR: 对Pandas、Polars和Dask三个Python数据处理库在深度学习训练和推理流水线中的性能进行对比分析


<details>
  <summary>Details</summary>
Motivation: 填补现有文献空白，研究这些库在数据加载、预处理和批次馈送等关键阶段与GPU大工作负载的交互情况

Method: 测量不同机器学习模型和数据集上的运行时、内存使用、磁盘使用和能耗（CPU和GPU）等关键性能指标

Result: 提供了三个库在深度学习流水线中的性能对比数据

Conclusion: 为深度学习实践中选择合适的数据处理库提供了实证依据

Abstract: This paper presents a detailed comparative analysis of the performance of three major Python data manipulation libraries - Pandas, Polars, and Dask - specifically when embedded within complete deep learning (DL) training and inference pipelines. The research bridges a gap in existing literature by studying how these libraries interact with substantial GPU workloads during critical phases like data loading, preprocessing, and batch feeding. The authors measured key performance indicators including runtime, memory usage, disk usage, and energy consumption (both CPU and GPU) across various machine learning models and datasets.

</details>


### [4] [An insight into the technical debt-fix trade off in software backporting](https://arxiv.org/abs/2511.09000)
*Jarin Tasnim,Debasish Chakroborti,Chanchal K. Roy,Kevin A. Schneider*

Main category: cs.SE

TL;DR: 分析了105,396个提交中的技术债务问题，发现在回移植过程中约4.3%的提交会引入新的技术债务，不同生态系统和开发阶段存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 研究稳定软件版本维护过程中回移植活动引入技术债务的情况，识别何时以及为何在回移植过程中会产生新的技术债务。

Method: 分析了87个仓库中31,076个回移植源的105,396个提交，涵盖Apache、Eclipse和Python三个软件生态系统。

Result: 4.3%的回移植引入了新的技术债务；Apache贡献了最多的绝对实例，而Python和Eclipse的债务提交比是Apache的近三倍；不同生态系统在不同发布周期阶段积累技术债务的模式不同。

Conclusion: 回移植过程中确实会引入技术债务，其发生率和模式受生态系统、发布周期阶段以及开发者经验、工作负荷和所有权状态等因素影响。

Abstract: Maintaining software is an ongoing process that stretches beyond the initial release. Stable software versions continuously evolve to fix bugs, add improvements, address security issues, and ensure compatibility. This ongoing support involves Backporting, which means taking a fix or update from a newer version and applying it to an older version of the same software. As software versions evolve, new technical debt can arise during backport maintenance activities. This study examines the technical debt involved in fixing 105,396 commits from 31,076 backport sources across 87 repositories in three software ecosystems (Apache, Eclipse, and Python). The goal is to identify when and why new technical debt arises during backporting in stable source code. Our results indicate that approximately 4.3% of backports introduce new technical debt. Apache contributes the most absolute instances, while Python and Eclipse exhibit nearly three times higher debt-to-commit ratios than Apache. Feature migrations make older Apache releases debt-prone in the early phase, whereas Python and Eclipse releases tend to accumulate technical debt mostly during the middle phase of their release cycles. Additionally, developers who are inexperienced, under high workloads, or non-owners are more likely to introduce technical debt during backporting.

</details>


### [5] [Test Plan Generation for Live Testing of Cloud Services](https://arxiv.org/abs/2511.09038)
*Oussama Jebbar,Ferhat Khendek,Maria Toeroe*

Main category: cs.SE

TL;DR: 提出一种自动化生成测试计划的方法，旨在减少生产环境中测试活动可能引起的服务中断


<details>
  <summary>Details</summary>
Motivation: 手动设计测试计划繁琐且容易出错，特别是在大型复杂系统中更为困难，需要避免测试活动对生产流量造成不可接受的干扰

Method: 开发自动化测试计划生成方法，包括测试配置选择/生成、测试配置部署规划、测试运行调度创建以及选择降低干扰风险的策略等

Result: 通过案例研究说明了该方法的各个方面，展示了其在生产环境中的应用

Conclusion: 自动化测试计划生成方法能够有效减少生产环境中测试活动可能引起的服务中断

Abstract: Live testing is performed in the production environment ideally without causing unacceptable disturbance to the production traffic. Thus, test activities have to be orchestrated properly to avoid interferences with the production traffic. A test plan is the road map that specifies how the test activities need to be orchestrated. Developing a test plan includes tasks such as test configuration selection/generation, test configuration deployment planning, creating the test runs schedule, choosing strategies to mitigate the risk of interferences, etc. The manual design of a test plan is tedious and error prone. This task becomes harder especially when the systems are large and complex. In this paper we propose an approach for automating test plans generation. With this approach we aim at reducing service disruption that may be induced by the testing activities in production. We illustrate our approach with a case study and discuss its different aspects.

</details>


### [6] [Vendor-Aware Industrial Agents: RAG-Enhanced LLMs for Secure On-Premise PLC Code Generation](https://arxiv.org/abs/2511.09122)
*Joschka Kersting,Michael Rummel,Gesa Benndorf*

Main category: cs.SE

TL;DR: 开发了一个用于工业PLC编程的低数据领域编码助手，通过RAG、多模型竞争和即时编译验证，在无需微调大模型的情况下实现高质量代码生成。


<details>
  <summary>Details</summary>
Motivation: PLC编程使用专有代码方言，难以训练编码助手；现有LLM不了解特定功能块和项目代码；工业客户不信任云提供商，需要本地化解决方案。

Method: 采用检索增强生成(RAG)方法，通过精心设计的提示工程和定向检索，让多个AI模型竞争，自动纠错并在聊天界面中直接编译验证代码有效性。

Result: 评估显示该编码助手能够通过编译统计和用户评分验证其有效性，在低数据领域中实现高质量代码生成。

Conclusion: RAG支持的编码助手结合提示工程和定向检索，可以在低数据工业领域中有效工作，无需微调大型模型。

Abstract: Programmable Logic Controllers are operated by proprietary code dialects; this makes it challenging to train coding assistants. Current LLMs are trained on large code datasets and are capable of writing IEC 61131-3 compatible code out of the box, but they neither know specific function blocks, nor related project code. Moreover, companies like Mitsubishi Electric and their customers do not trust cloud providers. Hence, an own coding agent is the desired solution to cope with this. In this study, we present our work on a low-data domain coding assistant solution for industrial use. We show how we achieved high quality code generation without fine-tuning large models and by fine-tuning small local models for edge device usage. Our tool lets several AI models compete with each other, uses reasoning, corrects bugs automatically and checks code validity by compiling it directly in the chat interface. We support our approach with an extensive evaluation that comes with code compilation statistics and user ratings. We found that a Retrieval-Augmented Generation (RAG) supported coding assistant can work in low-data domains by using extensive prompt engineering and directed retrieval.

</details>


### [7] [Leveraging Self-Paced Learning for Software Vulnerability Detection](https://arxiv.org/abs/2511.09212)
*Zeru Cheng,Yanjing Yang,He Zhang,Lanxin Yang,Jinghao Hu,Jinwei Xu,Bohan Liu,Haifeng Shen*

Main category: cs.SE

TL;DR: SPLVD是一种基于自步学习的软件漏洞检测方法，通过动态选择训练数据模拟人类从易到难的学习过程，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习漏洞检测方法准确率有限，主要原因是训练数据（源代码）质量低。

Method: 提出SPLVD方法，包含专门为漏洞检测设计的数据选择器，根据训练阶段动态选择源代码，优先学习简单代码，并在每个训练周期前重新计算代码难度并更新选择器。

Result: 在三个基准数据集（包含超过239K源代码，其中25K有漏洞）上，SPLVD分别达到89.2%、68.7%和43.5%的最高F1分数；在OpenHarmony项目上达到90.9%的最高精确度。

Conclusion: SPLVD通过自步学习方法有效提升了软件漏洞检测的准确性，在实际应用中表现出色。

Abstract: Software vulnerabilities are major risks to software systems. Recently, researchers have proposed many deep learning approaches to detect software vulnerabilities. However, their accuracy is limited in practice. One of the main causes is low-quality training data (i.e., source code). To this end, we propose a new approach: SPLVD (Self-Paced Learning for Software Vulnerability Detection). SPLVD dynamically selects source code for model training based on the stage of training, which simulates the human learning process progressing from easy to hard. SPLVD has a data selector that is specifically designed for the vulnerability detection task, which enables it to prioritize the learning of easy source code. Before each training epoch, SPLVD uses the data selector to recalculate the difficulty of the source code, select new training source code, and update the data selector. When evaluating SPLVD, we first use three benchmark datasets with over 239K source code in which 25K are vulnerable for standard evaluations. Experimental results demonstrate that SPLVD achieves the highest F1 of 89.2%, 68.7%, and 43.5%, respectively, outperforming the state-of-the-art approaches. Then we collect projects from OpenHarmony, a new ecosystem that has not been learned by general LLMs, to evaluate SPLVD further. SPLVD achieves the highest precision of 90.9%, demonstrating its practical effectiveness.

</details>


### [8] [AILINKPREVIEWER: Enhancing Code Reviews with LLM-Powered Link Previews](https://arxiv.org/abs/2511.09223)
*Panya Trakoolgerntong,Tao Xiao,Masanari Kondo,Chaiyong Ragkhitwetsagul,Morakot Choetkiertikul,Pattaraporn Sangaroonsilp,Yasutaka Kamei*

Main category: cs.SE

TL;DR: AILINKPREVIEWER工具使用LLM生成PR中链接的预览，通过结合PR元数据提供更丰富的上下文信息，提升代码审查效率。


<details>
  <summary>Details</summary>
Motivation: 代码审查中链接通常被忽略，限制了自动化任务的丰富性，增加了开发者的认知负担。

Method: 分析50个GitHub仓库，比较三种方法：上下文LLM摘要、非上下文LLM摘要和基于元数据的预览。

Result: 上下文摘要在BLEU、BERTScore等指标上表现更好，但用户研究显示多数参与者更喜欢非上下文摘要。

Conclusion: LLM驱动的链接预览有潜力提升代码审查效率，但在指标性能和用户偏好之间存在权衡。

Abstract: Code review is a key practice in software engineering, where developers evaluate code changes to ensure quality and maintainability. Links to issues and external resources are often included in Pull Requests (PRs) to provide additional context, yet they are typically discarded in automated tasks such as PR summarization and code review comment generation. This limits the richness of information available to reviewers and increases cognitive load by forcing context-switching. To address this gap, we present AILINKPREVIEWER, a tool that leverages Large Language Models (LLMs) to generate previews of links in PRs using PR metadata, including titles, descriptions, comments, and link body content. We analyzed 50 engineered GitHub repositories and compared three approaches: Contextual LLM summaries, Non-Contextual LLM summaries, and Metadata-based previews. The results in metrics such as BLEU, BERTScore, and compression ratio show that contextual summaries consistently outperform other methods. However, in a user study with seven participants, most preferred non-contextual summaries, suggesting a trade-off between metric performance and perceived usability. These findings demonstrate the potential of LLM-powered link previews to enhance code review efficiency and to provide richer context for developers and automation in software engineering.
  The video demo is available at https://www.youtube.com/watch?v=h2qH4RtrB3E, and the tool and its source code can be found at https://github.com/c4rtune/AILinkPreviewer.

</details>


### [9] [Leveraging Large Language Models for Use Case Model Generation from Software Requirements](https://arxiv.org/abs/2511.09231)
*Tobias Eisenreich,Nicholas Friedlaender,Stefan Wagner*

Main category: cs.SE

TL;DR: 本研究探索使用大型语言模型辅助用例建模，通过集成开源LLM和高级提示工程技术从软件需求中提取参与者和用例，相比传统手动方法可减少60%建模时间且保持模型质量。


<details>
  <summary>Details</summary>
Motivation: 手动创建用例模型耗时费力，实践中常被跳过，需要自动化方法来提高效率。

Method: 集成开源LLM，采用高级提示工程技术从软件需求中系统提取参与者和用例。

Result: 实验显示建模时间减少60%，模型质量与传统方法相当，参与者认为该方法提供了有价值的指导。

Conclusion: LLM辅助的用例建模方法能显著提高效率，同时保持模型质量，为软件开发过程提供有效支持。

Abstract: Use case modeling employs user-centered scenarios to outline system requirements. These help to achieve consensus among relevant stakeholders. Because the manual creation of use case models is demanding and time-consuming, it is often skipped in practice. This study explores the potential of Large Language Models (LLMs) to assist in this tedious process. The proposed method integrates an open-weight LLM to systematically extract actors and use cases from software requirements with advanced prompt engineering techniques. The method is evaluated using an exploratory study conducted with five professional software engineers, which compares traditional manual modeling to the proposed LLM-based approach. The results show a substantial acceleration, reducing the modeling time by 60\%. At the same time, the model quality remains on par. Besides improving the modeling efficiency, the participants indicated that the method provided valuable guidance in the process.

</details>


### [10] [Decoding the Configuration of AI Coding Agents: Insights from Claude Code Projects](https://arxiv.org/abs/2511.09268)
*Helio Victor F. Santos,Vitor Costa,Joao Eduardo Montandon,Marco Tulio Valente*

Main category: cs.SE

TL;DR: 对Claude Code配置文件的实证研究，分析了328个配置文件的结构和内容，重点关注软件工程关注点和实践规范。


<details>
  <summary>Details</summary>
Motivation: 虽然智能代码助手承诺带来前所未有的生产力提升，但其行为和效果严重依赖于定义架构约束、编码实践和工具使用策略的配置文件。然而，人们对这些配置工件的结构和内容知之甚少。

Method: 收集并分析了来自公共Claude Code项目的328个配置文件，识别了(i)它们指定的软件工程关注点和实践，以及(ii)这些关注点在单个文件中的共现情况。

Result: 结果强调了在智能体配置文件中定义广泛关注点和实践的重要性，特别是指定智能体应遵循的架构。

Conclusion: 配置文件中需要定义广泛的关注点和实践规范，其中架构规范的指定尤为重要。

Abstract: Agentic code assistants are a new generation of AI systems capable of performing end-to-end software engineering tasks. While these systems promise unprecedented productivity gains, their behavior and effectiveness depend heavily on configuration files that define architectural constraints, coding practices, and tool usage policies. However, little is known about the structure and content of these configuration artifacts. This paper presents an empirical study of the configuration ecosystem of Claude Code, one of the most widely used agentic coding systems. We collected and analyzed 328 configuration files from public Claude Code projects to identify (i) the software engineering concerns and practices they specify and (ii) how these concerns co-occur within individual files. The results highlight the importance of defining a wide range of concerns and practices in agent configuration files, with particular emphasis on specifying the architecture the agent should follow.

</details>


### [11] [Routesplain: Towards Faithful and Intervenable Routing for Software-related Tasks](https://arxiv.org/abs/2511.09373)
*Adam Štorek,Vikas Upadhyay,Marianne Menglin Liu,Daniel W. Peterson,Anshul Mittal,Sujeeth Bharadwaj,Fahad Shah,Dan Roth*

Main category: cs.SE

TL;DR: Routesplain是首个针对软件相关任务的LLM路由器，通过提取可解释概念进行路由决策，在准确性和成本方面优于单个模型，并达到或超过所有黑盒基线。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在软件任务中性能差异显著，但之前的路由工作主要关注通用目的的黑盒模型路由。需要专门针对软件相关任务的智能路由系统来提高响应质量并降低成本。

Method: Routesplain首先从每个查询中提取人类可解释的概念（如任务类型、领域、推理复杂度），然后仅基于这些概念进行路由决策，提供可理解且可信的推理过程。

Result: 在16个最先进的LLM和8个软件相关任务上的评估显示，Routesplain在准确性和成本方面都优于单个模型，并且等于或超过了所有黑盒基线方法。

Conclusion: Routesplain证明了基于概念的路由在软件任务中的有效性，概念级干预为进一步改进路由器提供了途径。

Abstract: LLMs now tackle a wide range of software-related tasks, yet we show that their performance varies markedly both across and within these tasks. Routing user queries to the appropriate LLMs can therefore help improve response quality while reducing cost. Prior work, however, has focused mainly on general-purpose LLM routing via black-box models. We introduce Routesplain, the first LLM router for software-related tasks, including multilingual code generation and repair, input/output prediction, and computer science QA. Unlike existing routing approaches, Routesplain first extracts human-interpretable concepts from each query (e.g., task, domain, reasoning complexity) and only routes based on these concepts, thereby providing intelligible, faithful rationales. We evaluate Routesplain on 16 state-of-the-art LLMs across eight software-related tasks; Routesplain outperforms individual models both in terms of accuracy and cost, and equals or surpasses all black-box baselines, with concept-level intervention highlighting avenues for further router improvements.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [12] [Soteria: Efficient Symbolic Execution as a Functional Library](https://arxiv.org/abs/2511.08729)
*Sacha-Élie Ayoun,Opale Sjöstedt,Azalea Raad*

Main category: cs.PL

TL;DR: Soteria是一个轻量级OCaml库，用于直接为源语言构建符号执行引擎，避免了中间语言的性能、准确性和功能支持方面的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有符号执行工具依赖中间语言来支持多种编程语言，但在实践中存在性能、准确性和语言特性支持之间的权衡。作者认为直接为每种源语言构建符号执行引擎更简单有效。

Method: 开发Soteria库，采用函数式编程风格，支持直接基于源语言语义构建符号执行引擎，提供可配置性、组合推理和易于实现的特性。

Result: 使用Soteria开发了Soteria^Rust（首个支持Rust Tree Borrows别名模型的符号执行引擎）和Soteria^C（组合式C语言符号执行引擎），在性能和bug检测数量上优于或与Kani、Pulse、CBMC和Gillian-C等最先进工具相当。

Conclusion: 证明了无需中间语言的妥协，也能实现高效、准确、表达性强的符号执行，并形式化了Soteria的理论基础并证明了其正确性。

Abstract: Symbolic execution (SE) tools often rely on intermediate languages (ILs) to support multiple programming languages, promising reusability and efficiency. In practice, this approach introduces trade-offs between performance, accuracy, and language feature support. We argue that building SE engines \emph{directly} for each source language is both simpler and more effective. We present Soteria, a lightweight OCaml library for writing SE engines in a functional style, without compromising on performance, accuracy or feature support. Soteria enables developers to construct SE engines that operate directly over source-language semantics, offering \emph{configurability}, compositional reasoning, and ease of implementation. Using Soteria, we develop Soteria$^{\text{Rust}}$, the \emph{first} Rust SE engine supporting Tree Borrows (the intricate aliasing model of Rust), and Soteria$^{\text{C}}$, a compositional SE engine for C. Both tools are competitive with or outperform state-of-the-art tools such as Kani, Pulse, CBMC and Gillian-C in performance and the number of bugs detected. We formalise the theoretical foundations of Soteria and prove its soundness, demonstrating that sound, efficient, accurate, and expressive SE can be achieved without the compromises of ILs.

</details>


### [13] [Galois Slicing as Automatic Differentiation](https://arxiv.org/abs/2511.09203)
*Robert Atkey,Roly Perera*

Main category: cs.PL

TL;DR: 本文通过类比Galois切片与可微编程，使用CHAD自动微分方法重新表述Galois切片，探索其在定量区间分析中的扩展应用。


<details>
  <summary>Details</summary>
Motivation: 探索Galois切片与可微编程之间的类比关系，将前向和后向切片实现视为一种自动微分，以澄清现有方法中的隐式选择。

Method: 使用Vákár等人的CHAD自动微分方法，通过范畴语义重新表述Galois切片技术。

Result: 成功将Galois切片重新表述为范畴语义形式，并探索了该方法在定量区间分析中的扩展应用。

Conclusion: 通过CHAD方法重新表述Galois切片，不仅澄清了现有方法中的选择，还为扩展到定量区间分析提供了理论基础。

Abstract: Galois slicing is a technique for program slicing for provenance, developed by Perera and collaborators. Galois slicing aims to explain program executions by demonstrating how to track approximations of the input and output forwards and backwards along a particular execution. In this paper, we explore an analogy between Galois slicing and differentiable programming, seeing the implementation of forwards and backwards slicing as a kind of automatic differentiation. Using the CHAD approach to automatic differentiation due to Vákár and collaborators, we reformulate Galois slicing via a categorical semantics. In doing so, we are able to explore extensions of the Galois slicing idea to quantitative interval analysis, and to clarify the implicit choices made in existing instantiations of this approach.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [14] [Tractable Weighted First-Order Model Counting with Bounded Treewidth Binary Evidence](https://arxiv.org/abs/2511.09174)
*Václav Kůla,Qipeng Kuang,Yuyi Wang,Yuanhong Wang,Ondřej Kuželka*

Main category: cs.LO

TL;DR: 本文提出了一种在领域大小上多项式时间计算两变量逻辑片段（FO²和C²）在二元证据条件下的加权一阶模型计数（WFOMC）算法，要求证据的Gaifman图具有有界树宽。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，即使对于原本在无证据情况下可处理的逻辑片段，在证据条件下计算WFOMC在领域大小上也是多项式时间不可行的（除非#P ⊆ FP）。本文旨在通过限制二元证据的图结构来突破这一障碍。

Method: 通过限制二元证据的Gaifman图具有有界树宽，开发了一种多项式时间算法来计算FO²和C²逻辑片段在证据条件下的WFOMC。

Result: 成功实现了在领域大小上多项式时间计算有界树宽二元证据条件下的WFOMC，并解决了有界度有界树宽图上的稳定座位安排问题这一开放问题。实验表明算法相比现有模型计数求解器具有更好的可扩展性。

Conclusion: 通过限制证据的图结构特性（有界树宽），可以有效解决证据条件下WFOMC的计算复杂性障碍，为组合问题的求解提供了新的有效方法。

Abstract: The Weighted First-Order Model Counting Problem (WFOMC) asks to compute the weighted sum of models of a given first-order logic sentence over a given domain. Conditioning WFOMC on evidence -- fixing the truth values of a set of ground literals -- has been shown impossible in time polynomial in the domain size (unless $\mathsf{\#P \subseteq FP}$) even for fragments of logic that are otherwise tractable for WFOMC without evidence. In this work, we address the barrier by restricting the binary evidence to the case where the underlying Gaifman graph has bounded treewidth. We present a polynomial-time algorithm in the domain size for computing WFOMC for the two-variable fragments $\text{FO}^2$ and $\text{C}^2$ conditioned on such binary evidence. Furthermore, we show the applicability of our algorithm in combinatorial problems by solving the stable seating arrangement problem on bounded-treewidth graphs of bounded degree, which was an open problem. We also conducted experiments to show the scalability of our algorithm compared to the existing model counting solvers.

</details>
