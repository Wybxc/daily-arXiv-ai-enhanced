<div id=toc></div>

# Table of Contents

- [cs.LO](#cs.LO) [Total: 8]
- [cs.PL](#cs.PL) [Total: 8]
- [cs.FL](#cs.FL) [Total: 2]
- [cs.SE](#cs.SE) [Total: 24]


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [1] [ReVEAL: GNN-Guided Reverse Engineering for Formal Verification of Optimized Multipliers](https://arxiv.org/abs/2512.22260)
*Chen Chen,Daniela Kaufmann,Chenhui Deng,Zhan Song,Hongce Zhang,Cunxi Yu*

Main category: cs.LO

TL;DR: ReVEAL：基于图学习的乘法器架构逆向工程方法，用于改进代数电路验证技术


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的方法在处理大规模优化乘法器时存在可扩展性和准确性限制，需要更强大的架构识别技术来改进代数电路验证

Method: 基于图学习的方法，利用结构图特征和学习驱动推理来识别架构模式，能够大规模处理优化乘法器

Result: 在多样化乘法器基准测试中展示应用性，相比传统基于规则方法，在可扩展性和准确性方面有改进

Conclusion: ReVEAL能够与现有验证流程无缝集成，支持下游代数证明策略，为乘法器验证提供了更强大的逆向工程能力

Abstract: We present ReVEAL, a graph-learning-based method for reverse engineering of multiplier architectures to improve algebraic circuit verification techniques. Our framework leverages structural graph features and learning-driven inference to identify architecture patterns at scale, enabling robust handling of large optimized multipliers. We demonstrate applicability across diverse multiplier benchmarks and show improvements in scalability and accuracy compared to traditional rule-based approaches. The method integrates smoothly with existing verification flows and supports downstream algebraic proof strategies.

</details>


### [2] [A Representation of Explicit Knowledge and Epistemic Indistinguishability in a Logic of Awareness](https://arxiv.org/abs/2512.22477)
*Yudai Kubono,Satoshi Tojo*

Main category: cs.LO

TL;DR: 本文针对Fagin和Halpern的认知逻辑框架中显式知识定义的问题，提出了基于认知不可区分性的新逻辑AIL，通过区分认知的隐式知识和显式知识来解决逻辑全知问题。


<details>
  <summary>Details</summary>
Motivation: Fagin和Halpern的认知逻辑框架通过引入认知概念来区分显式知识和隐式知识，但他们的定义在应用假言推理时可能产生不能被视为显式知识的不合理命题。因此需要改进显式知识的定义。

Method: 基于认知的不可区分性，重新定义显式知识：显式知识必须是认知的且隐式的，但认知的隐式知识不一定是显式的。使用初等几何的例子说明不同学生因对数学事实的认知不同而得到不同结果。形式化提出了AIL逻辑的语法和语义。

Result: 证明了AIL比Fagin和Halpern的逻辑具有更强的表达能力，并且后者可以嵌入到AIL中。为AIL提供了公理系统，并证明了其可靠性和完备性。

Conclusion: 提出的AIL逻辑框架改进了显式知识的定义，解决了原有框架中的问题，为认知逻辑提供了更精确的形式化工具。

Abstract: The logic of awareness, first proposed by Fagin and Halpern, addressed the problem of logical omniscience by introducing the notion of awareness and distinguishing explicit knowledge from implicit knowledge. In their framework, explicit knowledge was defined as the conjunction of implicit knowledge and awareness, each of which was represented by modal operators. Their definition, however, may derive undesirable propositions that cannot be considered explicit knowledge when Modus Ponens is applied within implicit knowledge. Hence, focusing on indistinguishability among possible worlds, dependent on awareness, we refine the definition of explicit knowledge. In our semantics, we require that the aware implicit knowledge is not necessarily explicit knowledge, though explicit knowledge must be aware as well as implicit. We employ an example of elementary geometry, where different students may or may not reach the final answer, depending on whether they are aware of learned mathematical facts. Thereafter, we formally present the syntax and the semantics of our language, named Awareness-Based Indistinguishability Logic ($\mathrm{AIL}$). We prove that $\mathrm{AIL}$ has more expressive power than the logic of Fagin and Halpern, and show that the latter is embeddable in $\mathrm{AIL}$. Furthermore, we provide an axiomatic system of $\mathrm{AIL}$ and prove its soundness and completeness.

</details>


### [3] [Verifying Asynchronous Hyperproperties in Reactive Systems](https://arxiv.org/abs/2512.23344)
*Raven Beutner,Bernd Finkbeiner*

Main category: cs.LO

TL;DR: 提出基于博弈的方法验证异步HyperLTL（A-HLTL）的∀*∃*公式，解决异步超属性模型检测问题


<details>
  <summary>Details</summary>
Motivation: 现有HyperLTL等逻辑只能同步比较执行轨迹，无法表达异步超属性（如观测确定性、非推断、停顿精化等），需要研究异步HyperLTL的模型检测方法

Method: 提出博弈方法，将验证视为验证者和反驳者之间的博弈，双方分别控制轨迹和停顿部分，验证者的获胜策略对应存在量化轨迹的具体证据和异步对齐

Result: 识别出博弈解释完备的片段，从而构成有限状态决策过程，为任意∀*∃* A-HLTL公式提供验证方法

Conclusion: 基于博弈的方法能够处理异步HyperLTL的模型检测问题，特别是∀*∃*公式，为异步超属性验证提供了新途径

Abstract: Hyperproperties are system properties that relate multiple execution traces and commonly occur when specifying information-flow and security policies. Logics like HyperLTL utilize explicit quantification over execution traces to express temporal hyperproperties in reactive systems, i.e., hyperproperties that reason about the temporal behavior along infinite executions. An often unwanted side-effect of such logics is that they compare the quantified traces synchronously. This prohibits the logics from expressing properties that compare multiple traces asynchronously, such as Zdancewic and Myers's observational determinism, McLean's non-inference, or stuttering refinement. We study the model-checking problem for a variant of asynchronous HyperLTL (A-HLTL), a temporal logic that can express hyperproperties where multiple traces are compared across timesteps. In addition to quantifying over system traces, A-HLTL features secondary quantification over stutterings of these traces. Consequently, A-HLTL allows for a succinct specification of many widely used asynchronous hyperproperties. Model-checking A-HLTL requires finding suitable stutterings, which, thus far, has been only possible for very restricted fragments or terminating systems. In this paper, we propose a novel game-based approach for the verification of arbitrary $\forall^*\exists^*$ A-HLTL formulas in reactive systems. In our method, we consider the verification as a game played between a verifier and a refuter, who challenge each other by controlling parts of the underlying traces and stutterings. A winning strategy for the verifier then corresponds to concrete witnesses for existentially quantified traces and asynchronous alignments for existentially quantified stutterings. We identify fragments for which our game-based interpretation is complete and thus constitutes a finite-state decision procedure.

</details>


### [4] [Many-valued coalgebraic dynamic logics: Safety and strong completeness via reducibility](https://arxiv.org/abs/2512.22851)
*Helle Hvid Hansen,Wolfgang Poiger*

Main category: cs.LO

TL;DR: 提出一个基于余代数的框架，用于研究动态模态逻辑（如PDL和博弈逻辑）的泛化，其中命题和语义结构都可以在真值代数A中取值。


<details>
  <summary>Details</summary>
Motivation: 现有动态模态逻辑（如PDL和博弈逻辑）通常基于经典二值逻辑。本文旨在将其扩展到多值/模糊逻辑框架中，允许命题和语义结构在更一般的真值代数中取值，从而处理不确定性、模糊性等现实场景。

Method: 采用余代数模态逻辑方法，通过A值谓词提升（其中A是FL_ew代数）来建模。将动作（抽象程序和博弈）表示为F-余代数，其中函子F表示某种类型的A加权系统。引入余代数操作和测试，特别关注可约化操作（复合动作的模态可以约化为组成动作的模态组合）。

Result: 证明了可约化操作对于互模拟和行为等价是安全的，并建立了通用的强完备性结果。由此获得了两个新结果：1）当A是有限链时，具有A值可达关系的二值无迭代PDL的强完备性；2）基于有限Lukasiewicz逻辑的多值无迭代博弈逻辑与多值策略的强完备性。

Conclusion: 提出的余代数框架成功地将动态模态逻辑扩展到多值设置，为处理模糊和不确定性的程序与博弈逻辑提供了理论基础，并通过可约化操作确保了良好的逻辑性质。

Abstract: We present a coalgebraic framework for studying generalisations of dynamic modal logics such as PDL and game logic in which both the propositions and the semantic structures can take values in an algebra $\mathbf{A}$ of truth-degrees. More precisely, we work with coalgebraic modal logic via $\mathbf{A}$-valued predicate liftings where $\mathbf{A}$ is a $\mathsf{FL}_{\mathrm{ew}}$-algebra, and interpret actions (abstracting programs and games) as $\mathsf{F}$-coalgebras where the functor $\mathsf{F}$ represents some type of $\mathbf{A}$-weighted system. We also allow combinations of crisp propositions with $\mathbf{A}$-weighted systems and vice versa. We introduce coalgebra operations and tests, with a focus on operations that are reducible in the sense that modalities for composed actions can be reduced to compositions of modalities for the constituent actions. We prove that reducible operations are safe for bisimulation and behavioural equivalence, and prove a general strong completeness result, from which we obtain new strong completeness results for $2$-valued iteration-free PDL with $\mathbf{A}$-valued accessibility relations when $\mathbf{A}$ is a finite chain, and for many-valued iteration-free game logic with many-valued strategies based on finite Lukasiewicz logic.

</details>


### [5] [PSPACE-Completeness of the Equational Theory of Relational Kleene Algebra with Graph Loop](https://arxiv.org/abs/2512.22930)
*Yoshiki Nakamura*

Main category: cs.LO

TL;DR: 本文证明了带图循环的关系Kleene代数的等式理论是PSpace完全的，并扩展到包含顶元、测试、逆和命名元的情况。同时发现带域的关系KAT等式理论是PSpace完全的，而带反域的关系KAT等式理论是ExpTime完全的。


<details>
  <summary>Details</summary>
Motivation: 研究关系Kleene代数及其扩展的等式理论的复杂度，特别是图循环算子对复杂度的影响，以及不同扩展（域与反域）导致的复杂度差异。

Method: 引入新的循环自动机模型，该模型在NFA基础上增加了测试当前顶点是否有循环的转移类型。通过该模型，将上述等式理论问题多项式时间归约到双向交替字符串自动机的语言包含问题。

Result: 1) 带图循环的关系Kleene代数的等式理论是PSpace完全的；2) 该结果扩展到包含顶元、测试、逆和命名元时仍然成立；3) 带域的关系KAT等式理论是PSpace完全的；4) 带反域的关系KAT等式理论是ExpTime完全的。

Conclusion: 图循环算子不会增加关系Kleene代数等式理论的复杂度（保持PSpace完全），但域与反域算子对关系KAT等式理论的复杂度影响不同：域算子保持PSpace完全，而反域算子将复杂度提升到ExpTime完全。

Abstract: In this paper, we show that the equational theory of relational Kleene algebra with \emph{graph loop} is PSpace-complete. Here, the graph loop is the unary operator that restricts a binary relation to the identity relation. We further show that this PSpace-completeness still holds by extending with top, tests, converse, and nominals. Notably, we also obtain that for Kleene algebra with tests (KAT), the equational theory of relational KAT with domain is PSpace-complete, whereas the equational theory of relational KAT with antidomain is ExpTime-complete.
  To this end, we introduce a novel automaton model on relational structures, named \emph{loop-automata}. Loop-automata are obtained from non-deterministic finite string automata (NFA) by adding a transition type that tests whether the current vertex has a loop. Using this model, we give a polynomial-time reduction from the above equational theories to the language inclusion problem for 2-way alternating string automata.

</details>


### [6] [Hypergraph Semantics for Doxastic Logics](https://arxiv.org/abs/2512.23088)
*Hans van Ditmarsch,Djanira Gomes,David Lehnherr,Valentin Müller,Thomas Studer*

Main category: cs.LO

TL;DR: 提出基于有向超图的新信念逻辑语义，扩展单纯形模型以处理信念而不仅仅是知识


<details>
  <summary>Details</summary>
Motivation: 现有单纯形模型只能处理知识，无法处理信念，需要新的语义模型来同时处理知识和信念

Method: 使用有向超图（可连接多个顶点的有向边）作为语义基础，构建信念逻辑系统，包括一致信念和仅内省信念

Result: 建立了有向超图模型的完备公理化系统，并提供了信念克里普克模型与有向超图模型之间的直接转换方法

Conclusion: 有向超图模型成功扩展了单纯形模型，既能保持知识逻辑特征，又能处理信念，为分布式计算中的信念建模提供了新工具

Abstract: Simplicial models have become a crucial tool for studying distributed computing. These models, however, are only able to account for the knowledge, but not for the beliefs of agents. We present a new semantics for logics of belief. Our semantics is based on directed hypergraphs, a generalization of ordinary directed graphs in which edges are able to connect more than two vertices. Directed hypergraph models preserve the characteristic features of simplicial models for epistemic logic, while also being able to account for the beliefs of agents. We provide systems of both consistent belief and merely introspective belief. The completeness of our axiomatizations is established by the construction of canonical hypergraph models. We also present direct conversions between doxastic Kripke models and directed hypergraph models.

</details>


### [7] [Checking Satisfiability of Hyperproperties using First-Order Logic](https://arxiv.org/abs/2512.23332)
*Raven Beutner,Bernd Finkbeiner*

Main category: cs.LO

TL;DR: FOLHyper是一个自动化工具，用于检查HyperLTL指定的超属性可满足性，通过将问题约简为等可满足的一阶逻辑公式，利用FOL求解器进行分析。


<details>
  <summary>Details</summary>
Motivation: 超属性涉及多个执行轨迹，在安全性和信息流属性规范中常见。检查超属性可满足性有重要应用，如测试安全属性是否矛盾、分析信息流策略间的蕴含和等价关系。

Method: FOLHyper将HyperLTL超属性的可满足性问题约简为等可满足的一阶逻辑公式，然后利用一阶逻辑求解器进行分析。该方法超越了HyperLTL的可判定∃*∀*片段。

Result: 实验表明FOLHyper在证明公式不可满足性方面特别有效，能够补充现有的有界可满足性方法。

Conclusion: FOLHyper通过利用一阶逻辑求解器，为超属性可满足性检查提供了强大工具，特别适用于不可满足性证明，扩展了可分析的HyperLTL公式范围。

Abstract: Hyperproperties are system properties that relate multiple execution traces and occur, e.g., when specifying security and information-flow properties. Checking if a hyperproperty is satisfiable has many important applications, such as testing if some security property is contradictory, or analyzing implications and equivalences between information-flow policies. In this paper, we present FOLHyper, a tool that can automatically check satisfiability of hyperproperties specified in the temporal logic HyperLTL. FOLHyper reduces the problem to an equisatisfiable first-order logic (FOL) formula, which allows us to leverage FOL solvers for the analysis of hyperproperties. As such, FOLHyper is applicable to many formulas beyond the decidable $\exists^*\forall^*$ fragment of HyperLTL. Our experiments show that FOLHyper is particularly useful for proving that a formula is unsatisfiable, and complements existing bounded approaches to satisfiability.

</details>


### [8] [Modelling of logical systems by means of their fragments](https://arxiv.org/abs/2512.23509)
*Mikhail Rybakov*

Main category: cs.LO

TL;DR: 论文研究了非经典逻辑的算法复杂性，发现命题逻辑通常可多项式时间归约到最多两个变量的片段，谓词逻辑通常可归约到只有1-2个一元谓词字母和2-3个体变量的片段。


<details>
  <summary>Details</summary>
Motivation: 研究非经典逻辑（特别是超直觉主义和模态系统）的算法复杂性，探索逻辑系统与其简化片段之间的可归约性关系。

Method: 通过建立多项式时间归约关系，分析命题逻辑到其变量受限片段的归约条件，以及谓词逻辑到谓词字母和变量受限片段的归约条件，并提供归约失败的反例。

Result: 证明了命题逻辑通常可归约到最多两个变量的片段（有时甚至是一个变量或无变量片段），谓词逻辑通常可归约到只有1-2个一元谓词字母和2-3个体变量的片段，并给出了多个逻辑的新复杂性界限。

Conclusion: 非经典逻辑系统通常具有比预期更强的可归约性，许多复杂逻辑可以简化到非常受限的片段而不损失表达能力，这为算法复杂性分析和逻辑系统设计提供了重要见解。

Abstract: This work investigates the algorithmic complexity of non-classical logics, focusing on superintuitionistic and modal systems. It is shown that propositional logics are usually polynomial-time reducible to their fragments with at most two variables (often to the one-variable or even variable-free fragments). Also, it is proved that predicate logics are usually reducible to their fragments with one or two unary predicate letters and two or three individual variables. The work describes conditions sufficient for such reductions and provides examples where they fail, establishing non-reducibility in those cases. Furthermore, the work provides new complexity bounds for several logics, results on Kripke-incompleteness of predicate calculi, and analogues of the classical theorems of Church and Trakhtenbrot for the logic of quasiary predicates.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [9] [Symbolic Specification and Reasoning for Quantum Data and Operations](https://arxiv.org/abs/2512.22383)
*Mingsheng Ying*

Main category: cs.PL

TL;DR: 提出符号算子逻辑SOL作为量子计算的形式化验证框架，将经典一阶逻辑嵌入到量子算子语言中，实现量子数据和操作的符号化规范与推理。


<details>
  <summary>Details</summary>
Motivation: 量子计算领域缺乏符号化规范与推理的形式化理论，限制了自动化验证技术的实际应用。需要建立统一框架来支持量子算法和程序的自动化验证。

Method: 提出符号算子逻辑SOL框架，将经典一阶逻辑语言嵌入到形式算子语言中，用于规范量子数据和操作（包括递归定义）。通过底层经典数据理论（如布尔代数或群论）进行推理。

Result: 建立了统一的符号化规范与推理框架SOL，使量子计算验证能够利用现有的经典计算自动化验证工具，为形式化验证和自动定理证明提供概念基础。

Conclusion: SOL框架为量子计算的形式化验证和自动定理证明提供了概念基础，有望在Lean、Coq等证明助手中支持量子计算和信息的验证工作。

Abstract: In quantum information and computation research, symbolic methods have been widely used for human specification and reasoning about quantum states and operations. At the same time, they are essential for ensuring the scalability and efficiency of automated reasoning and verification tools for quantum algorithms and programs. However, a formal theory for symbolic specification and reasoning about quantum data and operations is still lacking, which significantly limits the practical applicability of automated verification techniques in quantum computing.
  In this paper, we present a general logical framework, called Symbolic Operator Logic $\mathbf{SOL}$, which enables symbolic specification and reasoning about quantum data and operations. Within this framework, a classical first-order logical language is embedded into a language of formal operators used to specify quantum data and operations, including their recursive definitions. This embedding allows reasoning about their properties modulo a chosen theory of the underlying classical data (e.g., Boolean algebra or group theory), thereby leveraging existing automated verification tools developed for classical computing. It should be emphasised that this embedding of classical first-order logic into $\mathbf{SOL}$ is precisely what makes the symbolic method possible.
  We envision that this framework can provide a conceptual foundation for the formal verification and automated theorem proving of quantum computation and information in proof assistants such as Lean, Coq, and related systems.

</details>


### [10] [Eliminate Branches by Melding IR Instructions](https://arxiv.org/abs/2512.22390)
*Yuze Li,Srinivasan Ramachandra Sharma,Charitha Saumya,Ali R. Butt,Kirshanthan Sundararajah*

Main category: cs.PL

TL;DR: MERIT是一种编译器转换技术，通过对齐和融合不同分支路径中结构相似的操作来消除分支，避免分支预测错误带来的性能损失。


<details>
  <summary>Details</summary>
Motivation: 现代处理器中分支预测错误会导致严重的性能损失。虽然存在硬件预测器和配置文件引导技术，但对于具有不规则模式的数据依赖分支仍然具有挑战性。传统的if-conversion通过软件谓词消除分支，但在x86等架构上存在局限性，经常在包含内存指令的路径上失败，或者因完全推测大型分支体而产生过多的指令开销。

Method: MERIT是一种编译器转换，通过观察不同分支路径通常执行结构相似但操作数不同的操作，采用序列对齐技术发现合并机会，并使用安全的操作数级保护来确保语义正确性，无需硬件谓词。该方法在IR指令级别对齐和融合相似操作。

Result: 在四个基准测试套件的102个程序上实现为LLVM pass进行评估，MERIT实现了10.9%的几何平均加速比，与硬件分支预测器相比峰值改进达到32倍，同时减少了静态指令开销。

Conclusion: MERIT通过编译器级别的分支消除技术有效解决了数据依赖分支的性能问题，在保持语义正确性的同时显著提升了程序性能，证明了该方法在减少分支预测错误方面的有效性。

Abstract: Branch mispredictions cause catastrophic performance penalties in modern processors, leading to performance loss. While hardware predictors and profile-guided techniques exist, data-dependent branches with irregular patterns remain challenging. Traditional if-conversion eliminates branches via software predication but faces limitations on architectures like x86. It often fails on paths containing memory instructions or incurs excessive instruction overhead by fully speculating large branch bodies.
  This paper presents Melding IR Instructions (MERIT), a compiler transformation that eliminates branches by aligning and melding similar operations from divergent paths at the IR instruction level. By observing that divergent paths often perform structurally similar operations with different operands, MERIT adapts sequence alignment to discover merging opportunities and employs safe operand-level guarding to ensure semantic correctness without hardware predication. Implemented as an LLVM pass and evaluated on 102 programs from four benchmark suites, MERIT achieves a geometric mean speedup of 10.9% with peak improvements of 32x compared to hardware branch predictor, demonstrating the effectiveness with reduced static instruction overhead.

</details>


### [11] [A Bounded Game Semantics Checker for Precise Smart Contract Analysis](https://arxiv.org/abs/2512.22417)
*Vasileios Koutavas,Yu-Yang Lin,Nikos Tzevelekos*

Main category: cs.PL

TL;DR: 提出基于博弈语义的智能合约漏洞检测方法YulToolkit，实现无假阳性、边界完备的精确分析，可扩展到真实合约。


<details>
  <summary>Details</summary>
Motivation: 智能合约漏洞检测需要精确性（无假阳性）和可扩展性，现有方法难以同时满足这两个要求，特别是对于重入等复杂漏洞。

Method: 基于博弈语义建模合约与环境交互，将未知或恶意外部合约的推理简化为轨迹枚举，实现YulToolkit工具，支持Solidity编写的检测器并传播到Yul中间语言。

Result: YulToolkit在DAO、PredyPool、Lendf.Me等真实漏洞案例中成功检测到已知漏洞（生成触发违规的轨迹），修复后无违规报告，在基准合约上表现良好。

Conclusion: 边界博弈语义探索是智能合约分析工具箱中有效且精确的补充方法，特别适用于重入等在真实代码中难以精确检测的漏洞类型。

Abstract: We present a new approach to finding smart contract vulnerabilities that is precise (no false positives up to our EVM-Yul interpreter), bounded-complete, and, when instrumented with domain knowledge, scales to real-world contracts. Our method is based on game semantics, modelling computation as an interaction between a contract and its environment, reducing reasoning about unknown or malicious external contracts to trace enumeration. We implement this in a tool we refer to as YulToolkit, a bounded game-semantics checker for Yul, the intermediate language of Solidity. By exploring only feasible interactions, YulToolkit avoids over-approximation, and by relying on the theory of game semantics it achieves bounded completeness. To make exploration tractable, YulToolkit supports instrumentation written in Solidity and propagated to Yul, comparable in effort to creating a test harness. Unlike tests, however, our technique explores all admissible traces within the chosen parameters and bounds. We evaluate YulToolkit on three real-world incidents: The DAO, PredyPool, and Lendf.Me, as well as benchmark contracts. In all cases, YulToolkit detects the known vulnerabilities (producing a violation-triggering trace), and after applying fixes, reports no further violations within bounds. These results show that bounded game semantics exploration is an effective and precise addition to the smart contract analysis toolbox, particularly for vulnerabilities such as reentrancy that are hard to detect precisely in real code.

</details>


### [12] [Compiling Gradual Types with Evidence](https://arxiv.org/abs/2512.22684)
*José Luis Romero,Cristóbal Isla,Matías Toro,Éric Tanter*

Main category: cs.PL

TL;DR: 本文提出并实现了基于证据的编译器GrEv，证明证据语义可以实现高效的类型渐进编译，性能可与基于强制转换的编译器竞争甚至更优。


<details>
  <summary>Details</summary>
Motivation: 在结构类型语言中高效支持渐进类型具有挑战性。虽然AGT方法论在语言设计和语义方面取得了成果，但基于证据的语义是否能实现高效实现尚不明确。

Method: 设计、实现和评估基于证据的编译器GrEv，弥合形式语义与编译器实现之间的差距，识别新颖的单调语义，并在Grift基准测试套件上进行实证评估。

Result: GrEv编译器性能可与基于强制转换的编译器竞争甚至更快，在静态到动态谱系配置上表现出更好的稳定性。

Conclusion: 证据语义是实现高效渐进类型编译的可行途径，为探索AGT文献中许多高级渐进类型系统的高效实现打开了大门。

Abstract: Efficiently supporting sound gradual typing in a language with structural types is challenging. To date, the Grift compiler is the only close-to-the-metal implementation of gradual typing in this setting, exploiting coercions for runtime checks, and further extended with monotonic references for efficient access to statically-typed data structures. On the language design and semantics side, the Abstracting Gradual Typing (AGT) methodology has proven fruitful to elucidate existing designs and to innovate by deriving gradualizations of a wide variety of typing disciplines and language features. Grounded in abstract interpretation, the Curry-Howard inspired runtime semantics of AGT is based on the notion of evidence for consistent judgments that evolve during reduction, monitoring the plausibility of well-typedness. While expressive and versatile, it is unclear whether such evidence-based semantics are a viable route to realize an efficient implementation of gradual typing.
  In this work, we explore this question by designing, implementing, and evaluating an evidence-based compiler, called GrEv. We explain how to bridge the gap between the formal semantics and the GrEv compiler implementation, and identify novel monotonic semantics. We empirically evaluate the performance of GrEv on the Grift benchmark suite. The results show that an evidence-based compiler can be competitive with, and even faster than, a coercion-based compiler, exhibiting more stability across configurations on the static-to-dynamic spectrum. In addition to enriching the space of gradual typing compilers, this work opens a direct door to exploring efficient implementations of the many advanced gradual typing disciplines formally derived with AGT in the literature.

</details>


### [13] [Fancy Some Chips for Your TeaStore? Modeling the Control of an Adaptable Discrete System](https://arxiv.org/abs/2512.23496)
*Anna Gallone,Simon Bliudze,Sophie Cerf,Olga Kouchnarenko*

Main category: cs.PL

TL;DR: Chips是一种建模语言，用于简化由各种交织组件组成的复杂系统设计，结合控制理论和通用编程语言概念，生成健壮的基于组件的模型。


<details>
  <summary>Details</summary>
Motivation: 开发者在设计新Web应用时需要处理软件、硬件、网络、在线微服务等多种资源的约束，这些实体形成复杂的通信相互依赖系统。确保系统健壮性以提供良好服务质量非常重要。

Method: 引入Chips语言，采用功能块形式描述应用，混合控制理论和通用编程语言概念，支持系统化设计、建模和分析复杂系统项目。

Result: 使用改进的Adaptable TeaStore应用作为运行示例，展示了如何使用Chips进行复杂系统项目的系统化设计、建模和分析。

Conclusion: Chips语言有助于简化复杂系统的建模过程，通过结合控制理论和编程语言概念，能够生成健壮的组件化模型，提高系统设计的质量和效率。

Abstract: When designing new web applications, developers must cope with different kinds of constraints relative to the resources they rely on: software, hardware, network, online micro-services, or any combination of the mentioned entities. Together, these entities form a complex system of communicating interdependent processes, physical or logical. It is very desirable that such system ensures its robustness to provide a good quality of service. In this paper we introduce Chips, a language that aims at facilitating the design of models made of various entwined components. It allows the description of applications in the form of functional blocks. Chips mixes notions  from control theory and general purpose programming languages to generate robust component-based models. This paper presents how to use Chips to systematically design, model and analyse a complex system project, using a variation of the Adaptable TeaStore application as running example.

</details>


### [14] [Adaptable TeaStore: A Choreographic Approach](https://arxiv.org/abs/2512.23497)
*Giuseppe De Palma,Saverio Giallorenzo,Ivan Lanese,Gianluigi Zavattaro*

Main category: cs.PL

TL;DR: 使用AIOCJ编排语言实现Adaptable TeaStore，展示动态自适应微服务架构的构建时通信正确性保证


<details>
  <summary>Details</summary>
Motivation: Adaptable TeaStore作为自适应微服务架构的参考模型，需要实现不同配置间的动态转换，同时保证通信的正确性

Method: 基于AIOCJ编排语言实现Adaptable TeaStore，利用编排编程确保构建时通信正确性（无死锁等），支持运行时动态自适应

Result: 成功展示了AIOCJ方法的优势，包括构建时通信正确性保证和运行时动态自适应能力，但也识别了当前限制

Conclusion: AIOCJ为自适应微服务架构提供了有前景的方法，但需要进一步改进以更好地与现实云架构对齐

Abstract: The Adaptable TeaStore has recently been proposed as a reference model for adaptable microservice architectures. It includes different configurations, as well as scenarios requiring to transition between them. We describe an implementation of the Adaptable TeaStore based on AIOCJ, a choreographic language that allows one to program multiparty systems that can adapt at runtime to different conditions. Following the choreographic tradition, AIOCJ ensures by-construction correctness of communications (e.g., no deadlocks) before, during, and after adaptation. Adaptation is dynamic, and the adaptation scenarios need to be fully specified only at runtime. Using AIOCJ to model the Adaptable TeaStore, we showcase the strengths of the approach and its current limitations, providing suggestions for future directions for refining the paradigm (and the AIOCJ language, in particular), to better align it with real-world Cloud architectures.

</details>


### [15] [Beyond Per-Thread Lock Sets: Multi-Thread Critical Sections and Dynamic Deadlock Prediction](https://arxiv.org/abs/2512.23552)
*Martin Sulzmann*

Main category: cs.PL

TL;DR: 提出一种改进的锁集构造方法，通过跨线程临界区概念减少死锁预测中的误报和漏报。


<details>
  <summary>Details</summary>
Motivation: 传统基于单线程临界区的锁集构造方法会忽略其他线程中的锁获取事件，导致死锁预测出现误报和漏报。

Method: 1. 提出基于轨迹的跨线程临界区定义；2. 通过偏序关系进行可靠近似；3. 改进锁集构造算法；4. 集成到SPDOffline死锁预测器中。

Result: 改进的锁集构造能够消除DIRK死锁预测器的误报，并减少SPDOffline的漏报，在标准基准测试中性能不受影响。

Conclusion: 跨线程临界区的概念能够更准确地描述锁行为，改进的锁集构造方法在保持高效计算的同时提高了死锁预测的准确性。

Abstract: Lock sets are commonly used for dynamic analysis of deadlocks. The standard per-thread lock set construction only considers locks acquired in the same thread, but is unaware of locks acquired in another thread. This leads to false positives and false negatives. The underlying issue is that the commonly used notion of a critical section on which the lock set construction relies ignores events from other threads. We give a trace-based characterization of critical sections that drops this restriction. Critical sections are no longer restricted to a single thread and can cover multiple threads. Such forms of critical sections exist, are natural, and correct the standard formulation.
  We show how to soundly approximate the trace-based characterization via partial order relations. Thus, we obtain an improved lock set construction that can still be efficiently computed and allows us to remove false positives reported by the DIRK deadlock predictor and remove false negatives by extending the SPDOffline deadlock predictor. We integrate various lock set constructions with increased precision in an extension of SPDOffline. Our extensions remain sound (no false positives) but are more complete (fewer false negatives) w.r.t. SPDOffline. For an extensive standard benchmark suite we can also show that the performance is not affected.

</details>


### [16] [Automating the Analysis of Parsing Algorithms (and other Dynamic Programs)](https://arxiv.org/abs/2512.23665)
*Tim Vieira,Ryan Cotterell,Jason Eisner*

Main category: cs.PL

TL;DR: 开发了一个帮助程序员分析NLP算法复杂度和正确性的系统，能够推断类型、死代码、冗余代码以及参数化运行时和空间复杂度边界


<details>
  <summary>Details</summary>
Motivation: NLP算法研究需要高效处理复杂形式结构，算法设计者需要为其算法提供性能保证（如运行时间、空间复杂度上界），并确定算法推导量的必要属性以合成高效数据结构和验证类型错误

Method: 开发了一个系统来帮助程序员进行这类分析，该系统能够自动推断算法属性

Result: 将该系统应用于多个NLP算法，成功推断出类型、死代码、冗余代码以及参数化运行时和空间复杂度边界

Conclusion: 开发的系统能够有效帮助程序员分析NLP算法的复杂度和正确性问题，为算法设计和优化提供自动化支持

Abstract: Much algorithmic research in NLP aims to efficiently manipulate rich formal structures. An algorithm designer typically seeks to provide guarantees about their proposed algorithm -- for example, that its running time or space complexity is upper-bounded as a certain function of its input size. They may also wish to determine the necessary properties of the quantities derived by the algorithm to synthesize efficient data structures and verify type errors. In this paper, we develop a system for helping programmers to perform these types of analyses. We apply our system to a number of NLP algorithms and find that it successfully infers types, dead and redundant code, and parametric runtime and space complexity bounds.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [17] [Spatio-Temporal view on the Topological Functioning Model](https://arxiv.org/abs/2512.22889)
*Maria Spichkova*

Main category: cs.FL

TL;DR: 提出TopFunST框架，基于扩展的拓扑功能模型语言，用于分析系统功能特征间的拓扑依赖关系，并覆盖时空维度


<details>
  <summary>Details</summary>
Motivation: 现有拓扑功能模型(TFM)语言能够分析系统功能特征间的拓扑关系、功能交互和运行周期结构，但缺乏对时空维度的支持，而时空特性对许多系统至关重要

Method: 提出TopFunST框架，基于扩展的拓扑功能模型语言，能够分析系统功能特征间的拓扑依赖关系，并特别覆盖空间和时间维度

Result: 开发了TopFunST框架，解决了传统TFM语言无法处理时空维度的问题，为系统分析提供了更全面的拓扑功能建模能力

Conclusion: TopFunST框架扩展了拓扑功能模型语言，使其能够分析系统功能特征的拓扑依赖关系，并覆盖空间和时间维度，为系统分析提供了更完整的解决方案

Abstract: The spatial and temporal aspects of system properties are crucial for many types of systems. In this short paper, we present a TopFunST framework to analyse topological dependencies among features of the system, covering also spatial and temporal aspects. TopFunM is based on an extended version of the language of Topological Functioning Models (TFM). The TFM language describes the topological relationships among functional features of the system, which allows not only to analyse system functionality, as well as feature interactions and functioning cycle structures, but spatial and temporal aspects haven't been covered yet. This paper presents a solution to this problem.

</details>


### [18] [A pumping-like lemma for languages over infinite alphabets](https://arxiv.org/abs/2512.23403)
*Yoav Danieli*

Main category: cs.FL

TL;DR: 证明了单寄存器交替有限内存自动机接受语言的泵引理，并得出此类语言中词长度的集合是半线性的结论。


<details>
  <summary>Details</summary>
Motivation: 研究单寄存器交替有限内存自动机（1R-AFMA）接受的语言的结构性质，特别是其长度集合的特性。

Method: 证明了一种针对单寄存器交替有限内存自动机接受语言的泵引理（pumping lemma），作为分析工具。

Result: 成功证明了泵引理，并作为推论得出：此类语言中所有词的长度构成的集合是半线性的。

Conclusion: 单寄存器交替有限内存自动机接受的语言具有结构良好的长度特性，其长度集合呈现半线性模式，这为理解此类自动机的计算能力提供了重要洞见。

Abstract: We prove a kind of a pumping lemma for languages accepted by one-register alternating finite-memory automata. As a corollary, we obtain that the set of lengths of words in such languages is semi-linear.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [19] [Syntax Is Not Enough: An Empirical Study of Small Transformer Models for Neural Code Repair](https://arxiv.org/abs/2512.22216)
*Shaunak Samant*

Main category: cs.SE

TL;DR: 小型Transformer模型在Java程序修复中能生成语法正确的代码，但无法产生语义正确的修复，且常直接复制错误代码


<details>
  <summary>Details</summary>
Motivation: 研究小型Transformer模型是否能有效修复真实世界的Java程序错误，以及语法正确性是否可作为语义正确性的可靠代理指标

Method: 使用CodeXGLUE中的52,364个Java bug-fix对微调CodeT5-small模型（60.5M参数），通过AST解析评估token级别性能和语法有效性

Result: 模型收敛良好且语法正确率高（约94%生成语法有效的Java代码），但在精确匹配评估中修复正确率为零，约80%情况下直接复制错误输入

Conclusion: 语法正确性不能作为语义正确性的可靠代理，小型Transformer模型虽能生成语法有效的代码，但无法有效修复真实世界的程序错误

Abstract: Automated program repair using neural models has shown promising results on benchmark datasets, yet practical deployment remains limited. In this study, we examine whether a small transformer model can meaningfully repair real-world Java bugs and whether syntactic correctness is a reliable proxy for semantic correctness.
  We fine-tune CodeT5-small (60.5M parameters) on 52,364 Java bug-fix pairs from CodeXGLUE and evaluate both token-level performance and syntactic validity using AST parsing. While the model converges cleanly and achieves high grammatical correctness, producing syntactically valid Java code in approximately ninety-four percent of cases, it fails to generate correct repairs under exact-match evaluation, achieving zero exact matches. In approximately eighty percent of cases, the model reproduces the buggy input verbatim.

</details>


### [20] [Failure Analysis of Safety Controllers in Autonomous Vehicles Under Object-Based LiDAR Attacks](https://arxiv.org/abs/2512.22244)
*Daniyal Ganiuly,Nurzhau Bolatbek,Assel Smaiyl*

Main category: cs.SE

TL;DR: 本文系统分析了在高速公路驾驶场景下，基于对象的LiDAR攻击对纵向安全控制器的影响，发现即使短暂的LiDAR诱导物体幻觉也能触发不安全制动、对真实危险响应延迟和不稳定控制行为。


<details>
  <summary>Details</summary>
Motivation: 虽然先前研究表明LiDAR感知可以通过基于对象的欺骗和注入攻击进行操纵，但这些攻击对车辆安全控制器的影响尚未得到充分理解。自动驾驶车辆依赖LiDAR感知支持自适应巡航控制和自动紧急制动等安全关键控制功能，因此需要系统分析攻击对控制器安全性的影响。

Method: 采用高保真仿真框架，集成LiDAR感知、对象跟踪和闭环车辆控制，评估虚假和位移物体检测如何通过感知-规划-控制管道传播。研究聚焦于高速公路驾驶场景中的切入和跟车情况，其中对抗性物体引入持续感知错误而不直接修改车辆控制软件。

Result: 结果表明，即使短暂的LiDAR诱导物体幻觉也能触发不安全制动、对真实危险响应延迟和不稳定控制行为。在切入场景中，与良性条件相比，观察到不安全减速事件和碰撞时间违规明显增加，尽管控制器参数相同。分析进一步显示，控制器故障更受欺骗物体时间一致性的影响，而非单纯的空间不准确性。

Conclusion: 这些发现揭示了自动驾驶系统中感知鲁棒性与控制级安全保证之间的关键差距。通过明确描述对抗感知下的安全控制器故障模式，这项工作为设计攻击感知的安全机制和更鲁棒的LiDAR依赖自动驾驶车辆控制策略提供了实用见解。

Abstract: Autonomous vehicles rely on LiDAR based perception to support safety critical control functions such as adaptive cruise control and automatic emergency braking. While previous research has shown that LiDAR perception can be manipulated through object based spoofing and injection attacks, the impact of such attacks on vehicle safety controllers is still not well understood. This paper presents a systematic failure analysis of longitudinal safety controllers under object based LiDAR attacks in highway driving scenarios. The study focuses on realistic cut in and car following situations in which adversarial objects introduce persistent perception errors without directly modifying vehicle control software. A high fidelity simulation framework integrating LiDAR perception, object tracking, and closed loop vehicle control is used to evaluate how false and displaced object detections propagate through the perception planning and control pipeline. The results demonstrate that even short duration LiDAR induced object hallucinations can trigger unsafe braking, delayed responses to real hazards, and unstable control behavior. In cut in scenarios, a clear increase in unsafe deceleration events and time to collision violations is observed when compared to benign conditions, despite identical controller parameters. The analysis further shows that controller failures are more strongly influenced by the temporal consistency of spoofed objects than by spatial inaccuracies alone. These findings reveal a critical gap between perception robustness and control level safety guarantees in autonomous driving systems. By explicitly characterizing safety controller failure modes under adversarial perception, this work provides practical insights for the design of attack aware safety mechanisms and more resilient control strategies for LiDAR dependent autonomous vehicles.

</details>


### [21] [Hallucination Detection for LLM-based Text-to-SQL Generation via Two-Stage Metamorphic Testing](https://arxiv.org/abs/2512.22250)
*Bo Yang,Yinfen Xia,Weisong Sun,Yang Liu*

Main category: cs.SE

TL;DR: SQLHD：一种基于蜕变测试的Text-to-SQL幻觉检测方法，无需标准答案即可检测LLM生成的SQL查询中的错误。


<details>
  <summary>Details</summary>
Motivation: LLM在Text-to-SQL任务中会产生幻觉（不现实或不合理的内容），导致错误的SQL查询。现有检测方法针对传统深度学习模型设计，在LLM上应用有限，主要因为缺乏真实数据。需要一种无需标准答案的检测方法。

Method: 提出SQLHD方法，基于蜕变测试，将检测任务分为两个阶段：1）模式链接幻觉检测：通过8个结构感知的蜕变关系扰动比较词、实体、句子结构或数据库模式；2）逻辑合成幻觉检测：通过9个逻辑感知的蜕变关系突变前缀词、极值表达式、比较范围或整个数据库。每个阶段单独调用LLM生成模式映射或SQL构件，通过相应蜕变关系交叉检查后续输出与源输出，任何违规都被标记为幻觉。

Result: 实验结果显示SQLHD在F1分数上表现优异，范围从69.36%到82.76%。SQLHD在Text-to-SQL任务中有效识别幻觉，性能优于LLM自评估方法。

Conclusion: SQLHD是一种有效的Text-to-SQL幻觉检测方法，无需标准答案即可检测LLM生成的SQL查询中的错误，通过两阶段蜕变测试方法在模式链接和逻辑合成层面识别幻觉。

Abstract: In Text-to-SQL generation, large language models (LLMs) have shown strong generalization and adaptability. However, LLMs sometimes generate hallucinations, i.e.,unrealistic or illogical content, which leads to incorrect SQL queries and negatively impacts downstream applications. Detecting these hallucinations is particularly challenging. Existing Text-to-SQL error detection methods, which are tailored for traditional deep learning models, face significant limitations when applied to LLMs. This is primarily due to the scarcity of ground-truth data. To address this challenge, we propose SQLHD, a novel hallucination detection method based on metamorphic testing (MT) that does not require standard answers. SQLHD splits the detection task into two sequentiial stages: schema-linking hallucination detection via eight structure-aware Metamorphic Relations (MRs) that perturb comparative words, entities, sentence structure or database schema, and logical-synthesis hallucination detection via nine logic-aware MRs that mutate prefix words, extremum expressions, comparison ranges or the entire database. In each stage the LLM is invoked separately to generate schema mappings or SQL artefacts; the follow-up outputs are cross-checked against their source counterparts through the corresponding MRs, and any violation is flagged as a hallucination without requiring ground-truth SQL. The experimental results demonstrate our method's superior performance in terms of the F1-score, which ranges from 69.36\% to 82.76\%. Additionally, SQLHD demonstrates superior performance over LLM Self-Evaluation methods, effectively identifying hallucinations in Text-to-SQL tasks.

</details>


### [22] [Agentic Software Issue Resolution with Large Language Models: A Survey](https://arxiv.org/abs/2512.22256)
*Zhonghao Jiang,David Lo,Zhongxin Liu*

Main category: cs.SE

TL;DR: 该论文对基于LLM的智能体软件问题解决进行了系统综述，分析了126项最新研究，建立了涵盖基准、技术和实证研究三个维度的分类体系，并探讨了智能体强化学习带来的范式转变。


<details>
  <summary>Details</summary>
Motivation: 软件问题解决是软件维护的关键环节，传统单步方法难以应对复杂现实问题。随着LLM在推理和生成能力上的快速发展，基于LLM的智能体系统成为主流，但需要系统梳理这一新兴领域的研究现状和发展方向。

Method: 对126项最新研究进行系统综述，建立三维分类体系（基准、技术、实证研究），分析智能体强化学习带来的范式转变，总结关键挑战和未来方向。

Result: 建立了基于LLM的智能体软件问题解决的系统分类框架，识别了该领域的关键技术趋势，特别是智能体强化学习如何改变系统设计和训练范式，为未来研究提供了结构化指导。

Conclusion: 基于LLM的智能体系统在软件问题解决中展现出巨大潜力，但面临复杂推理、迭代探索和反馈驱动决策等挑战。智能体强化学习带来了范式转变，未来需要在基准建设、技术优化和实证验证等方面继续推进，以连接人工智能与软件工程领域。

Abstract: Software issue resolution aims to address real-world issues in software repositories (e.g., bug fixing and efficiency optimization) based on natural language descriptions provided by users, representing a key aspect of software maintenance. With the rapid development of large language models (LLMs) in reasoning and generative capabilities, LLM-based approaches have made significant progress in automated software issue resolution. However, real-world software issue resolution is inherently complex and requires long-horizon reasoning, iterative exploration, and feedback-driven decision making, which demand agentic capabilities beyond conventional single-step approaches. Recently, LLM-based agentic systems have become mainstream for software issue resolution. Advancements in agentic software issue resolution not only greatly enhance software maintenance efficiency and quality but also provide a realistic environment for validating agentic systems' reasoning, planning, and execution capabilities, bridging artificial intelligence and software engineering.
  This work presents a systematic survey of 126 recent studies at the forefront of LLM-based agentic software issue resolution research. It outlines the general workflow of the task and establishes a taxonomy across three dimensions: benchmarks, techniques, and empirical studies. Furthermore, it highlights how the emergence of agentic reinforcement learning has brought a paradigm shift in the design and training of agentic systems for software engineering. Finally, it summarizes key challenges and outlines promising directions for future research.

</details>


### [23] [Beyond Correctness: Exposing LLM-generated Logical Flaws in Reasoning via Multi-step Automated Theorem Proving](https://arxiv.org/abs/2512.23511)
*Xinyi Zheng,Ningke Li,Xiaokun Luan,Kailong Wang,Ling Shi,Meng Sun,Haoyu Wang*

Main category: cs.SE

TL;DR: MATP是一个通过多步自动定理证明系统验证LLM推理逻辑的评估框架，将自然语言推理转化为一阶逻辑，使用自动定理证明器评估逐步逻辑有效性，在推理步骤验证上比基于提示的方法提升超过42个百分点。


<details>
  <summary>Details</summary>
Motivation: LLM在医疗、法律、科学等高风险领域展现出强大推理能力，但其推理常包含被流畅语言掩盖的细微逻辑错误，现有方法如事实核查、自一致性方法和基于规则的验证无法检测多步推理中的复杂逻辑缺陷。

Method: MATP将自然语言推理转化为一阶逻辑，应用自动定理证明器评估逐步逻辑有效性，识别隐藏的逻辑错误并提供细粒度的推理正确性分类。

Result: 在包含10,830个推理实例的基准测试中（来自PrOntoQA-OOD、ProofWriter和FOLIO任务，由10个LLM生成），MATP在推理步骤验证上比基于提示的基线方法提升超过42个百分点，并揭示推理模型比通用模型产生更逻辑一致的输出。

Conclusion: MATP框架通过系统性的逻辑验证，能够有效检测LLM推理中的隐藏逻辑错误，显著提升LLM生成推理的可信度，为高风险应用中的LLM部署提供重要保障。

Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, leading to their adoption in high-stakes domains such as healthcare, law, and scientific research. However, their reasoning often contains subtle logical errors masked by fluent language, posing significant risks for critical applications. While existing approaches like fact-checking, self-consistency methods, and rule-based validation provide partial solutions, they fail to detect complex logical flaws in multi-step reasoning.
  To overcome these challenges, we present MATP, an evaluation framework for systematically verifying LLM reasoning via Multi-step Automatic Theorem Proving. MATP translates natural language reasoning into First-Order Logic (FOL) and applies automated theorem provers to assess step-by-step logical validity. This approach identifies hidden logical errors and provides fine-grained classifications of reasoning correctness. Evaluations on a benchmark comprising 10,830 reasoning instances generated by 10 LLMs across tasks from PrOntoQA-OOD, ProofWriter, and FOLIO show that MATP surpasses prompting-based baselines by over 42 percentage points in reasoning step verification. It further reveals model-level disparities, with reasoning models generating more logically coherent outputs than general models. These results demonstrate MATP's potential to enhance the trustworthiness of LLM-generated reasoning.

</details>


### [24] [AI-Generated Code Is Not Reproducible (Yet): An Empirical Study of Dependency Gaps in LLM-Based Coding Agents](https://arxiv.org/abs/2512.22387)
*Bhanu Prakash Vangala,Ali Adibifar,Tanu Malik,Ashish Gehani*

Main category: cs.SE

TL;DR: LLM生成的代码在干净环境中执行成功率仅68.3%，存在大量隐藏依赖，从声明到实际运行时依赖平均膨胀13.5倍


<details>
  <summary>Details</summary>
Motivation: 研究LLM作为编码代理对生成代码可重现性的影响，探索LLM生成的代码在仅使用模型指定依赖的干净环境中能否成功执行

Method: 评估三个先进LLM编码代理（Claude Code、OpenAI Codex、Gemini），使用100个标准化提示生成300个项目（Python、JavaScript、Java），引入三层依赖框架（声明依赖、工作依赖、运行时依赖）量化执行可重现性

Result: 仅68.3%的项目能开箱即用，语言间差异显著（Python 89.2%、Java 44.0%），从声明到实际运行时依赖平均膨胀13.5倍，存在大量隐藏依赖

Conclusion: LLM生成的代码存在严重的可重现性问题，依赖管理不完整，需要改进LLM的依赖规范能力以确保代码可执行性

Abstract: The rise of Large Language Models (LLMs) as coding agents promises to accelerate software development, but their impact on generated code reproducibility remains largely unexplored. This paper presents an empirical study investigating whether LLM-generated code can be executed successfully in a clean environment with only OS packages and using only the dependencies that the model specifies. We evaluate three state-of-the-art LLM coding agents (Claude Code, OpenAI Codex, and Gemini) across 300 projects generated from 100 standardized prompts in Python, JavaScript, and Java. We introduce a three-layer dependency framework (distinguishing between claimed, working, and runtime dependencies) to quantify execution reproducibility. Our results show that only 68.3% of projects execute out-of-the-box, with substantial variation across languages (Python 89.2%, Java 44.0%). We also find a 13.5 times average expansion from declared to actual runtime dependencies, revealing significant hidden dependencies.

</details>


### [25] [Building Software by Rolling the Dice: A Qualitative Study of Vibe Coding](https://arxiv.org/abs/2512.22418)
*Yi-Hung Chou,Boyuan Jiang,Yi Wen Chen,Mingyue Weng,Victoria Jackson,Thomas Zimmermann,James A. Jones*

Main category: cs.SE

TL;DR: 研究通过分析20个"氛围编程"视频，揭示了开发者使用LLM进行编程的实践谱系，从完全依赖AI到检查适配生成代码，所有人都需应对生成的随机性，调试常被描述为"掷骰子"。


<details>
  <summary>Details</summary>
Motivation: LLM正在重塑软件工程，催生了"氛围编程"现象，即开发者主要通过提示而非编写代码来构建软件。尽管被广泛宣传为生产力突破，但人们对从业者如何实际定义和参与这些实践知之甚少。

Method: 采用扎根理论研究法，分析了20个氛围编程视频，包括7个直播编码会话（约16小时，254个提示）和13个观点视频（约5小时），并辅以活动时长和提示意图的额外分析。

Result: 发现行为谱系：一些氛围编码者几乎完全依赖AI而不检查代码，而另一些则检查和适配生成输出。所有方法都必须应对生成的随机性，调试和优化常被描述为"掷骰子"。不同的心智模型（受专业知识和AI依赖影响）影响了提示策略、评估实践和信任水平。

Conclusion: 这些发现为软件工程未来的研究开辟了新方向，并为工具设计和教育提供了实践机会。

Abstract: Large language models (LLMs) are reshaping software engineering by enabling "vibe coding," in which developers build software primarily through prompts rather than writing code. Although widely publicized as a productivity breakthrough, little is known about how practitioners actually define and engage in these practices. To shed light on this emerging phenomenon, we conducted a grounded theory study of 20 vibe-coding videos, including 7 live-streamed coding sessions (about 16 hours, 254 prompts) and 13 opinion videos (about 5 hours), supported by additional analysis of activity durations and prompt intents. Our findings reveal a spectrum of behaviors: some vibe coders rely almost entirely on AI without inspecting code, while others examine and adapt generated outputs. Across approaches, all must contend with the stochastic nature of generation, with debugging and refinement often described as "rolling the dice." Further, divergent mental models, shaped by vibe coders' expertise and reliance on AI, influence prompting strategies, evaluation practices, and levels of trust. These findings open new directions for research on the future of software engineering and point to practical opportunities for tool design and education.

</details>


### [26] [GraphLocator: Graph-guided Causal Reasoning for Issue Localization](https://arxiv.org/abs/2512.22469)
*Wei Liu,Chao Peng,Pengfei Gao,Aofan Liu,Wei Zhang,Haiyan Zhao,Zhi Jin*

Main category: cs.SE

TL;DR: GraphLocator：通过因果结构发现和动态问题解耦解决软件问题定位中的症状-原因不匹配和一对多不匹配问题


<details>
  <summary>Details</summary>
Motivation: 软件问题定位任务面临两个主要挑战：症状-原因不匹配（描述不明确揭示根本原因）和一对多不匹配（单个问题对应多个相互依赖的代码实体），这导致自然语言问题描述与源代码实现之间存在语义鸿沟。

Method: 提出GraphLocator方法，构建因果问题图（CIG），顶点表示发现的子问题及其关联代码实体，边编码它们之间的因果依赖关系。采用两阶段工作流：症状顶点定位和动态CIG发现，先在仓库图上识别症状位置，然后通过迭代推理相邻顶点动态扩展CIG。

Result: 在三个真实数据集上的实验显示：1）相比基线，GraphLocator在函数级召回率平均提升+19.49%，精确率提升+11.89%；2）在症状-原因和一对多不匹配场景下分别实现召回率提升+16.44%和+19.18%，精确率提升+7.78%和+13.23%；3）生成的CIG在下游解决任务中带来28.74%的性能提升。

Conclusion: GraphLocator通过因果结构发现和动态问题解耦有效解决了软件问题定位中的两个关键不匹配问题，显著提升了定位准确性，并且生成的因果问题图对下游解决任务具有重要价值。

Abstract: The issue localization task aims to identify the locations in a software repository that requires modification given a natural language issue description. This task is fundamental yet challenging in automated software engineering due to the semantic gap between issue description and source code implementation. This gap manifests as two mismatches:(1) symptom-to-cause mismatches, where descriptions do not explicitly reveal underlying root causes; (2) one-to-many mismatches, where a single issue corresponds to multiple interdependent code entities. To address these two mismatches, we propose GraphLocator, an approach that mitigates symptom-to-cause mismatches through causal structure discovering and resolves one-to-many mismatches via dynamic issue disentangling. The key artifact is the causal issue graph (CIG), in which vertices represent discovered sub-issues along with their associated code entities, and edges encode the causal dependencies between them. The workflow of GraphLocator consists of two phases: symptom vertices locating and dynamic CIG discovering; it first identifies symptom locations on the repository graph, then dynamically expands the CIG by iteratively reasoning over neighboring vertices. Experiments on three real-world datasets demonstrates the effectiveness of GraphLocator: (1) Compared with baselines, GraphLocator achieves more accurate localization with average improvements of +19.49% in function-level recall and +11.89% in precision. (2) GraphLocator outperforms baselines on both symptom-to-cause and one-to-many mismatch scenarios, achieving recall improvement of +16.44% and +19.18%, precision improvement of +7.78% and +13.23%, respectively. (3) The CIG generated by GraphLocator yields the highest relative improvement, resulting in a 28.74% increase in performance on downstream resolving task.

</details>


### [27] [Isolating Compiler Faults via Multiple Pairs of Adversarial Compilation Configurations](https://arxiv.org/abs/2512.22538)
*Qingyang Li,Yibiao Yang,Maolin Sun,Jiangchang Wu,Qingkai Shi,Yuming Zhou*

Main category: cs.SE

TL;DR: MultiConf：一种通过构建多对对抗性编译配置来自动定位编译器故障的新方法，显著提升了GCC编译器故障定位的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 编译器是现代软件开发的基础，但编译器故障的定位非常困难，因为现代编译器基础设施复杂且规模庞大。现有方法难以将故障精确定位到具体的源文件。

Method: MultiConf通过构建多对对抗性编译配置（每个对包含一个失败配置和对应的通过配置，仅在少量细粒度选项上不同），使用轻量级构造过程生成失败配置，并通过选择性禁用bug相关细粒度选项推导通过配置。然后应用基于频谱的故障定位公式对编译器源文件进行可疑度排序，每个配置对独立产生排序，最后通过加权投票方案聚合得到最终排序。

Result: 在60个真实GCC编译器bug的基准测试中，MultiConf在Top-1文件级别成功定位了27个bug，相比现有最佳方法Odfl(20)和Basic(21)分别提升了35.0%和28.6%，在效果和效率上都显著优于现有技术。

Conclusion: MultiConf通过多对对抗性编译配置和加权投票聚合，提供了一种更准确、更鲁棒的编译器故障定位方法，能够有效处理现代编译器的复杂性和规模问题。

Abstract: Compilers are fundamental to modern software development, making the effective identification and resolution of compiler faults essential. However, localizing these faults to specific source files remains highly challenging due to the complexity and scale of modern compiler infrastructures. In this study, we propose MultiConf, a novel approach that automatically isolates compiler faults by constructing multiple pairs of adversarial compilation configurations. Each adversarial compilation configuration pair consists of a failing configuration and its corresponding passing configuration, which differ in only a small number of fine-grained options. MultiConf generates failing configurations through a lightweight construction process and derives the corresponding passing configurations by selectively disabling bug-related fine-grained options. We then employ a Spectrum-Based Fault Localization (SBFL) formula to rank the suspiciousness of compiler source files. Each adversarial configuration pair independently produces a ranking, which is subsequently aggregated using a weighted voting scheme to derive a final suspiciousness ranking, enabling more accurate and robust fault localization. We evaluate MultiConf on a benchmark of 60 real-world GCC compiler bugs. The results demonstrate that MultiConf significantly outperforms existing compiler fault localization techniques in both effectiveness and efficiency. In particular, MultiConf successfully localizes 27 out of 60 bugs at the Top-1 file level, representing improvements of 35.0% and 28.6% over the two state-of-the-art approaches, Odfl(20) and Basic(21), respectively.

</details>


### [28] [Rethinking the Capability of Fine-Tuned Language Models for Automated Vulnerability Repair](https://arxiv.org/abs/2512.22633)
*Woorim Han,Yeongjun Kwak,Miseon Yu,Kyeongmin Kim,Younghan Lee,Hyungon Moon,Yunheung Paek*

Main category: cs.SE

TL;DR: 该研究揭示了当前基于学习的自动化漏洞修复模型存在过拟合问题，评估方法存在缺陷，并提出新的测试基准来更准确评估模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的自动化漏洞修复技术虽然显示出潜力，但其修复未见漏洞的能力存疑。现有模型可能过拟合训练数据，且评估方法存在训练/验证/测试集不互斥的问题，同时基于匹配的评估指标无法考虑多种有效修复方式。

Method: 采用三种方法：1) 对测试集应用语义保持变换，检验模型是否学习到稳健的修复模式；2) 重新划分互斥的训练/验证/测试集，评估模型泛化能力；3) 引入L-AVRBench测试基准，克服基于匹配指标的局限性。

Result: 研究发现现有最先进模型确实存在过拟合问题，在互斥数据集上表现下降，基于匹配的评估指标无法准确反映模型真实修复能力。

Conclusion: 当前基于学习的自动化漏洞修复模型评估方法存在缺陷，需要更严格的评估标准和基于测试的基准来准确衡量模型真实修复能力，L-AVRBench为解决这些问题提供了新方向。

Abstract: Learning-based automated vulnerability repair (AVR) techniques that utilize fine-tuned language models have shown promise in generating vulnerability patches. However, questions remain about their ability to repair unseen vulnerabilities. Our empirical study reveals that state-of-the-art models often overfit to the training set and are evaluated using training, validation, and test sets that are not mutually exclusive. Furthermore, relying on match-based metrics that compare generated patches to reference fixes at the token level has some limitations, failing to account for the possibility of various valid ways to patch the vulnerability. In this paper, we examine the capabilities of state-of-the-art fine-tuned AVR models and the adequacy of match-based evaluation metrics in three ways. First, we apply semantic-preserving transformations to test sets in order to determine whether models truly learn robust vulnerability-repair patterns or simply rely on spurious features. Second, we re-split the training, validation, and test sets to be mutually exclusive and evaluate the models on the revised test set to assess their generalization capabilities. Third, we introduce L-AVRBench, a test-based benchmark tailored for learning-based AVR, to overcome the limitations of match-based metrics and examine the AVR models' true repair capabilities.

</details>


### [29] [CFIghter: Automated Control-Flow Integrity Enablement and Evaluation for Legacy C/C++ Systems](https://arxiv.org/abs/2512.22701)
*Sabine Houy,Bruno Kreyssig,Alexandre Bartel*

Main category: cs.SE

TL;DR: CFIghter：首个全自动系统，通过检测、分类和修复测试套件暴露的意外策略违规，在真实项目中实现严格类型化CFI，无需手动源代码修改。


<details>
  <summary>Details</summary>
Motivation: 基于编译器的控制流完整性（CFI）提供强大的前向边保护，但由于可见性不匹配、类型不一致和意外行为故障，在大型C/C++软件中部署仍然具有挑战性。

Method: CFIghter集成了全程序分析与引导式运行时监控，迭代应用最小必要的CFI执行调整，仅在需要的地方进行，直到所有测试通过或剩余故障被认为无法解决。

Result: 在四个GNU项目上评估，CFIghter解决了所有可见性相关的构建错误，在大型多库util-linux代码库中自动修复了95.8%的意外CFI违规，同时在超过89%的间接控制流站点保持严格执行。

Conclusion: 自动兼容性修复使严格的编译器CFI在成熟、模块化的C软件中实际可部署，无需手动源代码更改，仅依赖自动生成的可见性调整和必要的局部化执行范围。

Abstract: Compiler-based Control-Flow Integrity (CFI) offers strong forward-edge protection but remains challenging to deploy in large C/C++ software due to visibility mismatches, type inconsistencies, and unintended behavioral failures. We present CFIghter, the first fully automated system that enables strict, type-based CFI in real-world projects by detecting, classifying, and repairing unintended policy violations exposed by the test suite. CFIghter integrates whole-program analysis with guided runtime monitoring and iteratively applies the minimal necessary adjustments to CFI enforcement only where required, stopping once all tests pass or remaining failures are deemed unresolvable. We evaluate CFIghter on four GNU projects. It resolves all visibility-related build errors and automatically repairs 95.8% of unintended CFI violations in the large, multi-library util-linux codebase, while retaining strict enforcement at over 89% of indirect control-flow sites. Across all subjects, CFIghter preserves strict type-based CFI for the majority of the codebase without requiring manual source-code changes, relying only on automatically generated visibility adjustments and localized enforcement scopes where necessary. These results show that automated compatibility repair makes strict compiler CFI practically deployable in mature, modular C software.

</details>


### [30] [From Rookie to Expert: Manipulating LLMs for Automated Vulnerability Exploitation in Enterprise Software](https://arxiv.org/abs/2512.22753)
*Moustapha Awwalou Diouf,Maimouna Tamah Diao,Iyiola Emmanuel Olatunji,Abdoul Kader Kaboré,Jordan Samhi,Gervais Mendy,Samuel Ouya,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: LLMs使非程序员也能成为攻击者，通过RSA策略可100%成功生成漏洞利用代码，挑战了传统软件安全的基本假设。


<details>
  <summary>Details</summary>
Motivation: LLMs在民主化软件开发的同时，也破坏了数十年来软件工程的安全假设。传统安全模型基于攻击者需要技术专业知识的前提，但LLMs使非技术人员也能生成有效的攻击代码，这需要重新审视安全基础。

Method: 提出RSA策略（角色分配、场景伪装、行动诱导），通过社会工程学方法操纵LLMs绕过安全机制生成功能性的漏洞利用代码。在Odoo ERP平台上测试了5个主流LLM（GPT-4o、Gemini、Claude、Microsoft Copilot、DeepSeek）。

Result: 测试的CVE漏洞在3-4轮提示内都至少生成了一个可工作的利用代码，成功率100%。相比之前研究认为LLM辅助攻击需要大量人工努力，本研究完全消除了这种开销。

Conclusion: 这代表了软件工程的范式转变：技术与非技术人员的区分不再有效；漏洞描述的技术复杂性不再提供保护；构建软件的工具也能被操纵来破坏软件。安全实践必须重新设计，因为现在攻击只需要编写提示的能力，而不是理解代码。

Abstract: LLMs democratize software engineering by enabling non-programmers to create applications, but this same accessibility fundamentally undermines security assumptions that have guided software engineering for decades. We show in this work how publicly available LLMs can be socially engineered to transform novices into capable attackers, challenging the foundational principle that exploitation requires technical expertise. To that end, we propose RSA (Role-assignment, Scenario-pretexting, and Action-solicitation), a pretexting strategy that manipulates LLMs into generating functional exploits despite their safety mechanisms. Testing against Odoo -- a widely used ERP platform, we evaluated five mainstream LLMs (GPT-4o, Gemini, Claude, Microsoft Copilot, and DeepSeek) and achieved a 100% success rate: tested CVE yielded at least one working exploit within 3-4 prompting rounds. While prior work [13] found LLM-assisted attacks difficult and requiring manual effort, we demonstrate that this overhead can be eliminated entirely.
  Our findings invalidate core software engineering security principles: the distinction between technical and non-technical actors no longer provides valid threat models; technical complexity of vulnerability descriptions offers no protection when LLMs can abstract it away; and traditional security boundaries dissolve when the same tools that build software can be manipulated to break it. This represents a paradigm shift in software engineering -- we must redesign security practices for an era where exploitation requires only the ability to craft prompts, not understand code.
  Artifacts available at: https://anonymous.4open.science/r/From-Rookie-to-Attacker-D8B3.

</details>


### [31] [FasterPy: An LLM-based Code Execution Efficiency Optimization Framework](https://arxiv.org/abs/2512.22827)
*Yue Wu,Minghao Han,Ruiyin Li,Peng Liang,Amjed Tahir,Zengyang Li,Qiong Feng,Mojtaba Shahin*

Main category: cs.SE

TL;DR: FasterPy是一个基于大语言模型的Python代码优化框架，结合检索增强生成和低秩适应技术，在PIE基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的方法需要手动设计和维护特定性能bug的规则，劳动密集且适用范围有限。基于机器学习和深度学习的方法虽然有所改进，但依赖特定程序表示和精心构建的训练数据集，开发成本高且难以扩展。大语言模型在代码生成方面的卓越能力为自动化代码优化开辟了新途径。

Method: 提出FasterPy框架，结合检索增强生成（RAG）和低秩适应（LoRA）技术。RAG基于现有性能改进代码对和相应性能测量构建的知识库，LoRA用于增强代码优化性能。

Result: 在Performance Improving Code Edits（PIE）基准测试中，该方法在多个指标上优于现有模型。

Conclusion: FasterPy是一个低成本、高效的框架，能够有效利用大语言模型优化Python代码的执行效率，为自动化代码优化提供了新的解决方案。

Abstract: Code often suffers from performance bugs. These bugs necessitate the research and practice of code optimization. Traditional rule-based methods rely on manually designing and maintaining rules for specific performance bugs (e.g., redundant loops, repeated computations), making them labor-intensive and limited in applicability. In recent years, machine learning and deep learning-based methods have emerged as promising alternatives by learning optimization heuristics from annotated code corpora and performance measurements. However, these approaches usually depend on specific program representations and meticulously crafted training datasets, making them costly to develop and difficult to scale. With the booming of Large Language Models (LLMs), their remarkable capabilities in code generation have opened new avenues for automated code optimization. In this work, we proposed FasterPy, a low-cost and efficient framework that adapts LLMs to optimize the execution efficiency of Python code. FasterPy combines Retrieval-Augmented Generation (RAG), supported by a knowledge base constructed from existing performance-improving code pairs and corresponding performance measurements, with Low-Rank Adaptation (LoRA) to enhance code optimization performance. Our experimental results on the Performance Improving Code Edits (PIE) benchmark demonstrate that our method outperforms existing models on multiple metrics. The FasterPy tool and the experimental results are available at https://github.com/WuYue22/fasterpy.

</details>


### [32] [Towards the analysis of team members well-being](https://arxiv.org/abs/2512.22845)
*Zan Xu,Sari Nurfauziyyah,Anastasia Romanova,Kaamesh G S,Yiqun Gao,Maria Spichkova*

Main category: cs.SE

TL;DR: 该研究分析了软件开发团队成员的幸福感，开发了团队幸福感分析原型系统，强调认可和赞赏对团队幸福感的重要性。


<details>
  <summary>Details</summary>
Motivation: 软件开发团队成员的幸福感不仅影响工作生产力和绩效，还关系到员工的健康和个人生活。研究表明，团队成员是否感到被认可和赞赏是影响幸福感的重要因素。

Method: 开展了一个关于团队幸福感分析的项目，并在项目中开发了相应的原型系统。

Result: 论文展示了团队幸福感分析项目的结果以及在该项目中开发的原型系统。

Conclusion: 团队成员的认可和赞赏对软件开发团队的幸福感至关重要，该研究通过开发分析原型为团队幸福感评估提供了工具。

Abstract: Many recent research studies have focused on the well-being of software development team members, as this aspect may be critical not only for productivity and performance at work but also for the physical health and personal life of employees. Many studies agree that an important factor of team member well-being is whether team members feel appreciated and acknowledged for their contributions. This paper presents the results of a project on the team well-being analysis as well as the prototype developed within the project.

</details>


### [33] [Interpretable Gallbladder Ultrasound Diagnosis: A Lightweight Web-Mobile Software Platform with Real-Time XAI](https://arxiv.org/abs/2512.23033)
*Fuyad Hasan Bhoyan,Prashanta Sarker,Parsia Noor Ethila,Md. Emon Hossain,Md Kaviul Hossain,Md Humaion Kabir Mehedi*

Main category: cs.SE

TL;DR: 开发基于AI的胆囊疾病超声诊断软件，集成混合深度学习模型MobResTaNet，可实时分类10种胆囊疾病（9种疾病+正常），准确率达99.85%，参数量仅2.24M，通过可解释AI可视化提供透明诊断支持。


<details>
  <summary>Details</summary>
Motivation: 胆囊疾病的早期准确诊断至关重要，但超声图像解读具有挑战性，需要开发AI辅助诊断工具来提高诊断效率和准确性。

Method: 采用混合深度学习模型MobResTaNet，直接从超声图像分类10个类别（9种胆囊疾病类型+正常）。系统集成可解释AI（XAI）可视化，提供实时预测。使用HTML、CSS、JavaScript、Bootstrap和Flutter技术开发为Web和移动应用。

Result: 系统达到高达99.85%的准确率，模型参数仅2.24M。提供实时、可解释的预测，支持透明的临床决策，部署为高效、可访问的护理点诊断支持工具。

Conclusion: 该AI驱动诊断软件通过高效的深度学习模型和可解释AI技术，为胆囊疾病超声诊断提供了准确、实时、透明的辅助工具，有望改善临床决策和护理点诊断。

Abstract: Early and accurate detection of gallbladder diseases is crucial, yet ultrasound interpretation is challenging. To address this, an AI-driven diagnostic software integrates our hybrid deep learning model MobResTaNet to classify ten categories, nine gallbladder disease types and normal directly from ultrasound images. The system delivers interpretable, real-time predictions via Explainable AI (XAI) visualizations, supporting transparent clinical decision-making. It achieves up to 99.85% accuracy with only 2.24M parameters. Deployed as web and mobile applications using HTML, CSS, JavaScript, Bootstrap, and Flutter, the software provides efficient, accessible, and trustworthy diagnostic support at the point of care

</details>


### [34] [An Automated Grey Literature Extraction Tool for Software Engineering](https://arxiv.org/abs/2512.23066)
*Houcine Abdelkader Cherief,Brahim Mahmoudi,Zacharie Chenail-Larcher,Naouel Moha,Quentin Sti'evenart,Florent Avellaneda*

Main category: cs.SE

TL;DR: GLiSE是一个基于提示的工具，用于从GitHub、Stack Overflow和Google等软件工程网络来源大规模收集和评估灰色文献，通过语义分类器过滤和排序结果以提高相关性。


<details>
  <summary>Details</summary>
Motivation: 灰色文献对软件工程研究至关重要，但因其来源、格式和API的异构性，大规模收集和评估仍然困难，阻碍了可重复的大规模综合研究。

Method: GLiSE将研究主题提示转换为平台特定查询，从常见软件工程网络来源收集结果，并使用基于嵌入的语义分类器根据相关性过滤和排序结果。工具设计注重可重复性，所有设置基于配置，每个生成的查询都可访问。

Result: 论文提供了(i)GLiSE工具，(ii)一个经过整理的软件工程灰色文献搜索结果数据集，按语义相关性分类，以及(iii)对工具可用性的实证研究。

Conclusion: GLiSE解决了软件工程灰色文献大规模收集和评估的挑战，提供了可重复的工具和数据集，有助于促进该领域的研究。

Abstract: Grey literature is essential to software engineering research as it captures practices and decisions that rarely appear in academic venues. However, collecting and assessing it at scale remains difficult because of their heterogeneous sources, formats, and APIs that impede reproducible, large-scale synthesis. To address this issue, we present GLiSE, a prompt-driven tool that turns a research topic prompt into platform-specific queries, gathers results from common software-engineering web sources (GitHub, Stack Overflow) and Google Search, and uses embedding-based semantic classifiers to filter and rank results according to their relevance. GLiSE is designed for reproducibility with all settings being configuration-based, and every generated query being accessible. In this paper, (i) we present the GLiSE tool, (ii) provide a curated dataset of software engineering grey-literature search results classified by semantic relevance to their originating search intent, and (iii) conduct an empirical study on the usability of our tool.

</details>


### [35] [An Empirical Study of Generative AI Adoption in Software Engineering](https://arxiv.org/abs/2512.23327)
*Görkem Giray,Onur Demirörs,Marcos Kalinowski,Daniel Mendez*

Main category: cs.SE

TL;DR: 该研究调查了GenAI在软件工程中的采用现状，发现广泛采用但缺乏实证证据，工具深度集成到日常工作中，带来效率提升但也面临输出不可靠、验证负担等挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管GenAI工具在软件工程中日益普及，但缺乏关于其实际使用情况、效益、挑战以及更广泛组织和社会影响的实证证据。本研究旨在填补这一空白，了解GenAI在SE中的采用现状。

Method: 研究通过调查分析GenAI在软件工程中的采用状态，重点关注采用现状、相关效益和挑战、工具和技术的制度化，以及对SE专业人员和社区的长期影响预期。

Result: 研究发现GenAI工具被广泛采用并深度集成到日常SE工作中，特别是在实现、验证和确认、个人协助和维护相关任务中。实践者报告了显著效益，包括周期时间减少、质量改进、知识工作支持增强和生产力提升。但生产力质量的客观测量仍然有限。主要挑战包括输出不正确或不可靠、提示工程困难、验证负担、安全和隐私问题以及过度依赖风险。工具制度化普遍但差异大，重点关注工具访问而非培训和管理。实践者预期GenAI将重新定义而非取代其角色，对就业市场收缩和技能转变表达适度担忧。

Conclusion: GenAI在软件工程中已被广泛采用并深度集成，带来了显著效益但也面临重要挑战。需要更系统的制度化方法，包括培训和管理，同时实践者预期角色将重新定义而非被取代，表明GenAI作为辅助工具而非替代品的定位。

Abstract: Context. GenAI tools are being increasingly adopted by practitioners in SE, promising support for several SE activities. Despite increasing adoption, we still lack empirical evidence on how GenAI is used in practice, the benefits it provides, the challenges it introduces, and its broader organizational and societal implications. Objective. This study aims to provide an overview of the status of GenAI adoption in SE. It investigates the status of GenAI adoption, associated benefits and challenges, institutionalization of tools and techniques, and anticipated long term impacts on SE professionals and the community. Results. The results indicate a wide adoption of GenAI tools and how they are deeply integrated into daily SE work, particularly for implementation, verification and validation, personal assistance, and maintenance-related tasks. Practitioners report substantial benefits, most notably reduction in cycle time, quality improvements, enhanced support in knowledge work, and productivity gains. However, objective measurement of productivity and quality remains limited in practice. Significant challenges persist, including incorrect or unreliable outputs, prompt engineering difficulties, validation overhead, security and privacy concerns, and risks of overreliance. Institutionalization of tools and techniques seems to be common, but it varies considerably, with a strong focus on tool access and less emphasis on training and governance. Practitioners expect GenAI to redefine rather than replace their roles, while expressing moderate concern about job market contraction and skill shifts.

</details>


### [36] [Securing the AI Supply Chain: What Can We Learn From Developer-Reported Security Issues and Solutions of AI Projects?](https://arxiv.org/abs/2512.23385)
*The Anh Nguyen,Triet Huynh Minh Le,M. Ali Babar*

Main category: cs.SE

TL;DR: 该研究通过分析Hugging Face和GitHub上的开发者讨论，构建了一个包含312,868条安全讨论的数据集，识别出AI供应链中的32类安全问题和24种解决方案，揭示了AI组件复杂依赖性和黑盒特性带来的安全挑战。


<details>
  <summary>Details</summary>
Motivation: AI模型和应用的快速增长带来了日益复杂的安全环境，开发者不仅面临传统软件供应链问题，还要应对AI特有的安全威胁。然而，目前对实践中常见安全问题及其解决方案的了解有限，这阻碍了针对AI供应链各组件有效安全措施的开发。

Method: 研究通过结合关键词匹配和优化的distilBERT分类器构建分析管道，从Hugging Face和GitHub收集开发者讨论。该管道识别出312,868条安全相关讨论，然后对其中753个帖子进行主题分析，建立了细粒度的安全分类体系。

Result: 研究发现了涵盖四个主题的32类安全问题和24种解决方案：(1)系统和软件、(2)外部工具和生态系统、(3)模型、(4)数据。结果显示许多安全问题源于AI组件的复杂依赖性和黑盒特性，特别是模型和数据相关的挑战往往缺乏具体解决方案。

Conclusion: 该研究为开发者和研究人员提供了基于实证的指导，帮助他们应对AI供应链中的现实安全威胁。研究揭示了当前安全实践的现状，并指出了需要进一步关注和解决的领域，特别是模型和数据安全方面的解决方案不足问题。

Abstract: The rapid growth of Artificial Intelligence (AI) models and applications has led to an increasingly complex security landscape. Developers of AI projects must contend not only with traditional software supply chain issues but also with novel, AI-specific security threats. However, little is known about what security issues are commonly encountered and how they are resolved in practice. This gap hinders the development of effective security measures for each component of the AI supply chain. We bridge this gap by conducting an empirical investigation of developer-reported issues and solutions, based on discussions from Hugging Face and GitHub. To identify security-related discussions, we develop a pipeline that combines keyword matching with an optimal fine-tuned distilBERT classifier, which achieved the best performance in our extensive comparison of various deep learning and large language models. This pipeline produces a dataset of 312,868 security discussions, providing insights into the security reporting practices of AI applications and projects. We conduct a thematic analysis of 753 posts sampled from our dataset and uncover a fine-grained taxonomy of 32 security issues and 24 solutions across four themes: (1) System and Software, (2) External Tools and Ecosystem, (3) Model, and (4) Data. We reveal that many security issues arise from the complex dependencies and black-box nature of AI components. Notably, challenges related to Models and Data often lack concrete solutions. Our insights can offer evidence-based guidance for developers and researchers to address real-world security threats across the AI supply chain.

</details>


### [37] [An SLO Driven and Cost-Aware Autoscaling Framework for Kubernetes](https://arxiv.org/abs/2512.23415)
*Vinoth Punniyamoorthy,Bikesh Kumar,Sumit Saha,Lokesh Butra,Mayilsamy Palanigounder,Akash Kumar Agarwal,Kabilan Kannan*

Main category: cs.SE

TL;DR: 论文提出基于AIOps的Kubernetes智能扩缩容框架，通过多信号集成、SLO感知和成本控制，相比原生方案减少SLO违规31%、提升响应时间24%、降低成本18%


<details>
  <summary>Details</summary>
Motivation: Kubernetes原生扩缩容机制存在反应式行为、应用级信号利用有限、控制逻辑不透明等问题，导致生产环境中经常出现SLO违规和成本效率低下

Method: 提出安全可解释的多信号扩缩容框架，集成SLO感知和成本控制，结合轻量级需求预测，采用AIOps原则增强Kubernetes扩缩容能力

Result: 实验评估显示，相比默认和调优的Kubernetes基线，该方法减少SLO违规持续时间31%，提升扩缩容响应时间24%，降低基础设施成本18%

Conclusion: AIOps驱动的SLO优先扩缩容能显著提升Kubernetes云平台的可靠性、效率和操作可信度，为云原生应用提供更智能的资源管理方案

Abstract: Kubernetes provides native autoscaling mechanisms, including the Horizontal Pod Autoscaler, Vertical Pod Autoscaler, and node-level autoscalers, to enable elastic resource management for cloud-native applications. However, production environments frequently experience Service Level Objective violations and cost inefficiencies due to reactive scaling behavior, limited use of application-level signals, and opaque control logic. This paper investigates how Kubernetes autoscaling can be enhanced using AIOps principles to jointly satisfy SLO and cost constraints under diverse workload patterns without compromising safety or operational transparency. We present a gap-driven analysis of existing autoscaling approaches and propose a safe and explainable multi-signal autoscaling framework that integrates SLO-aware and cost-conscious control with lightweight demand forecasting. Experimental evaluation using representative microservice and event-driven workloads shows that the proposed approach reduces SLO violation duration by up to 31 percent, improves scaling response time by 24 percent, and lowers infrastructure cost by 18 percent compared to default and tuned Kubernetes autoscaling baselines, while maintaining stable and auditable control behavior. These results demonstrate that AIOps-driven, SLO-first autoscaling can significantly improve the reliability, efficiency, and operational trustworthiness of Kubernetes-based cloud platforms.

</details>


### [38] [Embedding Quality Assurance in project-based learning](https://arxiv.org/abs/2512.23488)
*Maria Spichkova*

Main category: cs.SE

TL;DR: 基于十多年在敏捷/Scrum环境下教授软件工程课程的经验，分享了软件质量教学的经验教训，并提供了在项目式学习中融入质量保证主题的建议


<details>
  <summary>Details</summary>
Motivation: 分享十多年在敏捷/Scrum环境下教授软件工程课程中软件质量方面的经验教训，为教育工作者提供实践指导

Method: 基于多年教学实践，分析在毕业年级软件开发项目和软件工程项目管理课程中教授软件质量的经验，总结教训并提出建议

Result: 提供了在敏捷/Scrum背景下的项目式学习中嵌入质量保证主题的具体建议和最佳实践

Conclusion: 在敏捷/Scrum环境下的软件工程教育中，需要系统性地将质量保证主题融入项目式学习，本文的经验教训和建议为教育工作者提供了有价值的指导

Abstract: In this paper, we share our lessons learned from more than a decade of teaching software quality aspects within Software Engineering (SE) courses, where the focus is on Agile/Scrum settings: final year software development projects and the course on SE Project Management. Based on the lessons learned, we also provide a number of recommendations on embedding quality assurance topics in the project-based learning with Agile/Scrum context.

</details>


### [39] [Adaptable Teastore with Energy Consumption Awareness: A Case Study](https://arxiv.org/abs/2512.23498)
*Henrique De Medeiros,Denisse Muñante,Sophie Chabridon,César Perdigão Batista,Denis Conan*

Main category: cs.SE

TL;DR: EnCoMSAS工具用于监控自适应性系统的能耗，支持运行时动态调整以降低能耗，对系统整体能耗影响较小。


<details>
  <summary>Details</summary>
Motivation: 数据中心能耗持续增长，云应用迁移和数字内容消费加剧能耗问题。动态自适应方法可在运行时降低软件应用能耗，但缺乏有效的能耗监控工具来实现能耗感知。

Method: 提出EnCoMSAS工具监控分布式云应用能耗，支持评估SAS变体的运行时能耗。使用Adaptable TeaStore案例进行实证研究，在Grid5000测试平台上模拟不同负载条件，收集推荐算法的能耗数据。

Result: EnCoMSAS能有效收集能耗数据支持运行时动态调整；CPU使用率与能耗相关性验证了测量有效性；能耗受算法复杂度和部署环境影响；EnCoMSAS对SAS生态系统整体能耗影响相对较小。

Conclusion: EnCoMSAS为自适应性系统提供了有效的能耗监控能力，支持将能效作为动态调整的主要目标，且监控工具本身对系统能耗影响有限。

Abstract: [Context and Motivation] Global energy consumption has been steadily increasing in recent years, with data centers emerging as major contributors. This growth is largely driven by the widespread migration of applications to the Cloud, alongside a rising number of users consuming digital content. Dynamic adaptation (or self-adaptive) approaches appear as a way to reduce, at runtime and under certain constraints, the energy consumption of software applications.
  [Question/Problem] Despite efforts to make energy-efficiency a primary goal in the dynamic adaptation of software applications, there is still a gap in understanding how to equip these self-adaptive software systems (SAS), which are dynamically adapted at runtime, with effective energy consumption monitoring tools that enable energy-awareness. Furthermore, the extent to which such an energy consumption monitoring tool impacts the overall energy consumption of the SAS ecosystem has not yet been thoroughly explored.
  [Methodology] To address this gap, we introduce the EnCoMSAS (Energy Consumption Monitoring for Self-Adaptive Systems) tool that allows to gather the energy consumed by distributed software applications deployed, for instance, in the Cloud. EnCoMSAS enables the evaluation of energy consumption of SAS variants at runtime. It allows to integrate energy-efficiency as a main goal in the analysis and execution of new adaptation plans for the SAS. In order to evaluate the effectiveness of EnCoMSAS and investigate its impact on the overall energy consumption of the SAS ecosystem, we conduct an empirical study by using the Adaptable TeaStore case study. Adaptable TeaStore is a self-adaptive extension of the TeaStore application, a microservice benchmarking application. For this study, we focus on the recommender service of Adaptable TeaStore. Regarding the experiments, we first equip Adaptable TeaStore with EnCoMSAS. Next, we execute Adaptable TeaStore by varying workload conditions that simulate users interactions. Finally, we use EnCoMSAS for gathering and assessing the energy consumption of the recommender algorithms of Adaptable TeaStore. To run these experiments, we use nodes of the Grid5000 testbed.
  [Results] The results show that EnCoMSAS is effective in collecting energy consumption of software applications for enabling dynamic adaptation at runtime. The observed correlation between CPU usage and energy consumption collected by EnCoMSAS provides evidence supporting the validity of the collected energy measurements. Moreover, we point out, through EnCoMSAS, that energy consumption is influenced not only by the algorithmic complexity but also by the characteristics of the deployment environment. Finally, the results show that the impact of EnCoMSAS on the overall energy consumption of the SAS ecosystem is comparatively modest with respect to the entire set of the TeaStore applications microservices.

</details>


### [40] [AdaptiFlow: An Extensible Framework for Event-Driven Autonomy in Cloud Microservices](https://arxiv.org/abs/2512.23499)
*Brice Arléon Zemtsop Ndadji,Simon Bliudze,Clément Quinton*

Main category: cs.SE

TL;DR: AdaptiFlow：基于自主计算原理的微服务自适应框架，通过解耦监控与执行，实现去中心化自适应，支持自愈、自保护和自优化三种场景。


<details>
  <summary>Details</summary>
Motivation: 现代云架构需要自适应能力来应对动态运行条件，但现有解决方案通常采用集中式控制模型，不适合微服务的去中心化特性。需要一种能够保持微服务架构独立性，同时实现系统级适应性的框架。

Method: AdaptiFlow框架基于MAPE-K循环的Monitor和Execute阶段，提供抽象层：1) Metrics Collectors统一收集基础设施/业务指标；2) Adaptation Actions作为声明式执行器进行运行时调整；3) 轻量级事件驱动和基于规则的机制用于自适应逻辑规范。

Result: 通过增强的Adaptable TeaStore基准测试验证，实现了三种自适应场景：自愈（数据库恢复）、自保护（DDoS缓解）和自优化（流量管理），每个服务只需最小代码修改。证明了去中心化自适应可以通过局部决策实现，无需全局协调。

Conclusion: AdaptiFlow将自主计算理论与云原生实践相结合，为构建弹性分布式系统提供了概念框架和具体工具。未来工作包括与形式化协调模型集成，以及应用基于AI代理的自适应技术来处理复杂场景。

Abstract: Modern cloud architectures demand self-adaptive capabilities to manage dynamic operational conditions. Yet, existing solutions often impose centralized control models ill-suited to microservices decentralized nature. This paper presents AdaptiFlow, a framework that leverages well-established principles of autonomous computing to provide abstraction layers focused on the Monitor and Execute phases of the MAPE-K loop. By decoupling metrics collection and action execution from adaptation logic, AdaptiFlow enables microservices to evolve into autonomous elements through standardized interfaces, preserving their architectural independence while enabling system-wide adaptability. The framework introduces: (1) Metrics Collectors for unified infrastructure/business metric gathering, (2) Adaptation Actions as declarative actuators for runtime adjustments, and (3) a lightweight Event-Driven and rule-based mechanism for adaptation logic specification. Validation through the enhanced Adaptable TeaStore benchmark demonstrates practical implementation of three adaptation scenarios targeting three levels of autonomy self-healing (database recovery), self-protection (DDoS mitigation), and self-optimization (traffic management) with minimal code modification per service. Key innovations include a workflow for service instrumentation and evidence that decentralized adaptation can emerge from localized decisions without global coordination. The work bridges autonomic computing theory with cloud-native practice, providing both a conceptual framework and concrete tools for building resilient distributed systems. Future work includes integration with formal coordination models and application of adaptation techniques relying on AI agents for proactive adaptation to address complex adaptation scenarios.

</details>


### [41] [Model-based Development for Autonomous Driving Software Considering Parallelization](https://arxiv.org/abs/2512.23575)
*Kenshin Obi,Takumi Onozawa,Hiroshi Fujimoto,Takuya Azumi*

Main category: cs.SE

TL;DR: 提出基于模型开发（MBD）的自动驾驶软件并行化方法，通过扩展现有MBP方法减少执行时间，满足实时性要求。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶软件需要处理多种功能和复杂环境，对实时性能要求很高，需要优化执行效率。

Method: 扩展现有的基于模型并行化（MBP）方法，采用模型驱动开发（MBD）流程对自动驾驶软件进行并行化处理。

Result: 执行时间得到减少，评估结果表明该方法适合自动驾驶软件开发，特别是在实现实时性能方面表现良好。

Conclusion: 提出的基于MBD的并行化方法能有效提升自动驾驶软件性能，满足实时性要求，适合复杂自动驾驶系统开发。

Abstract: In recent years, autonomous vehicles have attracted attention as one of the solutions to various social problems. However, autonomous driving software requires real-time performance as it considers a variety of functions and complex environments. Therefore, this paper proposes a parallelization method for autonomous driving software using the Model-Based Development (MBD) process. The proposed method extends the existing Model-Based Parallelizer (MBP) method to facilitate the implementation of complex processing. As a result, execution time was reduced. The evaluation results demonstrate that the proposed method is suitable for the development of autonomous driving software, particularly in achieving real-time performance.

</details>


### [42] [Parallelized Code Generation from Simulink Models for Event-driven and Timer-driven ROS 2 Nodes](https://arxiv.org/abs/2512.23605)
*Kenshin Obi,Ryo Yoshinaka,Hiroshi Fujimoto,Takuya Azumi*

Main category: cs.SE

TL;DR: 提出一个基于模型开发的框架，将ROS 2兼容的Simulink模型分类为事件驱动和定时器驱动类型，实现有针对性的并行化，支持多输入ROS 2模型的并行代码生成。


<details>
  <summary>Details</summary>
Motivation: 嵌入式系统（特别是自动驾驶系统）复杂度和规模增加，采用ROS 2和多核处理器。传统手动并行化面临数据完整性和并发问题挑战，而基于模型开发（MBD）在集成ROS 2等现代框架时遇到多输入场景的困难。

Method: 提出一个MBD框架，将ROS 2兼容的Simulink模型分类为事件驱动和定时器驱动类型，针对不同类型进行并行化处理，扩展传统MBD并行化能力，支持多输入ROS 2模型的并行代码生成。

Result: 评估结果显示，应用所提框架进行并行化后，所有模式都显示出执行时间的减少，证实了并行化的有效性。

Conclusion: 该框架成功解决了ROS 2多输入场景下的并行化挑战，扩展了MBD在ROS 2集成中的应用，提高了嵌入式系统的执行效率。

Abstract: In recent years, the complexity and scale of embedded systems, especially in the rapidly developing field of autonomous driving systems, have increased significantly. This has led to the adoption of software and hardware approaches such as Robot Operating System (ROS) 2 and multi-core processors. Traditional manual program parallelization faces challenges, including maintaining data integrity and avoiding concurrency issues such as deadlocks. While model-based development (MBD) automates this process, it encounters difficulties with the integration of modern frameworks such as ROS 2 in multi-input scenarios. This paper proposes an MBD framework to overcome these issues, categorizing ROS 2-compatible Simulink models into event-driven and timer-driven types for targeted parallelization. As a result, it extends the conventional parallelization by MBD and supports parallelized code generation for ROS 2-based models with multiple inputs. The evaluation results show that after applying parallelization with the proposed framework, all patterns show a reduction in execution time, confirming the effectiveness of parallelization.

</details>
