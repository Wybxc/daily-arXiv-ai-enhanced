<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.FL](#cs.FL) [Total: 2]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.SE](#cs.SE) [Total: 12]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Solvable Tuple Patterns and Their Applications to Program Verification](https://arxiv.org/abs/2508.20365)
*Naoki Kobayashi,Ryosuke Sato,Ayumi Shinohara,Ryo Yoshinaka*

Main category: cs.PL

TL;DR: 论文提出了可解元组模式(STPs)的概念，用于表达列表式递归数据结构的程序不变量，无需负样本即可从小量正样本中高效推断，并集成到CHC求解器中实现自动化程序验证。


<details>
  <summary>Details</summary>
Motivation: 尽管程序验证技术有所进展，但递归数据结构程序的完全自动化验证仍具挑战性，需要新的方法来高效推断程序不变量。

Method: 引入可解元组模式(STPs)表达数据结构间的不变量关系，开发STP推断算法仅需正样本，利用支持序列理论的SMT求解器验证推断的STP是否为归纳不变量，并将STP推断集成到支持列表式数据结构的CHC求解器中。

Result: 集成STP推断的CHC求解器在CHC-COMP 2025的ADT-LIN类别中以显著优势获胜，证明了该方法的有效性。

Conclusion: STPs为递归数据结构的程序验证提供了高效的不变量推断方法，仅需正样本即可工作，与CHC求解器的集成展示了其在自动化程序验证中的实用价值。

Abstract: Despite the recent progress of automated program verification techniques,
fully automated verification of programs manipulating recursive data structures
remains a challenge. We introduce the notion of solvable tuple patterns (STPs)
to express invariants between list-like recursive data structures. A
distinguishing feature of STPs is that they can be efficiently inferred from
only a small number of positive samples; no negative samples are required. An
SMT solver that supports the sequence theory can be used to check that an
inferred STP is indeed an inductive invariant. After presenting basic
properties of STPs and an STP inference algorithm, we show how to incorporate
the STP inference into a CHC (Constrained Horn Clauses) solver supporting
list-like data structures, which serves as a uniform backend for automated
program verification tools. A CHC solver incorporating the STP inference has
won the ADT-LIN category of CHC-COMP 2025 by a big margin.

</details>


### [2] [Static Factorisation of Probabilistic Programs With User-Labelled Sample Statements and While Loops](https://arxiv.org/abs/2508.20922)
*Markus Böck,Jürgen Cito*

Main category: cs.PL

TL;DR: 该论文研究了如何将包含样本语句和while循环的概率程序表示为贝叶斯网络，提出了基于控制流图的静态分析方法来近似程序中的随机变量依赖结构，并开发了程序切片技术来优化变分推断、Metropolis Hastings和顺序蒙特卡洛算法。


<details>
  <summary>Details</summary>
Motivation: 解决概率程序（特别是包含用户标记样本语句和while循环的程序）能否以及如何用图形化方式表示的问题，这类程序在Gen、Turing和Pyro等概率编程语言中很常见，但现有的贝叶斯网络表示方法对此类程序的支持有限。

Method: 扩展现有操作语义以支持语言特性，通过将程序转换为控制流图来定义静态分析，近似程序中随机变量的依赖结构，获得程序密度的静态因子分解，并开发程序切片技术。

Result: 获得了程序密度的静态因子分解表示，对于无循环和常量标签的程序等价于贝叶斯网络因子分解，对于通过循环或动态标签定义无限随机变量的程序提供了新颖的图形表示。程序切片技术成功实现了三种优化，经验证性能匹配或优于现有技术。

Conclusion: 该工作为包含样本语句和while循环的概率程序提供了系统的图形表示方法，并证明了基于此结构的优化技术的正确性和有效性，为概率程序的静态分析和优化提供了新途径。

Abstract: It is commonly known that any Bayesian network can be implemented as a
probabilistic program, but the reverse direction is not so clear. In this work,
we address the open question to what extent a probabilistic program with
user-labelled sample statements and while loops - features found in languages
like Gen, Turing, and Pyro - can be represented graphically. To this end, we
extend existing operational semantics to support these language features. By
translating a program to its control-flow graph, we define a sound static
analysis that approximates the dependency structure of the random variables in
the program. As a result, we obtain a static factorisation of the implicitly
defined program density, which is equivalent to the known Bayesian network
factorisation for programs without loops and constant labels, but constitutes a
novel graphical representation for programs that define an unbounded number of
random variables via loops or dynamic labels. We further develop a sound
program slicing technique to leverage this structure to statically enable three
well-known optimisations for the considered program class: we reduce the
variance of gradient estimates in variational inference and we speed up both
single-site Metropolis Hastings and sequential Monte Carlo. These optimisations
are proven correct and empirically shown to match or outperform existing
techniques.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [3] [Formal equivalence between global optimization consistency and random search](https://arxiv.org/abs/2508.20671)
*Gaëtan Serré*

Main category: cs.FL

TL;DR: 本文通过L∃∀N定理证明器和Mathlib库形式化证明了：任何随机迭代全局优化算法在Lipschitz连续函数上一致的充要条件是该算法必须对整个搜索空间进行采样。


<details>
  <summary>Details</summary>
Motivation: 为了形式化证明随机迭代全局优化算法的收敛性理论，需要建立一个既具有足够通用性（涵盖所有此类算法）又足够具体（可用于形式化证明）的算法定义框架。

Method: 将随机迭代全局优化算法定义为一个初始概率测度和一系列马尔可夫核的组合，这些核描述了给定先前采样点及其函数值时下一个采样点的分布。使用Ionescu-Tulcea定理构建算法有限和无限迭代序列的概率测度。

Result: 成功形式化证明了随机迭代全局优化算法在Lipschitz连续函数上一致的充要条件是该算法必须对整个搜索空间进行采样。

Conclusion: 该研究为随机优化算法的理论分析提供了严格的形式化基础，建立了算法一致性与全局采样之间的等价关系，为后续算法设计和分析提供了理论保证。

Abstract: We formalize a proof that any stochastic and iterative global optimization
algorithm is consistent over Lipschitz continuous functions if and only if it
samples the whole search space. To achieve this, we use the
L$\exists$$\forall$N theorem prover and the Mathlib library. The major
challenge of this formalization, apart from the technical aspects of the proof
itself, is to converge to a definition of a stochastic and iterative global
optimization algorithm that is both general enough to encompass all algorithms
of this type and specific enough to be used in a formal proof. We define such
an algorithm as a pair of an initial probability measure and a sequence of
Markov kernels that describe the distribution of the next point sampled by the
algorithm given the previous points and their evaluations. We then construct a
probability measure on finite and infinite sequences of iterations of the
algorithm using the Ionescu-Tulcea theorem.

</details>


### [4] [Evaluating Massively Parallel Algorithms for DFA Minimisation, Equivalence Checking and Inclusion Checking](https://arxiv.org/abs/2508.20735)
*Jan Heemstra,Jan Martens,Anton Wijs*

Main category: cs.FL

TL;DR: 本文研究了在GPU上并行化确定性有限自动机(DFA)最小化和等价性检查的算法，比较了四种并行最小化算法的性能，并提出了一种新的基于分区细化和部分传递闭包的混合算法。


<details>
  <summary>Details</summary>
Motivation: 随着GPU计算能力的发展，需要研究如何利用GPU的并行计算能力来加速DFA处理算法，特别是最小化和等价性检查这些计算密集型任务。

Method: 实现了四种GPU上的并行DFA最小化算法，包括基于分区细化的算法，并提出了一种结合分区细化和并行部分传递闭包的新混合算法。对于DFA等价性检查，采用了Hopcroft-Karp算法的并行变体，并利用GPUexplore模型检查器的并行机制。

Result: 理论时间复杂度最优的算法在实际GPU运行时资源消耗过大，而并行分区细化算法在实践中表现更好。新提出的混合算法在特定基准测试中具有更好的时间复杂度和实际性能。

Conclusion: GPU并行化DFA处理算法具有实际可行性，但需要平衡理论复杂度和实际资源约束。新提出的混合算法展示了在特定场景下的优势，为DFA并行处理提供了新的思路。

Abstract: We study parallel algorithms for the minimisation and equivalence checking of
Deterministic Finite Automata (DFAs). Regarding DFA minimisation, we implement
four different massively parallel algorithms on Graphics Processing
Units~(GPUs). Our results confirm the expectations that the algorithm with the
theoretically best time complexity is not practically suitable to run on GPUs
due to the large amount of resources needed. We empirically verify that
parallel partition refinement algorithms from the literature perform better in
practice, even though their time complexity is worse. Furthermore, we introduce
a novel algorithm based on partition refinement with an extra parallel partial
transitive closure step and show that on specific benchmarks it has better
run-time complexity and performs better in practice.
  In addition, we address checking the language equivalence and inclusion of
two DFAs. We consider the Hopcroft-Karp algorithm, and explain how a variant of
it can be parallelised for GPUs. We note that these problems can be encoded for
the GPU-accelerated model checker \GPUexplore, allowing the use its lockless
hash table and fine-grained parallel work distribution mechanism.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [5] [Exploiting Instantiations from Paramodulation Proofs in Isabelle/HOL](https://arxiv.org/abs/2508.20738)
*Lukas Bartl,Jasmin Blanchette,Tobias Nipkow*

Main category: cs.LO

TL;DR: Metis是Isabelle/HOL中的有序参数化证明器，新工具通过分析成功证明来推导变量实例化，提高Sledgehammer的成功率和证明速度


<details>
  <summary>Details</summary>
Motivation: 提高Sledgehammer工具的成功率，加速生成的证明过程，并帮助用户理解目标如何从引理推导

Method: 分析Metis成功证明来推导变量实例化

Result: 增加了Sledgehammer的成功率，提高了证明生成速度，帮助用户理解证明过程

Conclusion: 通过分析成功证明推导变量实例化的方法是有效的，能够显著提升证明辅助工具的性能和可用性

Abstract: Metis is an ordered paramodulation prover built into the Isabelle/HOL proof
assistant. It attempts to close the current goal using a given list of lemmas.
Typically these lemmas are found by Sledgehammer, a tool that integrates
external automatic provers. We present a new tool that analyzes successful
Metis proofs to derive variable instantiations. These increase Sledgehammer's
success rate, improve the speed of Sledgehammer-generated proofs, and help
users understand why a goal follows from the lemmas.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [6] [Evaluating LLMs on microservice-based applications: how complex is your specification?](https://arxiv.org/abs/2508.20119)
*Daniel M. Yellin*

Main category: cs.SE

TL;DR: 本文评估了LLM在真实世界微服务应用代码生成方面的能力，提出了难度评分标准和自动化测试框架，发现强LLM在中等难度任务表现良好但在高难度任务表现很差。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在真实世界微服务架构代码生成方面的进展，探索其在复杂业务场景下的能力边界。

Method: 定义了微服务应用的标准模板和难度评分指标，开发了自动化测试框架来评估LLM生成的代码质量。

Result: 强LLM（如GPT-3o-mini）在中等难度规范上表现良好，但在高难度规范上表现很差，主要由于复杂业务逻辑、外部服务集成、数据库操作和非功能性需求（如认证）的挑战。

Conclusion: LLM在真实世界代码生成方面仍有显著局限，特别是在处理复杂业务逻辑和系统集成时，需要进一步研究改进代码合成技术。

Abstract: In this paper we evaluate how far LLMs have advanced in generating code for
real-world problems. Specifically, we explore code synthesis for
microservice-based applications, a widely used architecture pattern. We define
a standard template for specifying these applications, and we propose a metric
for judging the difficulty level of a specification. The higher the score, the
more difficult it is to generate code for the specification. We develop a
framework to automate the process of testing LLM-synthesized code for a
microservice using unit tests. Our experimental results show that strong LLMs
(like GPT-3o-mini) do fairly well on medium difficulty specifications but do
very poorly on those of higher difficulty levels. This is due to more intricate
business logic, a greater use of external services, database integration and
inclusion of non-functional capabilities such as authentication. We analyzed
the errors in LLM-synthesized code and report on the key challenges LLMs face
in generating code for these specifications thereby suggesting future research
directions to improve code synthesis for real-world problems.

</details>


### [7] [Towards Better Correctness and Efficiency in Code Generation](https://arxiv.org/abs/2508.20124)
*Yunlong Feng,Yang Xu,Xiao Xu,Binyuan Hui,Junyang Lin*

Main category: cs.SE

TL;DR: 提出基于强化学习的效率导向框架，通过动态探索和误差不敏感方法，在保持代码正确性的同时显著提升运行效率


<details>
  <summary>Details</summary>
Motivation: 现有代码大语言模型生成的代码运行效率较差，限制了在性能敏感场景的实际应用

Method: 两阶段调优方法：第一阶段确保代码正确性，第二阶段通过动态探索和性能奖励进行效率优化，采用误差不敏感的强化学习方法

Result: 在7B模型上，代码正确性提升10.18%，运行效率提升7.75%，性能达到与更大模型相当的水平

Conclusion: 该框架有效解决了代码效率问题，通过平衡正确性和效率，为代码生成模型的实际应用提供了可行方案

Abstract: While code large language models have demonstrated remarkable progress in
code generation, the generated code often exhibits poor runtime efficiency,
limiting its practical application in performance-sensitive scenarios. To
address this limitation, we propose an efficiency-oriented reinforcement
learning framework guided by a novel performance reward. Based on this
framework, we take a deeper dive into the code efficiency problem, identifying
then proposing methods to overcome key bottlenecks: (1) Dynamic exploration
overcomes the static data constraints of offline fine-tuning, enabling the
discovery of more efficient code implementations. (2) The error-insensitive
reinforcement learning method and high-contrast efficiency signals are crucial
for mitigating systematic errors and achieving effective optimization. (3)
Online exploration is most effective when starting from a high-correctness
baseline, as this allows for efficiency improvements without sacrificing
accuracy. With these discoveries, we finally propose a two-stage tuning method,
which achieves high and balanced performance across correctness and efficiency.
The results of experiments show the effectiveness of the method, which improves
code correctness by 10.18\% and runtime efficiency by 7.75\% on a 7B model,
achieving performance comparable to much larger model.

</details>


### [8] [Boosting Skeleton-Driven SMT Solver Fuzzing by Leveraging LLM to Produce Formula Generators](https://arxiv.org/abs/2508.20340)
*Maolin Sun,Yibiao Yang,Yuming Zhou*

Main category: cs.SE

TL;DR: Chimera是一个基于LLM的SMT求解器模糊测试框架，通过生成可重用的项生成器而非直接生成公式，解决了现有方法中语法无效和计算开销大的问题。


<details>
  <summary>Details</summary>
Motivation: SMT求解器在现代系统和编程语言研究中至关重要，但现有测试技术难以跟上快速发展的求解器特性。基于LLM的方法虽然显示出潜力，但存在语法无效率高和计算开销大的问题。

Method: Chimera使用LLM从文档中自动提取SMT理论的上下文无关文法，并合成符合这些文法的可组合布尔项生成器。在模糊测试时，用LLM合成的生成器产生的项填充现有公式的结构骨架。

Result: 在Z3和cvc5两个主流SMT求解器上测试，Chimera发现了43个已确认的bug，其中40个已被开发者修复。

Conclusion: Chimera通过一次性LLM交互投资，显著降低了运行时成本，同时确保了语法有效性并促进了语义多样性，为SMT求解器的测试提供了高效解决方案。

Abstract: Satisfiability Modulo Theory (SMT) solvers are foundational to modern systems
and programming languages research, providing the foundation for tasks like
symbolic execution and automated verification. Because these solvers sit on the
critical path, their correctness is essential, and high-quality test formulas
are key to uncovering bugs. However, while prior testing techniques performed
well on earlier solver versions, they struggle to keep pace with rapidly
evolving features. Recent approaches based on Large Language Models (LLMs) show
promise in exploring advanced solver capabilities, but two obstacles remain:
nearly half of the generated formulas are syntactically invalid, and iterative
interactions with the LLMs introduce substantial computational overhead. In
this study, we present Chimera, a novel LLM-assisted fuzzing framework that
addresses both issues by shifting from direct formula generation to the
synthesis of reusable term (i.e., logical expression) generators. Particularly,
Chimera uses LLMs to (1) automatically extract context-free grammars (CFGs) for
SMT theories, including solver-specific extensions, from documentation, and (2)
synthesize composable Boolean term generators that adhere to these grammars.
During fuzzing, Chimera populates structural skeletons derived from existing
formulas with the terms iteratively produced by the LLM-synthesized generators.
This design ensures syntactic validity while promoting semantic diversity.
Notably, Chimera requires only one-time LLM interaction investment,
dramatically reducing runtime cost. We evaluated Chimera on two leading SMT
solvers: Z3 and cvc5. Our experiments show that Chimera has identified 43
confirmed bugs, 40 of which have already been fixed by developers.

</details>


### [9] [Adaptive Root Cause Localization for Microservice Systems with Multi-Agent Recursion-of-Thought](https://arxiv.org/abs/2508.20370)
*Lingzhe Zhang,Tong Jia,Kangjin Wang,Weijie Hong,Chiming Duan,Minghua He,Ying Li*

Main category: cs.SE

TL;DR: RCLAgent是一个基于多智能体递归思维框架的自适应根因定位方法，通过模拟SRE的递归、多维扩展和跨模态推理特性，在单次请求中实现优于现有方法的根因定位性能。


<details>
  <summary>Details</summary>
Motivation: 微服务系统日益复杂，故障频发，现有根因定位方法要么依赖预定义模式难以适应变化，要么缺乏可解释性。通过研究SRE的人工根因分析过程，发现其具有递归性、多维扩展和跨模态推理三个关键特征。

Method: 提出RCLAgent方法，采用多智能体递归思维框架，使用新颖的递归思维策略指导LLM推理过程，整合多智能体数据和工具辅助分析来精确定位根因。

Result: 在多个公开数据集上的实验评估表明，RCLAgent仅需单次请求就能实现根因定位，性能优于依赖多次请求聚合的最先进方法。

Conclusion: RCLAgent有效提升了复杂微服务环境中根因定位的效率和精度，证明了其在自适应根因分析中的有效性。

Abstract: As contemporary microservice systems become increasingly popular and
complex-often comprising hundreds or even thousands of fine-grained,
interdependent subsystems-they are facing more frequent failures. Ensuring
system reliability thus demands accurate root cause localization. While traces
and metrics have proven to be effective data sources for this task, existing
methods either heavily rely on pre-defined schemas, which struggle to adapt to
evolving operational contexts, or lack interpretability in their reasoning
process, thereby leaving Site Reliability Engineers (SREs) confused. In this
paper, we conduct a comprehensive study on how SREs localize the root cause of
failures, drawing insights from multiple professional SREs across different
organizations. Our investigation reveals that human root cause analysis
exhibits three key characteristics: recursiveness, multi-dimensional expansion,
and cross-modal reasoning. Motivated by these findings, we introduce RCLAgent,
an adaptive root cause localization method for microservice systems that
leverages a multi-agent recursion-of-thought framework. RCLAgent employs a
novel recursion-of-thought strategy to guide the LLM's reasoning process,
effectively integrating data from multiple agents and tool-assisted analysis to
accurately pinpoint the root cause. Experimental evaluations on various public
datasets demonstrate that RCLAgent achieves superior performance by localizing
the root cause using only a single request-outperforming state-of-the-art
methods that depend on aggregating multiple requests. These results underscore
the effectiveness of RCLAgent in enhancing the efficiency and precision of root
cause localization in complex microservice environments.

</details>


### [10] [AI and Agile Software Development: A Research Roadmap from the XP2025 Workshop](https://arxiv.org/abs/2508.20563)
*Zheying Zhang,Tomas Herda,Victoria Pichler,Pekka Abrahamsson,Geir K. Hanssen,Joshua Kerievsky,Alex Polyakov,Mohit Chandna,Marius Irgens,Kai-Kristian Kemell,Ayman Asad Khan,Crystal Kwok,Evan Leybourn,Munish Malik,Dorota Mleczko,Morteza Moalagh,Christopher Morales,Yuliia Pieskova,Daniel Planötscher,Mika Saari,Anastasiia Tkalich,Karl Josef Gstettner,Xiaofeng Wang*

Main category: cs.SE

TL;DR: XP2025研讨会总结报告，汇集30多位学者和从业者探讨GenAI与敏捷开发的挑战与机遇，识别了工具碎片化、治理、数据质量等痛点，并制定了多主题研究路线图


<details>
  <summary>Details</summary>
Motivation: 解决生成式人工智能(GenAI)与敏捷软件开发交叉领域的具体挑战和新兴机遇，促进两者的负责任、以人为本的整合

Method: 通过结构化、互动式分组讨论会议，汇集跨学科学术研究人员和行业从业者，共同分析问题并制定研究路线图

Result: 识别了工具碎片化、治理、数据质量、AI素养和提示工程技能差距等关键痛点，分析了根本原因和交叉关注点

Conclusion: 协作制定了包含短期可实施行动和长期愿景研究方向的多主题研究路线图，旨在指导未来研究并推动GenAI在敏捷实践中的负责任整合

Abstract: This paper synthesizes the key findings from a full-day XP2025 workshop on
"AI and Agile: From Frustration to Success", held in Brugg-Windisch,
Switzerland. The workshop brought together over 30 interdisciplinary academic
researchers and industry practitioners to tackle the concrete challenges and
emerging opportunities at the intersection of Generative Artificial
Intelligence (GenAI) and agile software development. Through structured,
interactive breakout sessions, participants identified shared pain points like
tool fragmentation, governance, data quality, and critical skills gaps in AI
literacy and prompt engineering. These issues were further analyzed, revealing
underlying causes and cross-cutting concerns. The workshop concluded by
collaboratively co-creating a multi-thematic research roadmap, articulating
both short-term, implementable actions and visionary, long-term research
directions. This cohesive agenda aims to guide future investigation and drive
the responsible, human-centered integration of GenAI into agile practices.

</details>


### [11] [Rethinking Testing for LLM Applications: Characteristics, Challenges, and a Lightweight Interaction Protocol](https://arxiv.org/abs/2508.20737)
*Wei Ma,Yixiao Yang,Qiang Hu,Shi Ying,Zhi Jin,Bo Du,Zhenchang Xing,Tianlin Li,Junjie Shi,Yang Liu,Linxiao Jiang*

Main category: cs.SE

TL;DR: 本文提出了大语言模型应用的三层测试架构，分析了传统测试方法在各层的适用性，并提出四种协同策略和可信质量保障框架。


<details>
  <summary>Details</summary>
Motivation: 大语言模型应用的非确定性、动态性和上下文依赖性给质量保障带来根本挑战，需要新的测试方法来应对这些挑战。

Method: 将LLM应用解构为三层架构：系统壳层、提示组织层和LLM推理核心层，分析各层的测试需求，提出四种协同策略（保留、转换、集成、运行时）和可信质量保障框架。

Result: 识别了软件工程和AI安全分析技术之间的结构性断层，提出了四个根本差异和6个核心挑战，并提出了实践指南和测试协议AICL。

Conclusion: 通过多层测试架构和协同策略，可以建立更有效的LLM应用质量保障体系，为标准化和工具化提供基础。

Abstract: Applications of Large Language Models~(LLMs) have evolved from simple text
generators into complex software systems that integrate retrieval augmentation,
tool invocation, and multi-turn interactions. Their inherent non-determinism,
dynamism, and context dependence pose fundamental challenges for quality
assurance. This paper decomposes LLM applications into a three-layer
architecture: \textbf{\textit{System Shell Layer}}, \textbf{\textit{Prompt
Orchestration Layer}}, and \textbf{\textit{LLM Inference Core}}. We then assess
the applicability of traditional software testing methods in each layer:
directly applicable at the shell layer, requiring semantic reinterpretation at
the orchestration layer, and necessitating paradigm shifts at the inference
core. A comparative analysis of Testing AI methods from the software
engineering community and safety analysis techniques from the AI community
reveals structural disconnects in testing unit abstraction, evaluation metrics,
and lifecycle management. We identify four fundamental differences that
underlie 6 core challenges. To address these, we propose four types of
collaborative strategies (\emph{Retain}, \emph{Translate}, \emph{Integrate},
and \emph{Runtime}) and explore a closed-loop, trustworthy quality assurance
framework that combines pre-deployment validation with runtime monitoring.
Based on these strategies, we offer practical guidance and a protocol proposal
to support the standardization and tooling of LLM application testing. We
propose a protocol \textbf{\textit{Agent Interaction Communication Language}}
(AICL) that is used to communicate between AI agents. AICL has the
test-oriented features and is easily integrated in the current agent framework.

</details>


### [12] [From Law to Gherkin: A Human-Centred Quasi-Experiment on the Quality of LLM-Generated Behavioural Specifications from Food-Safety Regulations](https://arxiv.org/abs/2508.20744)
*Shabnam Hassani,Mehrdad Sabetzadeh,Daniel Amyot*

Main category: cs.SE

TL;DR: LLMs能够从法律文本中生成高质量的Gherkin行为规范，显著减少人工工作量，为软件合规性提供结构化支持


<details>
  <summary>Details</summary>
Motivation: 法律文本采用技术中性语言编写，工程师手动创建合规需求文档工作量大且易出错，需要利用GenAI技术自动化这一过程

Method: 采用准实验设计，10名参与者评估Claude和Llama从食品安全法规生成的60个Gherkin规范，每个规范由2人评估5个标准，共120次评估

Result: 相关性75%最高评分，清晰度90%最高，完整性75%最高，单一性82%最高，时间节省68%最高。Llama在清晰度、完整性和时间节省方面略优，Claude在单一性方面更强

Conclusion: LLMs能够从法律文本生成高质量的Gherkin规范，减少人工工作量，为实施、保证和测试生成提供有用的结构化成果

Abstract: Context: Laws and regulations increasingly affect software design and quality
assurance, but legal texts are written in technology-neutral language. This
creates challenges for engineers who must develop compliance artifacts such as
requirements and acceptance criteria. Manual creation is labor-intensive,
error-prone, and requires domain expertise. Advances in Generative AI (GenAI),
especially Large Language Models (LLMs), offer a way to automate deriving such
artifacts.
  Objective: We present the first systematic human-subject study of LLMs'
ability to derive behavioral specifications from legal texts using a
quasi-experimental design. These specifications translate legal requirements
into a developer-friendly form.
  Methods: Ten participants evaluated specifications generated from food-safety
regulations by Claude and Llama. Using Gherkin, a structured BDD language, 60
specifications were produced. Each participant assessed 12 across five
criteria: Relevance, Clarity, Completeness, Singularity, and Time Savings. Each
specification was reviewed by two participants, yielding 120 assessments.
  Results: For Relevance, 75% of ratings were highest and 20% second-highest.
Clarity reached 90% highest. Completeness: 75% highest, 19% second.
Singularity: 82% highest, 12% second. Time Savings: 68% highest, 24% second. No
lowest ratings occurred. Mann-Whitney U tests showed no significant differences
across participants or models. Llama slightly outperformed Claude in Clarity,
Completeness, and Time Savings, while Claude was stronger in Singularity.
Feedback noted hallucinations and omissions but confirmed the utility of the
specifications.
  Conclusion: LLMs can generate high-quality Gherkin specifications from legal
texts, reducing manual effort and providing structured artifacts useful for
implementation, assurance, and test generation.

</details>


### [13] [Towards an Architectural Perspective for Sustainability: Bundle the Needs from Industry](https://arxiv.org/abs/2508.20774)
*Markus Funke,Patricia Lago*

Main category: cs.SE

TL;DR: 提出了可持续性视角愿景，通过系统文献回顾和专家焦点小组研究，为软件架构师提供结构化指导来处理可持续性质量属性


<details>
  <summary>Details</summary>
Motivation: 软件密集型系统中可持续性作为新兴质量属性日益重要，但架构师在设计阶段缺乏有效的结构化指导方法

Method: 采用雪球法对开创性文献进行系统回顾，并组织领域专家焦点小组讨论，制定可持续性视角框架

Result: 确认了不同视角元素在实践中的相关性，并明确了满足工业需求的可持续性视角的关键要素

Conclusion: 可持续性视角愿景为软件架构提供了独立于架构框架和行业背景的结构化方法，能够有效指导可持续性质量属性的处理

Abstract: Sustainability is increasingly recognized as an emerging quality property in
software-intensive systems, yet architects lack structured guidance to address
it effectively throughout the software design phase. Architectural
perspectives-an architectural knowledge artifact composed of concerns,
activities, tactics, pitfalls, and checklists-offer a promising approach to
tackle such emerging quality properties across architectural views and are also
independent of architecture frameworks and industry contexts. In this paper, we
present a sustainability perspective vision, i.e., a revised notion of
architectural perspective meant to be filled with its own elements to target
sustainability concerns. We formulate our sustainability perspective vision
through evidence from applying snowballing to seminal literature and from
conducting a focus group with experts in the field. Our findings confirm the
relevance of the different perspective elements in practice and highlight
implications for shaping a sustainability perspective that meets industrial
needs.

</details>


### [14] [Automated Test Oracles for Flaky Cyber-Physical System Simulators: Approach and Evaluation](https://arxiv.org/abs/2508.20902)
*Baharin A. Jodat,Khouloud Gaaloul,Mehrdad Sabetzadeh,Shiva Nejati*

Main category: cs.SE

TL;DR: 提出基于断言的测试预言方法，使用遗传编程和决策树技术为信息物理系统生成无需执行测试即可判断结果的测试预言，其中遗传编程配合Ochiai公式在准确性和抗干扰性方面表现最佳


<details>
  <summary>Details</summary>
Motivation: 信息物理系统仿真测试成本高且存在结果不一致问题，需要无需系统执行的自动化测试预言来降低测试成本，同时要求测试预言具有可解释性并能抵抗仿真器的不稳定性

Method: 提出基于断言的测试预言方法，使用两种生成技术：1）遗传编程（GP）结合频谱故障定位排名公式（Ochiai、Tarantula、Naish）作为适应度函数；2）决策树（DT）和决策规则（DR）

Result: 研究表明，使用GP配合Ochiai公式生成的测试预言准确率显著高于其他方法，即使在系统存在不稳定性情况下仍保持优势，平均准确率变化仅为4%，表现出良好的抗干扰性

Conclusion: 基于断言的测试预言方法能有效降低信息物理系统测试成本，GP配合Ochiai公式是最优选择，生成的测试预言具有高准确性和强鲁棒性，适用于航空航天、网络和自动驾驶等领域

Abstract: Simulation-based testing of cyber-physical systems (CPS) is costly due to the
time-consuming execution of CPS simulators. In addition, CPS simulators may be
flaky, leading to inconsistent test outcomes and requiring repeated test
re-execution for reliable test verdicts. Automated test oracles that do not
require system execution are therefore crucial for reducing testing costs.
Ideally, such test oracles should be interpretable to facilitate human
understanding of test verdicts, and they must be robust against the potential
flakiness of CPS simulators. In this article, we propose assertion-based test
oracles for CPS as sets of logical and arithmetic predicates defined over the
inputs of the system under test. Given a test input, our assertion-based test
oracle determines, without requiring test execution, whether the test passes,
fails, or if the oracle is inconclusive in predicting a verdict. We describe
two methods for generating assertion-based test oracles: one using genetic
programming~(GP) that employs well-known spectrum-based fault localization
(SBFL) ranking formulas, namely Ochiai, Tarantula, and Naish, as fitness
functions; and the other using decision trees (DT) and decision rules (DR). We
evaluate our assertion-based test oracles through case studies in the domains
of aerospace, networking and autonomous driving. We show that test oracles
generated using GP with Ochiai are significantly more accurate than those
obtained using GP with Tarantula and Naish or using DT or DR. Moreover, this
accuracy advantage remains even when accounting for the flakiness of the system
under test. We further show that the assertion-based test oracles generated by
GP with Ochiai are robust against flakiness with only 4% average variation in
their accuracy results across four different network and autonomous driving
systems with flaky behaviours.

</details>


### [15] [Deep Learning Based Concurrency Bug Detection and Localization](https://arxiv.org/abs/2508.20911)
*Zuocheng Feng,Kaiwen Zhang,Miaomiao Wang,Yiming Cheng,Yuandao Cai,Xiaofeng Li,Guanjun Liu*

Main category: cs.SE

TL;DR: 提出了一种基于图神经网络和预训练模型的新方法，用于并发bug检测和定位，通过构建专用数据集和并发感知代码属性图来提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在并发bug检测方面存在三个主要限制：缺乏大规模专用数据集、并发语义表示不足、二进制分类无法提供细粒度调试信息。

Method: 构建专用并发bug数据集，集成预训练模型和异构图神经网络，使用新的并发感知代码属性图(CCPG)来表征并发语义，并采用SubgraphX方法进行精确定位。

Result: 在多种评估设置下，相比最先进方法，平均准确率和精确度提升10%，召回率提升26%。

Conclusion: 该方法有效解决了并发bug检测和定位问题，通过结合预训练模型和图神经网络，显著提升了检测性能和调试效率。

Abstract: Concurrency bugs, caused by improper synchronization of shared resources in
multi-threaded or distributed systems, are notoriously hard to detect and thus
compromise software reliability and security. The existing deep learning
methods face three main limitations. First, there is an absence of large and
dedicated datasets of diverse concurrency bugs for them. Second, they lack
sufficient representation of concurrency semantics. Third, binary
classification results fail to provide finer-grained debug information such as
precise bug lines. To address these problems, we propose a novel method for
effective concurrency bug detection as well as localization. We construct a
dedicated concurrency bug dataset to facilitate model training and evaluation.
We then integrate a pre-trained model with a heterogeneous graph neural network
(GNN), by incorporating a new Concurrency-Aware Code Property Graph (CCPG) that
concisely and effectively characterizes concurrency semantics. To further
facilitate debugging, we employ SubgraphX, a GNN-based interpretability method,
which explores the graphs to precisely localize concurrency bugs, mapping them
to specific lines of source code. On average, our method demonstrates an
improvement of 10\% in accuracy and precision and 26\% in recall compared to
state-of-the-art methods across diverse evaluation settings.

</details>


### [16] [ConfLogger: Enhance Systems' Configuration Diagnosability through Configuration Logging](https://arxiv.org/abs/2508.20977)
*Shiwen Shan,Yintong Huo,Yuxin Su,Zhining Wang,Dan Li,Zibin Zheng*

Main category: cs.SE

TL;DR: 提出ConfLogger工具，通过配置感知的静态污点分析和LLM日志生成，增强软件配置可诊断性，在8个系统中验证有效，显著提升错误定位准确性和诊断效率


<details>
  <summary>Details</summary>
Motivation: 现代可配置系统虽然提供灵活性，但带来了配置相关问题。现有诊断方法专注于故障后分析，但缺乏对软件是否提供足够故障信息进行诊断的关注

Method: 1) 通过追踪整个项目中的配置相关数据流来识别配置敏感代码段；2) 通过分析配置代码上下文生成诊断日志语句；结合配置感知静态污点分析和基于LLM的日志生成

Result: 在8个流行软件系统中验证有效：ConfLogger增强的日志帮助诊断工具在30个静默配置错误场景中达到100%错误定位准确度，80%可通过暴露的显式配置信息直接解决；覆盖现有日志点74%，优于基线LLM日志器12-30%；在变量日志方面精度提升8.6%，召回率提升79.3%，F1提升26.2%；用户研究显示诊断时间加快1.25倍，故障排除准确率提升251.4%

Conclusion: ConfLogger通过统一的配置感知静态分析和LLM日志生成方法，有效增强了软件配置可诊断性，为配置相关问题的诊断提供了实用解决方案

Abstract: Modern configurable systems offer customization via intricate configuration
spaces, yet such flexibility introduces pervasive configuration-related issues
such as misconfigurations and latent softwarebugs. Existing diagnosability
supports focus on post-failure analysis of software behavior to identify
configuration issues, but none of these approaches look into whether the
software clue sufficient failure information for diagnosis. To fill in the
blank, we propose the idea of configuration logging to enhance existing logging
practices at the source code level. We develop ConfLogger, the first tool that
unifies configuration-aware static taint analysis with LLM-based log generation
to enhance software configuration diagnosability. Specifically, our method 1)
identifies configuration-sensitive code segments by tracing
configuration-related data flow in the whole project, and 2) generates
diagnostic log statements by analyzing configuration code contexts. Evaluation
results on eight popular software systems demonstrate the effectiveness of
ConfLogger to enhance configuration diagnosability. Specifically,
ConfLogger-enhanced logs successfully aid a log-based misconfiguration
diagnosis tool to achieve 100% accuracy on error localization in 30 silent
misconfiguration scenarios, with 80% directly resolvable through explicit
configuration information exposed. In addition, ConfLogger achieves 74%
coverage of existing logging points, outperforming baseline LLM-based loggers
by 12% and 30%. It also gains 8.6% higher in precision, 79.3% higher in recall,
and 26.2% higher in F1 compared to the state-of-the-art baseline in terms of
variable logging while also augmenting diagnostic value. A controlled user
study on 22 cases further validated its utility, speeding up diagnostic time by
1.25x and improving troubleshooting accuracy by 251.4%.

</details>


### [17] [Dynamics of Gender Bias in Software Engineering](https://arxiv.org/abs/2508.21050)
*Thomas J. Misa*

Main category: cs.SE

TL;DR: 该论文分析了软件工程领域的性别偏见问题，包括该领域的起源、工程专业性以及在ICSE会议中女性参与度的统计分析，发现了长达12年的性别排除现象。


<details>
  <summary>Details</summary>
Motivation: 软件工程领域根深蒂固地嵌入了工程学和计算机科学，可能存在两个领域都常见的性别偏见问题。研究动机在于探索软件工程领域中的性别偏见现象和女性参与情况。

Method: 采用了多种研究方法：1）调查软件工程的起源和工程专业性发展；2）分析该领域最近对性别问题的关注；3）对领军国际软件工程会议（ICSE）1976-2010年间的研究作者进行定量分析，识别统计上显著的性别排除年份。

Result: 研究发现在ICSE会议中存在长达12年的统计上显著的性别排除现象，说明软件工程领域确实存在性别偏见问题。

Conclusion: 软件工程领域存在系统性的性别偏见，需要通过政策层面的研究和干预措施来解决这些问题，以促进领域的性别平等和多元化。

Abstract: The field of software engineering is embedded in both engineering and
computer science, and may embody gender biases endemic to both. This paper
surveys software engineering's origins and its long-running attention to
engineering professionalism, profiling five leaders; it then examines the
field's recent attention to gender issues and gender bias. It next
quantitatively analyzes women's participation as research authors in the
field's leading International Conference of Software Engineering (1976-2010),
finding a dozen years with statistically significant gender exclusion. Policy
dimensions of research on gender bias in computing are suggested.

</details>
