<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 18]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.LO](#cs.LO) [Total: 6]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [How Software Engineers Engage with AI: A Pragmatic Process Model and Decision Framework Grounded in Industry Observations](https://arxiv.org/abs/2507.17930)
*Vahid Garousi,Zafar Jafarov*

Main category: cs.SE

TL;DR: 本文探讨了AI如何辅助软件工程，提出了一个实用的过程模型和决策框架，帮助开发者在AI工具使用中权衡效率与质量。


<details>
  <summary>Details</summary>
Motivation: 研究AI工具（如GitHub Copilot和ChatGPT）在软件工程中的实际应用，尤其是开发者如何信任、优化或拒绝AI生成的内容。

Method: 通过实践者报告和土耳其、阿塞拜疆三个行业环境的直接观察，提出了一个过程模型和2D决策框架。

Result: 模型和框架为开发者提供了结构化、轻量级的指导，支持更有效的人机协作。

Conclusion: 研究为AI在软件工程中的实际应用提供了实用指导，推动了人机协作的讨论。

Abstract: Artificial Intelligence (AI) has the potential to transform Software
Engineering (SE) by enhancing productivity, efficiency, and decision support.
Tools like GitHub Copilot and ChatGPT have given rise to "vibe coding"-an
exploratory, prompt-driven development style. Yet, how software engineers
engage with these tools in daily tasks, especially in deciding whether to
trust, refine, or reject AI-generated outputs, remains underexplored. This
paper presents two complementary contributions. First, a pragmatic process
model capturing real-world AI-assisted SE activities, including prompt design,
inspection, fallback, and refinement. Second, a 2D decision framework that
could help developers reason about trade-offs between effort saved and output
quality. Grounded in practitioner reports and direct observations in three
industry settings across Turkiye and Azerbaijan, our work illustrates how
engineers navigate AI use with human oversight. These models offer structured,
lightweight guidance to support more deliberate and effective use of AI tools
in SE, contributing to ongoing discussions on practical human-AI collaboration.

</details>


### [2] [Use as Directed? A Comparison of Software Tools Intended to Check Rigor and Transparency of Published Work](https://arxiv.org/abs/2507.17991)
*Peter Eckmann,Adrian Barnett,Alexandra Bannach-Brown,Elisa Pilar Bascunan Atria,Guillaume Cabanac,Louise Delwen Owen Franzen,Małgorzata Anna Gazda,Kaitlyn Hair,James Howison,Halil Kilicoglu,Cyril Labbe,Sarah McCann,Vladislav Nachev,Martijn Roelandse,Maia Salholz-Hillel,Robert Schulz,Gerben ter Riet,Colby Vorland,Anita Bandrowski,Tracey Weissgerber*

Main category: cs.SE

TL;DR: 论文比较了11种自动化工具在9种严谨性标准上的表现，发现某些工具在特定标准（如开放数据检测）上表现突出，而工具组合在其他标准（如纳入排除标准检测）上更优。


<details>
  <summary>Details</summary>
Motivation: 解决科学报告中标准化和透明度不足导致的再现性危机。

Method: 对11种自动化工具在9种严谨性标准上进行广泛比较。

Result: 某些工具在特定标准上表现优异，工具组合在某些标准上表现更佳。

Conclusion: 提出了工具开发的改进建议，并分享了代码和数据。

Abstract: The causes of the reproducibility crisis include lack of standardization and
transparency in scientific reporting. Checklists such as ARRIVE and CONSORT
seek to improve transparency, but they are not always followed by authors and
peer review often fails to identify missing items. To address these issues,
there are several automated tools that have been designed to check different
rigor criteria. We have conducted a broad comparison of 11 automated tools
across 9 different rigor criteria from the ScreenIT group. We found some
criteria, including detecting open data, where the combination of tools showed
a clear winner, a tool which performed much better than other tools. In other
cases, including detection of inclusion and exclusion criteria, the combination
of tools exceeded the performance of any one tool. We also identified key areas
where tool developers should focus their effort to make their tool maximally
useful. We conclude with a set of insights and recommendations for stakeholders
in the development of rigor and transparency detection tools. The code and data
for the study is available at https://github.com/PeterEckmann1/tool-comparison.

</details>


### [3] [An Empirical Study of GenAI Adoption in Open-Source Game Development: Tools, Tasks, and Developer Challenges](https://arxiv.org/abs/2507.18029)
*Xiang Echo Chen,Wenhan Zhu,Guoshuai Albert Shi,Michael W. Godfrey*

Main category: cs.SE

TL;DR: 研究探讨生成式AI（GenAI）在开源游戏开发中的应用，分析GitHub上的讨论以比较GenAI与传统AI（TradAI）及非AI话题的差异。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的能力逐渐改变游戏开发方式，但缺乏实际开发中的实证研究，尤其是在开源社区中的应用情况。

Method: 通过分析GitHub上的议题讨论，构建数据集并采用开放卡片分类和主题分析，比较GenAI、TradAI及非AI议题。

Result: 揭示了GenAI在开源游戏开发中的使用模式、开发者关注点及集成实践与传统方法的差异。

Conclusion: GenAI在游戏开发中展现出独特的工作流程和挑战，为未来研究提供了实证基础。

Abstract: The growing capabilities of generative AI (GenAI) have begun to reshape how
games are designed and developed, offering new tools for content creation,
gameplay simulation, and design ideation. While prior research has explored
traditional uses of AI in games, such as controlling agents or generating
procedural content. There is limited empirical understanding of how GenAI is
adopted by developers in real-world contexts, especially within the open-source
community. This study aims to explore how GenAI technologies are discussed,
adopted, and integrated into open-source game development by analyzing issue
discussions on GitHub. We investigate the tools, tasks, and challenges
associated with GenAI by comparing GenAI-related issues to those involving
traditional AI (TradAI) and NonAI topics. Our goal is to uncover how GenAI
differs from other approaches in terms of usage patterns, developer concerns,
and integration practices. To address this objective, we construct a dataset of
open-source game repositories that discuss AI-related topics. We apply open
card sorting and thematic analysis to a stratified sample of GitHub issues,
labelling each by type and content. These annotations enable comparative
analysis across GenAI, TradAI, and NonAI groups, and provide insight into how
GenAI is shaping the workflows and pain points of open-source game developers.

</details>


### [4] [Your ATs to Ts: MITRE ATT&CK Attack Technique to P-SSCRM Task Mapping](https://arxiv.org/abs/2507.18037)
*Sivana Hamer,Jacob Bowen,Md Nazmul Haque,Chris Madden,Laurie Williams*

Main category: cs.SE

TL;DR: 本文描述了MITRE ATT&CK攻击技术与P-SSCRM任务的映射，帮助软件组织识别如何通过任务缓解软件供应链攻击。


<details>
  <summary>Details</summary>
Motivation: 通过映射MITRE ATT&CK攻击技术与P-SSCRM任务，提供一种方法帮助软件组织识别和管理供应链攻击风险。

Method: 采用四种独立策略确定一致的映射关系，并将P-SSCRM任务与其他10个框架任务关联。

Result: 提供了MITRE ATT&CK与其他政府及行业框架之间的映射关系。

Conclusion: 该映射为软件组织提供了一种系统化的方法来管理供应链攻击风险。

Abstract: The MITRE Adversarial Tactics, Techniques and Common Knowledge (MITRE ATT&CK)
Attack Technique to Proactive Software Supply Chain Risk Management Framework
(P-SSCRM) Task mapping described in this document helps software organizations
to determine how different tasks mitigate the attack techniques of software
supply chain attacks. The mapping was created through four independent
strategies to find agreed-upon mappings. Because each P-SSCRM task is mapped to
one or more tasks from the 10 frameworks, the mapping we provide is also a
mapping between MITRE ATT&CK and other prominent government and industry
frameworks.

</details>


### [5] [Factors Impacting Faculty Adoption of Project-Based Learning in Computing Education: a Survey](https://arxiv.org/abs/2507.18039)
*Ahmad D. Suleiman,Yiming Tang,Daqing Hou*

Main category: cs.SE

TL;DR: 研究探讨了影响计算机教育工作者在软件工程和计算课程中采用项目式学习（PjBL）的因素，揭示了采用障碍及促进策略。


<details>
  <summary>Details</summary>
Motivation: 尽管PjBL能提升学生能力，但教师采用率不高，研究旨在探索障碍并提供解决方案。

Method: 采用混合方法，通过80名教师的在线调查（定量与定性问题）收集数据，并进行统计分析。

Result: PjBL受重视但采用受限，主要障碍包括规划管理、项目设计及缺乏支持；促进因素包括同行合作、专业发展和资源获取。

Conclusion: 需系统性支持结构以帮助教师实验和扩展PjBL实践。

Abstract: This research full paper investigates the factors influencing computing
educators' adoption of project-based learning (PjBL) in software engineering
and computing curricula. Recognized as a student-centered pedagogical approach,
PjBL has the potential to enhance student motivation, engagement, critical
thinking, collaboration, and problem-solving skills. Despite these benefits,
faculty adoption remains inconsistent due to challenges such as insufficient
institutional support, time constraints, limited training opportunities,
designing or sourcing projects, and aligning them with course objectives. This
research explores these barriers and investigates the strategies and resources
that facilitate a successful adoption. Using a mixed-methods approach, data
from 80 computing faculty were collected through an online survey comprising
closed-ended questions to quantify barriers, enablers, and resource needs,
along with an open-ended question to gather qualitative insights. Quantitative
data were analyzed using statistical methods, while qualitative responses
underwent thematic analysis. Results reveal that while PjBL is widely valued,
its adoption is often selective and impacted by challenges in planning and
managing the learning process, designing suitable projects, and a lack of
institutional support, such as time, funding, and teaching assistants. Faculty
are more likely to adopt or sustain PjBL when they have access to peer
collaboration, professional development, and institutional incentives. In
addition, sourcing projects from research, industry partnerships, and borrowing
from peers emerged as key facilitators for new projects. These findings
underscore the need for systemic support structures to empower faculty to
experiment with and scale PjBL practices.

</details>


### [6] [An Empirical Study of Complexity, Heterogeneity, and Compliance of GitHub Actions Workflows](https://arxiv.org/abs/2507.18062)
*Edward Abrokwah,Taher A. Ghaleb*

Main category: cs.SE

TL;DR: 该研究分析了GitHub Actions（GHA）工作流在开源项目中的结构、复杂性和合规性，旨在揭示其与最佳实践的差距。


<details>
  <summary>Details</summary>
Motivation: 尽管GHA提供了官方文档和社区最佳实践，但缺乏对开源项目中实际工作流如何遵循这些实践的实证研究。

Method: 通过分析Java、Python和C++仓库中的大量GHA工作流数据，研究其复杂性、模式、合规性及跨语言差异。

Result: 预计将发现对最佳实践的遵循情况以及改进空间。

Conclusion: 研究结果将为CI服务提供改进指南和文档的参考。

Abstract: Continuous Integration (CI) has evolved from a tooling strategy to a
fundamental mindset in modern CI engineering. It enables teams to develop,
test, and deliver software rapidly and collaboratively. Among CI services,
GitHub Actions (GHA) has emerged as a dominant service due to its deep
integration with GitHub and a vast ecosystem of reusable workflow actions.
Although GHA provides official documentation and community-supported best
practices, there appears to be limited empirical understanding of how
open-source real-world CI workflows align with such practices. Many workflows
might be unnecessarily complex and not aligned with the simplicity goals of CI
practices. This study will investigate the structure, complexity,
heterogeneity, and compliance of GHA workflows in open-source software
repositories. Using a large dataset of GHA workflows from Java, Python, and C++
repositories, our goal is to (a) identify workflow complexities, (b) analyze
recurring and heterogeneous structuring patterns, (c) assess compliance with
GHA best practices, and (d) uncover differences in CI pipeline design across
programming languages. Our findings are expected to reveal both areas of strong
adherence to best practices and areas for improvement where needed. These
insights will also have implications for CI services, as they will highlight
the need for clearer guidelines and comprehensive examples in CI documentation.

</details>


### [7] [Identifier Name Similarities: An Exploratory Study](https://arxiv.org/abs/2507.18081)
*Carol Wong,Mai Abe,Silvia De Benedictis,Marissa Halim,Anthony Peruma*

Main category: cs.SE

TL;DR: 研究探讨了标识符名称相似性对代码理解和协作的影响，并提出了分类法以分析其影响。


<details>
  <summary>Details</summary>
Motivation: 标识符名称对程序理解至关重要，但名称相似性可能导致误解和认知负担。

Method: 通过开发分类法，研究标识符名称相似性的不同形式。

Result: 初步分类法为研究者提供了分析名称相似性影响的平台。

Conclusion: 分类法有助于进一步研究和改进标识符名称的设计。

Abstract: Identifier names, which comprise a significant portion of the codebase, are
the cornerstone of effective program comprehension. However, research has shown
that poorly chosen names can significantly increase cognitive load and hinder
collaboration. Even names that appear readable in isolation may lead to
misunderstandings in contexts when they closely resemble other names in either
structure or functionality. In this exploratory study, we present our
preliminary findings on the occurrence of identifier name similarity in
software projects through the development of a taxonomy that categorizes
different forms of identifier name similarity. We envision our initial taxonomy
providing researchers with a platform to analyze and evaluate the impact of
identifier name similarity on code comprehension, maintainability, and
collaboration among developers, while also allowing for further refinement and
expansion of the taxonomy.

</details>


### [8] [Understanding the Supply Chain and Risks of Large Language Model Applications](https://arxiv.org/abs/2507.18105)
*Yujie Ma,Lili Quan,Xiaofei Xie,Qiang Hu,Jiongchi Yu,Yao Zhang,Sen Chen*

Main category: cs.SE

TL;DR: 论文提出了首个全面的LLM供应链安全数据集，分析了3,859个实际应用中的依赖关系和风险，发现供应链中存在显著漏洞，并提出了安全建议。


<details>
  <summary>Details</summary>
Motivation: 随着LLM系统的广泛应用，其复杂的供应链风险被忽视，缺乏系统性研究基准。

Method: 收集了3,859个LLM应用的数据，分析依赖关系（模型、数据集、库），并从公开漏洞数据库中提取1,555个风险问题。

Result: 发现LLM应用存在深度嵌套依赖和供应链中的显著漏洞。

Conclusion: 建议研究人员和开发者进行全面的安全分析，以构建更安全的LLM系统。

Abstract: The rise of Large Language Models (LLMs) has led to the widespread deployment
of LLM-based systems across diverse domains. As these systems proliferate,
understanding the risks associated with their complex supply chains is
increasingly important. LLM-based systems are not standalone as they rely on
interconnected supply chains involving pretrained models, third-party
libraries, datasets, and infrastructure. Yet, most risk assessments narrowly
focus on model or data level, overlooking broader supply chain vulnerabilities.
While recent studies have begun to address LLM supply chain risks, there
remains a lack of benchmarks for systematic research.
  To address this gap, we introduce the first comprehensive dataset for
analyzing and benchmarking LLM supply chain security. We collect 3,859
real-world LLM applications and perform interdependency analysis, identifying
109,211 models, 2,474 datasets, and 9,862 libraries. We extract model
fine-tuning paths, dataset reuse, and library reliance, mapping the ecosystem's
structure. To evaluate security, we gather 1,555 risk-related issues-50 for
applications, 325 for models, 18 for datasets, and 1,229 for libraries from
public vulnerability databases.
  Using this dataset, we empirically analyze component dependencies and risks.
Our findings reveal deeply nested dependencies in LLM applications and
significant vulnerabilities across the supply chain, underscoring the need for
comprehensive security analysis. We conclude with practical recommendations to
guide researchers and developers toward safer, more trustworthy LLM-enabled
systems.

</details>


### [9] [NoCode-bench: A Benchmark for Evaluating Natural Language-Driven Feature Addition](https://arxiv.org/abs/2507.18130)
*Le Deng,Zhonghao Jiang,Jialun Cao,Michael Pradel,Zhongxin Liu*

Main category: cs.SE

TL;DR: NoCode-bench是一个评估LLMs在自然语言驱动无代码开发中表现的基准，包含634个任务。实验显示，最佳LLMs的任务成功率仅为15.79%，表明LLMs尚无法完全支持无代码开发。


<details>
  <summary>Details</summary>
Motivation: 通过自然语言驱动无代码开发提高生产力和普及开发，利用LLMs实现这一目标。

Method: 引入NoCode-bench基准，包含634个任务和114k代码变更，评估LLMs在文档更新和代码实现配对任务中的表现。

Result: 最佳LLMs的任务成功率为15.79%，面临跨文件编辑、代码库理解和工具调用等挑战。

Conclusion: LLMs目前无法完全支持自然语言驱动的无代码开发，NoCode-bench为未来研究奠定了基础。

Abstract: Natural language-driven no-code development allows users to specify software
functionality using natural language (NL) instead of editing source code,
promising increased productivity and democratized development. Large language
models (LLMs) show potential in enabling this paradigm. In this context,
software documentation acts as an NL specification for functionality. This work
introduces NoCode-bench, a benchmark designed to evaluate LLMs on real-world
NL-driven feature addition tasks, consisting of 634 tasks across 10 projects
and 114k code changes. Each task pairs documentation updates with corresponding
code implementations, validated by developer-written test cases. A subset of
114 high-quality, human-verified instances, NoCode-bench Verified, ensures
reliable evaluation. Our experiments reveal that, despite high token usage, the
best LLMs achieve a task success rate of only 15.79%, highlighting challenges
in cross-file editing, codebase understanding, and tool calling. These findings
indicate that LLMs are not yet ready for fully NL-driven no-code development.
NoCode-bench lays the foundation for future advances in this area.

</details>


### [10] [SMECS: A Software Metadata Extraction and Curation Software](https://arxiv.org/abs/2507.18159)
*Stephan Ferenz,Aida Jafarbigloo,Oliver Werth,Astrid Nieße*

Main category: cs.SE

TL;DR: SMECS是一个帮助研究人员和研究软件工程师高效提取和编辑元数据的工具，支持FAIR原则。


<details>
  <summary>Details</summary>
Motivation: 高质量的元数据对实现FAIR原则至关重要，但手动创建费时费力。

Method: SMECS从GitHub等在线仓库提取元数据，并提供用户友好的界面进行编辑和导出为CodeMeta文件。

Result: 可用性实验证实SMECS提供了令人满意的用户体验。

Conclusion: SMECS通过简化元数据创建，支持研究软件的FAIR化。

Abstract: Metadata play a crucial role in adopting the FAIR principles for research
software and enables findability and reusability. However, creating
high-quality metadata can be resource-intensive for researchers and research
software engineers. To address this challenge, we developed the Software
Metadata Extraction and Curation Software (SMECS) which integrates the
extraction of metadata from existing sources together with a user-friendly
interface for metadata curation. SMECS extracts metadata from online
repositories such as GitHub and presents it to researchers through an
interactive interface for further curation and export as a CodeMeta file. The
usability of SMECS was evaluated through usability experiments which confirmed
that SMECS provides a satisfactory user experience. SMECS supports the
FAIRification of research software by simplifying metadata creation.

</details>


### [11] [GenAI for Automotive Software Development: From Requirements to Wheels](https://arxiv.org/abs/2507.18223)
*Nenad Petrovic,Fengjunjie Pan,Vahid Zolfaghari,Krzysztof Lebioda,Andre Schamschurko,Alois Knoll*

Main category: cs.SE

TL;DR: 本文提出了一种基于GenAI的汽车软件开发自动化方法，专注于自动驾驶和ADAS功能，利用LLM和RAG技术优化需求分析、测试场景生成和代码实现。


<details>
  <summary>Details</summary>
Motivation: 缩短ADAS功能的合规性和重新设计周期，减少开发和测试时间。

Method: 结合MDE和LLM进行需求一致性检查、测试场景生成、仿真代码和目标平台代码生成，并采用RAG增强测试场景生成。

Result: 实现了从需求到代码的自动化流程，优化了ADAS功能的开发和测试效率。

Conclusion: 该方法显著提升了ADAS相关功能的开发效率，缩短了合规性和测试周期。

Abstract: This paper introduces a GenAI-empowered approach to automated development of
automotive software, with emphasis on autonomous and Advanced Driver Assistance
Systems (ADAS) capabilities. The process starts with requirements as input,
while the main generated outputs are test scenario code for simulation
environment, together with implementation of desired ADAS capabilities
targeting hardware platform of the vehicle connected to testbench. Moreover, we
introduce additional steps for requirements consistency checking leveraging
Model-Driven Engineering (MDE). In the proposed workflow, Large Language Models
(LLMs) are used for model-based summarization of requirements (Ecore metamodel,
XMI model instance and OCL constraint creation), test scenario generation,
simulation code (Python) and target platform code generation (C++).
Additionally, Retrieval Augmented Generation (RAG) is adopted to enhance test
scenario generation from autonomous driving regulations-related documents. Our
approach aims shorter compliance and re-engineering cycles, as well as reduced
development and testing time when it comes to ADAS-related capabilities.

</details>


### [12] [An Empirical Study on Embodied Artificial Intelligence Robot (EAIR) Software Bugs](https://arxiv.org/abs/2507.18267)
*Zeqin Liao,Zibin Zheng,Peifan Reng,Henglong Liang,Zixu Gao,Zhixiang Chen,Wei Li,Yuhong Nan*

Main category: cs.SE

TL;DR: 本文首次系统研究了885个EAIR系统错误，揭示了18种根本原因、15种症状和13个受影响模块，为EAIR系统错误的修复提供了新见解。


<details>
  <summary>Details</summary>
Motivation: EAIR系统的程序正确性对其成功部署至关重要，但目前缺乏对EAIR系统错误的全面理解，阻碍了相关技术和实践的发展。

Method: 通过分析80个EAIR项目中的885个错误，研究其症状、根本原因和模块分布，并进行分类和映射。

Result: 发现8种EAIR特有症状和8种特有原因，多数与AI代理推理和决策相关，并构建了错误原因与模块的映射关系。

Conclusion: 研究为EAIR系统错误的预测、检测和修复提供了重要依据，有助于未来研究聚焦于易受特定错误类型影响的模块。

Abstract: Embodied Artificial Intelligence Robots (EAIR) is an emerging and rapidly
evolving technological domain. Ensuring their program correctness is
fundamental to their successful deployment. However, a general and in-depth
understanding of EAIR system bugs remains lacking, which hinders the
development of practices and techniques to tackle EAIR system bugs.
  To bridge this gap, we conducted the first systematic study of 885 EAIR
system bugs collected from 80 EAIR system projects to investigate their
symptoms, underlying causes, and module distribution. Our analysis takes
considerable effort, which classifies these bugs into 18 underlying causes, 15
distinct symptoms, and identifies 13 affected modules. It reveals several new
interesting findings and implications which help shed light on future research
on tackling or repairing EAIR system bugs. First, among the 15 identified
symptoms, our findings highlight 8 symptoms specific to EAIR systems, which is
characterized by severe functional failures and potential physical hazards.
Second, within the 18 underlying causes, we define 8 EAIR-specific causes, the
majority of which stem from the intricate issues of AI- agent reasoning and
decision making. Finally, to facilitate precise and efficient bug prediction,
detection, and repair, we constructed a mapping between underlying causes and
the modules in which they most frequently occur, which enables researchers to
focus diagnostic efforts on the modules most susceptible to specific bug types.

</details>


### [13] [Scheduzz: Constraint-based Fuzz Driver Generation with Dual Scheduling](https://arxiv.org/abs/2507.18289)
*Yan Li,Wenzhang Yang,Yuekun Wang,Jian Gao,Shaohua Wang,Yinxing Xue,Lijun Zhang*

Main category: cs.SE

TL;DR: Scheduzz是一种基于LLM的自动化库模糊测试技术，通过理解库的合理使用方式并提取API组合约束，优化计算资源利用，显著提高了覆盖率和效率。


<details>
  <summary>Details</summary>
Motivation: 传统库模糊测试需要专家手动编写高质量的模糊驱动程序，耗时且易出错，现有技术因缺乏对库使用规范的遵循而生成不合理驱动程序，浪费资源。

Method: Scheduzz利用LLM理解库的合理使用方式并提取API组合约束，采用双调度框架管理API组合和模糊驱动程序，将其建模为在线优化问题。

Result: 在33个真实库中评估，Scheduzz显著减少计算开销，覆盖率达到CKGFuzzer、Promptfuzz和OSS-Fuzz的1.62倍、1.50倍和1.89倍，并发现33个未知漏洞。

Conclusion: Scheduzz通过结合LLM和双调度框架，有效解决了库模糊测试中的资源浪费和低效问题，显著提升了测试效果。

Abstract: Fuzzing a library requires experts to understand the library usage well and
craft high-quality fuzz drivers, which is tricky and tedious. Therefore, many
techniques have been proposed to automatically generate fuzz drivers. However,
they fail to generate rational fuzz drivers due to the lack of adherence to
proper library usage conventions, such as ensuring a resource is closed after
being opened. To make things worse, existing library fuzzing techniques
unconditionally execute each driver, resulting in numerous irrational drivers
that waste computational resources while contributing little coverage and
generating false positive bug reports.
  To tackle these challenges, we propose a novel automatic library fuzzing
technique, Scheduzz, an LLM-based library fuzzing technique. It leverages LLMs
to understand rational usage of libraries and extract API combination
constraints. To optimize computational resource utilization, a dual scheduling
framework is implemented to efficiently manage API combinations and fuzz
drivers. The framework models driver generation and the corresponding fuzzing
campaign as an online optimization problem. Within the scheduling loop,
multiple API combinations are selected to generate fuzz drivers, while
simultaneously, various optimized fuzz drivers are scheduled for execution or
suspension.
  We implemented Scheduzz and evaluated it in 33 real-world libraries. Compared
to baseline approaches, Scheduzz significantly reduces computational overhead
and outperforms UTopia on 16 out of 21 libraries. It achieves 1.62x, 1.50x, and
1.89x higher overall coverage than the state-of-the-art techniques CKGFuzzer,
Promptfuzz, and the handcrafted project OSS-Fuzz, respectively. In addition,
Scheduzz discovered 33 previously unknown bugs in these well-tested libraries,
3 of which have been assigned CVEs.

</details>


### [14] [YATE: The Role of Test Repair in LLM-Based Unit Test Generation](https://arxiv.org/abs/2507.18316)
*Michael Konstantinou,Renzo Degiovanni,Jie M. Zhang,Mark Harman,Mike Papadakis*

Main category: cs.SE

TL;DR: 论文提出了一种名为YATE的简单技术，通过结合基于规则的静态分析和重新提示，修复语言模型生成的错误测试用例，显著提升了测试覆盖率和突变杀死率。


<details>
  <summary>Details</summary>
Motivation: 虽然语言模型在自动生成单元测试方面有效，但常产生语法或语义错误的测试用例，这些错误测试若被修复可能具有测试价值。

Method: YATE结合基于规则的静态分析和重新提示技术修复错误测试用例。

Result: 在6个开源项目上，YATE平均覆盖32.06%更多代码行，杀死21.77%更多突变体，优于其他四种LLM方法。

Conclusion: YATE以较低成本显著提升测试效果，证明了修复错误测试用例的价值。

Abstract: Recent advances in automated test generation utilises language models to
produce unit tests. While effective, language models tend to generate many
incorrect tests with respect to both syntax and semantics. Although such
incorrect tests can be easily detected and discarded, they constitute a "missed
opportunity" -- if fixed, they are often valuable as they directly add testing
value (they effectively target the underlying program logic to be tested) and
indirectly form good seeds for generating additional tests. To this end, we
propose a simple technique for repairing some of these incorrect tests through
a combination of rule-based static analysis and re-prompting. We evaluate this
simple approach, named YATE, on a set of 6 open-source projects and show that
it can effectively produce tests that cover on average 32.06% more lines and
kill 21.77% more mutants than a plain LLM-based method. We also compare YATE
with four other LLM-based methods, namely HITS, SYMPROMPT, TESTSPARK and
COVERUP and show that it produces tests that cover substantially more code.
YATE achieves 22% higher line coverage, 20% higher branch coverage and kill 20%
more mutants at a comparable cost (number of calls to LLMs).

</details>


### [15] [Gotta catch 'em all! Towards File Localisation from Issues at Large](https://arxiv.org/abs/2507.18319)
*Jesse Maarleveld,Jiapan Guo,Daniel Feitosa*

Main category: cs.SE

TL;DR: 论文提出了一种适用于所有类型问题的文件定位数据管道，并评估了传统信息检索方法的性能，发现针对特定问题的启发式方法在通用问题上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 研究目标是开发一种适用于所有类型问题的文件定位方法，避免现有研究仅针对特定类型问题的局限性。

Method: 提出了一种数据管道，用于创建问题文件定位数据集，并采用传统信息检索方法进行基线性能评估，同时通过统计分析研究已知偏差的影响。

Result: 结果显示，针对特定问题的启发式方法在通用问题上表现不佳，不同问题类型之间存在显著但较小的性能差异，且标识符的存在对大多数问题类型的性能影响较小。

Conclusion: 研究强调了开发通用模型的重要性，并建议开发能够适应项目特定特征的方法。

Abstract: Bug localisation, the study of developing methods to localise the files
requiring changes to resolve bugs, has been researched for a long time to
develop methods capable of saving developers' time. Recently, researchers are
starting to consider issues outside of bugs. Nevertheless, most existing
research into file localisation from issues focusses on bugs or uses other
selection methods to ensure only certain types of issues are considered as part
of the focus of the work. Our goal is to work on all issues at large, without
any specific selection.
  In this work, we provide a data pipeline for the creation of issue file
localisation datasets, capable of dealing with arbitrary branching and merging
practices. We provide a baseline performance evaluation for the file
localisation problem using traditional information retrieval approaches.
Finally, we use statistical analysis to investigate the influence of biases
known in the bug localisation community on our dataset.
  Our results show that methods designed using bug-specific heuristics perform
poorly on general issue types, indicating a need for research into general
purpose models. Furthermore, we find that there are small, but statistically
significant differences in performance between different issue types. Finally,
we find that the presence of identifiers have a small effect on performance for
most issue types. Many results are project-dependent, encouraging the
development of methods which can be tuned to project-specific characteristics.

</details>


### [16] [FMI Meets SystemC: A Framework for Cross-Tool Virtual Prototyping](https://arxiv.org/abs/2507.18339)
*Nils Bosbach,Meik Schmidt,Lukas Jünger,Matthias Berthold,Rainer Leupers*

Main category: cs.SE

TL;DR: 论文提出了一种新框架，通过FMI标准将SystemC虚拟平台（VP）集成到更广泛的协同仿真环境中，以支持未修改的目标软件运行并接收外部工具的环境输入数据。


<details>
  <summary>Details</summary>
Motivation: 随着系统复杂性增加，需要更全面的测试和虚拟原型设计。SystemC缺乏原生FMI支持，限制了其在协同仿真中的集成。

Method: 提出了一种新框架，利用FMI控制SystemC虚拟平台，并通过案例研究展示了如何通过FMI从外部工具获取温度数据。

Result: 框架成功实现了SystemC VP与外部工具的集成，支持未修改软件运行并接收环境输入数据，如温度、速度等。

Conclusion: 该框架为软件测试和验证提供了更高效的方法，有助于提前完成硬件认证（如ISO 26262）。

Abstract: As systems become more complex, the demand for thorough testing and virtual
prototyping grows. To simulate whole systems, multiple tools are usually needed
to cover different parts. These parts include the hardware of a system and the
environment with which the system interacts. The Functional Mock-up Interface
(FMI) standard for co-simulation can be used to connect these tools.
  The control part of modern systems is usually a computing unit, such as a
System-on-a-Chip (SoC) or Microcontroller Unit (MCU), which executes software
from a connected memory and interacts with peripherals. To develop software
without requiring access to physical hardware, full-system simulators, the
so-called Virtual Platforms (VPs), are commonly used. The IEEE-standardized
framework for VP development is SystemC TLM. SystemC provides interfaces and
concepts that enable modular design and model exchange. However, SystemC lacks
native FMI support, which limits the integration into broader co-simulation
environments.
  This paper presents a novel framework to control and interact with
SystemC-based VPs using the FMI. We present a case study showing how a
simulated temperature sensor in a SystemC simulation can obtain temperature
values from an external tool via FMI. This approach allows the unmodified
target software to run on the VP and receive realistic environmental input data
such as temperature, velocity, or acceleration values from other tools. Thus,
extensive software testing and verification is enabled. By having tests ready
and the software pre-tested using a VP once the physical hardware is available,
certifications like ISO 26262 can be done earlier.

</details>


### [17] [Automated Code Review Using Large Language Models with Symbolic Reasoning](https://arxiv.org/abs/2507.18476)
*Busra Icoz,Goksel Biricik*

Main category: cs.SE

TL;DR: 论文提出了一种结合符号推理技术和大型语言模型（LLMs）的混合方法，用于自动化代码审查，提高了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 手动代码审查主观性强且耗时，而现有的大型语言模型在逻辑推理能力上存在不足，无法完全理解和评估代码。

Method: 采用混合方法，结合符号推理技术和LLMs，使用CodexGlue数据集测试，比较了CodeT5、CodeBERT和GraphCodeBERT等模型。

Result: 实验结果表明，该方法提升了自动化代码审查的准确性和效率。

Conclusion: 结合符号推理和LLMs的混合方法是自动化代码审查的有效解决方案。

Abstract: Code review is one of the key processes in the software development lifecycle
and is essential to maintain code quality. However, manual code review is
subjective and time consuming. Given its rule-based nature, code review is well
suited for automation. In recent years, significant efforts have been made to
automate this process with the help of artificial intelligence. Recent
developments in Large Language Models (LLMs) have also emerged as a promising
tool in this area, but these models often lack the logical reasoning
capabilities needed to fully understand and evaluate code. To overcome this
limitation, this study proposes a hybrid approach that integrates symbolic
reasoning techniques with LLMs to automate the code review process. We tested
our approach using the CodexGlue dataset, comparing several models, including
CodeT5, CodeBERT, and GraphCodeBERT, to assess the effectiveness of combining
symbolic reasoning and prompting techniques with LLMs. Our results show that
this approach improves the accuracy and efficiency of automated code review.

</details>


### [18] [A Deep Dive into Retrieval-Augmented Generation for Code Completion: Experience on WeChat](https://arxiv.org/abs/2507.18515)
*Zezhou Yang,Ting Peng,Cuiyun Gao,Chaozheng Wang,Hailiang Huang,Yuetang Deng*

Main category: cs.SE

TL;DR: 论文研究了检索增强生成（RAG）方法在工业级闭源代码库（如微信）中的代码补全效果，发现相似性RAG优于标识符RAG，且结合词法和语义检索技术效果最佳。


<details>
  <summary>Details</summary>
Motivation: 探索RAG方法在闭源代码库中的表现，弥补开源与闭源代码库之间的分布差异研究空白。

Method: 对26个开源LLM（0.5B至671B参数）测试标识符RAG和相似性RAG，并采用词法（BM25）和语义（GTE-Qwen）检索技术。

Result: 相似性RAG表现更优，词法和语义检索结合效果最佳，开发者调查验证了RAG的实际效用。

Conclusion: RAG方法在闭源代码库中有效，相似性RAG结合多检索技术是优化代码补全的可行方案。

Abstract: Code completion, a crucial task in software engineering that enhances
developer productivity, has seen substantial improvements with the rapid
advancement of large language models (LLMs). In recent years,
retrieval-augmented generation (RAG) has emerged as a promising method to
enhance the code completion capabilities of LLMs, which leverages relevant
context from codebases without requiring model retraining. While existing
studies have demonstrated the effectiveness of RAG on public repositories and
benchmarks, the potential distribution shift between open-source and
closed-source codebases presents unique challenges that remain unexplored. To
mitigate the gap, we conduct an empirical study to investigate the performance
of widely-used RAG methods for code completion in the industrial-scale codebase
of WeChat, one of the largest proprietary software systems. Specifically, we
extensively explore two main types of RAG methods, namely identifier-based RAG
and similarity-based RAG, across 26 open-source LLMs ranging from 0.5B to 671B
parameters. For a more comprehensive analysis, we employ different retrieval
techniques for similarity-based RAG, including lexical and semantic retrieval.
Based on 1,669 internal repositories, we achieve several key findings: (1) both
RAG methods demonstrate effectiveness in closed-source repositories, with
similarity-based RAG showing superior performance, (2) the effectiveness of
similarity-based RAG improves with more advanced retrieval techniques, where
BM25 (lexical retrieval) and GTE-Qwen (semantic retrieval) achieve superior
performance, and (3) the combination of lexical and semantic retrieval
techniques yields optimal results, demonstrating complementary strengths.
Furthermore, we conduct a developer survey to validate the practical utility of
RAG methods in real-world development environments.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [19] [Time for Quiescence: Modelling quiescent behaviour in testing via time-outs in timed automata](https://arxiv.org/abs/2507.18205)
*Laura Brandán Briones,Marcus Gerhold,Petra van den Bos,Mariëlle Stoelinga*

Main category: cs.FL

TL;DR: 提出了一种提升操作符χᴹ，将时间引入LTS模型，避免使用复杂的时间自动机（TA），为工业实践中设置超时判定静止提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 工业实践中工程师偏好使用简单的模型（如LTS），但处理静止（无输出）时需设置超时。现有时间模型测试（MBT）依赖复杂的时间自动机，希望简化这一过程。

Method: 通过提升操作符χᴹ，在LTS中引入单一时钟和用户选择的时间界限M，用于判定静止。时钟值达到M时判定静止，输出需在M之前完成。

Result: 1. 实现符合ioco当且仅当其提升版本符合tioco_M；2. 提升操作符在ioco测试生成算法前后应用得到相同测试集；3. 提升后的TA测试套件与原LTS测试套件对所有实现给出相同判定。

Conclusion: χᴹ操作符为工业实践中超时判定静止提供了简化且形式化的方法，避免了复杂时间自动机的使用。

Abstract: Model-based testing (MBT) derives test suites from a behavioural
specification of the system under test. In practice, engineers favour simple
models, such as labelled transition systems (LTSs). However, to deal with
quiescence - the absence of observable output - in practice, a time-out needs
to be set to conclude observation of quiescence. Timed MBT exists, but it
typically relies on the full arsenal of timed automata (TA).
  We present a lifting operator $\chi^{\scriptstyle M}\!$ that adds timing
without the TA overhead: given an LTS, $\chi^{\scriptstyle M}\!$ introduces a
single clock for a user chosen time bound $M>0$ to declare quiescence. In the
timed automaton, the clock is used to model that outputs should happen before
the clock reaches value $M$, while quiescence occurs exactly at time $M$. This
way we provide a formal basis for the industrial practice of choosing a
time-out to conclude quiescence. Our contributions are threefold: (1) an
implementation conforms under $\mathbf{ioco}$ if and only if its lifted version
conforms under timed $\mathbf{tioco_M}$ (2) applying $\chi^{\scriptstyle M}\!$
before or after the standard $\mathbf{ioco}$ test-generation algorithm yields
the same set of tests, and (3) the lifted TA test suite and the original LTS
test suite deliver identical verdicts for every implementation.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [20] [Higher-Order Behavioural Conformances via Fibrations](https://arxiv.org/abs/2507.18509)
*Henning Urbat*

Main category: cs.PL

TL;DR: 本文提出了一种统一的范畴论方法，扩展了Howe方法，用于证明高阶语言中行为一致性的程序同余性。


<details>
  <summary>Details</summary>
Motivation: 随着具有定量特征（如概率性）的语言兴起，需要扩展共归纳方法以支持更精细的行为一致性概念，如行为距离。

Method: 采用抽象高阶规范（AHOS）建模语言，并通过纤维化建模行为一致性概念，提出了一种统一的范畴论框架。

Result: 在自然条件下，AHOS建模的语言操作模型中最大的行为（双）一致性形成同余性。

Conclusion: 该方法适用于推导概率高阶语言中的双相似性和行为伪度量同余性。

Abstract: Coinduction is a widely used technique for establishing behavioural
equivalence of programs in higher-order languages. In recent years, the rise of
languages with quantitative (e.g.~probabilistic) features has led to extensions
of coinductive methods to more refined types of behavioural conformances, most
notably notions of behavioural distance. To guarantee soundness of coinductive
reasoning, one needs to show that the behavioural conformance at hand forms a
program congruence, i.e. it is suitably compatible with the operations of the
language. This is usually achieved by a complex proof technique known as
\emph{Howe's method}, which needs to be carefully adapted to both the specific
language and the targeted notion of behavioural conformance. We develop a
uniform categorical approach to Howe's method that features two orthogonal
dimensions of abstraction: (1) the underlying higher-order language is modelled
by an \emph{abstract higher-order specification} (AHOS), a novel and very
general categorical account of operational semantics, and (2) notions of
behavioural conformance (such as relations or metrics) are modelled via
fibrations over the base category of an AHOS. Our main result is a fundamental
congruence theorem at this level of generality: Under natural conditions on the
categorical ingredients and the operational rules of a language modelled by an
AHOS, the greatest behavioural (bi)conformance on its operational model forms a
congruence. We illustrate our theory by deriving congruence of bisimilarity and
behavioural pseudometrics for probabilistic higher-order languages.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [21] [Program Logics via Distributive Monoidal Categories](https://arxiv.org/abs/2507.18238)
*Filippo Bonchi,Elena Di Lavore,Mario Román,Sam Staton*

Main category: cs.LO

TL;DR: 从命令式范畴的公理中推导出多种程序逻辑，包括正确性、不正确性和关系Hoare逻辑。


<details>
  <summary>Details</summary>
Motivation: 研究如何从统一的数学框架（命令式多范畴）中系统地推导程序逻辑，以提供更通用的理论基础。

Method: 引入命令式多范畴的内部语言，并基于此推导Dijkstra守卫命令语言的组合子，进一步从内部语言中导出程序逻辑规则。

Result: 成功从命令式范畴的公理中推导出多种程序逻辑，验证了方法的有效性。

Conclusion: 通过命令式多范畴的内部语言，可以系统地生成程序逻辑，为程序验证提供了新的理论工具。

Abstract: We derive multiple program logics, including correctness, incorrectness, and
relational Hoare logic, from the axioms of imperative categories: uniformly
traced distributive copy-discard categories. We introduce an internal language
for imperative multicategories, on top of which we derive combinators for an
adaptation of Dijkstra's guarded command language. Rules of program logics are
derived from this internal language.

</details>


### [22] [Resourceful Traces for Commuting Processes](https://arxiv.org/abs/2507.18246)
*Matthew Earnshaw,Chad Nester,Mario Román*

Main category: cs.LO

TL;DR: 论文提出了一种基于Mazurkiewicz轨迹的新表示方法，用于效应类别（广义Freyd类别），并通过图形演算展示了其应用。


<details>
  <summary>Details</summary>
Motivation: 探索如何将Mazurkiewicz轨迹的动作视为输入到输出的转换，而非简单的原子名称，以扩展效应类别的表示方法。

Method: 将轨迹动作建模为输入到输出的转换，并利用图形演算表示效应类别。

Result: 提出了一种新的效应类别表示方法，并构建了自由效应类别的交换张量积。

Conclusion: 该方法为效应类别的表示和组合提供了新的工具，特别适用于需要动作交换的系统。

Abstract: We show that, when the actions of a Mazurkiewicz trace are considered not
merely as atomic (i.e., mere names) but transformations from a specified type
of inputs to a specified type of outputs, we obtain a novel notion of
presentation for effectful categories (also known as generalised Freyd
categories), a well-known algebraic structure in the semantics of
side-effecting computation. Like the usual representation of traces as graphs,
our notion of presentation gives rise to a graphical calculus for effectful
categories. We use our presentations to give a construction of the commuting
tensor product of free effectful categories, capturing the combination of
systems in which the actions of each must commute with one another, while still
permitting exchange of resources

</details>


### [23] [Distributing Retractions, Weak Distributive Laws and Applications to Monads of Hyperspaces, Continuous Valuations and Measures](https://arxiv.org/abs/2507.18418)
*Jean Goubault-Larrecq*

Main category: cs.LO

TL;DR: 论文探讨了在范畴中通过弱分配律结合两个单子$S$和$T$，构建组合单子$U$的方法，并提出了验证$U$正确性的条件。


<details>
  <summary>Details</summary>
Motivation: 研究如何明确构建和验证组合单子$U$，特别是在已知$U$可能形式的情况下。

Method: 提出了一种称为“分配收缩”的条件，用于验证$U$的正确性，并证明其与弱分配律的一一对应关系。

Result: 在2-范畴框架下，分配收缩与弱分配律一一对应，并通过三个应用案例验证了方法的有效性。

Conclusion: 该方法成功应用于多个单子组合场景，并揭示了相关单子代数的结构。

Abstract: Given two monads $S$, $T$ on a category where idempotents split, and a weak
distributive law between them, one can build a combined monad $U$. Making
explicit what this monad $U$ is requires some effort. When we already have an
idea what $U$ should be, we show how to recognize that $U$ is indeed the
combined monad obtained from $S$ and $T$: it suffices to exhibit what we call a
distributing retraction of $ST$ onto $U$. We show that distributing retractions
and weak distributive laws are in one-to-one correspondence, in a 2-categorical
setting. We give three applications, where $S$ is the Smyth, Hoare or Plotkin
hyperspace monad, $T$ is a monad of continuous valuations, and $U$ is a monad
of previsions or of forks, depending on the case. As a byproduct, this allows
us to describe the algebras of monads of superlinear, resp. sublinear
previsions. In the category of compact Hausdorff spaces, the Plotkin hyperspace
monad is sometimes known as the Vietoris monad, the monad of probability
valuations coincides with the Radon monad, and we infer that the associated
combined monad is the monad of normalized forks.

</details>


### [24] [Well-Founded Coalgebras Meet König's Lemma](https://arxiv.org/abs/2507.18539)
*Henning Urbat,Thorsten Wißmann*

Main category: cs.LO

TL;DR: 论文提出了一个关于König引理的共代数版本，推广了有限分支树的概念，并展示了其在局部有限可表示范畴中的应用。


<details>
  <summary>Details</summary>
Motivation: König引理在数学和计算机科学中有广泛应用，但其传统形式限制了适用范围。通过共代数化和范畴化，可以扩展其适用性。

Method: 将König引理推广到有限共代数，并在局部有限可表示范畴中研究其性质。通过构造初始代数和递归共代数来验证结果。

Result: 证明了在温和条件下，每个良基共代数都是其有限生成子共代数的有向并，且良基共代数范畴是局部可表示的。

Conclusion: 该研究不仅扩展了König引理的应用范围，还提供了初始代数和递归共代数的新构造方法，为相关领域提供了新的理论工具。

Abstract: K\"onig's lemma is a fundamental result about trees with countless
applications in mathematics and computer science. In contrapositive form, it
states that if a tree is finitely branching and well-founded (i.e. has no
infinite paths), then it is finite. We present a coalgebraic version of
K\"onig's lemma featuring two dimensions of generalization: from finitely
branching trees to coalgebras for a finitary endofunctor H, and from the base
category of sets to a locally finitely presentable category C, such as the
category of posets, nominal sets, or convex sets. Our coalgebraic K\"onig's
lemma states that, under mild assumptions on C and H, every well-founded
coalgebra for H is the directed join of its well-founded subcoalgebras with
finitely generated state space -- in particular, the category of well-founded
coalgebras is locally presentable. As applications, we derive versions of
K\"onig's lemma for graphs in a topos as well as for nominal and convex
transition systems. Additionally, we show that the key construction underlying
the proof gives rise to two simple constructions of the initial algebra
(equivalently, the final recursive coalgebra) for the functor H: The initial
algebra is both the colimit of all well-founded and of all recursive coalgebras
with finitely presentable state space. Remarkably, this result holds even in
settings where well-founded coalgebras form a proper subclass of recursive
ones. The first construction of the initial algebra is entirely new, while for
the second one our approach yields a short and transparent new correctness
proof.

</details>


### [25] [Proceedings 19th International Workshop on the ACL2 Theorem Prover and Its Applications](https://arxiv.org/abs/2507.18567)
*Ruben Gamboa,Panagiotis Manolios*

Main category: cs.LO

TL;DR: ACL2研讨会是ACL2定理证明系统用户展示相关研究的主要技术论坛。ACL2是一个工业级自动推理系统，属于Boyer-Moore定理证明器家族的最新成员。


<details>
  <summary>Details</summary>
Motivation: 为ACL2用户提供一个交流研究的平台，推动ACL2及其应用的发展。

Method: 通过研讨会的形式，汇集ACL2用户的研究成果和技术分享。

Result: ACL2及其家族因Boyer、Kaufmann和Moore的工作获得2005年ACM软件系统奖。

Conclusion: ACL2研讨会是推动ACL2技术发展和应用的重要活动。

Abstract: The ACL2 Workshop series is the major technical forum for users of the ACL2
theorem proving system to present research related to the ACL2 theorem prover
and its applications. ACL2 is an industrial-strength automated reasoning
system, the latest in the Boyer-Moore family of theorem provers. The 2005 ACM
Software System Award was awarded to Boyer, Kaufmann, and Moore for their work
on ACL2 and the other theorem provers in the Boyer-Moore family.

</details>


### [26] [Approximate SMT Counting Beyond Discrete Domains](https://arxiv.org/abs/2507.18612)
*Arijit Shaw,Kuldeep S. Meel*

Main category: cs.LO

TL;DR: 论文介绍了pact，一种用于混合公式的SMT模型计数器，通过基于哈希的近似模型计数方法显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如bit-blasting）仅适用于离散变量，无法有效处理混合公式中的离散域投影问题。

Method: pact采用基于哈希的近似模型计数方法，通过优化的哈希函数和对数级SMT求解器调用次数来估计解的数量。

Result: 在14,202个测试实例中，pact成功完成了603个实例，而基线方法仅完成13个。

Conclusion: pact在混合公式的模型计数任务中表现出显著优势，为复杂公式的自动化推理提供了新工具。

Abstract: Satisfiability Modulo Theory (SMT) solvers have advanced automated reasoning,
solving complex formulas across discrete and continuous domains. Recent
progress in propositional model counting motivates extending SMT capabilities
toward model counting, especially for hybrid SMT formulas. Existing approaches,
like bit-blasting, are limited to discrete variables, highlighting the
challenge of counting solutions projected onto the discrete domain in hybrid
formulas.
  We introduce pact, an SMT model counter for hybrid formulas that uses
hashing-based approximate model counting to estimate solutions with theoretical
guarantees. pact makes a logarithmic number of SMT solver calls relative to the
projection variables, leveraging optimized hash functions. pact achieves
significant performance improvements over baselines on a large suite of
benchmarks. In particular, out of 14,202 instances, pact successfully finished
on 603 instances, while Baseline could only finish on 13 instances.

</details>
