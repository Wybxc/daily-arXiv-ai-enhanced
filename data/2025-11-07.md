<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]
- [cs.FL](#cs.FL) [Total: 2]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.LO](#cs.LO) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Tutorial Debriefing: Applied Statistical Causal Inference in Requirements Engineering](https://arxiv.org/abs/2511.03875)
*Julian Frattini,Hans-Martin Heyn,Robert Feldt,Richard Torkar*

Main category: cs.SE

TL;DR: 论文探讨了软件工程研究中知识向实践转化的必要性，强调需要证据证明工具、流程和指南与性能指标之间的因果关系，并提出了在无法进行随机对照试验时使用统计因果推断的方法。


<details>
  <summary>Details</summary>
Motivation: 软件工程研究社区致力于改善软件生产者和消费者的状况，这需要通过将研究成果转化为实践来实现。但这类转化的价值取决于能否证明贡献物（工具、流程、指南）与性能指标之间的因果关系。

Method: 提出了使用统计因果推断（SCI）从观察数据中获取因果关系的可靠过程，特别是在无法进行随机对照试验（由于法律、伦理或后勤限制）的情况下。

Result: 论文指出了在软件工程研究中建立因果关系的挑战，并提出了替代随机对照试验的统计因果推断方法。

Conclusion: 为了有效将软件工程研究成果转化为实践，需要可靠的因果证据。当随机对照试验不可行时，统计因果推断提供了一种从观察数据中获取这种证据的可行途径。

Abstract: As any scientific discipline, the software engineering (SE) research
community strives to contribute to the betterment of the target population of
our research: software producers and consumers. We will only achieve this
betterment if we manage to transfer the knowledge acquired during research into
practice. This transferal of knowledge may come in the form of tools,
processes, and guidelines for software developers. However, the value of these
contributions hinges on the assumption that applying them causes an improvement
of the development process, user experience, or other performance metrics. Such
a promise requires evidence of causal relationships between an exposure or
intervention (i.e., the contributed tool, process or guideline) and an outcome
(i.e., performance metrics). A straight-forward approach to obtaining this
evidence is via controlled experiments in which a sample of a population is
randomly divided into a group exposed to the new tool, process, or guideline,
and a control group. However, such randomized control trials may not be
legally, ethically, or logistically feasible. In these cases, we need a
reliable process for statistical causal inference (SCI) from observational
data.

</details>


### [2] [How Natural Language Proficiency Shapes GenAI Code for Software Engineering Tasks](https://arxiv.org/abs/2511.04115)
*Ruksit Rojpaisarnkit,Youmei Fan,Kenichi Matsumoto,Raula Gaikovina Kula*

Main category: cs.SE

TL;DR: 研究发现英语语言能力本身（独立于提示技术）会影响LLM生成代码的质量和正确性，高级英语提示能产生更正确的代码。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型工具在软件工程中的广泛应用，自然语言提示成为开发者与LLM的关键接口。虽然已有研究关注提示结构，但自然语言能力这一影响代码质量的因素尚未充分探索。

Method: 使用HumanEval数据集，系统地将164个编程任务的英语提示从基础到高级进行变化，测量生成代码的熟练度和正确性。

Result: LLM默认使用中级（B2）自然语言水平。虽然对代码熟练度的影响因模型而异，但高级英语提示在所有模型中始终产生更正确的代码。

Conclusion: 自然语言能力是控制代码生成的关键因素，帮助开发者定制AI输出并提高解决方案的可靠性。

Abstract: With the widespread adoption of Foundation Model (FM)-powered tools in
software engineering, the natural language prompt has become a critical
interface between developers and Large Language Models (LLMs). While much
research has focused on prompt structure, the natural language proficiency is
an underexplored factor that can influence the quality of generated code. This
paper investigates whether the English language proficiency itself independent
of the prompting technique affects the proficiency and correctness of code
generated by LLMs. Using the HumanEval dataset, we systematically varied the
English proficiency of prompts from basic to advanced for 164 programming tasks
and measured the resulting code proficiency and correctness. Our findings show
that LLMs default to an intermediate (B2) natural language level. While the
effect on the resulting code proficiency was model-dependent, we found that
higher-proficiency prompts consistently yielded more correct code across all
models. These results demonstrate that natural language proficiency is a key
lever for controlling code generation, helping developers tailor AI output and
improve the reliability of solutions.

</details>


### [3] [Collaborative Agents for Automated Program Repair in Ruby](https://arxiv.org/abs/2511.03925)
*Nikta Akbarpour,Mahdieh Sadat Benis,Fatemeh Hendijani Fard,Ali Ouni,Mohamed Aymen Saied*

Main category: cs.SE

TL;DR: RAMP是一个轻量级框架，将程序修复建模为反馈驱动的迭代过程，专门针对Ruby语言。它使用协作代理团队生成针对性测试、反思错误并改进候选修复方案，无需依赖大型多语言修复数据库或昂贵微调。


<details>
  <summary>Details</summary>
Motivation: 现有的自动程序修复方法计算成本高且主要关注少数语言，而Ruby作为广泛使用的Web开发语言在APR研究中关注不足，开发者面临持续挑战。

Method: RAMP采用多代理协作框架，通过轻量级提示和测试驱动反馈直接在Ruby上操作。代理团队生成针对性测试、反思错误并迭代改进修复方案，最多5次迭代收敛。

Result: 在XCodeEval基准测试中，RAMP在Ruby上达到67%的pass@1，优于先前方法。特别擅长修复错误答案、编译错误和运行时错误。

Conclusion: RAMP为多代理修复策略提供了新见解，为将基于LLM的调试工具扩展到研究不足的语言奠定了基础。

Abstract: Automated Program Repair (APR) has advanced rapidly with Large Language
Models (LLMs), but most existing methods remain computationally expensive, and
focused on a small set of languages. Ruby, despite its widespread use in web
development and the persistent challenges faced by its developers, has received
little attention in APR research. In this paper, we introduce RAMP, a novel
lightweight framework that formulates program repair as a feedback-driven,
iterative process for Ruby. RAMP employs a team of collaborative agents that
generate targeted tests, reflect on errors, and refine candidate fixes until a
correct solution is found. Unlike prior approaches, RAMP is designed to avoid
reliance on large multilingual repair databases or costly fine-tuning, instead
operating directly on Ruby through lightweight prompting and test-driven
feedback. Evaluation on the XCodeEval benchmark shows that RAMP achieves a
pass@1 of 67% on Ruby, outper-forming prior approaches. RAMP converges quickly
within five iterations, and ablation studies confirm that test generation and
self-reflection are key drivers of its performance. Further analysis shows that
RAMP is particularly effective at repairing wrong answers, compilation errors,
and runtime errors. Our approach provides new insights into multi-agent repair
strategies, and establishes a foundation for extending LLM-based debugging
tools to under-studied languages.

</details>


### [4] [PEFA-AI: Advancing Open-source LLMs for RTL generation using Progressive Error Feedback Agentic-AI](https://arxiv.org/abs/2511.03934)
*Athma Narayanan,Mahesh Subedar,Omesh Tickoo*

Main category: cs.SE

TL;DR: 提出了一种多智能体协作的RTL生成流程，通过渐进式错误反馈系统(PEFA)实现无需人工干预的自动化RTL设计，在开源和闭源LLMs上均取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 解决复杂RTL生成任务中需要人工干预的问题，通过智能体协作和自校正机制实现完全自动化的硬件设计流程。

Method: 使用多智能体框架，结合专用LLMs和硬件仿真工具，采用渐进式错误反馈系统(PEFA)进行迭代式错误修正，逐步增加设计复杂度。

Result: 在两个开源自然语言到RTL数据集上的基准测试显示，该方法在通过率和token效率方面均达到最先进水平，有效缩小了开源与闭源LLMs之间的性能差距。

Conclusion: 该智能体流程为自动化硬件设计提供了有效的解决方案，通过协作式错误修正机制实现了高质量的RTL生成，为硬件设计自动化开辟了新途径。

Abstract: We present an agentic flow consisting of multiple agents that combine
specialized LLMs and hardware simulation tools to collaboratively complete the
complex task of Register Transfer Level (RTL) generation without human
intervention. A key feature of the proposed flow is the progressive error
feedback system of agents (PEFA), a self-correcting mechanism that leverages
iterative error feedback to progressively increase the complexity of the
approach. The generated RTL includes checks for compilation, functional
correctness, and synthesizable constructs. To validate this adaptive approach
to code generation, benchmarking is performed using two opensource natural
language-to-RTL datasets. We demonstrate the benefits of the proposed approach
implemented on an open source agentic framework, using both open- and
closed-source LLMs, effectively bridging the performance gap between them.
Compared to previously published methods, our approach sets a new benchmark,
providing state-of-the-art pass rates while being efficient in token counts.

</details>


### [5] [PSD2Code: Automated Front-End Code Generation from Design Files via Multimodal Large Language Models](https://arxiv.org/abs/2511.04012)
*Yongxi Chen,Lei Chen*

Main category: cs.SE

TL;DR: PSD2Code是一种新颖的多模态方法，通过PSD文件解析和资源对齐生成生产就绪的React+SCSS代码，显著提升了代码相似性、视觉保真度和生产就绪度。


<details>
  <summary>Details</summary>
Motivation: 现有设计到代码生成方法存在结构不一致、资源错位和生产就绪度有限的问题，需要一种能够生成工业级前端代码的解决方案。

Method: 采用ParseAlignGenerate流水线，从PSD文件中提取层次结构、图层属性和元数据，使用基于约束的对齐策略确保生成元素与设计资源的一致性，并通过结构化提示构建增强可控性和代码质量。

Result: 综合评估显示在代码相似性、视觉保真度和生产就绪度等多个指标上显著优于现有方法，且在不同大语言模型上表现出强模型独立性。

Conclusion: 将结构化设计信息与多模态大语言模型相结合，为工业级代码生成提供了有效方法，标志着设计驱动自动化前端开发的重要进展。

Abstract: Design-to-code generation has emerged as a promising approach to bridge the
gap between design prototypes and deployable frontend code. However, existing
methods often suffer from structural inconsistencies, asset misalignment, and
limited production readiness. This paper presents PSD2Code, a novel multi-modal
approach that leverages PSD file parsing and asset alignment to generate
production-ready React+SCSS code. Our method introduces a ParseAlignGenerate
pipeline that extracts hierarchical structures, layer properties, and metadata
from PSD files, providing large language models with precise spatial
relationships and semantic groupings for frontend code generation. The system
employs a constraint-based alignment strategy that ensures consistency between
generated elements and design resources, while a structured prompt construction
enhances controllability and code quality. Comprehensive evaluation
demonstrates significant improvements over existing methods across multiple
metrics including code similarity, visual fidelity, and production readiness.
The method exhibits strong model independence across different large language
models, validating the effectiveness of integrating structured design
information with multimodal large language models for industrial-grade code
generation, marking an important step toward design-driven automated frontend
development.

</details>


### [6] [Specification-Guided Vulnerability Detection with Large Language Models](https://arxiv.org/abs/2511.04014)
*Hao Zhu,Jia Li,Cuiyun Gao,Jiaru Qian,Yihong Dong,Huanyu Liu,Lecheng Wang,Ziliang Wang,Xiaolong Hu,Ge Li*

Main category: cs.SE

TL;DR: VulInstruct是一个基于安全规范的漏洞检测方法，通过从历史漏洞中提取安全规范知识，帮助LLM理解代码的安全期望行为，显著提升漏洞检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在漏洞检测方面表现有限，主要原因是缺乏对安全规范的理解——即代码应该如何表现才能保持安全。当代码行为与这些期望不符时，就会产生漏洞，但这种知识很少在训练数据中明确体现。

Method: VulInstruct从两个角度构建规范知识库：(1)跨项目高质量补丁中的通用规范，捕捉基本安全行为；(2)特定仓库中重复违规的领域特定规范。通过检索相关历史案例和规范，使LLM能够基于期望的安全行为而非表面模式进行推理。

Result: 在PrimeVul数据集上，VulInstruct达到45.0% F1分数（提升32.7%）和37.7%召回率（提升50.8%），独特检测到24.3%的漏洞——比任何基线多2.4倍。在成对评估中实现32.3%相对提升，并发现了一个先前未知的高危漏洞(CVE-2025-56538)。

Conclusion: VulInstruct通过规范引导的方法显著提升了LLM的漏洞检测能力，证明了安全规范知识在代码安全分析中的重要性，具有实际应用价值。

Abstract: Large language models (LLMs) have achieved remarkable progress in code
understanding tasks. However, they demonstrate limited performance in
vulnerability detection and struggle to distinguish vulnerable code from
patched code. We argue that LLMs lack understanding of security specifications
-- the expectations about how code should behave to remain safe. When code
behavior differs from these expectations, it becomes a potential vulnerability.
However, such knowledge is rarely explicit in training data, leaving models
unable to reason about security flaws. We propose VulInstruct, a
specification-guided approach that systematically extracts security
specifications from historical vulnerabilities to detect new ones. VulInstruct
constructs a specification knowledge base from two perspectives: (i) General
specifications from high-quality patches across projects, capturing fundamental
safe behaviors; and (ii) Domain-specific specifications from repeated
violations in particular repositories relevant to the target code. VulInstruct
retrieves relevant past cases and specifications, enabling LLMs to reason about
expected safe behaviors rather than relying on surface patterns. We evaluate
VulInstruct under strict criteria requiring both correct predictions and valid
reasoning. On PrimeVul, VulInstruct achieves 45.0% F1-score (32.7% improvement)
and 37.7% recall (50.8% improvement) compared to baselines, while uniquely
detecting 24.3% of vulnerabilities -- 2.4x more than any baseline. In pair-wise
evaluation, VulInstruct achieves 32.3% relative improvement. VulInstruct also
discovered a previously unknown high-severity vulnerability (CVE-2025-56538) in
production code, demonstrating practical value for real-world vulnerability
discovery. All code and supplementary materials are available at
https://github.com/zhuhaopku/VulInstruct-temp.

</details>


### [7] [LLM-Driven Adaptive Source-Sink Identification and False Positive Mitigation for Static Analysis](https://arxiv.org/abs/2511.04023)
*Shiyin Lin*

Main category: cs.SE

TL;DR: AdaTaint是一个LLM驱动的污点分析框架，通过神经符号推理自适应推断源/汇规范并过滤虚假警报，相比现有方法平均减少43.7%误报并提升11.2%召回率。


<details>
  <summary>Details</summary>
Motivation: 静态分析在发现软件漏洞方面有效，但存在源-汇规范不完整和过多误报的问题。

Method: 结合LLM推理与符号验证，在程序事实和约束验证基础上自适应推断源/汇规范，过滤虚假警报。

Result: 在Juliet 1.3、SV-COMP风格C基准测试和三个大型实际项目中，相比CodeQL、Joern和纯LLM流水线，平均减少43.7%误报，提升11.2%召回率，同时保持有竞争力的运行时开销。

Conclusion: 将LLM推理与符号验证相结合，为实现更准确可靠的静态漏洞分析提供了实用路径。

Abstract: Static analysis is effective for discovering software vulnerabilities but
notoriously suffers from incomplete source--sink specifications and excessive
false positives (FPs). We present \textsc{AdaTaint}, an LLM-driven taint
analysis framework that adaptively infers source/sink specifications and
filters spurious alerts through neuro-symbolic reasoning. Unlike LLM-only
detectors, \textsc{AdaTaint} grounds model suggestions in program facts and
constraint validation, ensuring both adaptability and determinism.
  We evaluate \textsc{AdaTaint} on Juliet 1.3, SV-COMP-style C benchmarks, and
three large real-world projects. Results show that \textsc{AdaTaint} reduces
false positives by \textbf{43.7\%} on average and improves recall by
\textbf{11.2\%} compared to state-of-the-art baselines (CodeQL, Joern, and
LLM-only pipelines), while maintaining competitive runtime overhead. These
findings demonstrate that combining LLM inference with symbolic validation
offers a practical path toward more accurate and reliable static vulnerability
analysis.

</details>


### [8] [Benchmarking and Studying the LLM-based Agent System in End-to-End Software Development](https://arxiv.org/abs/2511.04064)
*Zhengran Zeng,Yixin Li,Rui Xie,Wei Ye,Shikun Zhang*

Main category: cs.SE

TL;DR: 提出了E2EDevBench基准和混合评估框架，通过控制实验发现当前软件开发智能体只能满足约50%需求，主要瓶颈在于需求遗漏和验证不足。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的端到端软件开发智能体评估存在基准过于简单和架构比较困难的问题，需要更科学的评估方法。

Method: 构建动态E2EDevBench基准，提出结合测试用例和LLM需求验证的混合评估框架，在统一基础上实现三种代表性架构进行对比研究。

Result: 最先进的智能体在基准上只能满足约50%需求，成功关键取决于任务分解和协作的架构策略，主要瓶颈是需求遗漏和自验证不足。

Conclusion: 为社区提供了更现实的基准和评估框架，揭示了软件开发智能体的当前能力和核心挑战，指导未来研究关注需求理解和规划改进。

Abstract: The development of LLM-based autonomous agents for end-to-end software
development represents a significant paradigm shift in software engineering.
However, the scientific evaluation of these systems is hampered by significant
challenges, including overly simplistic benchmarks and the difficulty of
conducting fair comparisons between different agent architectures due to
confounding implementation variables. To address these limitations, we first
construct a challenging and dynamically curated E2EDevBench to simulate
realistic development scenarios. Second, we propose a hybrid evaluation
framework that combines test-case-based functional assessment with
fine-grained, LLM-based requirement verification. Using this framework, we
conduct a controlled empirical study on three representative agent
architectures implemented upon a unified foundation to isolate the impact of
workflow design. Our findings reveal that state-of-the-art agents can fulfill
approximately 50\% of requirements on \bench{}, but their success is critically
dependent on the architectural strategy for task decomposition and
collaboration. Furthermore, our analysis indicates that the primary bottleneck
is the omission of requirements and inadequate self-verification. This work
provides the community with a more realistic benchmark, a comprehensive
evaluation framework, and crucial insights into the current capabilities and
core challenges of software development agents, guiding future research toward
enhancing requirement comprehension and planning.

</details>


### [9] [Are We Aligned? A Preliminary Investigation of the Alignment of Responsible AI Values between LLMs and Human Judgment](https://arxiv.org/abs/2511.04157)
*Asma Yamani,Malak Baslyman,Moataz Ahmed*

Main category: cs.SE

TL;DR: 本研究评估了23个大型语言模型在负责任AI价值观方面与人类群体的对齐程度，发现LLMs更接近AI从业者的价值观，但在声称的价值观与实际需求优先级之间存在不一致性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在软件工程任务中的广泛应用，需要评估它们在负责任AI价值观方面与人类判断的对齐程度，以确保AI辅助开发符合人类价值观。

Method: 评估23个LLMs在四个任务上的表现：选择关键负责任AI价值观、在具体情境中评分重要性、解决竞争价值观之间的权衡、以及优先体现这些价值观的软件需求。

Result: LLMs总体上更接近AI从业者的价值观，强调公平性、隐私、透明度、安全性和问责制，但在声称的价值观与实际需求优先级之间存在不一致性。

Conclusion: 研究强调了在需求工程中依赖LLMs而不进行人工监督的实际风险，并推动需要系统方法来基准测试、解释和监控AI辅助软件开发中的价值对齐。

Abstract: Large Language Models (LLMs) are increasingly employed in software
engineering tasks such as requirements elicitation, design, and evaluation,
raising critical questions regarding their alignment with human judgments on
responsible AI values. This study investigates how closely LLMs' value
preferences align with those of two human groups: a US-representative sample
and AI practitioners. We evaluate 23 LLMs across four tasks: (T1) selecting key
responsible AI values, (T2) rating their importance in specific contexts, (T3)
resolving trade-offs between competing values, and (T4) prioritizing software
requirements that embody those values. The results show that LLMs generally
align more closely with AI practitioners than with the US-representative
sample, emphasizing fairness, privacy, transparency, safety, and
accountability. However, inconsistencies appear between the values that LLMs
claim to uphold (Tasks 1-3) and the way they prioritize requirements (Task 4),
revealing gaps in faithfulness between stated and applied behavior. These
findings highlight the practical risk of relying on LLMs in requirements
engineering without human oversight and motivate the need for systematic
approaches to benchmark, interpret, and monitor value alignment in AI-assisted
software development.

</details>


### [10] [Explaining Software Vulnerabilities with Large Language Models](https://arxiv.org/abs/2511.04179)
*Oshando Johnson,Alexandra Fomina,Ranjith Krishnamurthy,Vaibhav Chaudhari,Rohith Kumar Shanmuganathan,Eric Bodden*

Main category: cs.SE

TL;DR: SAFE是一个IDE插件，使用GPT-4o来解释SAST工具检测到的安全漏洞的原因、影响和缓解策略，显著帮助开发人员理解和解决安全问题。


<details>
  <summary>Details</summary>
Motivation: SAST工具通常提供通用的警告信息，未能充分向开发人员传达重要信息，导致误解或忽视关键发现，因此需要提高SAST工具的可解释性。

Method: 提出了一种混合方法，利用LLMs（特别是GPT-4o）来处理SAST可解释性挑战，开发了名为SAFE的IDE插件。

Result: 专家用户研究表明，SAFE生成的解释能显著帮助初级到中级开发人员理解和解决安全漏洞，从而提高SAST工具的整体可用性。

Conclusion: 基于LLMs的方法可以有效解决SAST工具的可解释性问题，显著改善开发人员对安全漏洞的理解和处理能力。

Abstract: The prevalence of security vulnerabilities has prompted companies to adopt
static application security testing (SAST) tools for vulnerability detection.
Nevertheless, these tools frequently exhibit usability limitations, as their
generic warning messages do not sufficiently communicate important information
to developers, resulting in misunderstandings or oversight of critical
findings. In light of recent developments in Large Language Models (LLMs) and
their text generation capabilities, our work investigates a hybrid approach
that uses LLMs to tackle the SAST explainability challenges. In this paper, we
present SAFE, an Integrated Development Environment (IDE) plugin that leverages
GPT-4o to explain the causes, impacts, and mitigation strategies of
vulnerabilities detected by SAST tools. Our expert user study findings indicate
that the explanations generated by SAFE can significantly assist beginner to
intermediate developers in understanding and addressing security
vulnerabilities, thereby improving the overall usability of SAST tools.

</details>


### [11] [GITER: A Git-Based Declarative Exchange Model Using Kubernetes-Style Custom Resources](https://arxiv.org/abs/2511.04182)
*Christos Tranoris*

Main category: cs.SE

TL;DR: 提出了一种基于Git的轻量级可审计异步信息交换方法，用Git作为协调媒介替代传统API和消息代理，支持跨域、跨组织和隔离环境的协作。


<details>
  <summary>Details</summary>
Motivation: 传统API和消息代理在分布式实体间信息交换中存在复杂性和耦合度问题，需要一种更透明、可追溯且松耦合的通信方式。

Method: 采用基于Kubernetes Operators和Custom Resources的Git通信模型，通过共享仓库作为单一事实源，spec字段记录期望状态，status字段反映实际结果。

Result: 该方法利用Git原生特性（版本控制、提交签名、访问控制）确保透明度、可追溯性和可重现性，同时保持系统间的松耦合和自主性。

Conclusion: Git作为声明式通信基板具有显著优势，但也存在权衡，为跨域协作提供了新的解决方案。

Abstract: This paper introduces a lightweight and auditable method for asynchronous
information exchange between distributed entities using Git as the coordination
medium. The proposed approach replaces traditional APIs and message brokers
with a Git-based communication model built on the principles of Kubernetes
Operators and Custom Resources (CRs). Each participating entity, designated as
a Publisher or Consumer, interacts through a shared repository that serves as a
single source of truth, where the spec field captures the desired state and the
status field reflects the observed outcome. This pattern extends GitOps beyond
infrastructure management to support cross-domain, inter-organizational, and
air-gapped collaboration scenarios. By leveraging Git native features
(versioning, commit signing, and access control) the model ensures
transparency, traceability, and reproducibility while preserving loose coupling
and autonomy between systems. The paper discusses architectural principles,
implementation considerations, and comparisons with RESTful and broker-based
integrations, highlighting both the advantages and trade-offs of adopting Git
as a declarative communication substrate.

</details>


### [12] [A Tool for Benchmarking Large Language Models' Robustness in Assessing the Realism of Driving Scenarios](https://arxiv.org/abs/2511.04267)
*Jiahui Wu,Chengjie Lu,Aitor Arrieta,Shaukat Ali*

Main category: cs.SE

TL;DR: 提出了DriveRLR基准工具，用于评估LLMs在判断驾驶场景真实性方面的鲁棒性，通过生成变异场景和构建提示来测试不同LLMs的性能差异。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统安全性测试成本高，仿真测试需求增长，但评估仿真场景真实性困难，而LLMs具有强大的推理和泛化能力，可用于场景真实性评估。

Method: 开发DriveRLR工具，生成变异场景变体，构建提示，评估LLMs在判断驾驶场景真实性方面的能力和鲁棒性。

Result: 在DeepScenario数据集上验证，使用GPT-5、Llama 4 Maverick和Mistral Small 3.2三种LLMs，结果显示DriveRLR能有效揭示不同LLMs鲁棒性差异。

Conclusion: DriveRLR不仅可用于LLMs鲁棒性评估，还可作为场景生成的目标函数，支持基于仿真的自动驾驶系统测试工作流。

Abstract: In recent years, autonomous driving systems have made significant progress,
yet ensuring their safety remains a key challenge. To this end, scenario-based
testing offers a practical solution, and simulation-based methods have gained
traction due to the high cost and risk of real-world testing. However,
evaluating the realism of simulated scenarios remains difficult, creating
demand for effective assessment methods. Recent advances show that Large
Language Models (LLMs) possess strong reasoning and generalization
capabilities, suggesting their potential in assessing scenario realism through
scenario-related textual prompts. Motivated by this, we propose DriveRLR, a
benchmark tool to assess the robustness of LLMs in evaluating the realism of
driving scenarios. DriveRLR generates mutated scenario variants, constructs
prompts, which are then used to assess a given LLM's ability and robustness in
determining the realism of driving scenarios. We validate DriveRLR on the
DeepScenario dataset using three state-of-the-art LLMs: GPT-5, Llama 4
Maverick, and Mistral Small 3.2. Results show that DriveRLR effectively reveals
differences in the robustness of various LLMs, demonstrating its effectiveness
and practical value in scenario realism assessment. Beyond LLM robustness
evaluation, DriveRLR can serve as a practical component in applications such as
an objective function to guide scenario generation, supporting simulation-based
ADS testing workflows.

</details>


### [13] [Where Do LLMs Still Struggle? An In-Depth Analysis of Code Generation Benchmarks](https://arxiv.org/abs/2511.04355)
*Amir Molzam Sharifloo,Maedeh Heydari,Parsa Kazerooni,Daniel Maninger,Mira Mezini*

Main category: cs.SE

TL;DR: 本文分析了LLM在代码生成任务中的失败模式，通过研究四个流行基准测试中LLM持续失败的任务，识别出四种常见的弱点模式以及导致失败的任务复杂性因素。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试和排行榜主要提供LLM性能的定量排名，但缺乏对LLM持续失败任务的分析，这些信息对于理解当前局限性和指导更强大模型开发至关重要。

Method: 研究了四个流行代码生成基准测试中LLM最可能失败的任务，分析了解决方案代码的静态复杂性是否导致失败，并系统检查了114个LLM持续失败的任务。

Result: 分析揭示了LLM中四种反复出现的弱点模式，以及基准测试任务中最常导致失败的常见复杂性因素。

Conclusion: 识别LLM在代码生成中的系统性失败模式对于理解当前模型局限性和指导未来改进具有重要意义。

Abstract: Large Language Models (LLMs) have achieved remarkable success in code
generation, and the race to improve their performance has become a central
focus of AI research. Benchmarks and leaderboards are increasingly popular,
offering quantitative rankings of LLMs. However, they provide limited insight
into the tasks that LLMs consistently fail to solve - information that is
crucial for understanding current limitations and guiding the development of
more capable models. To address this gap, we examined code generation tasks
across four popular benchmarks, identifying those that major LLMs are most
likely to fail. To understand the causes of these failures, we investigated
whether the static complexity of solution code contributes to them, followed by
a systematic inspection of 114 tasks that LLMs consistently struggled with. Our
analysis revealed four recurring patterns of weaknesses in LLMs, as well as
common complications within benchmark tasks that most often lead to failure.

</details>


### [14] [Speed at the Cost of Quality? The Impact of LLM Agent Assistance on Software Development](https://arxiv.org/abs/2511.04427)
*Hao He,Courtney Miller,Shyam Agarwal,Christian Kästner,Bogdan Vasilescu*

Main category: cs.SE

TL;DR: 研究评估了Cursor LLM助手对软件开发速度和代码质量的影响，发现其能显著但短暂提升开发速度，同时导致静态分析警告和代码复杂度持续增加，最终影响长期开发效率。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM代理在软件开发中应用广泛且声称能提高生产力，但缺乏实证证据支持这些说法，需要量化评估其实际影响。

Method: 采用差分法设计，比较使用Cursor的GitHub项目与匹配的控制组项目，并使用面板广义矩估计方法分析长期影响。

Result: Cursor采用导致项目开发速度显著但短暂提升，同时静态分析警告和代码复杂度显著持续增加，这些质量下降因素是长期速度放缓的主要原因。

Conclusion: LLM助手在提升开发效率的同时会带来代码质量下降，这对软件工程实践者、LLM助手设计者和研究者都具有重要启示。

Abstract: Large language models (LLMs) have demonstrated the promise to revolutionize
the field of software engineering. Among other things, LLM agents are rapidly
gaining momentum in their application to software development, with
practitioners claiming a multifold productivity increase after adoption. Yet,
empirical evidence is lacking around these claims. In this paper, we estimate
the causal effect of adopting a widely popular LLM agent assistant, namely
Cursor, on development velocity and software quality. The estimation is enabled
by a state-of-the-art difference-in-differences design comparing
Cursor-adopting GitHub projects with a matched control group of similar GitHub
projects that do not use Cursor. We find that the adoption of Cursor leads to a
significant, large, but transient increase in project-level development
velocity, along with a significant and persistent increase in static analysis
warnings and code complexity. Further panel generalized method of moments
estimation reveals that the increase in static analysis warnings and code
complexity acts as a major factor causing long-term velocity slowdown. Our
study carries implications for software engineering practitioners, LLM agent
assistant designers, and researchers.

</details>


### [15] [EDIT-Bench: Evaluating LLM Abilities to Perform Real-World Instructed Code Edits](https://arxiv.org/abs/2511.04486)
*Wayne Chi,Valerie Chen,Ryan Shar,Aditya Mittal,Jenny Liang,Wei-Lin Chiang,Anastasios Nikolas Angelopoulos,Ion Stoica,Graham Neubig,Ameet Talwalkar,Chris Donahue*

Main category: cs.SE

TL;DR: EDIT-Bench是一个基于真实世界使用场景的代码编辑基准测试，包含545个问题，评估LLM在理解代码上下文、高亮代码和光标位置的基础上进行代码编辑的能力。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏直接评估LLM代码编辑能力的基准测试，现有数据集往往依赖人工生成的数据，需要基于真实世界使用场景的评估工具。

Method: 收集真实世界的用户指令和代码上下文，构建包含多种自然语言和编程语言、涵盖从错误修复到功能添加等多样化用例的基准测试。

Result: 评估了40个不同的LLM，只有5个模型得分超过60%，表明该基准具有挑战性。模型性能在不同指令类别间差异显著，上下文信息量对任务成功率影响可达11%。

Conclusion: EDIT-Bench是一个具有挑战性的代码编辑基准，强调了使用真实上下文进行评估的重要性，模型性能受上下文信息量的显著影响。

Abstract: Instructed code editing, where LLMs directly modify a developer's existing
code based on a user instruction, is becoming a widely used interaction mode in
AI coding assistants. However, few benchmarks directly evaluate this capability
and current datasets often rely on artificial sources. We introduce EDIT-Bench,
a benchmark for evaluating LLM code editing capabilities grounded in real-world
usage, i.e., user instructions and code contexts collected in the wild.
EDIT-Bench comprises of 545 problems, multiple natural and programming
languages, and a diverse set of real-world use cases, ranging from resolving
errors to adding features. EDIT-Bench introduces context-dependent problems
that require the model to understand code context, highlighted code, and cursor
position in addition to the user instruction. We evaluate 40 diverse LLMs and
observe that EDIT-Bench is a challenging set of problems where only 5 models
score over 60%. We find that model performance varies across different
categories of user instructions. Further, we find that varying levels of
contextual information greatly affect task success rate, with performance
varying up to 11%, indicating the importance of evaluating with realistic
context.

</details>


### [16] [Microservices Is Dying, A New Method for Module Division Based on Universal Interfaces](https://arxiv.org/abs/2511.04548)
*Qing Wang,Yong Zhang*

Main category: cs.SE

TL;DR: 提出了一种新的系统设计哲学和软件工程方法，通过计算模块独立性和设计通用接口来消除模块间依赖，实现了即使在单进程单体应用中也能动态加载、卸载或修改任何部分。


<details>
  <summary>Details</summary>
Motivation: 微服务虽然物理隔离了模块，但未能阻止依赖关系的传播和扩散，需要解决模块间耦合的根本原因。

Method: 从模块变更影响评估出发，提出计算模块独立性的概念方法，推导模块独立的必要条件，设计通用接口作为模块间的通用边界，并实现名为EIGHT的平台架构。

Result: 证明了只要保证模块独立性，即使在单进程单体应用中也能在运行时动态加载、卸载或修改任何部分。

Conclusion: 这种架构旨在为日益复杂的系统探索一条超越微服务和单体架构的新路径。

Abstract: Although microservices have physically isolated modules, they have failed to
prevent the propagation and diffusion of dependencies. To trace the root cause
of the inter-module coupling, this paper, starting from the impact assessment
approach for module changes, proposes a conceptual method for calculating
module independence and utilizes this method to derive the necessary conditions
for module independence. Then, a new system design philosophy and software
engineering methodology is proposed, aimed at eliminating dependencies between
modules. A specific pattern is employed to design a set of universal
interfaces, serving as a universal boundary between modules. Subsequently, this
method is used to implement a platform architecture named EIGHT, demonstrating
that, as long as module independence is guaranteed, even a monolithic
application within a single process can dynamically load, unload, or modify any
part at runtime. Finally, the paper concludes that this architecture aims to
explore a novel path for increasingly complex systems, beyond microservice and
monolithic architectures.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [17] [State Complexity of Multiple Concatenation](https://arxiv.org/abs/2511.03814)
*Jozef Jirásek,Galina Jirásková*

Main category: cs.FL

TL;DR: 本文改进了多正则语言连接的状态复杂性上界证明，解决了Caron等人提出的开放问题，证明了在三语言连接中三元字母表是最优的，并获得了单字母循环语言的紧上界。


<details>
  <summary>Details</summary>
Motivation: 改进多正则语言连接状态复杂性的现有证明，解决Caron等人提出的关于k个语言在k字母表上连接的最优字母表大小的开放问题。

Method: 使用更简单的证明方法构造见证语言，考虑部分语言可由二状态自动机识别的情况，分析不同字母表大小下的状态复杂性。

Result: 证明了k个语言在k+1字母表上的连接状态复杂性上界，解决了k个语言在k字母表上连接的开放问题，证明了三语言连接中三元字母表最优，获得了单字母循环语言的紧上界。

Conclusion: 本文简化了多语言连接状态复杂性的证明，解决了关键开放问题，确定了最优字母表大小，并为单字母语言提供了精确的上界分析。

Abstract: We describe witness languages meeting the upper bound on the state complexity
of the multiple concatenation of $k$ regular languages over an alphabet of size
$k+1$ with a significantly simpler proof than that in the literature. We also
consider the case where some languages may be recognized by two-state automata.
Then we show that one symbol can be saved, and we define witnesses for the
multiple concatenation of $k$ languages over a $k$-letter alphabet. This solves
an open problem stated by Caron et al. [2018, Fundam. Inform. 160, 255--279].
We prove that for the concatenation of three languages, the ternary alphabet is
optimal. We also show that a trivial upper bound on the state complexity of
multiple concatenation is asymptotically tight for ternary languages, and that
a lower bound remains exponential in the binary case. Finally, we obtain a
tight upper bound for unary cyclic languages and languages recognized by unary
automata that do not have final states in their tails.

</details>


### [18] [Explorability in Pushdown Automata](https://arxiv.org/abs/2511.04048)
*Ayaan Bedi,Karoliina Lehtinen*

Main category: cs.FL

TL;DR: 本文研究了下推自动机中的可探索性，这是一种对非确定性的度量，介于历史确定性和完全非确定性之间。可探索性形成了一个无限层次结构，每个层级都比前一个更强大但弱于完全非确定性。指数级可探索性恰好捕获了上下文无关语言。


<details>
  <summary>Details</summary>
Motivation: 研究下推自动机中非确定性的量化度量，探索介于历史确定性和完全非确定性之间的中间层次，为理解非确定性在计算模型中的作用提供新的视角。

Method: 引入可探索性概念，定义为在读取输入时只需跟踪k个并发运行即可构造接受运行的能力。研究不同可探索性层级的表达能力和简洁性，包括参数化可探索性概念。

Result: 发现可探索性形成无限层次结构，每个层级严格比前一个更强大；指数级可探索性恰好对应上下文无关语言；可探索PDA比历史确定性PDA更简洁，简洁性差距不可递归枚举。

Conclusion: 可探索性作为下推系统中非确定性的稳健且操作上有意义的度量，提供了对非确定性层次结构的深入理解，填补了历史确定性和完全非确定性之间的空白。

Abstract: We study explorability, a measure of nondeterminism in pushdown automata,
which generalises history-determinism. An automaton is k-explorable if, while
reading the input, it suffices to follow k concurrent runs, built step-by-step
based only on the input seen so far, to construct an accepting one, if it
exists. We show that the class of explorable PDAs lies strictly between
history-deterministic and fully nondeterministic PDAs in terms of both
expressiveness and succinctness. In fact increasing explorability induces an
infinite hierarchy: each level k defines a strictly more expressive class than
level k-1, yet the entire class remains less expressive than general
nondeterministic PDAs. We then introduce a parameterized notion of
explorability, where the number of runs may depend on input length, and show
that exponential explorability precisely captures the context-free languages.
Finally, we prove that explorable PDAs can be doubly exponentially more
succinct than history-deterministic ones, and that the succinctness gap between
deterministic and 2-explorable PDAs is not recursively enumerable. These
results position explorability as a robust and operationally meaningful measure
of nondeterminism for pushdown systems.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [19] [Modular abstract syntax trees (MAST): substitution tensors with second-class sorts](https://arxiv.org/abs/2511.03946)
*Marcelo P. Fiore,Ohad Kammar,Georg Moser,Sam Staton*

Main category: cs.PL

TL;DR: 该论文将Fiore、Plotkin和Turi的抽象语法理论扩展到处理具有第二类排序的语言，如CBV和CBPV编程演算，通过使用actegories而非monoidal categories来表征抽象语法。


<details>
  <summary>Details</summary>
Motivation: 现有抽象语法理论无法很好地处理具有第二类排序的语言，这些语言在变量上下文中禁止第二类排序的出现，需要新的理论框架。

Method: 采用actegories代替monoidal categories来表征抽象语法，通过双范畴论证重现了大部分理论发展，并将其应用于证明CBV变体的替换引理。

Result: 成功构建了适用于具有第二类排序语言的抽象语法理论框架，能够处理CBV和CBPV等编程演算的语法结构。

Conclusion: 通过actegories方法可以有效地扩展抽象语法理论以支持第二类排序，为相关编程语言的形式化分析提供了理论基础。

Abstract: We adapt Fiore, Plotkin, and Turi's treatment of abstract syntax with
binding, substitution, and holes to account for languages with second-class
sorts. These situations include programming calculi such as the Call-by-Value
lambda-calculus (CBV) and Levy's Call-by-Push-Value (CBPV). Prohibiting
second-class sorts from appearing in variable contexts changes the
characterisation of the abstract syntax from monoids in monoidal categories to
actions in actegories. We reproduce much of the development through
bicategorical arguments. We apply the resulting theory by proving substitution
lemmata for varieties of CBV.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [20] [An Automated Theorem Generator with Theoretical Foundation Based on Rectangular Standard Contradiction](https://arxiv.org/abs/2511.04092)
*Yang Xu,Peiyao Liu,Shuwei Chen,Jun Liu*

Main category: cs.LO

TL;DR: 本文提出了一种基于矩形标准矛盾的新型自动定理生成理论，证明了该结构的核心性质，并开发了高效的模板算法实现自动定理生成。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统生成非平凡且逻辑有效定理的严谨理论体系，需要填补这一关键空白。

Method: 基于标准矛盾概念，首次定义并证明矩形标准矛盾结构，利用其非冗余性和必然不可满足性，设计模板化ATG算法。

Result: 证明了矩形标准矛盾分割为前提集A和其补集的否定H时，可形成有效定理A⊢¬H，且所有此类定理逻辑等价。开发了矩形自动定理生成器。

Conclusion: 该研究使机器从'验证者'转变为'发现者'，为逻辑和人工智能基础研究开辟了新途径。

Abstract: Currently, there is a lack of rigorous theoretical system for systematically
generating non-trivial and logically valid theorems. Addressing this critical
gap, this paper conducts research to propose a novel automated theorem
generation theory and tool. Based on the concept of standard contradiction
which possesses unique deductive advantages, this paper defines and proves, for
the first time, a new logical structure known as rectangular standard
contradiction. Centered on this structure, a complete Automated Theorem
Generation (ATG) theory is put forward. Theoretical proofs clarify two core
properties of rectangular standard contradiction: first, it is a standard
contradiction (necessarily unsatisfiable); second, it exhibits non-redundancy
(the remaining clause set becomes satisfiable after removing any clause).
Leveraging these properties, this paper proves that partitioning a rectangular
standard contradiction into a premise subset $A$ and negation of its complement
$H$, a valid theorem $A \vdash \neg H$ can be formed, and all such theorems are
logically equivalent. To implement this theory, an efficient template-based ATG
algorithm is designed, and a Rectangular Automated Theorem Generator is
developed. This research enables machines to transition from "verifiers" to
"discoverers", opening up new avenues for fundamental research in the fields of
logic and artificial intelligence.

</details>


### [21] [Compact Quantitative Theories of Convex Algebras](https://arxiv.org/abs/2511.04201)
*Matteo Mio*

Main category: cs.LO

TL;DR: 引入紧凑定量等式理论概念，证明插值重心定量代数的理论是紧凑的，并以此为例获得其他凸代数的紧凑定量等式理论。


<details>
  <summary>Details</summary>
Motivation: 研究定量等式理论的紧凑性，确保所有结论都能通过有限证明推导出来。

Method: 定义紧凑定量等式理论，证明插值重心定量代数理论的紧凑性，并以此为基础推导其他凸代数的紧凑理论。

Result: 成功证明了插值重心定量代数理论是紧凑的，并获得了其他凸代数的紧凑定量等式理论。

Conclusion: 紧凑定量等式理论的概念是有效的，插值重心定量代数理论作为范例证明了这一概念的应用价值，为其他凸代数的定量理论提供了基础。

Abstract: We introduce the concept of compact quantitative equational theory. A
quantitative equational theory is defined to be compact if all its consequences
are derivable by means of finite proofs. We prove that the theory of
interpolative barycentric (also known as convex) quantitative algebras of
Mardare et. al. is compact. This serves as a paradigmatic example, used to
obtain other compact quantitative equational theories of convex algebras, each
axiomatizing some distance on finitely supported probability distributions.

</details>


### [22] [The Size of Interpolants in Modal Logics](https://arxiv.org/abs/2511.04577)
*Balder ten Cate,Louwe Kuijer,Frank Wolter*

Main category: cs.LO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We start a systematic investigation of the size of Craig interpolants,
uniform interpolants, and strongest implicates for (quasi-)normal modal logics.
Our main upper bound states that for tabular modal logics, the computation of
strongest implicates can be reduced in polynomial time to uniform interpolant
computation in classical propositional logic. Hence they are of polynomial
dag-size iff NP $\subseteq$ P$_{/\text{poly}}$. The reduction also holds for
Craig interpolants and uniform interpolants if the tabular modal logic has the
Craig interpolation property. Our main lower bound shows an unconditional
exponential lower bound on the size of Craig interpolants and strongest
implicates covering almost all non-tabular standard normal modal logics. For
normal modal logics contained in or containing S4 or GL we obtain the following
dichotomy: tabular logics have ``propositionally sized'' interpolants while for
non-tabular logics an unconditional exponential lower bound holds.

</details>
