<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 33]
- [cs.LO](#cs.LO) [Total: 5]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [SLMFix: Leveraging Small Language Models for Error Fixing with Reinforcement Learning](https://arxiv.org/abs/2511.19422)
*David Jiahao Fu,Aryan Gupta,Aaron Councilman,David Grove,Yu-Xiong Wang,Vikram Adve*

Main category: cs.SE

TL;DR: SLMFix是一个利用强化学习微调的小型语言模型来修复LLM生成代码中语法错误的代码生成管道，特别针对低资源编程语言，可替代传统微调方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在代码生成中仍会产生语法错误，尤其在低资源编程语言上表现不佳，且微调成本高昂，需要更高效的解决方案。

Method: 使用强化学习微调小型语言模型进行程序修复，奖励函数结合静态验证器和静态语义相似度指标。

Result: 在多个领域特定语言上实验证明有效，静态验证器通过率超过95%，在低资源编程语言上优于监督微调的7B模型。

Conclusion: SLMFix展示了作为传统微调方法替代方案的潜力，能显著提升LLM生成代码的质量。

Abstract: Recent advancements in large language models (LLMs) have shown very impressive capabilities in code generation across many programming languages. However, even state-of-the-art LLMs generate programs that contains syntactic errors and fail to complete the given tasks, especially for low-resource programming languages (LRPLs). In addition, high training cost makes finetuning LLMs unaffordable with constrained computational resources, further undermining the effectiveness of LLMs for code generation. In this work, we propose SLMFix, a novel code generation pipeline that leverages a small language model (SLM) finetuned using reinforcement learning (RL) techniques to fix syntactic errors in LLM-generated programs to improve the quality of LLM-generated programs for domain-specific languages (DSLs). In specific, we applied RL on the SLM for the program repair task using a reward calculated using both a static validator and a static semantic similarity metric. Our experimental results demonstrate the effectiveness and generalizability of our approach across multiple DSLs, achieving more than 95% pass rate on the static validator. Notably, SLMFix brings substantial improvement to the base model and outperforms supervised finetuning approach even for 7B models on a LRPL, showing the potential of our approach as an alternative to traditional finetuning approaches.

</details>


### [2] [The Software Engineering Simulations Lab: Agentic AI for RE Quality Simulations](https://arxiv.org/abs/2511.17762)
*Henning Femmer,Ivan Esau*

Main category: cs.SE

TL;DR: 本文提出使用代理AI模拟扩展需求工程研究工具箱，通过标准化代理在随机、动态、事件驱动的定性模拟中复制软件工程过程，以解决需求质量缺陷实证数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 需求工程中的质量评估仍主要依赖经验和直觉，缺乏实证数据支持。随着AI驱动开发的出现，需求质量因素可能发生变化，因为需求不仅被人类消费，也被AI代理消费，这可能导致不同的高效需求风格。

Method: 提出代理AI模拟方法，使用标准化代理在随机、动态、事件驱动的定性模拟中复制软件工程过程。开发了原型并进行了可行性研究。

Result: 初步研究表明，即使是简单的实现也能产生可执行的模拟，为需求工程研究的技术改进和更广泛应用提供了支持。

Conclusion: 代理AI模拟以其速度和简单性成为需求工程研究的有价值补充，尽管在复制人类行为方面存在局限性需要进一步研究。

Abstract: Context and motivation. Quality in Requirements Engineering (RE) is still predominantly anecdotal and intuition-driven. Creating a solid requirements quality model requires broad sets of empirical evidence to evaluate quality factors and their context. Problem. However, empirical data on the detailed effects of requirements quality defects is scarce, since it is costly to obtain. Furthermore, with the advent of AI-based development, the requirements quality factors may change: Requirements are no longer only consumed by humans, but increasingly also by AI agents, which might lead to a different efficient and effective requirements style. Principal ideas. We propose to extend the RE research toolbox with Agentic AI simulations, in which software engineering (SE) processes are replicated by standardized agents in stochastic, dynamic, event-driven, qualitative simulations. We argue that their speed and simplicity makes them a valuable addition to RE research, although limitations in replicating human behavior need to be studied and understood. Contribution. This paper contributes a first concept, a research roadmap, a prototype, and a first feasibility study for RE simulations with agentic AI. Study results indicate that even a naive implementation leads to executable simulations, encouraging technical improvements along with broader application in RE research.

</details>


### [3] [Validating API Design Requirements for Interoperability: A Static Analysis Approach Using OpenAPI](https://arxiv.org/abs/2511.17836)
*Edwin Sundberg,Thea Ekmark,Workneh Yilma Ayele*

Main category: cs.SE

TL;DR: 该研究开发了一个可配置的规则引擎来检测OpenAPI规范中的结构违规，支持API设计质量的自动化验证，改善API设计过程。


<details>
  <summary>Details</summary>
Motivation: RESTful API在企业软件开发中至关重要，但API设计质量评估目前主要是手动和临时过程，特别是在早期开发阶段。需要支持系统演进、服务互操作性和跨组织治理。

Method: 采用设计科学研究方法，通过文献回顾识别了75个API设计规则，并实现了一个可配置的规则引擎来检测OpenAPI规范中的结构违规。

Result: 评估显示S.E.O.R.A工具能够早期验证非功能性API需求，提供可操作和可追踪的反馈，与需求获取和质量保证过程良好对齐，自动化了原本需要手动检查的验证工作。

Conclusion: 该工作通过将设计原则操作化为可验证的约束并将其嵌入到实用的验证工具中，为需求工程做出了贡献。未来方向包括IDE集成、扩展规则覆盖范围和在真实环境中部署以支持敏捷API开发生命周期中的持续合规性。

Abstract: RESTful APIs are central in developing interoperable, modular, and maintainable software systems in enterprises today. Also, it is essential to support system evolution, service interoperability, and governance across organizational boundaries to ensure good quality and consistency of these APIs. However, evaluating API design quality, which is part of non-functional requirement tasks, remains a largely manual and ad hoc process, particularly during early development. Using a Design Science Research (DSR) methodology, we elicited user needs, identified 75 API design rules using a literature review, and implemented a configurable rule engine to detect structural violations in OpenAPI specifications. The proposed tool supports organizational adaptability by allowing rules to be customized, enabled, or disabled, enabling integration of domain-specific standards. The evaluation was conducted through structured experiments and thematic analysis involving industry experts. API quality validation contributes to aligning technical designs with requirements and enterprise architecture by strengthening interoperability and governance between enterprise systems. The results show that S.E.O.R.A facilitates early validation of non-functional API requirements, provides actionable and traceable feedback, and aligns well with requirements elicitation and quality assurance processes. It improves the API design process by automating checks that would otherwise require manual inspection, thus supporting consistent and reusable conformance practices. This work contributes to requirements engineering by operationalizing design principles as verifiable constraints and embedding them into a practical validation tool. Future directions include IDE integration, expanded rule coverage, and real-world deployment to support continuous compliance in agile API development lifecycles.

</details>


### [4] [A Low-Code Methodology for Developing AI Kiosks: a Case Study with the DIZEST Platform](https://arxiv.org/abs/2511.17853)
*SunMin Moon,Jangwon Gim,Chaerin Kim,Yeeun Kim,YoungJoo Kim,Kang Choi*

Main category: cs.SE

TL;DR: 提出基于DIZEST低代码架构的解决方案，通过AI增强自助服务系统，解决集成性差、结构僵化、性能瓶颈和缺乏协作框架等问题。


<details>
  <summary>Details</summary>
Motivation: 现代自助服务系统面临集成性差、结构僵化、性能瓶颈和缺乏协作框架等重大挑战，需要更高效的解决方案。

Method: 采用DIZEST低代码平台方法，支持直观的工作流设计和无缝AI集成，并与Jupyter Notebook、ComfyUI和Orange3等现有平台进行对比分析。

Result: DIZEST在关键评估标准上展现出优越性能，照片自助服务案例验证了该方法在提升互操作性、增强用户体验和提高部署灵活性方面的有效性。

Conclusion: DIZEST低代码架构为自助服务系统提供了有效的增强解决方案，显著改善了系统性能和用户体验。

Abstract: This paper presents a comprehensive study on enhancing kiosk systems through a low-code architecture, with a focus on AI-based implementations. Modern kiosk systems are confronted with significant challenges, including a lack of integration, structural rigidity, performance bottlenecks, and the absence of collaborative frameworks. To overcome these limitations, we propose a DIZEST-based approach methodology, a specialized low-code platform that enables intuitive workflow design and seamless AI integration. Through a comparative analysis with existing platforms, including Jupyter Notebook, ComfyUI, and Orange3, we demonstrate that DIZEST delivers superior performance across key evaluation criteria. Our photo kiosk case study further validates the effectiveness of this approach in improving interoperability, enhancing user experience, and increasing deployment flexibility.

</details>


### [5] [Synthesizing Precise Protocol Specs from Natural Language for Effective Test Generation](https://arxiv.org/abs/2511.17977)
*Kuangxiangzi Liu,Dhiman Chakraborty,Alexander Liggesmeyer,Andreas Zeller*

Main category: cs.SE

TL;DR: 提出一个两阶段管道，使用LLM从自然语言规范中提取协议元素，然后合成正式协议规范用于自动化测试生成


<details>
  <summary>Details</summary>
Motivation: 解决自然语言规范手动测试生成慢、易错、难扩展的问题，同时避免正式规范编写和维护的繁琐

Method: 两阶段方法：1) 从自然语言规范提取协议元素 2) 基于协议实现合成和精炼正式协议规范

Result: 在5个互联网协议上验证，平均恢复92.8%客户端和80.2%服务器消息类型，实现81.5%消息接受率

Conclusion: 该方法可行，优于端到端LLM测试生成，产生可检查、可追溯、人类可读的规范，无需LLM即可生成测试用例

Abstract: Safety- and security-critical systems have to be thoroughly tested against their specifications. The state of practice is to have _natural language_ specifications, from which test cases are derived manually - a process that is slow, error-prone, and difficult to scale. _Formal_ specifications, on the other hand, are well-suited for automated test generation, but are tedious to write and maintain. In this work, we propose a two-stage pipeline that uses large language models (LLMs) to bridge the gap: First, we extract _protocol elements_ from natural-language specifications; second, leveraging a protocol implementation, we synthesize and refine a formal _protocol specification_ from these elements, which we can then use to massively test further implementations.
  We see this two-stage approach to be superior to end-to-end LLM-based test generation, as 1. it produces an _inspectable specification_ that preserves traceability to the original text; 2. the generation of actual test cases _no longer requires an LLM_; 3. the resulting formal specs are _human-readable_, and can be reviewed, version-controlled, and incrementally refined; and 4. over time, we can build a _corpus_ of natural-language-to-formal-specification mappings that can be used to further train and refine LLMs for more automatic translations.
  Our prototype, AUTOSPEC, successfully demonstrated the feasibility of our approach on five widely used _internet protocols_ (SMTP, POP3, IMAP, FTP, and ManageSieve) by applying its methods on their _RFC specifications_ written in natural-language, and the recent _I/O grammar_ formalism for protocol specification and fuzzing. In its evaluation, AUTOSPEC recovers on average 92.8% of client and 80.2% of server message types, and achieves 81.5% message acceptance across diverse, real-world systems.

</details>


### [6] [Enhancing Automated Program Repair via Faulty Token Localization and Quality-Aware Patch Refinement](https://arxiv.org/abs/2511.18001)
*Jiaolong Kong,Xiaofei Xie,Yiheng Xiong,Yuekun Wang,Jian Wang*

Main category: cs.SE

TL;DR: TokenRepair是一个新颖的两级细化框架，通过整合内部反射和外部反馈来增强自动程序修复性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的程序修复技术主要依赖粗粒度的外部反馈（如测试结果），缺乏揭示补丁失败原因或识别错误代码部分的细粒度内部信号，导致修复效率低下、错误传播和性能不佳。

Method: TokenRepair首先通过分析上下文感知的令牌级不确定性波动来执行内部反射，识别补丁中的可疑或低置信度令牌；然后应用思维链引导的重写来仅细化这些定位的令牌，实现有针对性的细粒度修正；并采用质量感知的外部反馈机制来评估补丁质量并在细化前过滤低质量候选。

Result: 实验结果显示TokenRepair在Defects4J 1.2上正确修复了88个错误，在HumanEval-Java上修复了139个错误，在Defects4J 1.2上相比所有模型实现了8.2%到34.9%的显著改进，在HumanEval-Java上实现了3.3%到16.1%的改进。

Conclusion: TokenRepair通过整合内部令牌级不确定性和外部质量反馈，实现了最先进的程序修复性能，证明了细粒度内部信号在提升LLM-based APR效果中的重要性。

Abstract: Large language models (LLMs) have recently demonstrated strong potential for automated program repair (APR). However, existing LLM-based techniques primarily rely on coarse-grained external feedback (e.g.,test results) to guide iterative patch generation, while lacking fine-grained internal signals that reveal why a patch fails or which parts of the generated code are likely incorrect. This limitation often leads to inefficient refinement, error propagation, and suboptimal repair performance. In this work, we propose TokenRepair, a novel two-level refinement framework that enhances APR by integrating internal reflection for localizing potentially faulty tokens with external feedback for quality-aware patch refinement. Specifically, TokenRepair first performs internal reflection by analyzing context-aware token-level uncertainty fluctuations to identify suspicious or low-confidence tokens within a patch. It then applies Chain-of-Thought guided rewriting to refine only these localized tokens, enabling targeted and fine-grained correction. To further stabilize the iterative repair loop, TokenRepair incorporates a quality-aware external feedback mechanism that evaluates patch quality and filters out low-quality candidates before refinement. Experimental results show that TokenRepair achieves new state-of-the-art repair performance, correctly fixing 88 bugs on Defects4J 1.2 and 139 bugs on HumanEval-Java, demonstrating substantial improvements ranging from 8.2% to 34.9% across all models on Defects4J 1.2 and from 3.3% to 16.1% on HumanEval-Java.

</details>


### [7] [MASTEST: A LLM-Based Multi-Agent System For RESTful API Tests](https://arxiv.org/abs/2511.18038)
*Xiaoke Han,Hong Zhu*

Main category: cs.SE

TL;DR: MASTEST是一个多代理系统，结合LLM和编程代理，实现从API规范生成测试场景到执行测试、分析响应的完整RESTful API测试工作流，支持人工审核以确保质量。


<details>
  <summary>Details</summary>
Motivation: 随着云原生应用的发展，RESTful API测试日益重要。现有机器学习技术表明大语言模型可以自动执行各种测试活动，但需要完整的工具链来覆盖整个API测试工作流。

Method: 开发MASTEST多代理系统，结合LLM代理和编程代理，从OpenAPI Swagger规范生成单元和系统测试场景，创建Pytest测试脚本，执行测试并与Web服务交互，分析响应消息确定测试正确性并计算覆盖率。

Result: 在GPT-4o和DeepSeek V3.1 Reasoner上评估五个公共API，两个模型都表现出色：DeepSeek在数据类型正确性和状态码检测方面表现优异，GPT-4o在API操作覆盖率方面最佳。LLM生成的测试脚本保持100%语法正确性。

Conclusion: MASTEST系统证明了利用大语言模型进行自动化API测试的有效性和可行性，两种LLM在不同测试活动中各有优势，生成的测试脚本质量高且需要最少的人工编辑。

Abstract: Testing RESTful API is increasingly important in quality assurance of cloud-native applications. Recent advances in machine learning (ML) techniques have demonstrated that various testing activities can be performed automatically by large language models (LLMs) with reasonable accuracy. This paper develops a multi-agent system called MASTEST that combines LLM-based and programmed agents to form a complete tool chain that covers the whole workflow of API test starting from generating unit and system test scenarios from API specification in the OpenAPI Swagger format, to generating of Pytest test scripts, executing test scripts to interact with web services, to analysing web service response messages to determine test correctness and calculate test coverage. The system also supports the incorporation of human testers in reviewing and correcting LLM generated test artefacts to ensure the quality of testing activities. MASTEST system is evaluated on two LLMs, GPT-4o and DeepSeek V3.1 Reasoner with five public APIs. The performances of LLMs on various testing activities are measured by a wide range of metrics, including unit and system test scenario coverage and API operation coverage for the quality of generated test scenarios, data type correctness, status code coverage and script syntax correctness for the quality of LLM generated test scripts, as well as bug detection ability and usability of LLM generated test scenarios and scripts. Experiment results demonstrated that both DeepSeek and GPT-4o achieved a high overall performance. DeepSeek excels in data type correctness and status code detection, while GPT-4o performs best in API operation coverage. For both models, LLM generated test scripts maintained 100\% syntax correctness and only required minimal manual edits for semantic correctness. These findings indicate the effectiveness and feasibility of MASTEST.

</details>


### [8] [Event-Chain Analysis for Automated Driving and ADAS Systems: Ensuring Safety and Meeting Regulatory Timing Requirements](https://arxiv.org/abs/2511.18092)
*Sebastian Dingler,Philip Rehkop,Florian Mayer,Ralf Muenzenberger*

Main category: cs.SE

TL;DR: 本文提出了一种基于事件链建模的白盒方法，用于解决自动驾驶系统的时序约束挑战，确保符合国际法规要求。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统必须满足国际法规和标准规定的严格时序约束，而黑盒方法无法提供系统内部时序行为的透明洞察。

Method: 采用基于事件链建模的白盒方法，从感知、规划到执行和人机交互的每个功能组件进行时序分析。

Result: 通过详细案例研究，该方法能够早期识别合规问题、系统化参数优化，并通过概率分析生成定量证据。

Conclusion: 事件链分析方法增强了监管合规性，优化了系统设计，并支持基于模型的安全分析技术。

Abstract: Automated Driving Systems (ADS), including Advanced Driver Assistance Systems (ADAS), must fulfill not only high functional expectations but also stringent timing constraints mandated by international regulations and standards. Regulatory frameworks such as UN regulations, NCAP standards, ISO norms, and NHTSA guidelines impose strict bounds on system reaction times to ensure safe vehicle operation. This paper presents a structured, White-Box methodology based on Event-Chain Modeling to address these timing challenges. Unlike Black-Box approaches, Event-Chain Analysis offers transparent insights into the timing behavior of each functional component - from perception and planning to actuation and human interaction. This perspective is also aligned with multiple regulations, which require that homologation dossiers provide evidence that the chosen system architecture is suitable to ensure compliance with the specified requirements. Our methodology enables the derivation, modeling, and validation of end-to-end timing constraints at the architectural level and facilitates early verification through simulation. Through a detailed case study, we demonstrate how this Event-Chain-centric approach enhances regulatory compliance, optimizes system design, and supports model-based safety analysis techniques, with results showing early identification of compliance issues, systematic parameter optimization, and quantitative evidence generation through probabilistic analysis.

</details>


### [9] [Towards a General Framework for HTN Modeling with LLMs](https://arxiv.org/abs/2511.18165)
*Israel Puerta-Merino,Carlos Núñez-Molina,Pablo Mesejo,Juan Fernández-Olivares*

Main category: cs.SE

TL;DR: 本文提出了L2HP框架，扩展了L2P库以支持分层规划模型生成，并比较了LLM在自动规划和分层规划中的建模能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自动规划中的应用已被广泛探索，但在分层规划中的应用仍远未达到非分层架构的成熟水平，本文旨在填补这一研究空白。

Method: 提出L2HP框架作为L2P库的扩展，支持分层规划模型生成，并采用通用性和可扩展性的设计理念。在PlanBench数据集上进行实验，比较LLM在自动规划和分层规划中的建模能力。

Result: 在PlanBench数据集上，解析成功率在两种设置中相当（约36%），但分层情况下的语法有效性显著较低（1% vs 20%的实例）。

Conclusion: 分层规划对LLM提出了独特挑战，需要进一步研究来提高生成的分层规划模型质量。

Abstract: The use of Large Language Models (LLMs) for generating Automated Planning (AP) models has been widely explored; however, their application to Hierarchical Planning (HP) is still far from reaching the level of sophistication observed in non-hierarchical architectures. In this work, we try to address this gap. We present two main contributions. First, we propose L2HP, an extension of L2P (a library to LLM-driven PDDL models generation) that support HP model generation and follows a design philosophy of generality and extensibility. Second, we apply our framework to perform experiments where we compare the modeling capabilities of LLMs for AP and HP. On the PlanBench dataset, results show that parsing success is limited but comparable in both settings (around 36\%), while syntactic validity is substantially lower in the hierarchical case (1\% vs. 20\% of instances). These findings underscore the unique challenges HP presents for LLMs, highlighting the need for further research to improve the quality of generated HP models.

</details>


### [10] [Establishing Traceability Links between Release Notes & Software Artifacts: Practitioners' Perspectives](https://arxiv.org/abs/2511.18187)
*Sristy Sumana Nath,Banani Roy,Munima Jahan*

Main category: cs.SE

TL;DR: 开发了基于LLM的方法来自动建立发布说明与开发工件（PRs、commits、issues）之间的可追溯性链接，解决了开源环境中链接维护困难的问题。


<details>
  <summary>Details</summary>
Motivation: 开源环境中贡献者远程异步工作，建立和维护发布说明与开发工件之间的可追溯性链接容易出错、耗时且经常被忽视。实证研究发现47%的发布工件缺乏可追溯性链接，12%包含损坏链接。

Method: 首先分析发布说明的What、Why、How信息及其与PRs、commits、issues的对应关系；构建包含3500个验证链接实例的基准数据集；实现基于LLM的方法自动建立三对可追溯性链接；结合时间邻近特征。

Result: 使用Gemini 1.5 Pro的LLM方法在PR可追溯性恢复中达到了0.73的Precision@1值。在线调查显示16%受访者认为非常重要，68%认为比较重要。

Conclusion: 基于LLM的方法能有效自动建立发布说明与开发工件之间的可追溯性链接，具有较高的实用性和采用潜力。

Abstract: Maintaining traceability links between software release notes and corresponding development artifacts, e.g., pull requests (PRs), commits, and issues, is essential for managing technical debt and ensuring maintainability. However, in open-source environments where contributors work remotely and asynchronously, establishing and maintaining these links is often error-prone, time-consuming, and frequently overlooked. Our empirical study of GitHub repositories revealed that 47% of release artifacts lacked traceability links, and 12% contained broken links. To address this gap, we first analyzed release notes to identify their What, Why, and How information and assessed how these align with PRs, commits, and issues. We curated a benchmark dataset consisting of 3,500 filtered and validated traceability link instances. Then, we implemented LLM-based approaches to automatically establish traceability links of three pairs between release note contents & PRs, release note contents & PRs and release note contents & issues. By combining the time proximity feature, the LLM-based approach, e.g., Gemini 1.5 Pro, achieved a high Precision@1 value of 0.73 for PR traceability recovery. To evaluate the usability and adoption potential of this approach, we conducted an online survey involving 33 open-source practitioners. 16% of respondents rated as very important, and 68% as somewhat important for traceability maintenance.

</details>


### [11] [LLM Assisted Coding with Metamorphic Specification Mutation Agent](https://arxiv.org/abs/2511.18249)
*Mostafijur Rahman Akhond,Gias Uddin*

Main category: cs.SE

TL;DR: CodeMetaAgent (CMA) 是一个基于蜕变关系的LLM代理，通过系统化精炼任务规范和生成语义约束测试用例，提高代码生成的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在软件工程中由于用户规范不明确导致的模糊性和不一致性问题，提高LLM生成代码的可靠性。

Method: 使用蜕变关系驱动LLM代理，系统化精炼任务规范并生成语义约束测试用例，不同于传统将蜕变关系作为后验证的方法。

Result: 在HumanEval-Pro、MBPP-Pro和SWE-Bench_Lite数据集上评估，使用GPT-4o等模型，代码生成准确率提升高达17%，代码覆盖率提升高达99.81%。

Conclusion: 蜕变关系可以作为简单但有效的指导，辅助基于LLM的软件开发。

Abstract: Metamorphic Relations (MRs) serve as a foundational mechanism for generating semantically equivalent mutations. Software engineering has advanced significantly in recent years with the advent of Large Language Models (LLMs). However, the reliability of LLMs in software engineering is often compromised by ambiguities and inconsistencies due to improper user specification. To address this challenge, we present CodeMetaAgent (CMA), a metamorphic relation-driven LLM agent that systematically refines task specifications and generates semantically constrained test cases. Our proposed framework uses MRs with LLMs to improve generation consistency and reduce variability caused by specifications, unlike the traditional use of MRs as post validations. Our framework has been evaluated on the HumanEval-Pro, MBPP-Pro, and SWE-Bench_Lite datasets using the GPT-4o, Mistral Large, GPT-OSS, and Qwen3-Coder models. It improved code generation accuracy by up to 17% and achieved code coverage gains of up to 99.81%. These results show that metamorphic relations can be a simple but effective guide in assisting LLM-based software development.

</details>


### [12] [Can Large Language Models Solve Path Constraints in Symbolic Execution?](https://arxiv.org/abs/2511.18288)
*Wenhan Wang,Kaibo Liu,Zeyu Sun,An Ran Chen,Ge Li,Gang Huang,Lei Ma*

Main category: cs.SE

TL;DR: 本文研究使用大型语言模型替代传统SMT求解器来解决符号执行中的路径约束问题，在测试用例生成和路径分类任务上取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 传统基于SMT求解器的符号执行难以处理具有复杂数据结构或外部API调用的执行路径，限制了其在真实世界软件中的应用。

Method: 通过实证研究评估LLM在两种路径约束求解任务中的能力：生成测试输入以促进执行路径，以及判断给定执行路径是否可满足而不触发错误。构建了新的评估流程和基准数据集。

Result: 实验结果显示，最先进的LLM能够在生成和分类任务中解决路径约束，60%的生成测试用例能准确覆盖给定执行路径。LLM能够覆盖传统符号执行工具无法处理的真实世界仓库中的执行路径。

Conclusion: 这些发现突显了未来用LLM扩展符号执行技术以提升其能力和泛化性的可能性。

Abstract: Symbolic execution is an important software analysis technique which benefits downstream tasks such as software testing and debugging. However, several limitations hinder symbolic execution from application on real-world software. One of the limitations is the inability to solve diverse execution path constraints: traditional symbolic execution based on SMT solvers is difficult to handle execution paths with complex data structures or external API calls. In this paper, we focus on investigating the possibility of adopting large language models (LLM) for path constraint solving instead of traditional solver-based techniques in symbolic execution. We conduct an empirical study to evaluate the ability of LLMs in two types of path constraint solving: generating test inputs to facilitate an execution path, and determining whether a given execution path can be satisfied without triggering any bugs. We build new evaluation pipelines and benchmarks for two tasks: test case generation and path classification, which include data sources from both competition-level programs and real-world repositories. Our experiment results show that state-of-the-art LLMs are able to solve path constraints in both generation and classification tasks, with 60% of generated test cases that accurately cover the given execution path. Moreover, LLMs are capable of improving test coverage by covering execution paths in real-world repositories where traditional symbolic execution tools cannot be applied. These findings highlight the possibility of extending symbolic execution techniques with LLMs in the future to improve the ability and generalizability of symbolic execution.

</details>


### [13] [A Needle in a Haystack: Intent-driven Reusable Artifacts Recommendation with LLMs](https://arxiv.org/abs/2511.18343)
*Dongming Jin,Zhi Jin,Xiaohong Chen,Zheng Fang,Linyu Li,Yuanpeng He,Jia Li,Yirang Zhang,Yingtao Fang*

Main category: cs.SE

TL;DR: 本文提出了TreeRec框架，通过基于LLM的语义抽象将软件构件组织成层次化语义树，解决了传统方法和LLM在开源软件构件推荐中存在的精度低、推理成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 开源软件开发中，开发者面临大量可复用构件时难以找到满足需求的构件，现有推荐方法（包括基于检索、学习和LLM的方法）在精度和效率方面仍有不足。

Method: 提出TreeRec框架：1）构建IntentRecBench基准数据集；2）使用LLM进行语义抽象，将构件组织成层次化语义树；3）通过意图与功能对齐减少推理时间。

Result: 实验表明TreeRec能够持续提升不同LLM在多个生态系统中的推荐性能，证明了其泛化能力和实际部署潜力。

Conclusion: TreeRec通过语义树结构有效缓解了LLM在构件推荐中的精度和效率问题，为开源软件构件推荐提供了可行的解决方案。

Abstract: In open source software development, the reuse of existing artifacts has been widely adopted to avoid redundant implementation work. Reusable artifacts are considered more efficient and reliable than developing software components from scratch. However, when faced with a large number of reusable artifacts, developers often struggle to find artifacts that can meet their expected needs. To reduce this burden, retrieval-based and learning-based techniques have been proposed to automate artifact recommendations. Recently, Large Language Models (LLMs) have shown the potential to understand intentions, perform semantic alignment, and recommend usable artifacts. Nevertheless, their effectiveness has not been thoroughly explored. To fill this gap, we construct an intent-driven artifact recommendation benchmark named IntentRecBench, covering three representative open source ecosystems. Using IntentRecBench, we conduct a comprehensive comparative study of five popular LLMs and six traditional approaches in terms of precision and efficiency. Our results show that although LLMs outperform traditional methods, they still suffer from low precision and high inference cost due to the large candidate space. Inspired by the ontology-based semantic organization in software engineering, we propose TreeRec, a feature tree-guided recommendation framework to mitigate these issues. TreeRec leverages LLM-based semantic abstraction to organize artifacts into a hierarchical semantic tree, enabling intent and function alignment and reducing reasoning time. Extensive experiments demonstrate that TreeRec consistently improves the performance of diverse LLMs across ecosystems, highlighting its generalizability and potential for practical deployment.

</details>


### [14] [Evaluating perturbation robustnessof generative systems that use COBOL code inputs](https://arxiv.org/abs/2511.18488)
*Samuel Ackerman,Wesam Ibraheem,Orna Raz,Marcel Zalmanovici*

Main category: cs.SE

TL;DR: 提出了一个评估COBOL代码输入系统鲁棒性的框架，包括创建COBOL扰动方法库、构建变体扩展数据集，并通过可视化仪表板帮助调试系统输出。


<details>
  <summary>Details</summary>
Motivation: 使用大语言模型的系统对不影响输入含义的微小变化很敏感，这会降低系统的实用性。COBOL作为许多关键业务应用的编程语言，其代码通常无法用于LLM训练，因此评估其鲁棒性至关重要。

Method: 开发COBOL段落和完整程序的扰动方法库，创建任务基准数据集的变体扩展版本，通过计算系统输出指标的变化来评估鲁棒性，并提供动态可视化仪表板进行调试。

Result: 建立了一套完整的COBOL代码鲁棒性评估框架，包括扰动方法、评估指标和可视化工具，能够识别系统对输入变化的敏感性。

Conclusion: 该框架为评估和改进基于LLM的COBOL代码处理系统提供了有效工具，特别适用于翻译、代码生成等任务，有助于提高系统的稳定性和可靠性。

Abstract: Systems incorporating large language models (LLMs) as a component are known to be sensitive (i.e., non-robust) to minor input variations that do not change the meaning of the input; such sensitivity may reduce the system's usefulness. Here, we present a framework to evaluate robustness of systems using COBOL code as input; our application is translation between COBOL and Java programming languages, but the approach extends to other tasks such as code generation or explanation. Targeting robustness of systems with COBOL as input is essential yet challenging. Many business-critical applications are written in COBOL, yet these are typically proprietary legacy applications and their code is unavailable to LLMs for training. We develop a library of COBOL paragraph and full-program perturbation methods, and create variant-expanded versions of a benchmark dataset of examples for a specific task. The robustness of the LLM-based system is evaluated by measuring changes in values of individual and aggregate metrics calculated on the system's outputs. Finally, we present a series of dynamic table and chart visualization dashboards that assist in debugging the system's outputs, and monitoring and understanding root causes of the system's sensitivity to input variation. These tools can be further used to improve the system by, for instance, indicating variations that should be handled by pre-processing steps.

</details>


### [15] [HQPEF-Py: Metrics, Python Patterns, and Guidance for Evaluating Hybrid Quantum Programs](https://arxiv.org/abs/2511.18506)
*Michael Adjei Osei,Sidney Shapiro*

Main category: cs.SE

TL;DR: 提出了一个评估混合量子程序作为端到端工作流的框架，包括量子准备度评分、量子效用标准化加速比定义和时序漂移审计。


<details>
  <summary>Details</summary>
Motivation: 需要将混合量子程序作为完整工作流而非孤立设备或算法来评估，以全面理解其实际性能和价值。

Method: 基于混合量子程序评估框架，形式化工作流感知的量子准备度评分，定义质量约束下的标准化加速比作为量子效用指标，并提供混合管道的时序漂移审计。

Result: 开发了简洁的Python参考实现，展示了如何使用Qiskit或PennyLane等工具实例化这些指标和审计程序，同时保持匹配预算约束和可重复性。

Conclusion: 该框架为混合量子程序提供了系统性的端到端评估方法，有助于更准确地衡量量子计算在实际应用中的准备程度和效用价值。

Abstract: We study how to evaluate hybrid quantum programs as end-to-end workflows rather than as isolated devices or algorithms. Building on the Hybrid Quantum Program Evaluation Framework (HQPEF), we formalize a workflow-aware Quantum Readiness Level (QRL) score; define a normalized speedup under quality constraints for the Utility of Quantumness (UQ); and provide a timing-and-drift audit for hybrid pipelines. We complement these definitions with concise Python reference implementations that illustrate how to instantiate the metrics and audit procedures with state-of-the-art classical and quantum solvers (e.g., via Qiskit or PennyLane), while preserving matched-budget discipline and reproducibility.

</details>


### [16] [End-to-End Automated Logging via Multi-Agent Framework](https://arxiv.org/abs/2511.18528)
*Renyi Zhong,Yintong Huo,Wenwei Gu,Yichen Li,Michael R. Lyu*

Main category: cs.SE

TL;DR: Autologger是一个新颖的混合框架，通过Judger分类器判断是否需要日志记录，然后使用多智能体系统确定日志位置和内容，显著提升了日志记录的质量和准确性。


<details>
  <summary>Details</summary>
Motivation: 软件日志记录对系统可观测性至关重要，但开发者面临过度记录成本高和记录不足风险大的双重危机，现有自动化日志工具往往忽略了是否需要记录的基本决策。

Method: Autologger首先使用微调的分类器Judger准确判断方法是否需要新的日志语句，如果需要，则激活包含定位器(Locator)和生成器(Generator)的多智能体系统，利用程序分析和检索工具协同工作。

Result: 在三个成熟开源项目的大规模语料库上评估，Autologger在关键的是否需要记录决策上达到96.63%的F1分数，在端到端设置中，生成的日志语句整体质量比最强基线提高了16.13%。

Conclusion: Autologger框架具有通用性，能够持续提升各种骨干LLM的性能，有效解决了完整的端到端日志记录管道问题。

Abstract: Software logging is critical for system observability, yet developers face a dual crisis of costly overlogging and risky underlogging. Existing automated logging tools often overlook the fundamental whether-to-log decision and struggle with the composite nature of logging. In this paper, we propose Autologger, a novel hybrid framework that addresses the complete the end-to-end logging pipeline. Autologger first employs a fine-tuned classifier, the Judger, to accurately determine if a method requires new logging statements. If logging is needed, a multi-agent system is activated. The system includes specialized agents: a Locator dedicated to determining where to log, and a Generator focused on what to log. These agents work together, utilizing our designed program analysis and retrieval tools. We evaluate Autologger on a large corpus from three mature open-source projects against state-of-the-art baselines. Our results show that Autologger achieves 96.63\% F1-score on the crucial whether-to-log decision. In an end-to-end setting, Autologger improves the overall quality of generated logging statements by 16.13\% over the strongest baseline, as measured by an LLM-as-a-judge score. We also demonstrate that our framework is generalizable, consistently boosting the performance of various backbone LLMs.

</details>


### [17] [From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence](https://arxiv.org/abs/2511.18538)
*Jian Yang,Wei Zhang,Shark Liu,Jiajun Wu,Shawn Guo,Yizhi Li*

Main category: cs.SE

TL;DR: 本文提供了关于代码大语言模型的全面综合与实践指南，系统分析了从数据准备到后训练的完整模型生命周期，包括代码预训练、监督微调、强化学习和自主编码代理等关键技术。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在自动化软件开发中的革命性应用，从基于规则的系统发展到Transformer架构，性能从个位数提升到超过95%的成功率，需要系统总结代码LLMs的技术发展与实践应用。

Method: 通过一系列分析和探测实验，系统检查数据管理、代码预训练、监督微调、强化学习、自主编码代理等完整模型生命周期，分析通用LLMs和代码专用LLMs的能力。

Result: 对代码预训练、监督微调、强化学习进行了全面实验分析，涵盖扩展规律、框架选择、超参数敏感性、模型架构和数据集比较，识别了学术研究与实际部署之间的差距。

Conclusion: 本文为代码LLMs提供了全面的技术路线图，指出了代码正确性、安全性、大型代码库的上下文感知以及与开发工作流集成等关键研究方向，为未来研究与实践提供了指导。

Abstract: Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.

</details>


### [18] [Strategic Decision Framework for Enterprise LLM Adoption](https://arxiv.org/abs/2511.18589)
*Michael Trusov,Minha Hwang,Zainab Jamal,Swarup Chandra*

Main category: cs.SE

TL;DR: 本文提出了一个系统性的六步决策框架，帮助组织从初始应用选择到最终部署采用大型语言模型（LLMs），解决数据安全、开发方法、基础设施和部署策略等关键挑战。


<details>
  <summary>Details</summary>
Motivation: 组织在采用LLMs时缺乏明确的指导，面临数据安全、合规性、开发方法和部署策略等关键挑战，特别是在医疗、金融和软件行业等敏感领域。

Method: 基于广泛访谈和成功与失败实施案例的分析，开发了一个六步决策框架，通过关键决策点和真实案例提供实践指导。

Result: 该框架帮助组织将技术能力与业务目标对齐，在客户服务自动化、内容创建和高级分析等各种用例中确保安全高效的集成。

Conclusion: 该决策框架为业务领导者提供了实用的指导，使组织能够在采用LLMs时做出明智决策，同时确保跨不同用例的安全和高效集成。

Abstract: Organizations are rapidly adopting Large Language Models (LLMs) to transform their operations, yet they lack clear guidance on key decisions for adoption and implementation. While LLMs offer powerful capabilities in content generation, assisted coding, and process automation, businesses face critical challenges in data security, LLM solution development approach, infrastructure requirements, and deployment strategies. Healthcare providers must protect patient data while leveraging LLMs for medical analysis, financial institutions need to balance automated customer service with regulatory compliance, and software companies seek to enhance development productivity while maintaining code security.
  This article presents a systematic six-step decision framework for LLM adoption, helping organizations navigate from initial application selection to final deployment. Based on extensive interviews and analysis of successful and failed implementations, our framework provides practical guidance for business leaders to align technological capabilities with business objectives. Through key decision points and real-world examples from both B2B and B2C contexts, organizations can make informed decisions about LLM adoption while ensuring secure and efficient integration across various use cases, from customer service automation to content creation and advanced analytics.

</details>


### [19] [From Reviewers' Lens: Understanding Bug Bounty Report Invalid Reasons with LLMs](https://arxiv.org/abs/2511.18608)
*Jiangrui Zheng,Yingming Zhou,Ali Abdullah Ahmad,Hanqing Yao,Xueqing Liu*

Main category: cs.SE

TL;DR: 本文研究了AI生成的漏洞报告被标记为无效的原因，通过分析9,942份漏洞赏金报告，发现现有LLM模型在检测无效报告方面存在困难，并提出结合拒绝原因分类的RAG框架来改进无效报告识别。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成的漏洞报告增多，需要帮助漏洞猎人理解报告被标记为无效的原因，以提高报告质量并减轻评审负担。

Method: 收集9,942份已披露的漏洞赏金报告，评估GPT-5、DeepSeek和微调RoBERTa等模型识别无效报告的能力，构建信息泄露漏洞拒绝原因分类法，并集成到检索增强生成(RAG)框架中。

Result: 现有LLM模型总体准确率高，但在检测无效案例时表现不佳，倾向于过度接受报告。结合拒绝原因分类的RAG方法显著提高了分类一致性和减少了偏见。

Conclusion: 无效报告识别具有挑战性，将LLM与结构化评审知识相结合可以支持更透明和一致的漏洞报告评审。

Abstract: Bug bounty platforms (e.g., HackerOne, BugCrowd) leverage crowd-sourced vulnerability discovery to improve continuous coverage, reduce the cost of discovery, and serve as an integral complement to internal red teams. With the rise of AI-generated bug reports, little work exists to help bug hunters understand why these reports are labeled as invalid. To improve report quality and reduce reviewers' burden, it is critical to predict invalid reports and interpret invalid reasons.
  In this work, we conduct an empirical study with the purpose of helping bug hunters understand the validity of reports. We collect a dataset of 9,942 disclosed bug bounty reports, including 1,400 invalid reports, and evaluate whether state-of-the-art large language models can identify invalid reports. While models such as GPT-5, DeepSeek, and a fine-tuned RoBERTa achieve strong overall accuracy, they consistently struggle to detect invalid cases, showing a tendency to over-accept reports. To improve invalidity detection, we build a taxonomy of rejection reasons for Information Disclosure vulnerabilities and incorporate it into a retrieval-augmented generation (RAG) framework. This approach substantially improves classification consistency and reduces bias. We also examine whether reviewer decisions may be influenced by factors beyond the content of the report. Our analysis shows that reporters with higher reputations tend to receive more favorable outcomes in borderline cases, suggesting that perceived expertise can influence review judgments.
  Overall, our findings highlight the challenges of invalid report identification and show that combining LLMs with structured reviewer knowledge can support more transparent and consistent vulnerability report review.

</details>


### [20] [Leveraging Discrete Choice Experiments for User-Centric Requirements Prioritization in mHealth Applications](https://arxiv.org/abs/2511.18625)
*Wei Wang,Hourieh Khalajzadeh,John Grundy,Anuradha Madugalla,Humphrey O. Obie*

Main category: cs.SE

TL;DR: 本研究通过离散选择实验发现，用户偏好可用性高、控制性强、调整频率低、变化幅度小的自适应移动健康应用设计，而频繁使用的功能和照护者参与会降低其价值。


<details>
  <summary>Details</summary>
Motivation: 移动健康应用在慢性病管理中面临可用性和可访问性挑战，自适应用户界面可提供个性化解决方案，但需要了解用户偏好和权衡以促进广泛采纳。

Method: 对186名患有慢性病并使用移动健康应用的用户进行离散选择实验，使用混合logit模型分析偏好异质性，并进行亚组分析。

Result: 保持可用性同时确保对调整的控制、低频调整和小规模变化是促进自适应设计采纳的关键因素，而频繁使用的功能和照护者参与会降低其感知价值。

Conclusion: 本研究通过数据驱动方法量化用户偏好，识别关键权衡，为开发自适应移动健康应用提供指导，并为软件工程领域的需求优先级排序奠定基础。

Abstract: Mobile health (mHealth) applications are widely used for chronic disease management, but usability and accessibility challenges persist due to the diverse needs of users. Adaptive User Interfaces (AUIs) offer a personalized solution to enhance user experience, yet barriers to adoption remain. Understanding user preferences and trade-offs is essential to ensure widespread acceptance of adaptation designs. This study identifies key factors influencing user preferences and trade-offs in mHealth adaptation design. A Discrete Choice Experiment (DCE) was conducted with 186 participants who have chronic diseases and use mHealth applications. Participants were asked to select preferred adaptation designs from choices featuring six attributes with varying levels. A mixed logit model was used to analyze preference heterogeneity and determine the factors most likely influencing adoption. Additionally, subgroup analyses were performed to explore differences by age, gender, health conditions, and coping mechanisms. Maintaining usability while ensuring controllability over adaptations, infrequent adaptations, and small-scale changes are key factors that facilitate the adoption of adaptive mHealth app designs. In contrast, frequently used functions and caregiver involvement can diminish the perceived value of such adaptations. This study employs a data-driven approach to quantify user preferences, identify key trade-offs, and reveal variations across demographic and behavioral subgroups through preference heterogeneity modeling. Furthermore, our results offer valuable guidance for developing future adaptive mHealth applications and lay the groundwork for continued exploration into requirements prioritization within the field of software engineering.

</details>


### [21] [ChroniUXMag: A Persona-Driven Framework for Inclusive mHealth Requirements Engineering](https://arxiv.org/abs/2511.18634)
*Wei Wang,Devi Karolita,Hourieh Khalajzadeh,John Grundy,Anuradha Madugalla,Humphrey O. Obie*

Main category: cs.SE

TL;DR: ChroniUXMag是一个用于移动健康应用包容性需求分析的框架，通过系统化方法识别和评估慢性病患者使用mHealth时的包容性障碍。


<details>
  <summary>Details</summary>
Motivation: 传统需求工程方法往往忽视慢性病患者动态变化的健康需求，导致mHealth应用在可访问性、包容性和持续参与度方面存在挑战。

Method: 基于InclusiveMag和GenderMag原则，通过文献综述、焦点小组、访谈和大规模调查识别包容性维度，并将其整合为人物角色和认知走查表。

Result: 识别出13个影响mHealth使用的社会技术复杂性维度，包括信任、数字素养、依赖性和文化背景等，支持基于人物角色的结构化评估。

Conclusion: ChroniUXMag为mHealth需求工程提供了可复制的、基于证据的包容性嵌入方法，能够发现传统可用性评估经常遗漏的包容性障碍。

Abstract: Mobile health (mHealth) applications are increasingly adopted for chronic disease management, yet they face persistent challenges related to accessibility, inclusivity, and sustained engagement. Patients' needs evolve dynamically with their health progression, adherence, and caregiver support, creating unique requirements engineering (RE) challenges that traditional approaches often overlook. This study introduces ChroniUXMag, a framework for eliciting and analysing inclusivity requirements in mHealth design. Building on InclusiveMag and GenderMag principles, the framework aims to help researchers and practitioners systematically capture and evaluate factors that influence how individuals with chronic conditions perceive, trust, and interact with mHealth systems. The framework was developed through two stages of the InclusiveMag process. In the first stage, inclusivity facets were identified through a systematic literature review, focus groups, interviews, and a large-scale survey. In the second stage, these facets were synthesised into personas representing diverse health situations, attitudes, and digital practices, and integrated into an adapted cognitive walkthrough form. Thirteen facets were identified that capture the socio-technical complexity of mHealth use, including trust, digital literacy, dependency, and cultural context. These facets support structured, persona-driven evaluations that reveal inclusivity barriers often missed by traditional usability assessments. ChroniUXMag contributes to RE by offering a reproducible, evidence-based approach for embedding inclusivity into mHealth requirements. Future work will extend the third stage Apply through practitioner-led evaluation in real-world design contexts.

</details>


### [22] [Summary-Mediated Repair: Can LLMs use code summarisation as a tool for program repair?](https://arxiv.org/abs/2511.18782)
*Lukas Twist*

Main category: cs.SE

TL;DR: 提出了一种基于代码摘要的程序修复方法，通过自然语言代码摘要作为中间步骤来帮助LLMs修复实现层面的bug，在HumanEvalPack和MBPP基准测试中修复了最多65%的未见过错误。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在基准测试中表现良好，但生成的代码经常包含难以察觉的实现层面bug。LLMs在代码摘要时能捕捉高级意图但可能忽略低级噪声，因此利用代码摘要作为中间步骤来辅助程序修复。

Method: 提出了摘要介导修复方法，这是一个仅使用提示的管道，将自然语言代码摘要作为显式中间步骤。评估了多种摘要风格，发现错误感知诊断摘要效果最好。

Result: 在八个生产级LLMs和两个函数级基准测试中，错误感知诊断摘要修复了最多65%的未见过错误，平均比直接修复基线高出5%，但整体改进较为有限且依赖于具体LLM。

Conclusion: 代码摘要可以作为一种廉价、人类可解释的诊断工具集成到程序修复管道中，而不是作为独立的万能解决方案。

Abstract: Large Language Models (LLMs) often produce code with subtle implementation-level bugs despite strong benchmark performance. These errors are hard for LLMs to spot and can have large behavioural effects; yet when asked to summarise code, LLMs can frequently surface high-level intent and sometimes overlook this low-level noise. Motivated by this, we propose summary-mediated repair, a prompt-only pipeline for program repair that leverages natural-language code summarisation as an explicit intermediate step, extending previous work that has already shown code summarisation to be a useful intermediary for downstream tasks. We evaluate our method across eight production-grade LLMs on two function level benchmarks (HumanEvalPack and MBPP), comparing several summary styles against a direct repair baseline. Error-aware diagnostic summaries consistently yield the largest gains - repairing up to 65% of unseen errors, on average of 5% more than the baseline - though overall improvements are modest and LLM-dependent. Our results position summaries as a cheap, human-interpretable diagnostic artefact that can be integrated into program-repair pipelines rather than a stand-alone fix-all.

</details>


### [23] [Optimizing LLM Code Suggestions: Feedback-Driven Timing with Lightweight State Bounds](https://arxiv.org/abs/2511.18842)
*Mohammad Nour Al Awad,Sergey Ivanov,Olga Tikhonova*

Main category: cs.SE

TL;DR: 提出了自适应时机机制，根据开发者实时反馈动态调整代码建议的延迟时间，显著提高了建议接受率并减少了浪费的推理调用


<details>
  <summary>Details</summary>
Motivation: LLM在代码自动补全中生成上下文感知建议，但何时呈现这些建议尚未充分探索，常导致中断或浪费推理调用

Method: 结合逻辑变换的最近接受率和有界延迟范围，基于开发者认知状态的高级二元预测来动态调整建议延迟

Result: 在专业开发者两个月的部署中，建议接受率从无延迟的4.9%提高到静态延迟的15.4%，再到自适应时机的18.6%；盲拒绝率从8.3%降至0.36%；浪费推理调用减少75%

Conclusion: 自适应时机机制使基于LLM的代码助手在实践中更高效和成本效益更高

Abstract: Large Language Models (LLMs) have transformed code auto-completion by generating context-aware suggestions. Yet, deciding when to present these suggestions remains underexplored, often leading to interruptions or wasted inference calls. We propose an adaptive timing mechanism that dynamically adjusts the delay before offering a suggestion based on real-time developer feedback. Our suggested method combines a logistic transform of recent acceptance rates with a bounded delay range, anchored by a high-level binary prediction of the developer's cognitive state. In a two-month deployment with professional developers, our system improved suggestion acceptance from 4.9% with no delay to 15.4% with static delays, and to 18.6% with adaptive timing-while reducing blind rejections (rejections without being read) from 8.3% to 0.36%. Together, these improvements increase acceptance and substantially reduce wasted inference calls by 75%, making LLM-based code assistants more efficient and cost-effective in practice.

</details>


### [24] [Pre-Filtering Code Suggestions using Developer Behavioral Telemetry to Optimize LLM-Assisted Programming](https://arxiv.org/abs/2511.18849)
*Mohammad Nour Al Awad,Sergey Ivanov,Olga Tikhonova*

Main category: cs.SE

TL;DR: 提出轻量级预过滤模型，在调用LLM前预测代码建议被接受的可能性，使用开发者行为遥测数据提升接受率并减少低价值LLM调用


<details>
  <summary>Details</summary>
Motivation: LLM代码建议被大量忽略，造成计算浪费、延迟增加和不必要的中断，需要更智能的调用时机判断

Method: 基于开发者实时行为遥测（如打字速度、文件导航、编辑活动）构建轻量级预过滤模型，在调用LLM前预测建议接受概率

Result: 在VS Code插件中部署4个月，接受率从18.4%提升至34.2%，抑制了35%的低价值LLM调用

Conclusion: 仅使用行为信号就能显著改善LLM辅助编程的用户体验和系统效率，证明了时序感知、隐私保护的适配机制的价值

Abstract: Large Language Models (LLMs) are increasingly integrated into code editors to provide AI-powered code suggestions. Yet many of these suggestions are ignored, resulting in wasted computation, increased latency, and unnecessary interruptions. We introduce a lightweight pre-filtering model that predicts the likelihood of suggestion acceptance before invoking the LLM, using only real-time developer telemetry such as typing speed, file navigation, and editing activity. Deployed in a production-grade Visual Studio Code plugin over four months of naturalistic use, our approach nearly doubled acceptance rates (18.4% -> 34.2%) while suppressing 35% of low-value LLM calls. These findings demonstrate that behavioral signals alone can meaningfully improve both user experience and system efficiency in LLM-assisted programming, highlighting the value of timing-aware, privacy-preserving adaptation mechanisms. The filter operates solely on pre-invocation editor telemetry and never inspects code or prompts.

</details>


### [25] [Time Travel: LLM-Assisted Semantic Behavior Localization with Git Bisect](https://arxiv.org/abs/2511.18854)
*Yujing Wang,Weize Hong*

Main category: cs.SE

TL;DR: 提出一个将大语言模型集成到Git bisect过程中的新框架，用于语义故障定位，在存在不稳定测试和非单调回归的情况下提高成功率并减少平均二分时间。


<details>
  <summary>Details</summary>
Motivation: 传统二分法假设确定性谓词和二进制失败状态，但在现代软件开发中常因不稳定测试、非单调回归以及与上游仓库的语义分歧而违反这些假设。

Method: 通过结构化思维链推理增强二分遍历，在噪声条件下逐提交分析；评估多个开源和专有LLM，使用QLoRA在语义标记差异数据集上微调DeepSeekCoderV2；采用弱监督工作流减少标注开销，结合人工循环校正和自一致性过滤。

Result: 在多个开源项目上的实验显示，成功率从74.2%提升到80.6%，绝对增益6.4个百分点，显著减少失败遍历，平均二分时间最多减少2倍。

Conclusion: 讨论了针对提交级行为分析的时间推理、提示设计和微调策略，展示了LLM在软件故障定位中的有效性。

Abstract: We present a novel framework that integrates Large Language Models (LLMs) into the Git bisect process for semantic fault localization. Traditional bisect assumes deterministic predicates and binary failure states assumptions often violated in modern software development due to flaky tests, nonmonotonic regressions, and semantic divergence from upstream repositories. Our system augments bisect traversal with structured chain of thought reasoning, enabling commit by commit analysis under noisy conditions. We evaluate multiple open source and proprietary LLMs for their suitability and fine tune DeepSeekCoderV2 using QLoRA on a curated dataset of semantically labeled diffs. We adopt a weak supervision workflow to reduce annotation overhead, incorporating human in the loop corrections and self consistency filtering. Experiments across multiple open source projects show a 6.4 point absolute gain in success rate from 74.2 to 80.6 percent, leading to significantly fewer failed traversals and by experiment up to 2x reduction in average bisect time. We conclude with discussions on temporal reasoning, prompt design, and finetuning strategies tailored for commit level behavior analysis.

</details>


### [26] [VecIntrinBench: Benchmarking Cross-Architecture Intrinsic Code Migration for RISC-V Vector](https://arxiv.org/abs/2511.18867)
*Liutong Han,Chu Kang,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: VecIntrinBench是首个包含RISC-V Vector扩展的内在函数基准测试，包含50个函数级任务，系统评估了各种代码迁移方法，发现先进大语言模型在RISC-V代码迁移中能达到基于规则映射方法的效果，且性能更优。


<details>
  <summary>Details</summary>
Motivation: RISC-V软件生态系统对算法库迁移有巨大需求，但目前缺乏对新兴RISC-V架构的内在函数代码基准测试，没有能全面评估RVV扩展内在函数迁移能力的基准。

Method: 提出VecIntrinBench基准测试，包含50个来自开源仓库的函数级任务，实现为标量、RVV内在函数、Arm Neon内在函数和x86内在函数，并提供全面的功能和性能测试用例。

Result: 系统评估显示，先进大语言模型在RISC-V代码迁移中能达到基于规则映射方法的效果，同时提供更优的性能。

Conclusion: VecIntrinBench填补了RVV扩展内在函数基准测试的空白，为社区和开发者提供了评估代码迁移能力的工具，并分析了LLM在代码迁移领域的未来发展方向。

Abstract: Intrinsic functions are specialized functions provided by the compiler that efficiently operate on architecture-specific hardware, allowing programmers to write optimized code in a high-level language that fully exploits hardware features. Using intrinsics to vectorize core code blocks is a standard optimization method in high-performance libraries, often requiring specific vector optimization implementations for multiple mainstream architectures. The promising RISC-V software ecosystem has a significant demand for algorithm library migration and adaptation. Translating existing intrinsic functions to RISC-V Vector (RVV) intrinsic functions across architectures is currently a mainstream approach. Rule-based intrinsic mapping methods and LLM-based code generation can help developers address the code migration challenge. However, existing intrinsic code benchmarks focus on mainstream SIMD intrinsics and lack support for the emerging RISC-V architecture. There is currently no benchmark that comprehensively evaluates the intrinsic migration capabilities for the RVV extension. To fill this gap, we propose VecIntrinBench, the first intrinsic benchmark encompassing RVV extensions. It includes 50 function-level tasks from open source repositories, implemented as scalars, RVV intrinsics, Arm Neon intrinsics, and x86 intrinsics, along with comprehensive functional and performance test cases. We systematically evaluated various code migration approaches on VecIntrinBench, yielding a series of insightful findings. The results demonstrate that advanced Large Language Models (LLMs) achieve a similar effect as rule-based mapping approaches for RISC-V code migration, while also delivering superior performance. We further analyze the reasons and identify future directions for LLM development in the code migration field. The VecIntrinBench is open-sourced to benefit the broader community and developers.

</details>


### [27] [Optimization-Aware Test Generation for Deep Learning Compilers](https://arxiv.org/abs/2511.18918)
*Qingchao Shen,Zan Wang,Haoyang Ma,Yongqiang Tian,Lili Huang,Zibo Xiao,Junjie Chen,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: OATest是一种针对深度学习编译器优化阶段测试的新方法，通过合成优化感知的计算图来检测编译器错误，在TVM和ONNXRuntime中发现了58个未知bug。


<details>
  <summary>Details</summary>
Motivation: 深度学习编译器在DL生态系统中至关重要，但现有方法难以测试其核心的优化阶段，因为难以生成优化感知的测试用例。

Method: OATest结合从文档化测试中提取的模式，将其融入种子计算图，采用边重用策略建立模式与上下文间的强连接，并使用辅助层添加策略解决约束破坏问题。

Result: OATest在TVM和ONNXRuntime中检测到更多bug，达到更高代码覆盖率，发现了58个未知bug，其中36个已被开发者确认或修复。

Conclusion: OATest能有效测试DL编译器的优化阶段，显著优于现有方法，为编译器可靠性提供了有力保障。

Abstract: Deep Learning (DL) compilers have been widely utilized to optimize DL models for efficient deployment across various hardware. Due to their vital role in the DL ecosystem, ensuring their reliability and security is critical. However, existing approaches have limitations in testing optimization stages, which is the core functionality of DL compilers, due to the difficulty in generating optimization-aware tests. In this paper, we proposed OATest, a novel approach for synthesizing optimization-aware computational graphs. The approach combines patterns extracted from documented tests for optimization and incorporates them into seed computational graphs, enabling broader exploration of optimization paths. To guarantee the optimization-awareness of generated graphs, OATest introduces the edges reusing strategy to establish strong connections between patterns and contexts. Additionally, to solve the validity challenge for the generated graphs, OATest employs an auxiliary layers addition strategy to resolve broken constraints. Equipped with two distinct test oracles, OATest applies differential testing to evaluate the two widely used DL compilers (i.e., TVM and ONNXRuntime). Our experimental results show that OATest outperforms the state-of-the-art method by detecting more bugs and achieving higher code coverage in TVM and ONNXRutimes. Additionally, OATest uncovers 58 previously unknown bugs, 36 of which have been confirmed or fixed by developers.

</details>


### [28] [LLM-Driven Kernel Evolution: Automating Driver Updates in Linux](https://arxiv.org/abs/2511.18924)
*Arina Kharlamova,Jiawen Liu,Tianyi Zhang,Xinrui Yang,Humaid Alqasimi,Youcheng Sun,Chun Jason Xue*

Main category: cs.SE

TL;DR: DRIVEBENCH是一个可执行的内核-驱动协同演化案例语料库，AUTODRIVER是一个基于LLM的闭环系统，用于自动化驱动维护。该系统通过提示工程、多智能体协作、静态分析和迭代验证，确保生成的补丁在语法、功能和语义上与内核约定一致。


<details>
  <summary>Details</summary>
Motivation: Linux内核演化通过API/ABI变更、语义转变和安全强化更新破坏驱动程序兼容性，需要自动化工具来解决驱动维护问题。

Method: 构建DRIVEBENCH语料库（包含235个验证案例），开发AUTODRIVER系统，集成提示工程、多智能体协作、静态分析和迭代验证方法。

Result: 在55个案例评估中，AUTODRIVER实现56.4%的编译成功率；基于QEMU的启动验证表明编译后的补丁在大多数情况下保留了驱动初始化功能。

Conclusion: 通过发布DRIVEBENCH和工具，为可重复研究提供了基础，并为Linux内核与驱动程序的持续、安全协同演化提供了实用路径。

Abstract: Linux kernel evolution breaks drivers through API/ABI changes, semantic shifts, and security-hardening updates. We introduce DRIVEBENCH, an executable corpus of kernel$\rightarrow$driver co-evolution cases, and AUTODRIVER, a closed-loop, LLM-driven system for automating driver maintenance. The system integrates prompt engineering, multi-agent collaboration, static analysis, and iterative validation to ensure that generated patches are not only syntactically correct but also functionally and semantically consistent with kernel conventions. The corpus spans v5.10-v6.10 with 235 validated cases drawn from 612 candidates. In evaluation across 55 cases, AUTODRIVER achieves 56.4% compilation success; QEMU-based boot verification indicates that compiled patches preserve driver initialization in most instances. By releasing DRIVEBENCH and tooling, we enable reproducible research and a practical route to continuous, safe co-evolution of drivers with the Linux kernel.

</details>


### [29] [LLMAID: Identifying AI Capabilities in Android Apps with LLMs](https://arxiv.org/abs/2511.19059)
*Pei Liu,Terry Zhuo,Jiawei Deng,Thong James,Shidong Pan,Sherry Xu,Zhenchang Xing,Qinghua Lu,Xiaoning Du,Hongyu Zhang*

Main category: cs.SE

TL;DR: 提出了LLMAID方法，利用大语言模型自动检测移动应用中的AI能力，相比现有基于规则的方法能识别242%更多的真实AI应用，准确率和召回率均超过90%。


<details>
  <summary>Details</summary>
Motivation: 移动应用中AI集成日益普遍，但AI的幻觉和可靠性问题引发担忧。现有方法依赖人工检查和规则启发式，成本高且难以适应先进AI技术。

Method: LLMAID包含四个主要任务：候选提取、知识库交互、AI能力分析与检测、AI服务总结。应用于4,201个Android应用数据集。

Result: 相比最先进的基于规则方法，LLMAID能识别242%更多的真实AI应用；AI相关组件检测的精确率和召回率均超过90%；开发者认为LLMAID生成的AI服务总结比原始应用描述更有信息量。

Conclusion: LLMAID能有效自动检测移动应用中的AI能力，实证分析显示AI功能主要集中在计算机视觉领域（54.80%），其中目标检测是最常见任务（25.19%）。

Abstract: Recent advancements in artificial intelligence (AI) and its widespread integration into mobile software applications have received significant attention, highlighting the growing prominence of AI capabilities in modern software systems. However, the inherent hallucination and reliability issues of AI continue to raise persistent concerns. Consequently, application users and regulators increasingly ask critical questions such as: Does the application incorporate AI capabilities? and What specific types of AI functionalities are embedded? Preliminary efforts have been made to identify AI capabilities in mobile software; however, existing approaches mainly rely on manual inspection and rule-based heuristics. These methods are not only costly and time-consuming but also struggle to adapt advanced AI techniques.
  To address the limitations of existing methods, we propose LLMAID (Large Language Model for AI Discovery). LLMAID includes four main tasks: (1) candidate extraction, (2) knowledge base interaction, (3) AI capability analysis and detection, and (4) AI service summarization. We apply LLMAID to a dataset of 4,201 Android applications and demonstrate that it identifies 242% more real-world AI apps than state-of-the-art rule-based approaches. Our experiments show that LLM4AID achieves high precision and recall, both exceeding 90%, in detecting AI-related components. Additionally, a user study indicates that developers find the AI service summaries generated by LLMAID to be more informative and preferable to the original app descriptions. Finally, we leverage LLMAID to perform an empirical analysis of AI capabilities across Android apps. The results reveal a strong concentration of AI functionality in computer vision (54.80%), with object detection emerging as the most common task (25.19%).

</details>


### [30] [Can LLMs Recover Program Semantics? A Systematic Evaluation with Symbolic Execution](https://arxiv.org/abs/2511.19130)
*Rong Feng,Suman Saha*

Main category: cs.SE

TL;DR: 本研究探讨了结合符号执行和LLM进行程序反混淆的有效性，通过四种混淆变换构建基准测试，发现GPT-4.1-mini表现最佳，且加入KLEE符号执行工件能显著提升语义保持和编译成功率。


<details>
  <summary>Details</summary>
Motivation: 混淆给软件工程任务（如程序理解、维护、测试和漏洞检测）带来持续挑战，现有分析工具和LLM难以恢复原始语义。

Method: 构建包含四种混淆变换（控制流平坦化、不透明谓词、算术编码、分支编码）的C程序基准，比较三种最先进LLM在两种训练配置下的表现：基于混淆/原始代码对的基线微调，以及加入KLEE符号执行工件（SMT约束、路径统计、测试用例）的增强微调。

Result: GPT-4.1-mini在整体反混淆中表现最强，加入KLEE工件能持续提升所有模型的语义保持和编译成功率。

Conclusion: 反混淆是更广泛的软件工程关注点，结合LLM和符号执行可以增强在混淆存在情况下的自动化测试、静态分析和程序理解能力。

Abstract: Obfuscation poses a persistent challenge for software engineering tasks such as program comprehension, maintenance, testing, and vulnerability detection. While compiler optimizations and third-party code often introduce transformations that obscure program intent, existing analysis tools and large language models (LLMs) struggle to recover the original semantics. In this work, we investigate whether LLMs, when fine-tuned with symbolic execution artifacts, can effectively deobfuscate programs and restore analyzability. We construct a benchmark by applying four widely studied transformations-control-flow flattening, opaque predicates, arithmetic encoding, and branch encoding-across diverse C programs from TUM Obfuscation Benchmarks, the LLVM test suite, and algorithmic repositories. We then compare three state-of-the-art LLMs under two training configurations: baseline fine-tuning on obfuscated/original code pairs, and enhanced fine-tuning with additional KLEE artifacts such as SMT constraints, path statistics, and test cases. Our evaluation examines syntactic correctness (compilation success), semantic fidelity (behavioral equivalence under symbolic execution), and code quality (readability and structure). Results show that GPT-4.1-mini achieves the strongest deobfuscation overall, and that incorporating KLEE artifacts consistently improves semantic preservation and compilation success across models. These findings highlight deobfuscation as a broader software engineering concern, demonstrating that combining LLMs with symbolic execution can strengthen automated testing, static analysis, and program comprehension in the presence of obfuscation.

</details>


### [31] [LLMs-Powered Real-Time Fault Injection: An Approach Toward Intelligent Fault Test Cases Generation](https://arxiv.org/abs/2511.19132)
*Mohammad Abboush,Ahmad Hatahet,Andreas Rausch*

Main category: cs.SE

TL;DR: 提出了一种基于大语言模型(LLMs)的故障测试用例生成方法，用于汽车软件系统的实时故障注入测试，显著提高了测试效率和安全性评估。


<details>
  <summary>Details</summary>
Motivation: 当前故障注入方法需要手动识别故障属性，对于复杂系统来说成本高、耗时长且劳动密集。

Method: 利用大语言模型从功能安全需求自动生成故障测试用例，考虑了代表性和覆盖率标准，并比较了不同LLM模型的性能。

Result: GPT-4o模型在功能安全需求分类和故障测试用例生成方面表现最佳，F1分数分别达到88%和97.5%。生成的测试用例在硬件在环系统上实时执行验证。

Conclusion: 该方法优化了实时测试流程，降低了成本，同时增强了复杂安全关键汽车软件系统的安全性能。

Abstract: A well-known testing method for the safety evaluation and real-time validation of automotive software systems (ASSs) is Fault Injection (FI). In accordance with the ISO 26262 standard, the faults are introduced artificially for the purpose of analyzing the safety properties and verifying the safety mechanisms during the development phase. However, the current FI method and tools have a significant limitation in that they require manual identification of FI attributes, including fault type, location and time. The more complex the system, the more expensive, time-consuming and labour-intensive the process. To address the aforementioned challenge, a novel Large Language Models (LLMs)-assisted fault test cases (TCs) generation approach for utilization during real-time FI tests is proposed in this paper. To this end, considering the representativeness and coverage criteria, the applicability of various LLMs to create fault TCs from the functional safety requirements (FSRs) has been investigated. Through the validation results of LLMs, the superiority of the proposed approach utilizing gpt-4o in comparison to other state-of-the-art models has been demonstrated. Specifically, the proposed approach exhibits high performance in terms of FSRs classification and fault TCs generation with F1-score of 88% and 97.5%, respectively. To illustrate the proposed approach, the generated fault TCs were executed in real time on a hardware-in-the-loop system, where a high-fidelity automotive system model served as a case study. This novel approach offers a means of optimizing the real-time testing process, thereby reducing costs while simultaneously enhancing the safety properties of complex safety-critical ASSs.

</details>


### [32] [Synthesizing Test Cases for Narrowing Specification Candidates](https://arxiv.org/abs/2511.19177)
*Alcino Cunha,Nuno Macedo*

Main category: cs.SE

TL;DR: 提出一种技术，通过生成测试用例来帮助用户从多个候选形式化规范中选择最佳的一个。


<details>
  <summary>Details</summary>
Motivation: 当存在多个候选形式化规范时，需要一种系统化的方法来帮助用户选择最合适的规范。

Method: 提出两种基于求解器的算法：一种生成最小测试套件，另一种不保证最小性但更高效。两种算法都在原型工具中实现，用于Alloy规范的选择。

Result: 评估显示，最优算法对许多实际问题足够高效，非最优算法可扩展到数十个候选规范，同时生成合理大小的测试套件。

Conclusion: 该技术能有效帮助用户从候选规范集中选择最佳规范，两种算法在不同规模问题上各有优势。

Abstract: This paper proposes a technique to help choose the best formal specification candidate among a set of alternatives. Given a set of specifications, our technique generates a suite of test cases that, once classified by the user as desirable or not, narrows down the set of candidates to at most one specification. Two alternative solver-based algorithms are proposed, one that generates a minimal test suite, and another that does not ensure minimality. Both algorithms were implemented in a prototype that can be used generate test suites to help choose among alternative Alloy specifications. Our evaluation of this prototype against a large set of problems showed that the optimal algorithm is efficient enough for many practical problems, and that the non-optimal algorithm can scale up to dozens of candidate specifications while still generating reasonably sized test suites.

</details>


### [33] [Prompt Less, Smile More: MTP with Semantic Engineering in Lieu of Prompt Engineering](https://arxiv.org/abs/2511.19427)
*Jayanaka L. Dantanarayana,Savini Kashmira,Thakee Nathees,Zichen Zhang,Krisztian Flautner,Lingjia Tang,Jason Mars*

Main category: cs.SE

TL;DR: 提出语义工程方法，通过语义上下文注释在代码中嵌入自然语言上下文，增强程序语义表达，从而改进基于LLM的AI集成编程中的提示生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法如MTP仅依赖静态代码语义生成提示，但实际应用需要更多上下文线索、开发者意图和领域特定推理，这些超出了纯代码语义的表达能力。

Method: 引入语义工程方法，提出语义上下文注释机制，允许开发者在程序结构中直接嵌入自然语言上下文，并将其集成到Jac编程语言中，扩展MTP以在提示生成时包含这些丰富语义。

Result: 评估显示语义工程显著提高了提示保真度，在性能上与提示工程相当，但需要更少的开发者工作量。

Conclusion: 语义工程为AI集成编程提供了一种轻量级方法，能够更准确地反映开发者意图，而无需完全手动设计提示。

Abstract: AI-Integrated programming is emerging as a foundational paradigm for building intelligent systems with large language models (LLMs). Recent approaches such as Meaning Typed Programming (MTP) automate prompt generation by leveraging the semantics already present in code. However, many real-world applications depend on contextual cues, developer intent, and domain-specific reasoning that extend beyond what static code semantics alone can express. To address this limitation, we introduce Semantic Engineering, a lightweight method for enriching program semantics so that LLM-based systems can more accurately reflect developer intent without requiring full manual prompt design. We present Semantic Context Annotations (SemTexts), a language-level mechanism that allows developers to embed natural-language context directly into program constructs. Integrated into the Jac programming language, Semantic Engineering extends MTP to incorporate these enriched semantics during prompt generation. We further introduce a benchmark suite designed to reflect realistic AI-Integrated application scenarios. Our evaluation shows that Semantic Engineering substantially improves prompt fidelity, achieving performance comparable to Prompt Engineering while requiring significantly less developer effort.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [34] [Uncertainty Removal in Verification of Nonlinear Systems against Signal Temporal Logic via Incremental Reachability Analysis](https://arxiv.org/abs/2511.17617)
*Antoine Besset,Joris Tillet,Julien Alexandre dit Sandretto*

Main category: cs.LO

TL;DR: 提出了一种验证连续时间非线性系统在不确定性下满足信号时序逻辑规范的框架，通过可达性分析和布尔区间算术解决不确定性问题。


<details>
  <summary>Details</summary>
Motivation: 针对连续时间非线性系统在不确定性下验证STL规范时，由于可达集过度近似或模拟不完整导致的不确定满足问题。

Method: 基于可达性分析，扩展STL语义为布尔区间算术，将满足信号分解为带不确定性标记的单元组件，在满足树中传播，仅细化贡献不确定性的可达集。

Result: 非线性振荡器的案例研究表明，该方法显著减少了满足模糊性。

Conclusion: 该框架通过不确定性标记和选择性细化，有效解决了STL验证中的不确定性问题，支持在线和离线监控。

Abstract: A framework is presented for the verification of Signal Temporal Logic (STL) specifications over continuous-time nonlinear systems under uncertainty. Based on reachability analysis, the proposed method addresses indeterminate satisfaction caused by over-approximated reachable sets or incomplete simulations. STL semantics is extended via Boolean interval arithmetic, enabling the decomposition of satisfaction signals into unitary components with traceable uncertainty markers. These are propagated through the satisfaction tree, supporting precise identification even in nested formulas. To improve efficiency, only the reachable sets contributing to uncertainty are refined, identified through the associated markers. The framework allows online or offline monitoring to adapt to incremental system evolution while avoiding unnecessary recomputation. A case study on a nonlinear oscillator demonstrates a significant reduction in satisfaction ambiguity, highlighting the effectiveness of the approach.

</details>


### [35] [Comparing Labeled Markov Chains: A Cantor-Kantorovich Approach](https://arxiv.org/abs/2511.18103)
*Adrien Banse,Alessandro Abate,Raphaël M. Jungers*

Main category: cs.LO

TL;DR: 本文研究了标记马尔可夫链的Cantor-Kantorovich距离，证明其可表示为有限时域总变差距离的折现和，分析了计算复杂性、连续性性质和近似性，并提供了可计算近似方案。


<details>
  <summary>Details</summary>
Motivation: 比较两个标记马尔可夫链是评估抽象精度或量化模型扰动影响的关键挑战，需要研究CK距离的理论基础及其与现有距离的关系。

Method: 将CK距离框架化为有限时域总变差距离的折现和，分析其计算复杂性、连续性性质和近似性，并提供可计算近似方案。

Result: 证明CK距离的精确计算是#P难的，建立了CK距离与近似关系的上界，并证明有界CK距离意味着有限时域轨迹概率的有界误差。

Conclusion: 为CK距离提供了严格的理论基础，阐明了其与现有距离的关系，并确认其近似计算同样是#P难的。

Abstract: Labeled Markov Chains (or LMCs for short) are useful mathematical objects to model complex probabilistic languages. A central challenge is to compare two LMCs, for example to assess the accuracy of an abstraction or to quantify the effect of model perturbations. In this work, we study the recently introduced Cantor-Kantorovich (or CK) distance. In particular we show that the latter can be framed as a discounted sum of finite-horizon Total Variation distances, making it an instance of discounted linear distance, but arising from the natural Cantor topology. Building on the latter observation, we analyze the properties of the CK distance along three dimensions: computational complexity, continuity properties and approximation. More precisely, we show that the exact computation of the CK distance is #P-hard. We also provide an upper bound on the CK distance as a function of the approximation relation between the two LMCs, and show that a bounded CK distance implies a bounded error between probabilities of finite-horizon traces. Finally, we provide a computable approximation scheme, and show that the latter is also #P-hard. Altogether, our results provide a rigorous theoretical foundation for the CK distance and clarify its relationship with existing distances.

</details>


### [36] [Formalizing Computational Paths and Fundamental Groups in Lean](https://arxiv.org/abs/2511.19142)
*Arthur F. Ramos,Anjolina G. de Oliveira,Ruy J. G. B. de Queiroz,Tiago M. L. de Veras*

Main category: cs.LO

TL;DR: 在Lean 4中形式化计算路径理论，构建可重用的库，并应用于代数拓扑中的基本群计算


<details>
  <summary>Details</summary>
Motivation: 将命题等式视为显式路径，源自de Queiroz的工作，为等式提供弱群胚结构，并支持同伦理论的计算

Method: 在Lean 4中形式化计算路径理论，包括路径形成、组合、逆和重写系统，构建ComputationalPathsLean库，并应用于圆环和环面的基本群计算

Result: 成功证明圆的基群同构于整数，环面的基群同构于两个整数副本的乘积，展示了计算路径方法在现代证明助手中的应用能力

Conclusion: 计算路径方法能够扩展到非平凡的同伦计算，为同伦类型理论在证明助手中的应用提供了实用工具

Abstract: Computational paths treat propositional equality as explicit paths built from labelled deduction steps and rewrite rules. This view originates in work by de Queiroz and collaborators and yields a weak groupoid structure for equality, together with a computational account of homotopy inspired by homotopy type theory. In this paper we present a complete mechanization of this framework in Lean 4 and show how it supports concrete homotopy theoretic computations. Our contributions are threefold. First, we formalize the theory of computational paths in Lean, including path formation, composition, inverses, and a rewrite system that identifies redundant or trivial paths. We prove that equality types with computational paths carry a weak groupoid structure in the sense of the original theory. Second, we organize this material into a reusable Lean library, ComputationalPathsLean, which exposes an interface for paths, rewrites, and loop spaces. This library allows later developments to treat computational paths as a drop-in replacement for propositional equality when reasoning about homotopical structure. Third, we apply the library to two canonical examples in algebraic topology. We give Lean proofs that the fundamental group of the circle is isomorphic to the integers and that the fundamental group of the torus is isomorphic to the product of two copies of the integers, both via computational paths. These case studies demonstrate that the computational paths approach scales to nontrivial homotopical computations in a modern proof assistant. All the definitions and proofs described here are available in an open-source Lean 4 repository.

</details>


### [37] [A General (Uniform) Relational Semantics for Sentential Logics](https://arxiv.org/abs/2511.18458)
*Chrysafis Hartonas*

Main category: cs.LO

TL;DR: 提出了一个通用的关系语义框架，通过改变关系结构的公理化和组成部分，为经典和非经典句子逻辑提供统一语义。


<details>
  <summary>Details</summary>
Motivation: 为经典和非经典句子逻辑建立统一的关系语义框架，扩展Jónsson-Tarski表示理论到更一般的代数结构。

Method: 将Jónsson-Tarski对布尔代数与算子的表示和二元性推广到偏序集、半格或有界格（带或不带分配律）与拟算子的情况，使用无选择构造构建典范扩展。

Result: 开发了适用于各种逻辑系统的统一关系语义框架，并建立了逻辑扩展与模态对应之间的对应关系。

Conclusion: 该框架为广泛的逻辑系统提供了统一的关系语义基础，并支持通过广义Sahlqvist-van Benthem算法进行对应计算。

Abstract: We present a general relational semantics framework which, by varying the axiomatization and components of the relational structures, provides a uniform semantics for sentential logics, classical and non-classical alike. The approach we take rests on a generalization of the Jónsson-Tarski representation (and duality) for Boolean algebras with operators to the cases of posets, semilattices, or bounded lattices (with, or without distribution) with quasi-operators. Completeness proofs rely on a choice-free construction of canonical extensions for the algebras in the quasivarieties of the equivalent algebraic semantics of the logics. Correspondence results for axiomatic extensions of the logics of implication that we study rely on a fully abstract translation into their modal companions and they are calculated using a generalized Sahlqvist - van Benthem algorithm.

</details>


### [38] [A SAT-based Approach for Specification, Analysis, and Justification of Reductions between NP-complete Problems](https://arxiv.org/abs/2511.18639)
*Predrag Janičić*

Main category: cs.LO

TL;DR: 提出了一种基于URSA系统的NP完全问题归约开发、分析和验证新方法


<details>
  <summary>Details</summary>
Motivation: 现有系统在NP完全问题归约方面存在局限性，需要更有效的开发、分析和验证方法

Method: 使用URSA系统（基于SAT的约束求解器），并整合了区别于现有系统的特性

Result: 开发了一种新颖的归约方法，具有区别于现有系统的独特功能

Conclusion: 该方法为NP完全问题归约提供了新的工具和途径，具有实际应用价值

Abstract: We propose a novel approach for the development, analysis, and verification of reductions between NP-complete problems. This method uses the URSA system, a SAT-based constraint solver and incorporates features that distinguish it from existing related systems.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [39] [TensorRight: Automated Verification of Tensor Graph Rewrites](https://arxiv.org/abs/2511.17838)
*Jai Arora,Sirui Lu,Devansh Jain,Tianfan Xu,Farzin Houshmand,Phitchaya Mangpo Phothilimthana,Mohsen Lesani,Praveen Narayanan,Karthik Srinivasa Murthy,Rastislav Bodik,Amit Sabne,Charith Mendis*

Main category: cs.PL

TL;DR: TensorRight是首个能自动验证任意维度和大小张量图重写规则的系统，通过引入聚合轴概念和边界验证方法，解决了现有系统无法处理无界张量验证的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的张量编译器重写规则验证系统只能处理具体维度的张量，无法为任意维度和大小的张量提供语义保持的保证，存在验证空白。

Method: 引入TensorRight DSL核心语言，使用聚合轴定义表示重写规则；通过证明存在张量维度边界，将无界验证转化为有限个有界验证问题；利用符号执行和SMT求解器自动验证。

Result: 在XLA代数简化器的175个规则中，TensorRight能证明115个规则的正确性，而最接近的自动有界验证系统只能表达18个规则。

Conclusion: TensorRight填补了张量图重写规则无界验证的空白，为张量编译器提供了首个完全自动化的任意维度验证解决方案。

Abstract: Tensor compilers, essential for generating efficient code for deep learning models across various applications, employ tensor graph rewrites as one of the key optimizations. These rewrites optimize tensor computational graphs with the expectation of preserving semantics for tensors of arbitrary rank and size. Despite this expectation, to the best of our knowledge, there does not exist a fully automated verification system to prove the soundness of these rewrites for tensors of arbitrary rank and size. Previous works, while successful in verifying rewrites with tensors of concrete rank, do not provide guarantees in the unbounded setting.
  To fill this gap, we introduce TensorRight, the first automatic verification system that can verify tensor graph rewrites for input tensors of arbitrary rank and size. We introduce a core language, TensorRight DSL, to represent rewrite rules using a novel axis definition, called aggregated-axis, which allows us to reason about an unbounded number of axes. We achieve unbounded verification by proving that there exists a bound on tensor ranks, under which bounded verification of all instances implies the correctness of the rewrite rule in the unbounded setting. We derive an algorithm to compute this rank using the denotational semantics of TensorRight DSL. TensorRight employs this algorithm to generate a finite number of bounded-verification proof obligations, which are then dispatched to an SMT solver using symbolic execution to automatically verify the correctness of the rewrite rules. We evaluate TensorRight's verification capabilities by implementing rewrite rules present in XLA's algebraic simplifier. The results demonstrate that TensorRight can prove the correctness of 115 out of 175 rules in their full generality, while the closest automatic, bounded-verification system can express only 18 of these rules.

</details>
