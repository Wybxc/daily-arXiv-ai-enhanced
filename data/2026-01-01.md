<div id=toc></div>

# Table of Contents

- [cs.FL](#cs.FL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 23]
- [cs.PL](#cs.PL) [Total: 4]
- [cs.LO](#cs.LO) [Total: 5]


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [1] [On Good-for-MDPs Automata](https://arxiv.org/abs/2202.07629)
*Sven Schewe,Qiyi Tang,Tansholpan Zhanabekova*

Main category: cs.FL

TL;DR: GFM自动机可判定性得到证明，EXPTIME决策过程，PSPACE-hard，与GFG自动机存在指数级简洁性差距


<details>
  <summary>Details</summary>
Motivation: GFM自动机在MDP模型检查和强化学习中具有重要作用，类似于GFG自动机在反应式合成中的作用。虽然已有技术可以构建GFM自动机，但缺乏判断自动机是否具有GFM性质的决策过程。同时，GFM自动机与其他受限非确定性自动机之间的简洁性关系也不明确。

Method: 提出了GFM性质的决策过程，证明其可判定性，提供EXPTIME决策算法和PSPACE-hard下界证明。通过理论分析比较GFM自动机与GFG自动机、普通非确定性自动机之间的简洁性关系。

Result: 证明GFM性质是可判定的，给出EXPTIME决策过程和PSPACE-hard下界。发现GFM自动机与GFG自动机之间存在指数级简洁性差距，GFM自动机与普通非确定性自动机之间也存在指数级差距，即使后者限制为分离安全性或明确可达性自动机。

Conclusion: GFM自动机的可判定性问题得到解决，填补了理论空白。GFM自动机在简洁性上显著优于GFG自动机和普通非确定性自动机，这为MDP模型检查和强化学习应用提供了更高效的自动机表示方法。

Abstract: Nondeterministic good-for-MDPs (GFM) automata are for MDP model checking and reinforcement learning what good-for-games (GFG) automata are for reactive synthesis: a more compact alternative to deterministic automata that displays nondeterminism, but only so much that it can be resolved locally, such that a syntactic product can be analysed.
  GFM has recently been introduced as a property for reinforcement learning, where the simpler Büchi acceptance conditions it allows to use is key. However, while there are classic and novel techniques to obtain automata that are GFM, there has not been a decision procedure for checking whether or not an automaton is GFM. We show that GFM-ness is decidable and provide an EXPTIME decision procedure as well as a PSPACE-hardness proof.
  We also compare the succinctness of GFM automata with other types of automata with restricted nondeterminism. The first natural comparison point are GFG automata. Deterministic automata are GFG, and GFG automata are GFM, but not vice versa. This raises the question of how these classes relate in terms of succinctness. GFG automata are known to be exponentially more succinct than deterministic automata, but the gap between GFM and GFG automata as well as the gap between ordinary nondeterministic automata and those that are GFM have been open. We establish that these gaps are exponential, and sharpen this result by showing that the latter gap remains exponential when restricting the nondeterministic automata to separating safety or unambiguous reachability automata.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [AgenticTCAD: A LLM-based Multi-Agent Framework for Automated TCAD Code Generation and Device Optimization](https://arxiv.org/abs/2512.23742)
*Guangxi Fan,Tianliang Ma,Xuguang Sun,Xun Wang,Kain Lu Low,Leilai Shao*

Main category: cs.SE

TL;DR: 提出AgenticTCAD框架，通过多智能体系统实现自然语言驱动的TCAD代码生成与器件设计自动化，显著提升2nm纳米片FET设计效率


<details>
  <summary>Details</summary>
Motivation: 随着先进工艺节点持续微缩，设计-技术协同优化变得日益重要，但TCAD仿真领域缺乏开源资源，阻碍语言模型生成有效的TCAD代码

Method: 构建专家策划的开源TCAD数据集，微调领域专用模型用于TCAD代码生成，提出AgenticTCAD多智能体框架，实现自然语言驱动的端到端自动化器件设计与优化

Result: 在2nm纳米片FET设计中，AgenticTCAD在4.2小时内达到IRDS-2024器件规格要求，而人类专家使用商业工具需要7.1天

Conclusion: AgenticTCAD框架显著提升了TCAD器件设计与优化的效率，为解决先进工艺节点下的设计挑战提供了有效的自动化解决方案

Abstract: With the continued scaling of advanced technology nodes, the design-technology co-optimization (DTCO) paradigm has become increasingly critical, rendering efficient device design and optimization essential. In the domain of TCAD simulation, however, the scarcity of open-source resources hinders language models from generating valid TCAD code. To overcome this limitation, we construct an open-source TCAD dataset curated by experts and fine-tune a domain-specific model for TCAD code generation. Building on this foundation, we propose AgenticTCAD, a natural language - driven multi-agent framework that enables end-to-end automated device design and optimization. Validation on a 2 nm nanosheet FET (NS-FET) design shows that AgenticTCAD achieves the International Roadmap for Devices and Systems (IRDS)-2024 device specifications within 4.2 hours, whereas human experts required 7.1 days with commercial tools.

</details>


### [3] [Developing controlled natural language for formal specification patterns using AI assistants](https://arxiv.org/abs/2512.24159)
*Natalia Garanina,Vladimir Zyubin,Igor Anureev*

Main category: cs.SE

TL;DR: 提出一种基于AI助手系统构建需求受控自然语言的方法，利用形式化规约模式中的逻辑属性生成自然语言需求模式


<details>
  <summary>Details</summary>
Motivation: 需要一种系统化的方法来构建受控自然语言，以支持基于形式化规约模式的需求工程，提高需求表达的一致性和可形式化性

Method: 三阶段方法：1) 编译利用形式化模板所有属性的通用自然语言需求模式；2) 使用AI助手生成自然语言需求模式语料库，通过部分评估属性进行简化；3) 基于结果模式的语法结构分析形式化受控自然语言的语法

Result: 该方法已针对事件驱动的时序需求进行了测试验证

Conclusion: 该方法能够有效构建受控自然语言，支持基于形式化规约模式的需求工程，特别适用于事件驱动的时序需求场景

Abstract: Using an AI assistant, we developed a method for systematically constructing controlled natural language for requirements based on formal specification patterns containing logical attributes. The method involves three stages: 1) compiling a generalized natural language requirement pattern that utilizes all attributes of the formal specification template; 2) generating, using the AI assistant, a corpus of natural language requirement patterns, reduced by partially evaluating attributes (the developed prompt utilizes the generalized template, attribute definitions, and specific formal semantics of the requirement patterns); and 3) formalizing the syntax of the controlled natural language based on an analysis of the grammatical structure of the resulting patterns. The method has been tested for event-driven temporal requirements.

</details>


### [4] [Hybrid-Code: A Privacy-Preserving, Redundant Multi-Agent Framework for Reliable Local Clinical Coding](https://arxiv.org/abs/2512.23743)
*Yunguo Yu*

Main category: cs.SE

TL;DR: Hybrid-Code：一个混合神经符号多智能体框架，用于本地临床编码，通过冗余和验证确保生产可靠性，解决云端LLM的隐私和延迟问题。


<details>
  <summary>Details</summary>
Motivation: 云端大型语言模型用于临床编码存在隐私风险和延迟瓶颈，不适合本地医疗部署。需要一种既能保证可靠性又能保护隐私的解决方案。

Method: 提出Hybrid-Code混合神经符号多智能体框架：包含Coder智能体（使用BioMistral-7B进行语义推理，失败时回退到确定性关键词匹配）和Auditor智能体（基于257个代码知识库和临床证据验证代码）。

Result: 在1,000份MIMIC-III出院摘要上评估：知识库内接受输出无幻觉代码，验证率24.47%，覆盖率34.11%（95% CI: 31.2%--37.0%），语言模型利用率86%+。Auditor过滤无效格式代码，提供基于证据的质量控制（拒绝率75.53%），确保患者数据不离开医院防火墙。

Conclusion: 混合架构结合语言模型语义理解（成功时）、确定性回退（模型失败时）和符号验证（始终活跃），确保可靠性和隐私保护。关键发现：在生产医疗系统中，通过冗余实现的可靠性比纯模型性能更有价值。

Abstract: Clinical coding automation using cloud-based Large Language Models (LLMs) poses privacy risks and latency bottlenecks, rendering them unsuitable for on-premise healthcare deployment. We introduce Hybrid-Code, a hybrid neuro-symbolic multi-agent framework for local clinical coding that ensures production reliability through redundancy and verification. Our system comprises two agents: a Coder that attempts language model-based semantic reasoning using BioMistral-7B but falls back to deterministic keyword matching when model output is unreliable, ensuring pipeline completion; and an Auditor that verifies codes against a 257-code knowledge base and clinical evidence. Evaluating on 1,000 MIMIC-III discharge summaries, we demonstrate no hallucinated codes among accepted outputs within the knowledge base, 24.47% verification rate, and 34.11% coverage (95% CI: 31.2%--37.0%) with 86%+ language model utilization. The Auditor filtered invalid format codes and provided evidence-based quality control (75.53% rejection rate) while ensuring no patient data leaves the hospital firewall. The hybrid architecture -- combining language model semantic understanding (when successful), deterministic fallback (when the model fails), and symbolic verification (always active) -- ensures both reliability and privacy preservation, addressing critical barriers to AI adoption in healthcare. Our key finding is that reliability through redundancy is more valuable than pure model performance in production healthcare systems, where system failures are unacceptable.

</details>


### [5] [DEFT: Differentiable Automatic Test Pattern Generation](https://arxiv.org/abs/2512.23746)
*Wei Li,Yan Zou,Yixin Liang,José Moura,Shawn Blanton*

Main category: cs.SE

TL;DR: DEFT是一种基于可微优化的新型ATPG方法，将离散测试生成问题转化为连续优化任务，显著提升难检测故障的覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现代集成电路复杂度导致测试模式数量激增，大部分模式针对少数难检测故障，需要新的ATPG算法来提高对这类故障的测试效率。

Method: 将离散ATPG问题重新表述为连续优化任务，引入数学基础的重参数化方法，使连续目标与离散故障检测语义对齐；集成定制CUDA内核实现高效前向-反向传播，应用梯度归一化缓解梯度消失问题。

Result: 在相同模式预算和可比运行时间下，DEFT在两种工业基准测试中分别将难检测故障检测率平均提高21.1%和48.9%；支持部分赋值模式生成，产生比特数减少19.3%的模式同时检测到35%更多故障。

Conclusion: DEFT是一种有前景且有效的ATPG引擎，为现有启发式方法提供了有价值的补充，在难检测故障覆盖方面表现出色。

Abstract: Modern IC complexity drives test pattern growth, with the majority of patterns targeting a small set of hard-to-detect (HTD) faults. This motivates new ATPG algorithms to improve test effectiveness specifically for HTD faults. This paper presents DEFT (Differentiable Automatic Test Pattern Generation), a new ATPG approach that reformulates the discrete ATPG problem as a continuous optimization task. DEFT introduces a mathematically grounded reparameterization that aligns the expected continuous objective with discrete fault-detection semantics, enabling reliable gradient-based pattern generation. To ensure scalability and stability on deep circuit graphs, DEFT integrates a custom CUDA kernel for efficient forward-backward propagation and applies gradient normalization to mitigate vanishing gradients. Compared to a leading commercial tool on two industrial benchmarks, DEFT improves HTD fault detection by 21.1% and 48.9% on average under the same pattern budget and comparable runtime. DEFT also supports practical ATPG settings such as partial assignment pattern generation, producing patterns with 19.3% fewer 0/1 bits while still detecting 35% more faults. These results indicate DEFT is a promising and effective ATPG engine, offering a valuable complement to existing heuristic.

</details>


### [6] [State-of-the-art Small Language Coder Model: Mify-Coder](https://arxiv.org/abs/2512.23747)
*Abhinav Parmar,Abhisek Panigrahi,Abhishek Kumar Dwivedi,Abhishek Bhattacharya,Adarsh Ramachandra,Aditya Choudhary,Aditya Garg,Aditya Raj,Alankrit Bhatt,Alpesh Yadav,Anant Vishnu,Ananthu Pillai,Ankush Kumar,Aryan Patnaik,Aswatha Narayanan S,Avanish Raj Singh,Bhavya Shree Gadda,Brijesh Pankajbhai Kachhadiya,Buggala Jahnavi,Chidurala Nithin Krishna,Chintan Shah,Chunduru Akshaya,Debarshi Banerjee,Debrup Dey,Deepa R.,Deepika B G,Faiz ur Rahman,Gagan Gayari,Gudhi Jagadeesh Kumar Naidu,Gursimar Singh,Harshal Tyagi,Harshini K,James Mani Vathalloor,Jayarama Nettar,Jayashree Gajjam,Joe Walter Sugil George,Kamalakara Sri Krishna Tadepalli,Kamalkumar Rathinasamy,Karan Chaurasia,Karthikeyan S,Kashish Arora,Kaushal Desai,Khushboo Buwade,Kiran Manjrekar,Malikireddy Venkata Sai Likhitha,Manjunath A,Mitali Mahavir Bedmutha,Mohammed Rafee Tarafdar,Nikhil Tiwari,Nikitha K Gigi,Pavan Ravikumar,Pendyala Swarnanjali,Piyush Anand,Prakash Chandrasekar,Prasanna Bhalchandra Gawade,Prasanth Sivan,Preeti Khurana,Priyanshi Babbar,Rajab Ali Mondal,Rajesh Kumar Vissapragada,Rajeshwari Ganesan,Rajeswari Koppisetti,Ramjee R.,Ramkumar Thiruppathisamy,Rani G. S.,S Reka,Samarth Gupta,Sandeep Reddy Kothakota,Sarathy K,Sathyanarayana Sampath Kumar,Saurabh Kumar,Shashank Khasare,Shenbaga Devi Venkatesh Kumar,Shiva Rama Krishna Parvatham,Shoeb Shaikh,Shrishanmathi A,Shubham Pathak,Sree Samhita Koppaka,Sreenivasa Raghavan K S,Sreeram Venkatasubramanian,Suprabha Desai Bojja,Swetha R,Syed Ahmed,Chinmai Harshitha Thota,Tushar Yadav,Veeravelly Kusumitha,V V S S Prasanth Patnaik,Vidya Sri Sesetti,Vijayakeerthi K,Vikram Raj Bakshi,Vinay K K,Vinoth Kumar Loganathan,Vipin Tiwari,Vivek Kumar Shrivastav,V Venkata Sri Datta Charan,Wasim Akhtar Khan*

Main category: cs.SE

TL;DR: Mify-Coder是一个2.5B参数的代码模型，通过计算最优策略在4.2T tokens上训练，在代码生成和函数调用任务上超越更大基线模型，证明紧凑模型能达到前沿性能。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过计算最优策略和高质量数据，让紧凑模型在代码生成和代理驱动工作流中达到前沿模型的性能水平，同时实现高效部署。

Method: 基于Mify-2.5B基础模型，采用计算最优策略，结合高质量人工数据和通过代理设计提示生成的合成数据，使用企业级评估数据集迭代优化，通过LLM质量过滤提高数据密度，探索CPT-SFT目标、数据混合和采样动态。

Result: Mify-Coder在标准编码和函数调用基准测试中显著优于更大的基线模型，达到可比的准确性和安全性，量化变体可在标准桌面环境部署而无需专用硬件。

Conclusion: 通过原则性的数据和计算纪律，小型模型能够实现具有竞争力的准确性、效率和安全性合规，为代码智能提供高效部署解决方案。

Abstract: We present Mify-Coder, a 2.5B-parameter code model trained on 4.2T tokens using a compute-optimal strategy built on the Mify-2.5B foundation model. Mify-Coder achieves comparable accuracy and safety while significantly outperforming much larger baseline models on standard coding and function-calling benchmarks, demonstrating that compact models can match frontier-grade models in code generation and agent-driven workflows. Our training pipeline combines high-quality curated sources with synthetic data generated through agentically designed prompts, refined iteratively using enterprise-grade evaluation datasets. LLM-based quality filtering further enhances data density, enabling frugal yet effective training. Through disciplined exploration of CPT-SFT objectives, data mixtures, and sampling dynamics, we deliver frontier-grade code intelligence within a single continuous training trajectory. Empirical evidence shows that principled data and compute discipline allow smaller models to achieve competitive accuracy, efficiency, and safety compliance. Quantized variants of Mify-Coder enable deployment on standard desktop environments without requiring specialized hardware.

</details>


### [7] [Uncovering Discrimination Clusters: Quantifying and Explaining Systematic Fairness Violations](https://arxiv.org/abs/2512.23769)
*Ranit Debnath Akash,Ashish Kumar,Verya Monjezi,Ashutosh Trivedi,Gang,Tan,Saeid Tizpaz-Niari*

Main category: cs.SE

TL;DR: 提出歧视聚类概念，超越个体公平性检查，检测系统性偏见模式，并开发HyFair混合方法结合形式验证与随机搜索来发现和解释歧视聚类。


<details>
  <summary>Details</summary>
Motivation: 个体公平性只关注单对个体的相似性，无法捕捉系统性或聚类歧视模式。现有方法可能遗漏影响整个子群体的偏见模式，需要更全面的不公平性检测方法。

Method: 提出歧视聚类概念，检测输入空间中受保护特征微小扰动导致k个显著不同结果聚类的区域。开发HyFair混合技术：结合形式符号分析（SMT和MILP求解器）验证个体公平性，使用随机搜索发现歧视聚类。对高k不公平性输入，提出基于决策树的解释方法。

Result: HyFair在公平性验证和局部解释方法上优于现有最先进方法，能够发现形式方法难以检测的严重违规，并提供可解释的决策树风格解释。

Conclusion: 歧视聚类概念扩展了个体公平性框架，HyFair方法结合形式保证与实用检测能力，能够揭示算法偏见模式，为公平性评估提供更全面的工具。

Abstract: Fairness in algorithmic decision-making is often framed in terms of individual fairness, which requires that similar individuals receive similar outcomes. A system violates individual fairness if there exists a pair of inputs differing only in protected attributes (such as race or gender) that lead to significantly different outcomes-for example, one favorable and the other unfavorable. While this notion highlights isolated instances of unfairness, it fails to capture broader patterns of systematic or clustered discrimination that may affect entire subgroups. We introduce and motivate the concept of discrimination clustering, a generalization of individual fairness violations. Rather than detecting single counterfactual disparities, we seek to uncover regions of the input space where small perturbations in protected features lead to k-significantly distinct clusters of outcomes. That is, for a given input, we identify a local neighborhood-differing only in protected attributes-whose members' outputs separate into many distinct clusters. These clusters reveal significant arbitrariness in treatment solely based on protected attributes that help expose patterns of algorithmic bias that elude pairwise fairness checks. We present HyFair, a hybrid technique that combines formal symbolic analysis (via SMT and MILP solvers) to certify individual fairness with randomized search to discover discriminatory clusters. This combination enables both formal guarantees-when no counterexamples exist-and the detection of severe violations that are computationally challenging for symbolic methods alone. Given a set of inputs exhibiting high k-unfairness, we introduce a novel explanation method to generate interpretable, decision-tree-style artifacts. Our experiments demonstrate that HyFair outperforms state-of-the-art fairness verification and local explanation methods.

</details>


### [8] [Test Case Specification Techniques and System Testing Tools in the Automotive Industry: A Review](https://arxiv.org/abs/2512.23780)
*Denesa Zyberaj,Pascal Hirmer,Marco Aiello,Stefan Wagner*

Main category: cs.SE

TL;DR: 论文提出汽车软件测试挑战分析框架，通过系统文献综述和行业实践，构建技术/工具选择目录，推荐模型化规划、可互操作工具链等解决方案。


<details>
  <summary>Details</summary>
Motivation: 汽车领域向软件中心开发转型，导致嵌入式系统复杂度增加，测试能力紧张。现有标准缺乏跨异构、遗留约束工具链的连贯系统测试方法，实践依赖个人经验而非系统策略。

Method: 通过系统文献综述(SLR)结合行业经验，识别挑战和需求，映射到测试用例规范技术和测试工具，使用PRISMA评估其在汽车测试中的适用性。

Result: 识别出9个跨生命周期的重复挑战领域，如需求质量与可追溯性、可变性管理、工具链碎片化。构建优先标准目录，推荐模型化规划、可互操作工具链等解决方案。

Conclusion: 贡献是支持技术/工具选择的精选目录，可为未来测试框架和改进提供信息。推荐模型化规划、可互操作可追溯工具链、需求提升、实用自动化和虚拟化等策略。

Abstract: The automotive domain is shifting to software-centric development to meet regulation, market pressure, and feature velocity. This shift increases embedded systems' complexity and strains testing capacity. Despite relevant standards, a coherent system-testing methodology that spans heterogeneous, legacy-constrained toolchains remains elusive, and practice often depends on individual expertise rather than a systematic strategy. We derive challenges and requirements from a systematic literature review (SLR), complemented by industry experience and practice. We map them to test case specification techniques and testing tools, evaluating their suitability for automotive testing using PRISMA. Our contribution is a curated catalog that supports technique/tool selection and can inform future testing frameworks and improvements. We synthesize nine recurring challenge areas across the life cycle, such as requirements quality and traceability, variability management, and toolchain fragmentation. We then provide a prioritized criteria catalog that recommends model-based planning, interoperable and traceable toolchains, requirements uplift, pragmatic automation and virtualization, targeted AI and formal methods, actionable metrics, and lightweight organizational practices.

</details>


### [9] [A Systematic Mapping on Software Fairness: Focus, Trends and Industrial Context](https://arxiv.org/abs/2512.23782)
*Kessia Nepomuceno,Fabio Petrillo*

Main category: cs.SE

TL;DR: 对软件工程中公平性研究的系统文献综述，分析了95篇论文的研究趋势、焦点和工业可行性，发现该领域主要关注后处理方法和群体公平性，缺乏早期干预和工业应用。


<details>
  <summary>Details</summary>
Motivation: 随着软件工程领域的发展，系统公平性已成为关键问题。尽管已有一些公平性指南，但对确保软件系统公平性的研究解决方案仍缺乏全面理解。本文旨在通过系统文献综述探索和分类软件工程中公平性解决方案的最新进展。

Method: 开发了一个从新视角组织软件公平性研究的分类框架，应用于95篇选定研究，分析其工业采用潜力。重点关注三个维度：研究趋势、研究焦点和工业环境可行性。

Result: 研究发现：软件公平性研究正在扩展，但主要集中于方法和算法；主要关注后处理和群体公平性，较少关注早期干预、个体公平性指标和偏见根源理解；研究主要在学术界，工业合作有限，技术成熟度低至中等，工业可转移性仍较远。

Conclusion: 需要在软件开发生命周期的所有阶段纳入公平性考虑，并促进学术界与工业界更深入合作。该分析为未来研究和软件系统公平性的实际应用提供了基础。

Abstract: Context: Fairness in systems has emerged as a critical concern in software engineering, garnering increasing attention as the field has advanced in recent years. While several guidelines have been proposed to address fairness, achieving a comprehensive understanding of research solutions for ensuring fairness in software systems remains challenging. Objectives: This paper presents a systematic literature mapping to explore and categorize current advancements in fairness solutions within software engineering, focusing on three key dimensions: research trends, research focus, and viability in industrial contexts. Methods: We develop a classification framework to organize research on software fairness from a fresh perspective, applying it to 95 selected studies and analyzing their potential for industrial adoption. Results: Our findings reveal that software fairness research is expanding, yet it remains heavily focused on methods and algorithms. It primarily focuses on post-processing and group fairness, with less emphasis on early-stage interventions, individual fairness metrics, and understanding bias root causes. Additionally fairness research remains largely academic, with limited industry collaboration and low to medium Technology Readiness Level (TRL), indicating that industrial transferability remains distant. Conclusion: Our results underscore the need to incorporate fairness considerations across all stages of the software development life-cycle and to foster greater collaboration between academia and industry. This analysis provides a comprehensive overview of the field, offering a foundation to guide future research and practical applications of fairness in software systems.

</details>


### [10] [From Correctness to Collaboration: Toward a Human-Centered Framework for Evaluating AI Agent Behavior in Software Engineering](https://arxiv.org/abs/2512.23844)
*Tao Dong,Harini Sampath,Ja Young Lee,Sherry Y. Shi,Andrew Macvean*

Main category: cs.SE

TL;DR: 该论文提出了一种评估LLM作为软件工程协作伙伴的新框架，包括行为分类法和上下文自适应行为框架，以超越传统代码正确性评估，关注人机协作动态。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估方法滞后，现有基准主要关注代码正确性，无法捕捉人机协作所需的细微交互行为。需要建立更全面的评估框架来反映AI作为软件工程协作伙伴的实际需求。

Method: 1. 分析91组用户定义的代理规则，构建企业软件工程中理想代理行为的基础分类法；2. 引入上下文自适应行为框架，通过两个经验推导的维度（时间视野和工作类型）展示行为期望如何动态变化。

Result: 提出了包含四个关键期望的行为分类法：遵守标准和流程、确保代码质量和可靠性、有效解决问题、与用户协作。同时建立了上下文自适应行为框架，揭示了行为期望如何随时间视野（从即时需求到未来理想）和工作类型（从企业生产到快速原型）而变化。

Conclusion: 该研究为人机协作的下一代AI代理设计和评估提供了以人为中心的基础，将领域焦点从生成代码的正确性转向真正的协作智能动态，填补了当前评估方法的空白。

Abstract: As Large Language Models (LLMs) evolve from code generators into collaborative partners for software engineers, our methods for evaluation are lagging. Current benchmarks, focused on code correctness, fail to capture the nuanced, interactive behaviors essential for successful human-AI partnership. To bridge this evaluation gap, this paper makes two core contributions. First, we present a foundational taxonomy of desirable agent behaviors for enterprise software engineering, derived from an analysis of 91 sets of user-defined agent rules. This taxonomy defines four key expectations of agent behavior: Adhere to Standards and Processes, Ensure Code Quality and Reliability, Solving Problems Effectively, and Collaborating with the User.
  Second, recognizing that these expectations are not static, we introduce the Context-Adaptive Behavior (CAB) Framework. This emerging framework reveals how behavioral expectations shift along two empirically-derived axes: the Time Horizon (from immediate needs to future ideals), established through interviews with 15 expert engineers, and the Type of Work (from enterprise production to rapid prototyping, for example), identified through a prompt analysis of a prototyping agent. Together, these contributions offer a human-centered foundation for designing and evaluating the next generation of AI agents, moving the field's focus from the correctness of generated code toward the dynamics of true collaborative intelligence.

</details>


### [11] [A Tale of 1001 LoC: Potential Runtime Error-Guided Specification Synthesis for Verifying Large-Scale Programs](https://arxiv.org/abs/2512.24594)
*Zhongyi Wang,Tengjie Lin,Mingshuai Chen,Haokun Li,Mingqi Yang,Xiao Yi,Shengchao Qin,Yixing Luo,Xiaofeng Li,Bin Gu,Liqiang Lu,Jianwei Yin*

Main category: cs.SE

TL;DR: Preguss是一个模块化、细粒度的框架，通过结合静态分析和演绎验证，自动化生成和精化形式化规约，显著减少了人工验证工作量。


<details>
  <summary>Details</summary>
Motivation: 大规模软硬件系统的全自动化验证是形式化方法的终极目标。虽然大语言模型在形式化验证自动化方面展现出潜力，但在长上下文推理和复杂过程间规约推断方面存在可扩展性问题。

Method: Preguss采用分治策略，结合静态分析和演绎验证：1) 基于潜在运行时错误构建和优先化验证单元；2) 在单元级别使用LLM辅助合成过程间规约。

Result: Preguss显著优于最先进的基于LLM的方法，能够对超过1000行代码的真实世界程序实现高度自动化的RTE无错误验证，减少80.6%~88.9%的人工验证工作量。

Conclusion: Preguss通过模块化、细粒度的框架有效解决了LLM在形式化验证中的可扩展性问题，为实现大规模系统自动化验证提供了有前景的解决方案。

Abstract: Fully automated verification of large-scale software and hardware systems is arguably the holy grail of formal methods. Large language models (LLMs) have recently demonstrated their potential for enhancing the degree of automation in formal verification by, e.g., generating formal specifications as essential to deductive verification, yet exhibit poor scalability due to long-context reasoning limitations and, more importantly, the difficulty of inferring complex, interprocedural specifications. This paper presents Preguss -- a modular, fine-grained framework for automating the generation and refinement of formal specifications. Preguss synergizes between static analysis and deductive verification by steering two components in a divide-and-conquer fashion: (i) potential runtime error-guided construction and prioritization of verification units, and (ii) LLM-aided synthesis of interprocedural specifications at the unit level. We show that Preguss substantially outperforms state-of-the-art LLM-based approaches and, in particular, it enables highly automated RTE-freeness verification for real-world programs with over a thousand LoC, with a reduction of 80.6%~88.9% human verification effort.

</details>


### [12] [From Illusion to Insight: Change-Aware File-Level Software Defect Prediction Using Agentic AI](https://arxiv.org/abs/2512.23875)
*Mohsen Hesamolhokama,Behnam Rohani,Amirahmad Shafiee,MohammadAmin Fazli,Jafar Habibi*

Main category: cs.SE

TL;DR: 传统软件缺陷预测存在准确率幻觉问题，本文提出基于代码变更的预测方法和多智能体辩论框架，显著提升对缺陷引入的敏感性。


<details>
  <summary>Details</summary>
Motivation: 传统文件级软件缺陷预测存在准确率幻觉问题，因为大多数文件在版本间持续存在且缺陷标签保持不变，标准评估方法奖励的是标签持久性偏差而非对代码变更的推理能力。

Method: 将SDP重新定义为变更感知预测任务，模型基于连续项目版本中文件的代码变更进行推理，而非依赖静态文件快照。提出基于LLM的变更感知多智能体辩论框架。

Result: 实验表明传统模型在F1指标上表现虚高，但在关键的缺陷转换案例上失败；而变更感知推理和多智能体辩论框架在演化子集上表现更均衡，显著提升对缺陷引入的敏感性。

Conclusion: 当前SDP评估实践存在根本性缺陷，实际缺陷预测需要变更感知推理。提出的方法能更准确地识别代码变更引入的缺陷。

Abstract: Much of the reported progress in file-level software defect prediction (SDP) is, in reality, nothing but an illusion of accuracy. Over the last decades, machine learning and deep learning models have reported increasing performance across software versions. However, since most files persist across releases and retain their defect labels, standard evaluation rewards label-persistence bias rather than reasoning about code changes. To address this issue, we reformulate SDP as a change-aware prediction task, in which models reason over code changes of a file within successive project versions, rather than relying on static file snapshots. Building on this formulation, we propose an LLM-driven, change-aware, multi-agent debate framework. Our experiments on multiple PROMISE projects show that traditional models achieve inflated F1, while failing on rare but critical defect-transition cases. In contrast, our change-aware reasoning and multi-agent debate framework yields more balanced performance across evolution subsets and significantly improves sensitivity to defect introductions. These results highlight fundamental flaws in current SDP evaluation practices and emphasize the need for change-aware reasoning in practical defect prediction. The source code is publicly available.

</details>


### [13] [Coding With AI: From a Reflection on Industrial Practices to Future Computer Science and Software Engineering Education](https://arxiv.org/abs/2512.23982)
*Hung-Fu Chang,MohammadShokrolah Shirazi,Lizhou Cao,Supannika Koolmanojwong Mobasser*

Main category: cs.SE

TL;DR: 该研究通过分析57个YouTube视频，探讨了LLM编程工具在专业实践中的使用、相关风险以及对开发工作流程的影响，特别关注对计算机教育的启示。


<details>
  <summary>Details</summary>
Motivation: 先前研究主要关注个体层面或教育环境中的AI编程，缺乏对工业从业者视角的深入探索。本研究旨在填补这一空白，了解LLM编码工具在专业实践中的实际应用、相关风险以及工作流程的转变。

Method: 对2024年末至2025年间发布的57个精选YouTube视频进行定性分析，这些视频记录了从业者的反思和经验。经过筛选和质量评估后，分析比较了基于LLM的编程与传统编程，识别了新兴风险，并描述了不断演变的工作流程。

Result: 研究揭示了AI编程实践的定义、显著的生产力提升和降低的入门门槛。从业者报告开发瓶颈转向代码审查，并关注代码质量、可维护性、安全漏洞、伦理问题、基础问题解决技能的侵蚀以及初级工程师准备不足等问题。

Conclusion: 基于这些发现，讨论了计算机科学和软件工程教育的启示，主张课程转向问题解决、架构思维、代码审查以及早期整合LLM工具的项目式学习。该研究提供了基于行业的AI编程视角，并为教育实践与快速演变的专业现实对齐提供了指导。

Abstract: Recent advances in large language models (LLMs) have introduced new paradigms in software development, including vibe coding, AI-assisted coding, and agentic coding, fundamentally reshaping how software is designed, implemented, and maintained. Prior research has primarily examined AI-based coding at the individual level or in educational settings, leaving industrial practitioners' perspectives underexplored. This paper addresses this gap by investigating how LLM coding tools are used in professional practice, the associated concerns and risks, and the resulting transformations in development workflows, with particular attention to implications for computing education. We conducted a qualitative analysis of 57 curated YouTube videos published between late 2024 and 2025, capturing reflections and experiences shared by practitioners. Following a filtering and quality assessment process, the selected sources were analyzed to compare LLM-based and traditional programming, identify emerging risks, and characterize evolving workflows. Our findings reveal definitions of AI-based coding practices, notable productivity gains, and lowered barriers to entry. Practitioners also report a shift in development bottlenecks toward code review and concerns regarding code quality, maintainability, security vulnerabilities, ethical issues, erosion of foundational problem-solving skills, and insufficient preparation of entry-level engineers. Building on these insights, we discuss implications for computer science and software engineering education and argue for curricular shifts toward problem-solving, architectural thinking, code review, and early project-based learning that integrates LLM tools. This study offers an industry-grounded perspective on AI-based coding and provides guidance for aligning educational practices with rapidly evolving professional realities.

</details>


### [14] [CoHalLo: code hallucination localization via probing hidden layer vector](https://arxiv.org/abs/2512.24183)
*Nan Jia,Wangchao Sang,Pengfei Lin,Xiangping Chen,Yuan Huang,Yi Liu,Mingliang Li*

Main category: cs.SE

TL;DR: CoHalLo：一种通过探测幻觉检测模型的隐藏层向量来实现行级代码幻觉定位的新方法，通过比较预测的抽象语法树与原始语法树来定位幻觉代码行。


<details>
  <summary>Details</summary>
Motivation: 现有代码幻觉检测方法大多停留在粗粒度检测，缺乏专门的细粒度幻觉定位技术，难以帮助开发者高效定位具体包含幻觉的代码行。

Method: 首先在人工标注数据集上微调幻觉检测模型，使其学习代码语法特征；然后设计探针网络将高维潜在向量投影到低维语法子空间，生成向量元组并重建预测的抽象语法树；通过比较预测语法树与原始语法树来识别与幻觉相关的关键语法结构，从而定位幻觉代码行。

Result: CoHalLo在Top-1准确率达到0.4253，Top-3准确率0.6149，Top-5准确率0.7356，Top-10准确率0.8333，IFA为5.73，Recall@1% Effort为0.052721，Effort@20% Recall为0.155269，均优于基线方法。

Conclusion: CoHalLo通过探测幻觉检测模型的隐藏层向量，成功实现了行级代码幻觉定位，为开发者提供了更精细的代码可靠性改进工具。

Abstract: The localization of code hallucinations aims to identify specific lines of code containing hallucinations, helping developers to improve the reliability of AI-generated code more efficiently. Although recent studies have adopted several methods to detect code hallucination, most of these approaches remain limited to coarse-grained detection and lack specialized techniques for fine-grained hallucination localization. This study introduces a novel method, called CoHalLo, which achieves line-level code hallucination localization by probing the hidden-layer vectors from hallucination detection models. CoHalLo uncovers the key syntactic information driving the model's hallucination judgments and locates the hallucinating code lines accordingly. Specifically, we first fine-tune the hallucination detection model on manually annotated datasets to ensure that it learns features pertinent to code syntactic information. Subsequently, we designed a probe network that projects high-dimensional latent vectors onto a low-dimensional syntactic subspace, generating vector tuples and reconstructing the predicted abstract syntax tree (P-AST). By comparing P-AST with the original abstract syntax tree (O-AST) extracted from the input AI-generated code, we identify the key syntactic structures associated with hallucinations. This information is then used to pinpoint hallucinated code lines. To evaluate CoHalLo's performance, we manually collected a dataset of code hallucinations. The experimental results show that CoHalLo achieves a Top-1 accuracy of 0.4253, Top-3 accuracy of 0.6149, Top-5 accuracy of 0.7356, Top-10 accuracy of 0.8333, IFA of 5.73, Recall@1% Effort of 0.052721, and Effort@20% Recall of 0.155269, which outperforms the baseline methods.

</details>


### [15] ["Game Changer" or "Overenthusiastic Drunk Acquaintance"? Generative AI Use by Blind and Low Vision Software Professionals in the Workplace](https://arxiv.org/abs/2512.24462)
*Yoonha Cha,Victoria Jackson,Lauren Shu,Stacy Branham,André van der Hoek*

Main category: cs.SE

TL;DR: 研究通过39次访谈发现，视障软件开发者使用生成式AI能提高生产力和可访问性，但也面临幻觉风险和组织政策限制，需要权衡高风险与高回报。


<details>
  <summary>Details</summary>
Motivation: 软件行业广泛采用生成式AI，但视障软件专业人员的独特视角尚未被研究。他们面临技术和协作可访问性挑战，需要了解AI对他们工作的影响。

Method: 采用定性研究方法，对39名视障软件专业人员进行半结构化访谈，了解生成式AI引入对他们工作的意义。

Result: 视障开发者将AI用于多种开发任务，获得生产力提升和可访问性改善，但比明眼同事更易受AI幻觉影响，且有时受组织政策限制。

Conclusion: 视障软件专业人员在决定是否及何时使用AI工具时，需要仔细权衡高风险与高回报，面临独特的权衡决策。

Abstract: The software development workplace poses numerous technical and collaborative accessibility challenges for blind and low vision software professionals (BLVSPs). Though Generative AI (GenAI) is increasingly adopted within the software development industry and has been a rapidly growing topic of interest in research, to date, the unique perspectives of BLVSPs have yet to be consulted. We report on a qualitative study involving 39 semi-structured interviews with BLVSPs about what the introduction of GenAI has meant for their work. We found that BLVSPs used GenAI for many software development tasks, resulting in benefits such as increased productivity and accessibility. However, significant costs were also accompanied by GenAI use as they were more vulnerable to hallucinations than their sighted colleagues. Sometimes, organizational policies prevented use. Based on our findings, we discuss the higher-risks and higher-returns that BLVSPs had to carefully weigh when deciding whether and when to use GenAI tools for work.

</details>


### [16] [A Magnified View into Heterogeneous-ISA Thread Migration Performance without State Transformation](https://arxiv.org/abs/2512.24530)
*Nikolaos Mavrogeorgis,Christos Vasiladiotis,Pei Mu,Amir Khordadi,Björn Franke,Antonio Barbalace*

Main category: cs.SE

TL;DR: Unifico是一个多ISA编译器，通过保持跨架构的相同栈布局来消除异构ISA处理器中的运行时栈转换开销，显著减少二进制大小并降低性能影响。


<details>
  <summary>Details</summary>
Motivation: 异构ISA处理器设计需要软件支持来桥接ISA异构性，但缺乏支持异构ISA目标的编译工具链阻碍了该领域研究。特别是运行时程序栈在不同架构间的转换成本过高。

Method: 设计开发Unifico编译器，使用LLVM基础设施实现，生成在两种架构上执行时保持相同栈布局的二进制文件，避免运行时栈转换，维护统一的ABI和虚拟地址空间，目前针对x86-64和ARMv8 ISA。

Result: 在计算密集型NAS基准测试中，Unifico引入的平均开销小于6%（高端处理器）和10%（低端处理器）。相比最先进的Popcorn编译器，将二进制大小开销从~200%降低到~10%，同时消除了ISA迁移期间的栈转换开销。

Conclusion: Unifico通过消除运行时栈转换开销，为异构ISA处理器提供了高效的编译支持，其设计特性可以进一步优化以减轻性能影响，显著改进了现有解决方案。

Abstract: Heterogeneous-ISA processor designs have attracted considerable research interest. However, unlike their homogeneous-ISA counterparts, explicit software support for bridging ISA heterogeneity is required. The lack of a compilation toolchain ready to support heterogeneous-ISA targets has been a major factor hindering research in this exciting emerging area. For any such compiler, "getting right" the mechanics involved in state transformation upon migration and doing this efficiently is of critical importance. In particular, any runtime conversion of the current program stack from one architecture to another would be prohibitively expensive. In this paper, we design and develop Unifico, a new multi-ISA compiler that generates binaries that maintain the same stack layout during their execution on either architecture. Unifico avoids the need for runtime stack transformation, thus eliminating overheads associated with ISA migration. Additional responsibilities of the Unifico compiler backend include maintenance of a uniform ABI and virtual address space across ISAs. Unifico is implemented using the LLVM compiler infrastructure, and we are currently targeting the x86-64 and ARMv8 ISAs. We have evaluated Unifico across a range of compute-intensive NAS benchmarks and show its minimal impact on overall execution time, where less than 6% (10%) overhead is introduced on average for high-end (low-end) processors. We also analyze the performance impact of Unifico's key design features and demonstrate that they can be further optimized to mitigate this impact. When compared against the state-of-the-art Popcorn compiler, Unifico reduces binary size overhead from ~200% to ~10%, whilst eliminating the stack transformation overhead during ISA migration.

</details>


### [17] [Localized Calibrated Uncertainty in Code Language Models](https://arxiv.org/abs/2512.24560)
*David Gros,Prem Devanbu*

Main category: cs.SE

TL;DR: 该论文提出了一种定位LLM生成代码与用户意图偏差的技术，通过创建"最小意图对齐补丁"数据集，并比较多种方法（白盒探测、黑盒反思和自一致性）来评估哪些代码行需要编辑的概率校准能力。


<details>
  <summary>Details</summary>
Motivation: LLM生成的代码可能偏离用户意图，需要监督和编辑。为了支持这一过程，需要技术来定位生成代码中可能存在的偏差，以便更高效地进行修正。

Method: 1. 创建"最小意图对齐补丁"数据集，包含修复后的LLM生成程序，使用测试用例验证正确性；2. 比较多种技术：白盒探测（提出高效任意跨度查询技术）、黑盒反思和自一致性方法；3. 评估这些技术为代码编辑部分分配校准概率的能力。

Result: 使用小型监督模型的探测方法能够实现较低的校准误差和约0.2的Brier技能分数，在评估由大几个数量级的模型生成的代码编辑行时表现良好。仅基于代码训练的探测方法在允许新概率缩放的情况下，显示出对自然语言错误的泛化迹象。

Conclusion: 该研究为LLM生成代码的意图对齐提供了有效的定位技术，小型监督模型能够有效识别大模型生成代码中的偏差。这些技术与AI监督和控制相关，显示出跨领域泛化的潜力。

Abstract: Large Language models (LLMs) can generate complicated source code from natural language prompts. However, LLMs can generate output that deviates from what the user wants, requiring supervision and editing. To support this process, we offer techniques to localize where generations might be misaligned from user intent. We first create a dataset of "Minimal Intent Aligning Patches" of repaired LLM generated programs. Each program uses test cases to verify correctness. After creating a dataset of programs, we measure how well various techniques can assign a well-calibrated probability to indicate which parts of code will be edited in a minimal patch (i.e., give a probability that corresponds with empirical odds it is edited). We compare white-box probing (where we propose a technique for efficient arbitrary-span querying), against black-box reflective and self-consistency based approaches. We find probes with a small supervisor model can achieve low calibration error and Brier Skill Score of approx 0.2 estimating edited lines on code generated by models many orders of magnitude larger. We discuss the generalizability of the techniques, and the connections to AI oversight and control, finding a probe trained only on code shows some signs of generalizing to natural language errors if new probability scaling is allowed.

</details>


### [18] [On the Effectiveness of Training Data Optimization for LLM-based Code Generation: An Empirical Study](https://arxiv.org/abs/2512.24570)
*Shiqi Kuang,Zhao Tian,Tao Xiao,Dong Wang,Junjie Chen*

Main category: cs.SE

TL;DR: 该研究首次系统评估了五种常用训练数据优化技术及其组合对LLM代码生成的影响，发现数据合成在功能正确性上最有效，而数据合成与重构的组合整体表现最佳。


<details>
  <summary>Details</summary>
Motivation: 尽管已有多种训练数据优化技术被提出以提升LLM代码生成质量，但这些技术的整体效果尚未得到系统评估，需要填补这一研究空白。

Method: 进行了首个大规模实证研究，在三个基准测试和四个LLM上评估了五种广泛使用的训练数据优化技术及其两两组合对代码生成的影响。

Result: 数据合成在提升功能正确性和减少代码异味方面最有效；大多数组合不能进一步改善功能正确性，但能有效提升代码质量；数据合成与数据重构的组合整体性能最强。

Conclusion: 该研究为系统理解训练数据优化和组合策略迈出了第一步，为未来LLM代码生成的研究和部署提供了实用指导。

Abstract: Large language models (LLMs) have achieved remarkable progress in code generation, largely driven by the availability of high-quality code datasets for effective training. To further improve data quality, numerous training data optimization techniques have been proposed; however, their overall effectiveness has not been systematically evaluated. To bridge this gap, we conduct the first large-scale empirical study, examining five widely-used training data optimization techniques and their pairwise combinations for LLM-based code generation across three benchmarks and four LLMs. Our results show that data synthesis is the most effective technique for improving functional correctness and reducing code smells, although it performs relatively worse on code maintainability compared to data refactoring, cleaning, and selection. Regarding combinations, we find that most combinations do not further improve functional correctness but can effectively enhance code quality (code smells and maintainability). Among all combinations, data synthesis combined with data refactoring achieves the strongest overall performance. Furthermore, our fine-grained analysis reinforces these findings and provides deeper insights into how individual techniques and their combinations influence code generation effectiveness. Overall, this work represents a first step toward a systematic understanding of training data optimization and combination strategies, offering practical guidance for future research and deployment in LLM-based code generation.

</details>


### [19] [How Do Agentic AI Systems Address Performance Optimizations? A BERTopic-Based Analysis of Pull Requests](https://arxiv.org/abs/2512.24630)
*Md Nahidul Islam Opu,Shahidul Islam,Muhammad Asaduzzaman,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: AI代理生成的性能相关PR研究：识别52个性能主题，发现优化类型影响PR接受率和审查时间，主要在开发阶段而非维护阶段进行性能优化


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究关注AI生成软件的正确性和性能，但AI代理系统在实践中如何具体处理性能问题尚不清楚，需要实证研究来理解AI代理的性能优化行为

Method: 使用LLM辅助检测和BERTopic主题建模方法，分析AI代理生成的性能相关pull requests，识别性能优化主题并进行分类

Result: 识别出52个性能相关主题，分为10个高层类别；AI代理在软件栈各层应用性能优化；优化类型显著影响PR接受率和审查时间；性能优化主要在开发阶段而非维护阶段

Conclusion: 研究为评估和改进AI代理系统的性能优化行为及审查结果提供了实证证据，有助于理解AI代理在实际开发中的性能优化实践

Abstract: LLM-based software engineering is influencing modern software development. In addition to correctness, prior studies have also examined the performance of software artifacts generated by AI agents. However, it is unclear how exactly the agentic AI systems address performance concerns in practice. In this paper, we present an empirical study of performance-related pull requests generated by AI agents. Using LLM-assisted detection and BERTopic-based topic modeling, we identified 52 performance-related topics grouped into 10 higher-level categories. Our results show that AI agents apply performance optimizations across diverse layers of the software stack and that the type of optimization significantly affects pull request acceptance rates and review times. We also found that performance optimization by AI agents primarily occurs during the development phase, with less focus on the maintenance phase. Our findings provide empirical evidence that can support the evaluation and improvement of agentic AI systems with respect to their performance optimization behaviors and review outcomes.

</details>


### [20] [DynaFix: Iterative Automated Program Repair Driven by Execution-Level Dynamic Information](https://arxiv.org/abs/2512.24635)
*Zhili Huang,Ling Xu,Chao Liu,Weifeng Sun,Xu Zhang,Yan Lei,Meng Yan,Hongyu Zhang*

Main category: cs.SE

TL;DR: DynaFix：一种基于执行级动态信息的自动程序修复方法，通过迭代利用运行时信息（变量状态、控制流路径、调用栈）来指导LLM生成补丁，显著提升修复效果


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的APR方法主要依赖静态分析，忽略运行时行为；即使尝试融入动态信号，也仅限于训练/微调阶段或单次注入提示，无法迭代利用执行信息。现有迭代修复框架通常依赖粗粒度反馈（通过/失败、异常类型），未能有效利用细粒度执行级信息，限制了模型模拟人类逐步调试的能力

Method: DynaFix是一种执行级动态信息驱动的APR方法，在每轮修复中捕获变量状态、控制流路径、调用栈等运行时信息，将其转化为结构化提示来指导LLM生成候选补丁。如果补丁验证失败，重新执行修改后的程序收集新的执行信息进行下一次尝试，形成类似人类逐步调试的迭代循环

Result: 在Defects4J v1.2和v2.0基准测试中，DynaFix修复了186个单函数bug，比最先进基线提升10%，包括38个先前未修复的bug。最多35次尝试内获得正确补丁，相比现有方法减少70%的补丁搜索空间

Conclusion: DynaFix通过迭代利用执行级动态信息，有效模拟人类逐步调试过程，在修复复杂bug方面展现出显著的效果和效率优势，为基于LLM的APR方法提供了新的方向

Abstract: Automated Program Repair (APR) aims to automatically generate correct patches for buggy programs. Recent approaches leveraging large language models (LLMs) have shown promise but face limitations. Most rely solely on static analysis, ignoring runtime behaviors. Some attempt to incorporate dynamic signals, but these are often restricted to training or fine-tuning, or injected only once into the repair prompt, without iterative use. This fails to fully capture program execution. Current iterative repair frameworks typically rely on coarse-grained feedback, such as pass/fail results or exception types, and do not leverage fine-grained execution-level information effectively. As a result, models struggle to simulate human stepwise debugging, limiting their effectiveness in multi-step reasoning and complex bug repair.
  To address these challenges, we propose DynaFix, an execution-level dynamic information-driven APR method that iteratively leverages runtime information to refine the repair process. In each repair round, DynaFix captures execution-level dynamic information such as variable states, control-flow paths, and call stacks, transforming them into structured prompts to guide LLMs in generating candidate patches. If a patch fails validation, DynaFix re-executes the modified program to collect new execution information for the next attempt. This iterative loop incrementally improves patches based on updated feedback, similar to the stepwise debugging practices of human developers. We evaluate DynaFix on the Defects4J v1.2 and v2.0 benchmarks. DynaFix repairs 186 single-function bugs, a 10% improvement over state-of-the-art baselines, including 38 bugs previously unrepaired. It achieves correct patches within at most 35 attempts, reducing the patch search space by 70% compared with existing methods, thereby demonstrating both effectiveness and efficiency in repairing complex bugs.

</details>


### [21] [How Do Agentic AI Systems Deal With Software Energy Concerns? A Pull Request-Based Study](https://arxiv.org/abs/2512.24636)
*Tanjum Motin Mitul,Md. Masud Mazumder,Md Nahidul Islam Opu,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 研究分析AI编码代理在软件开发中对能源问题的认知和处理能力，发现虽然AI代理本身能耗高，但在生成软件工件时表现出能源意识，不过能源优化相关的PR接受率较低。


<details>
  <summary>Details</summary>
Motivation: 随着软件工程进入SE 3.0时代，AI编码代理日益自动化软件开发流程。然而，这些代理如何识别和处理软件能源问题尚不明确，而这个问题因大规模数据中心、能耗高的语言模型和电池受限设备而日益重要。

Method: 使用公开数据集分析代理撰写的pull requests（PRs），识别出216个明确涉及能源的PRs，进行主题分析，推导出能源感知工作的分类法，并进一步分析应用的优化技术。

Result: 大多数优化技术与既定的研究建议一致。虽然构建和运行这些AI代理本身能耗很高，但令人鼓舞的是，它们在生成软件工件时表现出能源意识。然而，与优化相关的PRs接受率低于其他PRs，主要原因是它们对可维护性产生负面影响。

Conclusion: AI编码代理在软件开发中展现出能源意识，但能源优化相关的修改在实践中的接受度受到可维护性考虑的限制，需要在能源效率和软件质量之间找到平衡。

Abstract: As Software Engineering enters its new era (SE 3.0), AI coding agents increasingly automate software development workflows. However, it remains unclear how exactly these agents recognize and address software energy concerns-an issue growing in importance due to large-scale data centers, energy-hungry language models, and battery-constrained devices. In this paper, we examined the energy awareness of agent-authored pull requests (PRs) using a publicly available dataset. We identified 216 energy-explicit PRs and conducted a thematic analysis, deriving a taxonomy of energy-aware work. Our further analysis of the applied optimization techniques shows that most align with established research recommendations. Although building and running these agents is highly energy intensive, encouragingly, the results indicate that they exhibit energy awareness when generating software artifacts. However, optimization-related PRs are accepted less frequently than others, largely due to their negative impact on maintainability.

</details>


### [22] [Characterizing Bugs and Quality Attributes in Quantum Software: A Large-Scale Empirical Study](https://arxiv.org/abs/2512.24656)
*Mir Mohammad Yousuf,Shabir Ahmad Sofi*

Main category: cs.SE

TL;DR: 首次对123个开源量子仓库进行生态系统规模纵向分析，发现全栈库和编译器缺陷最多，量子特定bug主要影响性能和维护性，自动化测试可减少约60%缺陷。


<details>
  <summary>Details</summary>
Motivation: 量子软件工程需要确保混合量子-经典系统的可靠性和可维护性，但缺乏关于真实量子项目中bug如何出现及影响质量的实证证据。

Method: 混合方法：结合仓库挖掘、静态代码分析、问题元数据提取和验证过的基于规则的分类框架，分析2012-2024年间32,296个已验证的bug报告。

Result: 全栈库和编译器因电路、门和转换相关问题缺陷最多；模拟器主要受测量和噪声建模错误影响；经典bug影响可用性和互操作性，量子特定bug主要降低性能、可维护性和可靠性；缺陷密度在2017-2021年达到峰值后下降；自动化测试可减少约60%缺陷。

Conclusion: 首次大规模数据驱动的量子软件缺陷特征分析，为改进量子软件工程的测试、文档和维护实践提供实证指导，显示生态系统正在成熟。

Abstract: Quantum Software Engineering (QSE) is essential for ensuring the reliability and maintainability of hybrid quantum-classical systems, yet empirical evidence on how bugs emerge and affect quality in real-world quantum projects remains limited. This study presents the first ecosystem-scale longitudinal analysis of software defects across 123 open source quantum repositories from 2012 to 2024, spanning eight functional categories, including full-stack libraries, simulators, annealing, algorithms, compilers, assembly, cryptography, and experimental computing. Using a mixed method approach combining repository mining, static code analysis, issue metadata extraction, and a validated rule-based classification framework, we analyze 32,296 verified bug reports. Results show that full-stack libraries and compilers are the most defect-prone categories due to circuit, gate, and transpilation-related issues, while simulators are mainly affected by measurement and noise modeling errors. Classical bugs primarily impact usability and interoperability, whereas quantum-specific bugs disproportionately degrade performance, maintainability, and reliability. Longitudinal analysis indicates ecosystem maturation, with defect densities peaking between 2017 and 2021 and declining thereafter. High-severity defects cluster in cryptography, experimental computing, and compiler toolchains. Repositories employing automated testing detect more defects and resolve issues faster. A negative binomial regression further shows that automated testing is associated with an approximate 60 percent reduction in expected defect incidence. Overall, this work provides the first large-scale data-driven characterization of quantum software defects and offers empirical guidance for improving testing, documentation, and maintainability practices in QSE.

</details>


### [23] [Feature Slice Matching for Precise Bug Detection](https://arxiv.org/abs/2512.24858)
*Ke Ma,Jianjun Huang,Wei You,Bin Liang,Jingzheng Wu,Yanjun Wu,Yuanjun Gong*

Main category: cs.SE

TL;DR: MATUS通过特征切片和端到端目标噪声抑制，提升基于相似度的bug检测精度，在Linux内核中发现31个未知bug


<details>
  <summary>Details</summary>
Motivation: 现有基于函数相似度的bug检测方法中，与bug无关的语句会产生噪声干扰，而现有工作未能有效消除目标代码中的噪声，影响了检测性能

Method: 从buggy查询和目标代码中提取特征切片来表示（潜在）bug逻辑的语义特征；利用buggy代码的先验知识指导目标切片，以端到端方式精确定位目标中的切片标准；将所有特征切片嵌入并基于向量相似度进行比较

Result: MATUS在真实项目bug检测中具有优势且效率可接受；在Linux内核中发现了31个未知bug，全部得到内核开发者确认，其中11个被分配了CVE编号

Conclusion: MATUS通过抑制目标噪声实现了更精确的基于相似度的bug检测，在实际项目中验证了其有效性

Abstract: Measuring the function similarity to detect bugs is effective, but the statements unrelated to the bugs can impede the performance due to the noise interference. Suppressing the noise interference in existing works does not manage the tough job, i.e., eliminating the noise in the targets. In this paper, we propose MATUS to mitigate the target noise for precise bug detection based on similarity measurement. Feature slices are extracted from both the buggy query and the targets to represent the semantic feature of (potential) bug logics. In particular, MATUS guides the target slicing with the prior knowledge from the buggy code, in an end-to-end way to pinpoint the slicing criterion in the targets. All feature slices are embedded and compared based on the vector similarity. Buggy candidates are audited to confirm unknown bugs in the targets. Experiments show that MATUS holds advantages in bug detection for real-world projects with acceptable efficiency. In total, MATUS has spotted 31 unknown bugs in the Linux kernel. All of them have been confirmed by the kernel developers, and 11 have been assigned CVEs.

</details>


### [24] [Securing High-Concurrency Ticket Sales: A Framework Based on Microservice](https://arxiv.org/abs/2512.24941)
*Zhiyong Zhang,Xiaoyan Zhang,Xiaoqi Li*

Main category: cs.SE

TL;DR: 基于微服务架构的铁路售票系统，采用B/S架构和Spring Cloud技术，解决高并发场景下的系统稳定性问题，实现全流程在线购票功能。


<details>
  <summary>Details</summary>
Motivation: 传统聚合架构在节假日等高并发场景下无法满足用户需求，存在容错性不足、能力低下等问题，需要采用微服务架构来确保系统在高并发下的稳定性和数据一致性。

Method: 采用B/S架构和Spring Cloud微服务框架进行系统开发，集成多种安全设计方法，实现实时列车查询、动态座位更新、在线选座、购票等功能，并整合多种中间件组件。

Result: 系统开发完成后进行核心接口测试，测试数据证明系统在高并发场景下具有良好的能力和稳定性，能够快速响应用户请求。

Conclusion: 通过微服务架构和多重安全设计，成功开发出能够应对高并发场景的铁路售票系统，解决了传统线下购票的排队时间长、信息延迟等问题，实现了全流程在线化。

Abstract: The railway ticketing system is one of the most important public service infrastructure. In peak periods such as holidays, it is often faced with the challenge of high concurrency scenarios because of a large number of users accessing at the same time. The traditional aggregation architecture can not meet the peak user requirements because of its insufficient fault tolerance and low ability. Therefore, the system needs to use microservice architecture for development, and add multiple security methods to ensure that the system can have good stability and data consistency under high concurrency scenarios, and can respond quickly to user requests. This paper introduces the use of B/S architecture and Spring Cloud to design and develop a railway ticket purchase system that can maintain stability and reliability under high concurrency scenarios, and formulate multiple security design methods for the system. This system integrates a range of functions, such as real-time train inquiries, dynamic seat updates, online seat selection, and ticket purchasing, effectively addressing common problems associated with offline ticket purchasing, such as long queues and delayed information. It enables a complete online process from inquiry and booking to payment and refunds. Furthermore, the "add passenger" function allows users to purchase tickets for others, extending the convenience of online ticketing to people with limited internet access. The system design prioritizes security and stability, while also focusing on high performance, and achieves these goals through a carefully designed architecture and the integration of multiple middleware components. After the completion of the system development, the core interface of the system is tested, and then the results are analyzed. The test data proves that the system has good ability and stability under high concurrency.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [25] [Enforcing Temporal Constraints for LLM Agents](https://arxiv.org/abs/2512.23738)
*Adharsh Kamath,Sishen Zhang,Calvin Xu,Shubham Ugare,Gagandeep Singh,Sasa Misailovic*

Main category: cs.PL

TL;DR: Agent-C 是一个为LLM智能体提供运行时安全保障的框架，确保智能体遵守时序安全策略，在检测到违规行为时生成合规替代方案，实现100%安全合规。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体被部署在安全关键应用中，但现有防护系统无法防止违反时序安全策略的行为。现有防护措施依赖不精确的自然语言指令或事后监控，无法提供智能体满足时序约束的形式化保证。

Method: Agent-C引入了一个用于表达时序属性的领域特定语言，将规范转换为一阶逻辑，并使用SMT求解在token生成过程中检测不合规的智能体行为。当LLM尝试生成不合规的工具调用时，Agent-C利用约束生成技术确保每个生成的行动都符合规范，并为不合规行为生成合规替代方案。

Result: 在零售客户服务和航空票务系统等实际应用中，Agent-C实现了完美的安全性（100%合规，0%危害），同时相比最先进的防护措施和无限制智能体提高了任务效用。在Claude Sonnet 4.5上合规率从77.4%提升到100%，在GPT-5上从83.7%提升到100%，同时效用也有所提高。

Conclusion: Agent-C为可靠的智能体推理设定了新的最先进前沿，通过形式化方法和运行时保证，确保LLM智能体在安全关键应用中遵守时序安全策略。

Abstract: LLM-based agents are deployed in safety-critical applications, yet current guardrail systems fail to prevent violations of temporal safety policies, requirements that govern the ordering and sequencing of agent actions. For instance, agents may access sensitive data before authenticating users or process refunds to unauthorized payment methods, violations that require reasoning about sequences of action rather than an individual action. Existing guardrails rely on imprecise natural language instructions or post-hoc monitoring, and provide no formal guarantees that agents will satisfy temporal constraints. We present Agent-C, a novel framework that provides run-time guarantees ensuring LLM agents adhere to formal temporal safety properties. Agent-C introduces a domain-specific language for expressing temporal properties (e.g., authenticate before accessing data), translates specifications to first-order logic, and uses SMT solving to detect non-compliant agent actions during token generation. When the LLM attempts to generate a non-compliant tool call, Agent-C leverages constrained generation techniques to ensure that every action generated by the LLM complies with the specification, and to generate a compliant alternative to a non-compliant agent action. We evaluate Agent-C across two real-world applications: retail customer service and airline ticket reservation system, and multiple language models (open and closed-source). Our results demonstrate that Agent-C achieves perfect safety (100% conformance, 0% harm), while improving task utility compared to state-of-the-art guardrails and unrestricted agents. On SoTA closed-source models, Agent-C improves conformance (77.4% to 100% for Claude Sonnet 4.5 and 83.7% to 100% for GPT-5), while simultaneously increasing utility (71.8% to 75.2% and 66.1% to 70.6%, respectively), representing a new SoTA frontier for reliable agentic reasoning.

</details>


### [26] [Towards representation agnostic probabilistic programming](https://arxiv.org/abs/2512.23740)
*Ole Fenske,Maximilian Popko,Sebastian Bader,Thomas Kirste*

Main category: cs.PL

TL;DR: 提出一种因子抽象作为通用接口，支持不同表示形式的混合使用，实现表示无关的概率编程


<details>
  <summary>Details</summary>
Motivation: 当前概率编程语言和工具将模型表示与特定推理算法紧密耦合，限制了新表示形式或混合离散-连续模型的实验

Method: 引入具有五个基本操作的因子抽象，作为操作因子的通用接口，无论其底层表示形式如何

Result: 实现了表示无关的概率编程，用户可以在统一框架内自由混合不同表示形式，支持当前工具无法充分表达的复杂混合模型

Conclusion: 因子抽象为概率编程提供了灵活的基础，支持多种表示形式的混合使用，扩展了概率编程的表达能力

Abstract: Current probabilistic programming languages and tools tightly couple model representations with specific inference algorithms, preventing experimentation with novel representations or mixed discrete-continuous models. We introduce a factor abstraction with five fundamental operations that serve as a universal interface for manipulating factors regardless of their underlying representation. This enables representation-agnostic probabilistic programming where users can freely mix different representations (e.g. discrete tables, Gaussians distributions, sample-based approaches) within a single unified framework, allowing practical inference in complex hybrid models that current toolkits cannot adequately express.

</details>


### [27] [VGC: A High-Performance Zone-Based Garbage Collector Architecture for Python with Partitioning and Parallel Execution](https://arxiv.org/abs/2512.23768)
*Abdulla M*

Main category: cs.PL

TL;DR: VGC提出了一种新颖的双层垃圾回收框架，通过主动层（运行时并发标记清除）和被动层（编译时预测性内存映射）优化内存管理，在并行系统中减少暂停时间30%，降低内存使用25%。


<details>
  <summary>Details</summary>
Motivation: 传统垃圾回收器在不同系统（从资源受限的嵌入式设备到高性能并行架构）中性能表现不佳，存在暂停时间长、内存碎片化、可预测性差等问题，需要一种更高效、低开销的内存管理方案。

Method: 采用双层架构：Active VGC在运行时使用并发标记清除策略管理动态对象，减少并行工作负载的暂停时间；Passive VGC在编译时通过预测性内存映射优化静态对象分配，将对象对齐到缓存边界以减少碎片化。

Result: 与分代收集器相比，在多线程基准测试中暂停时间减少高达30%；内存使用总量减少高达25%；提供可预测的内存访问模式；改善现代并行应用程序的可扩展性。

Conclusion: VGC通过整合编译时和运行时优化，为内存密集型系统提供了一个鲁棒且适应性强的解决方案，适用于从低级到高级编程环境的各种场景。

Abstract: The Virtual Garbage Collector (VGC) introduces a novel memory management framework designed to optimize performance across diverse systems, ranging from resource constrained embedded devices to high performance parallel architectures. Unlike conventional garbage collectors, VGC employs a dual layer architecture consisting of Active VGC and Passive VGC to enable efficient, low overhead memory management. Active VGC dynamically manages runtime objects using a concurrent mark and sweep strategy tailored for parallel workloads, reducing pause times by up to 30 percent compared to generational collectors in multithreaded benchmarks. Passive VGC operates at compile time and optimizes static object allocation through predictive memory mapping, minimizing fragmentation by aligning objects to cache boundaries. This separation of responsibilities ensures predictable memory access patterns, reduces total memory usage by up to 25 percent, and improves scalability for modern parallel applications. By integrating compile time and runtime optimizations, VGC provides a robust and adaptable solution for memory intensive systems across both low level and high level programming environments.

</details>


### [28] [State Space Estimation for DPOR-based Model Checkers](https://arxiv.org/abs/2512.23996)
*A. R. Balasubramanian,Mohammad Hossein Khoshechin Jorshari,Rupak Majumdar,Umang Mathur,Minjian Zhang*

Main category: cs.PL

TL;DR: 提出首个可证明的多项式时间无偏估计器，用于计算并发程序的Mazurkiewicz迹等价类数量，解决模型检查资源分配问题。


<details>
  <summary>Details</summary>
Motivation: 并发程序的迹等价类数量估计对基于枚举的模型检查至关重要，它决定了检查所需时间和搜索空间覆盖率，但现有方法无法高效解决这一#P难问题。

Method: 将无状态最优DPOR算法转化为无偏估计器，将其探索过程视为有界深度和宽度的树，应用Knuth经典估计器，并通过随机枚举控制方差，维护每层部分路径的小种群。

Result: 在JMC模型检查器中实现，在共享内存基准测试中，即使状态空间有10^5-10^6个类，也能在数百次试验内获得稳定估计（通常在20%误差范围内）。

Conclusion: 首次提供了可证明的多项式时间无偏估计器来计数迹，这对分配模型检查资源具有重要实际意义，同时展示了如何扩展该机制来估计模型检查成本。

Abstract: We study the estimation problem for concurrent programs: given a bounded program $P$, estimate the number of Mazurkiewicz trace-equivalence classes induced by its interleavings. This quantity informs two practical questions for enumeration-based model checking: how long a model checking run is likely to take, and what fraction of the search space has been covered so far. We first show the counting problem is #P-hard even for restricted programs and, unless $P=NP$, inapproximable within any subexponential factor, ruling out efficient exact or randomized approximation algorithms. We give a Monte Carlo approach to obtain a poly-time unbiased estimator: we convert a stateless optimal DPOR algorithm into an unbiased estimator by viewing its exploration as a bounded-depth, bounded-width tree whose leaves are the maximal Mazurkiewicz traces. A classical estimator by Knuth, when run on this tree, yields an unbiased estimate. To control the variance, we apply stochastic enumeration by maintaining a small population of partial paths per depth whose evolution is coupled. We have implemented our estimator in the JMC model checker and evaluated it on shared-memory benchmarks. With modest budgets, our estimator yields stable estimates, typically within a 20% band, within a few hundred trials, even when the state space has $10^5$--$10^6$ classes. We also show how the same machinery estimates model-checking cost by weighting all explored graphs, not only complete traces. Our algorithms provide the first provable poly-time unbiased estimators for counting traces, a problem of considerable importance when allocating model checking resources.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [29] [A precise proof of the n-variable Bekic principle](https://arxiv.org/abs/2512.24038)
*Jun Xu*

Main category: cs.LO

TL;DR: 证明n元Bekič原理：向量形式的n维不动点可按字典序分解为嵌套坐标不动点


<details>
  <summary>Details</summary>
Motivation: Bekič原理是程序语义和不动点理论中的经典结果，将向量不动点分解为标量不动点。本文旨在将这一原理推广到n维情况，为高阶不动点计算提供理论基础。

Method: 采用归纳法证明。从二维Bekič原理出发，通过数学归纳将结果推广到任意维度n，展示如何将向量不动点按字典序分解为嵌套坐标不动点。

Result: 成功证明了n元Bekič原理，建立了向量不动点与嵌套坐标不动点之间的等价关系，为高阶不动点计算提供了严格的数学基础。

Conclusion: n元Bekič原理的证明完善了不动点理论的框架，为程序语义、形式验证等领域的高阶不动点分析提供了有力工具。

Abstract: We provide a proof of the $n$-ary Bekič principle, which states that a vectorial fixpoint of size $n$ can be written in terms of nested fixpoints in each coordinate according to lexicographic order. The proof is inductive.

</details>


### [30] [Proof-Carrying PWL Verification for ReLU Networks: Convex-Hull Semantics, Exact \SMT/\MILP Encodings, and Symbolic Certificate Checking](https://arxiv.org/abs/2512.24339)
*Chandrasekhar Gokavarapu*

Main category: cs.LO

TL;DR: 提出一个可证明的ReLU神经网络验证框架，通过代数见证（LP对偶乘子和Farkas不可行性证书）提供可独立验证的安全证据。


<details>
  <summary>Details</summary>
Motivation: 现有的神经网络验证方法虽然能证明安全性，但缺乏可独立验证的证据。需要一种不仅能证明正确性，还能提供可检查证据的验证框架。

Method: 1) 将ReLU网络形式化为激活模式索引的多面体并集；2) 提出精确的SMT/MILP编码和ReLU的凸包松弛；3) 引入证书演算，其中边界收紧、稳定化、强化和剪枝步骤生成显式代数见证。

Result: 开发了一个符号证书检查器、保持有效性的规范化规则，以及将区域证书组合为全局安全证明的框架。

Conclusion: 该工作为ReLU神经网络验证提供了一个可证明的框架，通过代数见证提供可独立验证的安全证据，增强了验证结果的可信度。

Abstract: ReLU networks are piecewise-linear (PWL), enabling exact symbolic verification via \SMT(\LRA) or \MILP. However, safety claims in certification pipelines require not only correctness but also \emph{checkable evidence}. We develop a proof-carrying verification core for PWL neural constraints: (i) we formalize ReLU networks as unions of polyhedra indexed by activation patterns; (ii) we present exact \SMT/\MILP encodings and the canonical convex-hull relaxation for each bounded ReLU; and (iii) we introduce a certificate calculus in which bound tightening, stabilization, strengthening, and pruning steps emit explicit algebraic witnesses (LP dual multipliers and Farkas infeasibility certificates). Crucially, these witnesses are \emph{symbolic objects} that admit independent verification in exact arithmetic over $\Q$. We provide a symbolic certificate checker, normalization rules that preserve validity, and a compositional view of region-wise certificates as a global proof artifact for universal safety.

</details>


### [31] [LeanCat: A Benchmark Suite for Formal Category Theory in Lean (Part I: 1-Categories)](https://arxiv.org/abs/2512.24796)
*Rongge Xu,Hui Dai,Yiming Fu,Jiedong Jiang,Tianjiao Nie,Hongwei Wang,Junkai Wang,Holiverse Yang,Jiatong Yang,Zhi-Hao Zhang*

Main category: cs.LO

TL;DR: LeanCat是一个用于范畴论形式化的Lean基准测试，包含100个形式化任务，旨在评估LLM在结构化、接口级推理方面的能力，作为AI和人类在Lean中实现可靠研究级形式化的检查点。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试未能充分衡量现代数学中组织化的抽象和库中介推理，特别是范畴论作为数学结构的统一语言和现代证明工程的核心层。需要创建能够测试结构性、接口级推理能力的基准。

Method: 引入LeanCat基准测试，包含100个完全形式化的陈述级任务，通过LLM辅助+人工评分过程按主题家族和三个难度等级进行策划。评估最佳模型在pass@1和pass@4下的表现，并评估使用LeanExplore搜索Mathlib的LeanBridge方法。

Result: 最佳模型在pass@1下解决8.25%的任务（简单/中等/高难度分别为32.50%/4.17%/0.00%），在pass@4下解决12.00%的任务（50.00%/4.76%/0.00%）。使用LeanExplore搜索Mathlib的LeanBridge方法相比单模型基线有持续提升。

Conclusion: LeanCat作为一个紧凑、可重复使用的检查点，可用于跟踪AI和人类在Lean中实现可靠研究级形式化的进展，特别强调范畴论作为数学结构统一语言的形式化能力评估。

Abstract: Large language models (LLMs) have made rapid progress in formal theorem proving, yet current benchmarks under-measure the kind of abstraction and library-mediated reasoning that organizes modern mathematics. In parallel with FATE's emphasis on frontier algebra, we introduce LeanCat, a Lean benchmark for category-theoretic formalization -- a unifying language for mathematical structure and a core layer of modern proof engineering -- serving as a stress test of structural, interface-level reasoning. Part I: 1-Categories contains 100 fully formalized statement-level tasks, curated into topic families and three difficulty tiers via an LLM-assisted + human grading process. The best model solves 8.25% of tasks at pass@1 (32.50%/4.17%/0.00% by Easy/Medium/High) and 12.00% at pass@4 (50.00%/4.76%/0.00%). We also evaluate LeanBridge which use LeanExplore to search Mathlib, and observe consistent gains over single-model baselines. LeanCat is intended as a compact, reusable checkpoint for tracking both AI and human progress toward reliable, research-level formalization in Lean.

</details>


### [32] [Open Horn Type Theory](https://arxiv.org/abs/2512.24498)
*Iman Poernomo*

Main category: cs.LO

TL;DR: OHTT扩展依赖类型理论，引入"相干性"和"间隙"两个原始判断形式，通过"传输角"捕获HoTT无法表达的阻碍，应用于拓扑、语义和逻辑领域。


<details>
  <summary>Details</summary>
Motivation: 现有同伦类型理论(HoTT)的Kan条件保证所有传输都成功，无法表达某些阻碍现象。需要一种能形式化表达"非相干性"的类型理论，捕捉拓扑、语义和逻辑中的阻碍结构。

Method: 扩展依赖类型理论，引入两个原始判断形式：相干性和间隙，受互斥律约束。构建"传输角"配置，开发破裂单纯集和破裂Kan复形的语义模型。将HoTT嵌入为OHTT的相干片段。

Result: 建立了OHTT理论框架，能表达三类阻碍：拓扑阻碍(单值性、和乐、示性类)、语义阻碍(多义性、意义纤维化)、逻辑阻碍(资源敏感可导性、子结构失败)。间隙见证作为正结构而非证明缺失。

Conclusion: OHTT通过引入相干性/间隙的三分法，超越了HoTT的二元可导/不可导区分，能形式化表达各种阻碍现象，为类型理论提供了更丰富的表达能力。

Abstract: We introduce Open Horn Type Theory (OHTT), an extension of dependent type theory with two primitive judgment forms: coherence and gap, subject to a mutual exclusion law. Unlike classical or intuitionistic negation, gap is not defined via implication but is a primitive witness of non-coherence. Judgments may also be open -- neither coherent nor gapped -- yielding a trichotomy that generalizes the binary derivable/underivable distinction. The central construction is the transport horn: a configuration where a term and a path both cohere, but transport along the path is witnessed as gapped. This captures obstructions that Homotopy Type Theory (HoTT) cannot express, since HoTT's Kan condition guarantees all transport succeeds. We develop the semantics via ruptured simplicial sets -- simplicial sets equipped with coherence and gap structure -- and ruptured Kan complexes, which model types where some horns fill, some are gap-witnessed, and some remain open. We show that HoTT embeds as the coherent fragment of OHTT, recovered by imposing totality. Three classes of obstructions are developed in detail: topological (monodromy, holonomy, characteristic classes), semantic (polysemy, meaning fibrations), and logical (resource-sensitive derivability, substructural failure). In each case, the gap witness is positive structure -- not absence of proof, but certified obstruction.

</details>


### [33] [A Modal Logic for Possibilistic Reasoning with Fuzzy Formal Contexts](https://arxiv.org/abs/2512.24980)
*Prosenjit Howlader,Churn-Jung Liau*

Main category: cs.LO

TL;DR: 提出一种用于模糊形式背景可能性推理的双排序加权模态逻辑，包含必要性和充分性模态算子，并推广了形式概念分析的三个核心概念


<details>
  <summary>Details</summary>
Motivation: 为了在模糊形式背景下进行可能性推理，需要一种能够处理模糊形式背景的逻辑框架，同时能够推广形式概念分析中的核心概念

Method: 引入双排序加权模态逻辑，包含必要性和充分性两种加权模态算子，基于可能性理论在模糊形式背景中解释公式，并给出公理化系统

Result: 建立了相对于所有模糊背景模型类的声音公理化系统，必要性片段和充分性片段分别具有完备性，逻辑语言能够表示推广的形式概念、对象导向概念和属性导向概念

Conclusion: 该逻辑框架为模糊形式背景下的可能性推理提供了有效的工具，能够推广形式概念分析的核心概念，并具有扩展到多关系模糊背景推理的潜力

Abstract: We introduce a two-sort weighted modal logic for possibilistic reasoning with fuzzy formal contexts. The syntax of the logic includes two types of weighted modal operators corresponding to classical necessity ($\Box$) and sufficiency ($\boxminus$) modalities and its formulas are interpreted in fuzzy formal contexts based on possibility theory. We present its axiomatization that is \emph{sound} with respect to the class of all fuzzy context models. In addition, both the necessity and sufficiency fragments of the logic are also individually complete with respect to the class of all fuzzy context models. We highlight the expressive power of the logic with some illustrative examples. As a formal context is the basic construct of formal concept analysis (FCA), we generalize three main notions in FCA, i.e., formal concepts, object oriented concepts, and property oriented concepts, to their corresponding $c$-cut concepts in fuzzy formal contexts. Then, we show that our logical language can represent all three of these generalized notions. Finally, we demonstrate the possibility of extending our logic to reasoning with multi-relational fuzzy contexts, in which the Boolean combinations of different fuzzy relations are allowed.

</details>
