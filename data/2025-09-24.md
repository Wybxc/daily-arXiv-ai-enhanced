<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 7]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.LO](#cs.LO) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [CoRaCMG: Contextual Retrieval-Augmented Framework for Commit Message Generation](https://arxiv.org/abs/2509.18337)
*Bo Xiong,Linghao Zhang,Chong Wang,Peng Liang*

Main category: cs.SE

TL;DR: 本文提出CoRaCMG框架，通过检索相似的diff-message对来增强LLM生成提交消息的性能，显著提升了多种评估指标。


<details>
  <summary>Details</summary>
Motivation: 提交消息在记录代码变更意图中起关键作用，但现有提交消息往往质量低下、模糊或不完整，限制了其实用性。虽然LLM在自动生成提交消息方面显示出潜力，但性能仍有局限。

Method: CoRaCMG框架包含三个阶段：(1)检索相似的diff-message对；(2)将检索到的对与查询diff结合成结构化提示；(3)通过LLM生成对应的提交消息。

Result: 实验表明CoRaCMG显著提升了LLM在BLEU、Rouge-L、METEOR和CIDEr四个指标上的性能。DeepSeek-R1在加入单个检索示例后，BLEU相对提升76%，CIDEr提升71%。GPT-4o在加入单个示例后BLEU提升89%。

Conclusion: 性能提升主要归因于模型能够从检索到的示例对中学习人类编写提交消息的术语和写作风格，且当使用超过三个示例时性能提升趋于平稳，表明存在收益递减现象。

Abstract: Commit messages play a key role in documenting the intent behind code
changes. However, they are often low-quality, vague, or incomplete, limiting
their usefulness. Commit Message Generation (CMG) aims to automatically
generate descriptive commit messages from code diffs to reduce developers'
effort and improve message quality. Although recent advances in LLMs have shown
promise in automating CMG, their performance remains limited. This paper aims
to enhance CMG performance by retrieving similar diff-message pairs to guide
LLMs to generate commit messages that are more precise and informative. We
proposed CoRaCMG, a Contextual Retrieval-augmented framework for Commit Message
Generation, structured in three phases: (1) Retrieve: retrieving the similar
diff-message pairs; (2) Augment: combining them with the query diff into a
structured prompt; and (3) Generate: generating commit messages corresponding
to the query diff via LLMs. CoRaCMG enables LLMs to learn project-specific
terminologies and writing styles from the retrieved diff-message pairs, thereby
producing high-quality commit messages. We evaluated our method on various
LLMs, including closed-source GPT models and open-source DeepSeek models.
Experimental results show that CoRaCMG significantly boosts LLM performance
across four metrics (BLEU, Rouge-L, METEOR, and CIDEr). Specifically,
DeepSeek-R1 achieves relative improvements of 76% in BLEU and 71% in CIDEr when
augmented with a single retrieved example pair. After incorporating the single
example pair, GPT-4o achieves the highest improvement rate, with BLEU
increasing by 89%. Moreover, performance gains plateau after more than three
examples are used, indicating diminishing returns. Further analysis shows that
the improvements are attributed to the model's ability to capture the
terminologies and writing styles of human-written commit messages from the
retrieved example pairs.

</details>


### [2] [Reading Between the Lines: Scalable User Feedback via Implicit Sentiment in Developer Prompts](https://arxiv.org/abs/2509.18361)
*Daye Nam,Malgorzata Salawa,Satish Chandra*

Main category: cs.SE

TL;DR: 提出使用开发者提示的情感分析来评估对话AI助手满意度，该方法比显式用户反馈率高13倍以上


<details>
  <summary>Details</summary>
Motivation: 现有方法难以大规模评估开发者对对话AI助手的满意度，用户研究不可扩展，而大规模定量信号又过于浅层或稀疏

Method: 分析372名专业开发者的工业使用日志，通过情感分析识别开发者提示中的隐式满意度信号

Result: 该方法能在约8%的交互中识别信号，比显式用户反馈率高13倍以上，即使使用现成的情感分析方法也能达到合理准确度

Conclusion: 这种新方法为构建更全面的开发者体验理解开辟了新方向，可以补充现有反馈渠道

Abstract: Evaluating developer satisfaction with conversational AI assistants at scale
is critical but challenging. User studies provide rich insights, but are
unscalable, while large-scale quantitative signals from logs or in-product
ratings are often too shallow or sparse to be reliable. To address this gap, we
propose and evaluate a new approach: using sentiment analysis of developer
prompts to identify implicit signals of user satisfaction. With an analysis of
industrial usage logs of 372 professional developers, we show that this
approach can identify a signal in ~8% of all interactions, a rate more than 13
times higher than explicit user feedback, with reasonable accuracy even with an
off-the-shelf sentiment analysis approach. This new practical approach to
complement existing feedback channels would open up new directions for building
a more comprehensive understanding of the developer experience at scale.

</details>


### [3] [SC2Tools: StarCraft II Toolset and Dataset API](https://arxiv.org/abs/2509.18454)
*Andrzej Białecki,Piotr Białecki,Piotr Sowiński,Mateusz Budziak,Jan Gajewski*

Main category: cs.SE

TL;DR: 本文介绍了SC2Tools工具集，用于简化StarCraft 2等游戏的数据集创建和处理工作，旨在降低游戏和电竞研究的技术门槛。


<details>
  <summary>Details</summary>
Motivation: 游戏作为完全受控的模拟环境在强化学习研究中具有重要价值，但数据收集和预处理的技术负担阻碍了非技术背景的研究者参与游戏和电竞研究领域。

Method: 开发了模块化的SC2Tools工具集，包含多个子模块用于处理和生产大型数据集，提供了PyTorch和PyTorch Lightning API接口。

Result: 创建了迄今为止最大的StarCraft 2锦标赛数据集，工具集不仅限于StarCraft 2，还可用于其他类型数据的处理。

Conclusion: 减轻数据收集和预处理负担对于技术能力较弱的研究者参与游戏研究至关重要，该工具为StarCraft 2实验工作流程的标准化提供了基础工作。

Abstract: Computer games, as fully controlled simulated environments, have been
utilized in significant scientific studies demonstrating the application of
Reinforcement Learning (RL). Gaming and esports are key areas influenced by the
application of Artificial Intelligence (AI) and Machine Learning (ML) solutions
at scale. Tooling simplifies scientific workloads and is essential for
developing the gaming and esports research area.
  In this work, we present ``SC2Tools'', a toolset containing multiple
submodules responsible for working with, and producing larger datasets. We
provide a modular structure of the implemented tooling, leaving room for future
extensions where needed. Additionally, some of the tools are not StarCraft~2
exclusive and can be used with other types of data for dataset creation.
  The tools we present were leveraged in creating one of the largest
StarCraft~2 tournament datasets to date with a separate PyTorch and PyTorch
Lightning application programming interface (API) for easy access to the data.
  We conclude that alleviating the burden of data collection, preprocessing,
and custom code development is essential for less technically proficient
researchers to engage in the growing gaming and esports research area. Finally,
our solution provides some foundational work toward normalizing experiment
workflow in StarCraft~2

</details>


### [4] [Locking Down Science Gateways](https://arxiv.org/abs/2509.18548)
*Steven R Brandt,Max Morris,Patrick Diehl,Christopher Bowen,Jacob Tucker,Lauren Bristol,Golden G. Richard III*

Main category: cs.SE

TL;DR: 本文探讨了Linux内核新安全特性Landlock在科学网关应用中的实用性，通过修改三个成熟的科学代码来验证其安全效果，并实现了一个基于Landlock的安全科学网关。


<details>
  <summary>Details</summary>
Motivation: 科学网关应用在启动MPI时需要网络访问，但为了安全考虑，在读取用户提供的参数文件前应撤销网络访问权限。Landlock作为Linux内核的新安全特性，能够实现运行中进程的资源访问控制。

Method: 修改并锁定三个成熟的科学代码：爱因斯坦工具包（研究相对论天体物理动力学）、Octo-Tiger（研究非相对论天体物理动力学）和FUKA（相对论代码的初始数据求解器），实现基于Landlock的FUKA科学网关。

Result: 成功验证了Landlock在科学网关安全防护中的有效性，实现了无需用户认证的安全保护机制。

Conclusion: Landlock是一个有效的安全工具，能够为科学网关应用提供必要的安全防护，特别是在控制网络访问权限方面表现出色。

Abstract: The most recent Linux kernels have a new feature for securing applications:
Landlock. Like Seccomp before it, Landlock makes it possible for a running
process to give up access to resources. For applications running as Science
Gateways, network access is required while starting up MPI, but for the sake of
security, it should be taken away prior to the reading of user-supplied
parameter files. We explore the usefulness of Landlock by modifying and locking
down three mature scientific codes: The Einstein Toolkit (a code that studies
the dynamics of relativistic astrophysics, e.g. neutron star collisions),
Octo-Tiger (a code for studying the dynamics of non-relativistic astrophysics,
e.g. white dwarfs), and FUKA (an initial data solver for relativistic codes).
Finally, we implement a fully-functioning FUKA science gateway that relies on
Landlock (instead of user authentication) for security.

</details>


### [5] [SR-Eval: Evaluating LLMs on Code Generation under Stepwise Requirement Refinement](https://arxiv.org/abs/2509.18808)
*Zexun Zhan,Shuzheng Gao,Ruida Hu,Cuiyun Gao*

Main category: cs.SE

TL;DR: SR-Eval是一个专门评估大语言模型在逐步需求细化下迭代代码生成能力的基准测试，涵盖函数级和仓库级任务，结果显示当前模型在此任务上表现仍然具有挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要将代码生成视为静态、单轮问题，忽视了真实软件开发中的逐步需求变化和迭代工作流程，限制了理解LLMs支持真实开发工作流程的能力。

Method: 采用多智能体需求生成方法模拟开发过程，从最终需求中恢复多轮交互过程，并使用语义感知判别性测试用例生成组件确保每轮评估的一致性和判别性。

Result: 评估11个代表性LLMs，结果显示迭代代码生成任务极具挑战性：最佳模型在函数级任务完成率仅为22.67%，仓库级任务为20.00%。提示策略对性能有显著影响。

Conclusion: 逐步需求细化下的迭代代码生成仍然是一个高度挑战性的任务，需要开发更先进的方法来提升LLMs在此类任务上的表现。

Abstract: Large language models (LLMs) have achieved remarkable progress in code
generation. However, existing benchmarks mainly formalize the task as a static,
single-turn problem, overlooking the stepwise requirement changes and iterative
workflows in real-world software development. This mismatch limits the
understanding of how well LLMs can support real-world development workflows.
Constructing such iterative benchmarks is challenging due to the lack of public
interaction traces and the difficulty of creating discriminative, turn-specific
test cases.
  To bridge this gap, we present SR-Eval, a benchmark specifically designed to
assess LLMs on iterative code generation under Stepwise requirements
Refinement. SR-Eval spans both function-level and repository-level tasks in
Python and Java, enabling fine-grained and progressive evaluation across
evolving requirements. The construction of SR-Eval follows a carefully designed
pipeline that first leverages a multi-agent-based requirement generation method
to simulate the development process and recover the multi-round interaction
process from final requirements, then employs a semantic-aware discriminative
test case generation component to ensure discriminative and consistent
evaluation at each turn. SR-Eval comprises 443 multi-turn tasks and 1,857
questions at both function and repository levels. Using SR-Eval, we evaluate 11
representative LLMs with three prompting strategies that simulate different
usage patterns. Results show that iterative code generation under stepwise
requirement refinement remains highly challenging: the best-performing model
achieves only 22.67% completion rate on function-level tasks and 20.00% on
repository-level tasks. We further observe that prompting strategies
substantially influence performance, highlighting the need for the development
of advanced methods.

</details>


### [6] [On the Soundness and Consistency of LLM Agents for Executing Test Cases Written in Natural Language](https://arxiv.org/abs/2509.19136)
*Sébastien Salva,Redha Taguelmimt*

Main category: cs.SE

TL;DR: 本文研究了使用大语言模型直接执行自然语言测试用例的方法，通过引入护栏机制和专用代理来解决自然语言测试用例的不健全性和执行不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 自然语言测试用例作为替代手动编写可执行测试脚本的潜在方向，具有开发成本低和维护容易的优势，但其固有的不健全性和执行不一致性限制了实际应用。

Method: 提出了一种带有护栏机制和专用代理的算法，动态验证每个测试步骤的正确执行，并引入了评估LLM测试执行能力的指标和执行一致性的量化方法。

Result: 实验评估了8个公开可用的LLM（3B到70B参数），结果显示Meta Llama 3.1 70B在自然语言测试用例执行中表现出可接受的能力，执行一致性高于3-sigma水平。

Conclusion: 当前LLM代理在GUI测试中具有潜力但仍存在局限性，提出的方法能够有效提高自然语言测试用例的执行可靠性和一致性。

Abstract: The use of natural language (NL) test cases for validating graphical user
interface (GUI) applications is emerging as a promising direction to manually
written executable test scripts, which are costly to develop and difficult to
maintain. Recent advances in large language models (LLMs) have opened the
possibility of the direct execution of NL test cases by LLM agents. This paper
investigates this direction, focusing on the impact on NL test case unsoundness
and on test case execution consistency. NL test cases are inherently unsound,
as they may yield false failures due to ambiguous instructions or unpredictable
agent behaviour. Furthermore, repeated executions of the same NL test case may
lead to inconsistent outcomes, undermining test reliability. To address these
challenges, we propose an algorithm for executing NL test cases with guardrail
mechanisms and specialised agents that dynamically verify the correct execution
of each test step. We introduce measures to evaluate the capabilities of LLMs
in test execution and one measure to quantify execution consistency. We propose
a definition of weak unsoundness to characterise contexts in which NL test case
execution remains acceptable, with respect to the industrial quality levels Six
Sigma. Our experimental evaluation with eight publicly available LLMs, ranging
from 3B to 70B parameters, demonstrates both the potential and current
limitations of current LLM agents for GUI testing. Our experiments show that
Meta Llama 3.1 70B demonstrates acceptable capabilities in NL test case
execution with high execution consistency (above the level 3-sigma). We provide
prototype tools, test suites, and results.

</details>


### [7] [An Empirical Study of Testing Practices in Open Source AI Agent Frameworks and Agentic Applications](https://arxiv.org/abs/2509.19185)
*Mohammed Mehedi Hasan,Hao Li,Emad Fallahzadeh,Gopi Krishnan Rajbahadur,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本文首次对AI代理生态系统中的测试实践进行大规模实证研究，分析了39个开源代理框架和439个代理应用，发现测试努力存在根本性倒置：确定性组件消耗超过70%的测试努力，而基于基础模型的计划主体仅获得不到5%的测试关注。


<details>
  <summary>Details</summary>
Motivation: 基础模型(FM)驱动的AI代理在广泛应用中面临非确定性和不可重现性的测试挑战，目前缺乏对开发者如何验证这些代理内部正确性的理解。

Method: 通过对39个开源代理框架和439个代理应用进行实证分析，识别出十种不同的测试模式，并将这些模式映射到代理框架和应用的规范架构组件中。

Result: 研究发现传统测试模式被广泛采用，而新颖的代理特定方法很少使用；测试努力主要集中在确定性组件上，而触发组件（提示）和FM基础的计划主体测试严重不足。

Conclusion: 框架开发者应改进对新颖测试方法的支持，应用开发者必须采用提示回归测试，研究人员应探索采用障碍，以构建更健壮可靠的AI代理。

Abstract: Foundation model (FM)-based AI agents are rapidly gaining adoption across
diverse domains, but their inherent non-determinism and non-reproducibility
pose testing and quality assurance challenges. While recent benchmarks provide
task-level evaluations, there is limited understanding of how developers verify
the internal correctness of these agents during development.
  To address this gap, we conduct the first large-scale empirical study of
testing practices in the AI agent ecosystem, analyzing 39 open-source agent
frameworks and 439 agentic applications. We identify ten distinct testing
patterns and find that novel, agent-specific methods like DeepEval are seldom
used (around 1%), while traditional patterns like negative and membership
testing are widely adapted to manage FM uncertainty. By mapping these patterns
to canonical architectural components of agent frameworks and agentic
applications, we uncover a fundamental inversion of testing effort:
deterministic components like Resource Artifacts (tools) and Coordination
Artifacts (workflows) consume over 70% of testing effort, while the FM-based
Plan Body receives less than 5%. Crucially, this reveals a critical blind spot,
as the Trigger component (prompts) remains neglected, appearing in around 1% of
all tests.
  Our findings offer the first empirical testing baseline in FM-based agent
frameworks and agentic applications, revealing a rational but incomplete
adaptation to non-determinism. To address it, framework developers should
improve support for novel testing methods, application developers must adopt
prompt regression testing, and researchers should explore barriers to adoption.
Strengthening these practices is vital for building more robust and dependable
AI agents.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [8] [A Layered Implementation Framework for Regular Languages](https://arxiv.org/abs/2509.18232)
*Baudouin Le Charlier*

Main category: cs.FL

TL;DR: 本文提出了一个处理正则语言表示的系统，该系统分为两个层次结构，能够以更紧凑、高效和集成的方式表示正则语言。系统通过规范化正则表达式和等价类统一机制，实现了大规模正则语言集合的统一表示。


<details>
  <summary>Details</summary>
Motivation: 为了更有效地处理和表示正则语言，需要一种能够将正则表达式以紧凑、高效且统一的方式表示的系统。传统方法在处理大规模正则语言集合时存在效率低下和表示冗余的问题。

Method: 系统采用两层结构：第一层通过规范化正则表达式确保语法导数的有限性，并用整数标识符唯一表示；第二层通过等价类统一机制，选择每个等价类中最短表达式作为代表，建立表达式与其导数之间的关系方程。

Result: 实验结果表明，该框架能够有效表示大规模正则语言集合，其中不同标识符对应不同的语言，每个语言都由一个小型表达式和最小确定性自动机表示。

Conclusion: 提出的两层系统框架为正则语言的表示和处理提供了一种高效、统一的解决方案，特别适用于大规模正则语言集合的管理和操作。

Abstract: I present the most fundamental features of an implemented system designed to
manipulate representations of regular languages. The system is structured into
two layers, allowing regular languages to be represented in an increasingly
compact, efficient, and integrated way. Both layers are first presented at a
high level, adequate to design and prove the correctness of abstract
algorithms. Then, their low-level implementations are described meticulously.
  At the high level, the first layer offers a notion of normalized regular
expressions ensuring that the set of all syntactic derivatives of an expression
is finite. At the low level, normalized expressions are uniquely represented by
identifiers, i.e. by standard integers.
  The second layer, called the background, introduces additional notions to
record, integrate, and simplify things computed within the first layer. At the
high level, normalized expressions denoting the same regular language can be
unified by grouping them into equivalence classes. One shortest expression is
chosen in each class as its representative, which can be used to form equations
relating expressions to their derivatives.
  This paper also presents extensive experimental results to demonstrate the
usefulness of the proposed framework and, in particular, the fact that it makes
it possible to represent large sets of regular languages in a unified way where
distinct identifiers designate different languages, represented by both a small
expression and a minimal deteministic automaton.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [9] [A Verified Compiler for Quantum Simulation](https://arxiv.org/abs/2509.18583)
*Liyi Li,Fenfen An,Federico Zahariev,Zhi Xiang Chong,Amr Sabry,Mark Gordon*

Main category: cs.PL

TL;DR: QBlue是一个用于哈密顿量模拟的高层形式化验证编译框架，基于二次量子化形式，提供类型系统和编译正确性保证。


<details>
  <summary>Details</summary>
Motivation: 现有哈密顿量模拟编译器通常基于低层泡利算子表示，限制了可编程性且缺乏编译管道的正确性形式化保证。

Method: 基于二次量子化形式，使用产生和湮灭算子描述量子粒子系统，包含跟踪粒子类型和强制厄米结构的类型系统，支持数字和模拟量子电路编译。

Result: QBlue是首个端到端验证的二次量子化哈密顿量模拟编译器，所有组件都在Rocq证明框架中完全机械化。

Conclusion: QBlue提供了一个安全、正确且表达性强的高层哈密顿量模拟编译框架，填补了现有编译器的形式化验证空白。

Abstract: Hamiltonian simulation is a central application of quantum computing, with
significant potential in modeling physical systems and solving complex
optimization problems. Existing compilers for such simulations typically focus
on low-level representations based on Pauli operators, limiting programmability
and offering no formal guarantees of correctness across the compilation
pipeline. We introduce QBlue, a high-level, formally verified framework for
compiling Hamiltonian simulations. QBlue is based on the formalism of second
quantization, which provides a natural and expressive way to describe quantum
particle systems using creation and annihilation operators. To ensure safety
and correctness, QBlue includes a type system that tracks particle types and
enforces Hermitian structure. The framework supports compilation to both
digital and analog quantum circuits and captures multiple layers of semantics,
from static constraints to dynamic evolution. All components of QBlue,
including its language design, type system, and compilation correctness, are
fully mechanized in the Rocq proof framework, making it the first end-to-end
verified compiler for second-quantized Hamiltonian simulation.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [10] [Proceedings Seventh International Conference on Applied Category Theory 2024](https://arxiv.org/abs/2509.18357)
*Michael Johnson,David Jaz Myers*

Main category: cs.LO

TL;DR: 第七届应用范畴论国际会议论文集，涵盖从纯理论到应用的广泛科学和工程学科贡献


<details>
  <summary>Details</summary>
Motivation: 促进范畴论在不同学科领域的应用和交流，展示范畴论在科学和工程中的实用价值

Method: 通过学术会议形式汇集多学科研究成果，包括理论探讨和实际应用案例

Result: 成功组织了涵盖经典力学、量子物理、概率论、语言学、决策理论、机器学习、流行病学、热力学、工程学和逻辑学等多个领域的学术交流

Conclusion: 范畴论作为一种强大的数学工具，在跨学科研究中展现出广泛的应用潜力和价值

Abstract: Proceedings of the Seventh International Conference on Applied Category
Theory, held at the University of Oxford on 17 - 21 June 2024. The
contributions to ACT 2024 ranged from pure to applied and included
contributions in a wide range of disciplines in science and engineering. ACT
2024 included talks in classical mechanics, quantum physics, probability
theory, linguistics, decision theory, machine learning, epidemiology,
thermodynamics, engineering, and logic.

</details>


### [11] [Singleton algorithms for the Constraint Satisfaction Problem](https://arxiv.org/abs/2509.18434)
*Dmitriy Zhuk*

Main category: cs.LO

TL;DR: 本文研究了约束满足问题（CSP）的单例版本算法，通过minion同态和调色板块对称多态性来表征其能力，证明了BLP+AIP算法的单例版本可以解决域大小不超过7的所有可处理CSP问题。


<details>
  <summary>Details</summary>
Motivation: 研究CSP算法单例版本的增强能力，探索线性规划方法的局限性，并寻找更强大的算法变体。

Method: 使用minion同态理论、Hales-Jewett定理和调色板块对称多态性来表征单例版本算法的能力，并通过具体CSP模板进行验证。

Result: 证明了对于有限关系结构，minion条件等价于存在具有特定对称性的多态性；建立了BLP+AIP单例版本算法可以解决域大小≤7的所有可处理CSP问题。

Conclusion: 单例版本算法比标准算法更强大，调色板块对称多态性为理解CSP复杂性提供了优雅的理论框架，展示了线性规划方法的局限性。

Abstract: A natural strengthening of an algorithm for the (promise) constraint
satisfaction problem is its singleton version: we first fix a constraint to
some tuple from the constraint relation, then run the algorithm, and remove the
tuple from the constraint if the answer is negative. We characterize the power
of the singleton versions of standard universal algorithms for the (promise)
CSP over a fixed template in terms of the existence of a minion homomorphism.
Using the Hales-Jewett theorem, we show that for finite relational structures
this minion condition is equivalent to the existence of polymorphisms with
certain symmetries, called palette block symmetric polymorphisms. By proving
the existence of such polymorphisms we establish that the singleton version of
the BLP+AIP algorithm solves all tractable CSPs over domains of size at most 7.
Finally, by providing concrete CSP templates, we illustrate the limitations of
linear programming, the power of the singleton versions, and the elegance of
the palette block symmetric polymorphisms.

</details>
