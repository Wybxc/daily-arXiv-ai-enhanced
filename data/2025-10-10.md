<div id=toc></div>

# Table of Contents

- [cs.LO](#cs.LO) [Total: 7]
- [cs.SE](#cs.SE) [Total: 9]
- [cs.FL](#cs.FL) [Total: 2]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [1] [A Zone-Based Algorithm for Timed Parity Games](https://arxiv.org/abs/2510.07361)
*Gilles Geeraerts,Frédéric Herbreteau,Jean-François Raskin,Alexis Reynouard*

Main category: cs.LO

TL;DR: 本文基于"The Element of Surprise in Timed Games"的语义重新审视定时游戏，修改了原始语义以解决控制器合成中的反直觉问题，并提出了高效的基于区域的算法。


<details>
  <summary>Details</summary>
Motivation: 原始语义在控制器合成中存在反直觉的情况，需要改进语义以更好地支持控制器合成应用。

Method: 修改了原始定时游戏语义，开发了基于区域的高效算法，使用UppAal的区域库实现原型。

Result: 算法成功解决了定时奇偶游戏问题，原型证明了基于区域的算法对于奇偶目标和丰富定时交互语义的可行性。

Conclusion: 提出的修改语义和基于区域的算法为定时游戏提供了更直观的控制器合成方法和高效实现途径。

Abstract: This paper revisits timed games by building upon the semantics introduced in
"The Element of Surprise in Timed Games". We introduce some modifications to
this semantics for two primary reasons: firstly, we recognize instances where
the original semantics appears counterintuitive in the context of controller
synthesis; secondly, we present methods to develop efficient zone-based
algorithms. Our algorithm successfully addresses timed parity games, and we
have implemented it using UppAal's zone library. This prototype effectively
demonstrates the feasibility of a zone-based algorithm for parity objectives
and a rich semantics for timed interactions between the players.

</details>


### [2] [Homomorphism Problems in Graph Databases and Automatic Structures](https://arxiv.org/abs/2510.07422)
*Rémi Morvan*

Main category: cs.LO

TL;DR: 该论文研究了同态问题在两个互补领域中的核心作用：有限图结构数据的数据库查询和（可能无限的）结构上的约束求解。第一部分研究查询最小化问题，第二部分研究自动结构上的同态问题。


<details>
  <summary>Details</summary>
Motivation: 基于合取查询评估与同态存在之间的等价关系，研究结构保持映射在数据库查询和约束求解中的基础作用，探索查询最小化和无限结构上的可判定性问题。

Method: 第一部分：研究合取正则路径查询的最小化问题，考虑原子数量和查询图树宽两个度量标准；第二部分：研究自动结构上的同态问题，使用代数语言理论和伪变种逻辑方法。

Result: 证明了查询最小化问题在两种度量下都是可判定的，并为实践中使用的大量查询片段提供了高效算法；揭示了自动结构上同态问题的二分性：一些在非确定性对数空间可判定，而更多情况是不可判定的；证明了对于任何表现良好的逻辑，自动结构是否可用该逻辑描述是可判定的。

Conclusion: 同态问题在数据库查询和约束求解中具有核心地位，查询最小化是可判定的且存在高效算法，自动结构上的同态问题虽然普遍不可判定，但其逻辑可描述性是可判定的。

Abstract: This thesis investigates the central role of homomorphism problems
(structure-preserving maps) in two complementary domains: database querying
over finite, graph-shaped data, and constraint solving over (potentially
infinite) structures. Building on the well-known equivalence between
conjunctive query evaluation and homomorphism existence, the first part focuses
on conjunctive regular path queries, a standard extension of conjunctive
queries that incorporates regular-path predicates. We study the fundamental
problem of query minimization under two measures: the number of atoms
(constraints) and the tree-width of the query graph. In both cases, we prove
the problem to be decidable, and provide efficient algorithms for a large
fragment of queries used in practice. The second part of the thesis lifts
homomorphism problems to automatic structures, which are infinite structures
describable by finite automata. We highlight a dichotomy, between homomorphism
problems over automatic structures that are decidable in non-deterministic
logarithmic space, and those that are undecidable (proving to be the more
common case). In contrast to this prevalence of undecidability, we then focus
on the language-theoretic properties of these structures, and show, relying on
a novel algebraic language theory, that for any well-behaved logic (a
pseudovariety), whether an automatic structure can be described in this logic
is decidable.

</details>


### [3] [Verifying Graph Neural Networks with Readout is Intractable](https://arxiv.org/abs/2510.08045)
*Artem Chernobrovkin,Marco Sälzer,François Schwarzentruber,Nicolas Troquard*

Main category: cs.LO

TL;DR: 提出了一个用于量化聚合-组合图神经网络（ACR-GNNs）的逻辑语言，证明了量化GNN验证任务的（co)NEXPTIME完全性，表明其计算不可行性，同时实验显示量化模型轻量且保持良好性能。


<details>
  <summary>Details</summary>
Motivation: 量化图神经网络的验证任务在计算上具有挑战性，需要开发逻辑框架来确保GNN基系统的安全性。

Method: 引入一个逻辑语言来推理量化ACR-GNNs，提供逻辑特征化，并用于证明量化GNN验证任务的复杂性。

Result: 证明量化GNN验证是(co)NEXPTIME完全的，实验表明量化ACR-GNN模型轻量且保持与非量化模型相当的准确性和泛化能力。

Conclusion: 量化GNN验证在计算上不可行，但量化模型在保持性能的同时显著减少了计算资源需求。

Abstract: We introduce a logical language for reasoning about quantized
aggregate-combine graph neural networks with global readout (ACR-GNNs). We
provide a logical characterization and use it to prove that verification tasks
for quantized GNNs with readout are (co)NEXPTIME-complete. This result implies
that the verification of quantized GNNs is computationally intractable,
prompting substantial research efforts toward ensuring the safety of GNN-based
systems. We also experimentally demonstrate that quantized ACR-GNN models are
lightweight while maintaining good accuracy and generalization capabilities
with respect to non-quantized models.

</details>


### [4] [Implication Problems over Positive Semirings](https://arxiv.org/abs/2510.08112)
*Minna Hirvonen*

Main category: cs.LO

TL;DR: 研究半环团队语义中的各种依赖概念，包括功能依赖、包含依赖、边际同一性和独立性，并考察其蕴含问题的公理化。


<details>
  <summary>Details</summary>
Motivation: 为数据库理论和概率论中的依赖概念提供统一的半环团队语义框架，能够同时研究不同语义（如关系、包和概率语义）下的依赖问题。

Method: 使用半环团队语义作为通用框架，将数据库关系中的元组用正半环元素进行标注，推广各种依赖概念并研究其蕴含问题的公理化。

Result: 建立了半环团队语义下多种依赖概念的通用框架，能够统一处理不同语义下的依赖蕴含问题。

Conclusion: 半环团队语义为研究不同语义下的依赖概念提供了统一的数学框架，通过选择不同的半环可以获得特定的语义解释。

Abstract: We study various notions of dependency in semiring team semantics. Semiring
teams are essentially database relations, where each tuple is annotated with
some element from a positive semiring. We consider semiring generalizations of
several dependency notions from database theory and probability theory,
including functional and inclusion dependencies, marginal identity, and
(probabilistic) independence. We examine axiomatizations of implication
problems, which are rule-based characterizations for the logical implication
and inference of new dependencies from a given set of dependencies. Semiring
team semantics provides a general framework, where different implication
problems can be studied simultaneously for various semirings. The choice of the
semiring leads to a specific semantic interpretation of the dependencies, and
hence different semirings offer a way to study different semantics (e.g.,
relational, bag, and probabilistic semantics) in a unified framework.

</details>


### [5] [Complexity Results in Team Semantics: Nonemptiness Is Not So Complex](https://arxiv.org/abs/2510.08122)
*Aleksi Anttila,Juha Kontinen,Fan Yang*

Main category: cs.LO

TL;DR: 该论文研究了团队语义中凸逻辑的复杂性理论性质，重点关注带有非空原子NE的经典命题逻辑扩展，证明了其可满足性问题为NP完全，有效性问题是coNP完全，模型检测问题在P中。


<details>
  <summary>Details</summary>
Motivation: 研究团队语义中凸逻辑的计算复杂性，特别是带有非空原子NE的逻辑扩展，填补该领域复杂性分析的空白。

Method: 通过复杂性理论分析，证明带有NE原子的命题逻辑的可满足性、有效性和模型检测问题的计算复杂度。

Result: 可满足性问题为NP完全，有效性问题为coNP完全，模型检测问题在P类中。

Conclusion: 该凸逻辑扩展具有可处理的计算复杂度特性，为团队语义中逻辑的复杂性分析提供了重要基准。

Abstract: We initiate the study of the complexity-theoretic properties of convex logics
in team semantics. We focus on the extension of classical propositional logic
with the nonemptiness atom NE, a logic known to be both convex and union
closed. We show that the satisfiability problem for this logic is NP-complete,
that its validity problem is coNP-complete, and that its model-checking problem
is in P.

</details>


### [6] [Compression for Coinductive Infinitary Rewriting: A Generic Approach, with Applications to Cut-Elimination for Non-Wellfounded Proofs](https://arxiv.org/abs/2510.08420)
*Rémy Cerda,Alexis Saurin*

Main category: cs.LO

TL;DR: 本文研究了无穷重写系统中的压缩性质，证明了任意序数长度的重写序列可以压缩为长度不超过ω的等价序列，并给出了基于共归纳的通用证明方法。


<details>
  <summary>Details</summary>
Motivation: 无穷重写系统是描述非终止但具有生产力的重写系统动态行为的便利框架。压缩性质是该框架中高度期望的特性，能够将任意序数长度的重写序列压缩为可数的等价序列。

Method: 扩展了基于共归纳的无穷重写表示方法，设计了一个通用的压缩证明框架，通过特征化分解证明过程，识别出可压缩无穷重写系统应满足的关键性质。

Result: 成功证明了无穷重写系统的压缩性质，特别为无穷λ演算的共归纳表示提供了理论依据，并应用于非良基证明系统μMALL∞中的消去序列压缩。

Conclusion: 提出的通用压缩证明框架适用于多种无穷重写系统，包括一阶重写、无穷λ演算和线性逻辑证明系统，为这些系统的理论研究提供了重要工具。

Abstract: Infinitary rewriting, i.e. rewriting featuring possibly infinite terms and
sequences of reduction, is a convenient framework for describing the dynamics
of non-terminating but productive rewriting systems. In its original definition
based on metric convergence of ordinal-indexed sequences of rewriting steps, a
highly desirable property of an infinitary rewriting system is Compression,
i.e. the fact that rewriting sequences of arbitrary ordinal length can always
be 'compressed' to equivalent sequences of length at most {\omega}.
  Since then, the standard examples of infinitary rewriting systems have been
given another equivalent presentation based on coinduction. In this work, we
extend this presentation to the rewriting of arbitrary non-wellfounded
derivations and we investigate compression in this setting. We design a generic
proof of compression, relying on a characterisation factorising most of the
proof and identifying the key property a compressible infinitary rewriting
system should enjoy.
  As running examples, we discuss first-order rewriting and infinitary
{\lambda}-calculi. For the latter, compression can in particular be seen as a
justification of its coinductive presentation in the literature. As a more
advanced example, we also address compression of cut-elimination sequences in
the non-wellfounded proof system {\mu}MALL{\infty} for multiplicative-additive
linear logics with fixed points, which is a key lemma of several
cut-elimination results for similar proof systems.

</details>


### [7] [Dynamic Automated Deduction by Contradiction Separation: The Standard Extension Algorithm](https://arxiv.org/abs/2510.08468)
*Yang Xu,Xingxing He,Shuwei Chen,Jun Liu,Xiaomei Zhong*

Main category: cs.LO

TL;DR: 本文提出了标准扩展算法，这是第一个明确实现矛盾分离推理的程序化方法，通过互补文字扩展动态构建矛盾，将CSE理论操作化用于可满足性和不可满足性检查。


<details>
  <summary>Details</summary>
Motivation: 经典基于归结的系统（如Prover9、E、Vampire）依赖二元推理，限制了证明搜索中的多子句协同。虽然CSE框架理论上克服了这一限制，但原始工作未说明矛盾如何算法化构建和扩展。

Method: 提出标准扩展算法，通过互补文字扩展动态构建矛盾，在统一算法中实现可满足性和不可满足性检查。算法的正确性和完备性得到形式化证明。

Result: 基于CSE的系统（CSE、CSE-E、CSI-E、CSI-Enig）在主要自动推理竞赛（CASC）中的表现间接支持了该算法的有效性。

Conclusion: 标准扩展机制为动态、多子句的自动推理提供了稳健且经过实践验证的基础。

Abstract: Automated deduction seeks to enable machines to reason with mathematical
precision and logical completeness. Classical resolution-based systems, such as
Prover9, E, and Vampire, rely on binary inference, which inherently limits
multi-clause synergy during proof search. The Contradiction Separation
Extension (CSE) framework, introduced by Xu et al. (2018), overcame this
theoretical limitation by extending deduction beyond binary inference. However,
the original work did not specify how contradictions are algorithmically
constructed and extended in practice. This paper presents the Standard
Extension algorithm, the first explicit procedural realization of contradiction
separation reasoning. The proposed method dynamically constructs contradictions
through complementary literal extension, thereby operationalizing the CSE
theory within a unified algorithm for satisfiability and unsatisfiability
checking. The algorithm's soundness and completeness are formally proven, and
its effectiveness is supported indirectly through the performance of CSE-based
systems, including CSE, CSE-E, CSI-E, and CSI-Enig in major automated reasoning
competitions (CASC) in the last few years. These results confirm that the
Standard Extension mechanism constitutes a robust and practically validated
foundation for dynamic, multi-clause automated deduction.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [8] [Modeling Developer Burnout with GenAI Adoption](https://arxiv.org/abs/2510.07435)
*Zixuan Feng,Sadia Afroz,Anita Sarma*

Main category: cs.SE

TL;DR: 研究发现GenAI采用通过增加工作需求加剧开发者倦怠，但工作资源和积极认知可缓解此效应


<details>
  <summary>Details</summary>
Motivation: 研究GenAI采用与开发者倦怠的关系，关注AI工具在提升生产力同时可能带来的负面影响

Method: 采用混合方法研究设计，调查442名开发者，使用PLS-SEM和回归分析建模，辅以定性分析

Result: GenAI采用通过增加工作需求导致倦怠加重，但工作资源和积极认知可减轻这种负面影响

Conclusion: GenAI采用既是挑战也是机遇，组织应关注工作资源建设以缓解AI带来的倦怠风险

Abstract: Generative AI (GenAI) is rapidly reshaping software development workflows.
While prior studies emphasize productivity gains, the adoption of GenAI also
introduces new pressures that may harm developers' well-being. In this paper,
we investigate the relationship between the adoption of GenAI and developers'
burnout. We utilized the Job Demands--Resources (JD--R) model as the analytic
lens in our empirical study. We employed a concurrent embedded mixed-methods
research design, integrating quantitative and qualitative evidence. We first
surveyed 442 developers across diverse organizations, roles, and levels of
experience. We then employed Partial Least Squares--Structural Equation
Modeling (PLS-SEM) and regression to model the relationships among job demands,
job resources, and burnout, complemented by a qualitative analysis of
open-ended responses to contextualize the quantitative findings. Our results
show that GenAI adoption heightens burnout by increasing job demands, while job
resources and positive perceptions of GenAI mitigate these effects, reframing
adoption as an opportunity.

</details>


### [9] [HotBugs.jar: A Benchmark of Hot Fixes for Time-Critical Bugs](https://arxiv.org/abs/2510.07529)
*Carol Hanna,Federica Sarro,Mark Harman,Justyna Petke*

Main category: cs.SE

TL;DR: HotBugs.jar是首个专注于真实世界热修复的数据集，包含679个手动验证的热修复案例，其中110个可复现，为快速调试、自动修复和系统弹性研究提供基准。


<details>
  <summary>Details</summary>
Motivation: 尽管热修复对生产系统至关重要，但现有评估基准缺乏专门针对热修复的数据集，阻碍了相关工具的研究和发展。

Method: 通过挖掘10个Apache项目的19万次提交和15万份问题报告，识别746个热修复补丁，经手动评估确认679个真实热修复，其中110个可复现测试。

Result: 构建了HotBugs.jar数据集，包含679个手动验证的热修复案例（110个可复现），每个案例包含错误和修复版本、测试套件和元数据。

Conclusion: HotBugs.jar填补了热修复研究领域的空白，已被SBSE会议采纳为官方挑战数据集，将推动快速调试、自动修复和系统弹性工具的研究。

Abstract: Hot fixes are urgent, unplanned changes deployed to production systems to
address time-critical issues. Despite their importance, no existing evaluation
benchmark focuses specifically on hot fixes. We present HotBugs$.$jar, the
first dataset dedicated to real-world hot fixes. From an initial mining of 10
active Apache projects totaling over 190K commits and 150K issue reports, we
identified 746 software patches that met our hot-fix criteria. After manual
evaluation, 679 were confirmed as genuine hot fixes, of which 110 are
reproducible using a test suite. Building upon the Bugs$.$jar framework,
HotBugs$.$jar integrates these 110 reproducible cases and makes available all
679 manually validated hot fixes, each enriched with comprehensive metadata to
support future research. Each hot fix was systematically identified using Jira
issue data, validated by independent reviewers, and packaged in a reproducible
format with buggy and fixed versions, test suites, and metadata. HotBugs$.$jar
has already been adopted as the official challenge dataset for the Search-Based
Software Engineering (SBSE) Conference Challenge Track, demonstrating its
immediate impact. This benchmark enables the study and evaluation of tools for
rapid debugging, automated repair, and production-grade resilience in modern
software systems to drive research in this essential area forward.

</details>


### [10] [RustAssure: Differential Symbolic Testing for LLM-Transpiled C-to-Rust Code](https://arxiv.org/abs/2510.07604)
*Yubo Bai,Tapti Palit*

Main category: cs.SE

TL;DR: RustAssure是一个使用大语言模型自动将C代码转换为Rust的系统，通过提示工程生成惯用安全的Rust代码，并采用差分符号测试验证语义等价性。


<details>
  <summary>Details</summary>
Motivation: 现有C代码库需要转换为Rust才能利用其内存安全特性，但手动转换成本高且易出错。

Method: 使用LLM进行C到Rust的自动转换，结合提示工程技术生成高质量代码，并通过差分符号测试验证转换后的语义等价性。

Result: 在5个真实应用中，系统为89.8%的C函数生成了可编译的Rust代码，其中69.9%的函数在C和Rust版本间产生了等价的符号返回值。

Conclusion: RustAssure能够有效自动化C到Rust的转换过程，生成高质量且语义等价的Rust代码。

Abstract: Rust is a memory-safe programming language that significantly improves
software security. Existing codebases written in unsafe memory languages, such
as C, must first be transpiled to Rust to take advantage of Rust's improved
safety guarantees. RustAssure presents a system that uses Large Language Models
(LLMs) to automatically transpile existing C codebases to Rust. RustAssure uses
prompt engineering techniques to maximize the chances of the LLM generating
idiomatic and safe Rust code. Moreover, because LLMs often generate code with
subtle bugs that can be missed under traditional unit or fuzz testing,
RustAssure performs differential symbolic testing to establish the semantic
similarity between the original C and LLM-transpiled Rust code. We evaluated
RustAssure with five real-world applications and libraries, and showed that our
system is able to generate compilable Rust functions for 89.8% of all C
functions, of which 69.9% produced equivalent symbolic return values for both
the C and Rust functions.

</details>


### [11] [AppForge: From Assistant to Independent Developer -- Are GPTs Ready for Software Development?](https://arxiv.org/abs/2510.07740)
*Dezhi Ran,Yuan Cao,Mengzhou Wu,Simin Chen,Yuzhe Guo,Jun Ren,Zihe Song,Hao Yu,Jialei Wei,Linyi Li,Wei Yang,Baishakhi Ray,Tao Xie*

Main category: cs.SE

TL;DR: APPFORGE是一个评估LLMs构建完整软件系统能力的基准，包含101个真实Android应用开发问题，测试显示当前最佳模型仅能开发18.8%功能正确的应用。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估函数级代码生成，而真实软件开发需要协调多个组件、管理状态生命周期和异步操作，缺乏评估LLMs构建完整软件系统能力的基准。

Method: 设计多智能体系统自动从应用文档中总结主要功能并导航应用合成测试用例，构建包含测试用例的自动化评估框架。

Result: 评估12个主流LLMs，所有模型表现均不佳，最佳模型GPT-5仅能开发18.8%功能正确的应用。

Conclusion: 当前LLMs在处理复杂、多组件的软件工程挑战方面存在根本性局限。

Abstract: Large language models (LLMs) have demonstrated remarkable capability in
function-level code generation tasks. Unlike isolated functions, real-world
applications demand reasoning over the entire software system: developers must
orchestrate how different components interact, maintain consistency across
states over time, and ensure the application behaves correctly within the
lifecycle and framework constraints. Yet, no existing benchmark adequately
evaluates whether LLMs can bridge this gap and construct entire software
systems from scratch. To address this gap, we propose APPFORGE, a benchmark
consisting of 101 software development problems drawn from real-world Android
apps. Given a natural language specification detailing the app functionality, a
language model is tasked with implementing the functionality into an Android
app from scratch. Developing an Android app from scratch requires understanding
and coordinating app states, lifecycle management, and asynchronous operations,
calling for LLMs to generate context-aware, robust, and maintainable code. To
construct APPFORGE, we design a multi-agent system to automatically summarize
the main functionalities from app documents and navigate the app to synthesize
test cases validating the functional correctness of app implementation.
Following rigorous manual verification by Android development experts, APPFORGE
incorporates the test cases within an automated evaluation framework that
enables reproducible assessment without human intervention, making it easily
adoptable for future research. Our evaluation on 12 flagship LLMs show that all
evaluated models achieve low effectiveness, with the best-performing model
(GPT-5) developing only 18.8% functionally correct applications, highlighting
fundamental limitations in current models' ability to handle complex,
multi-component software engineering challenges.

</details>


### [12] [Interleaved Learning and Exploration: A Self-Adaptive Fuzz Testing Framework for MLIR](https://arxiv.org/abs/2510.07815)
*Zeyu Sun,Jingjing Liang,Weiyi Wang,Chenyao Suo,Junjie Chen,Fanjiang Xu*

Main category: cs.SE

TL;DR: FLEX是一个基于神经网络的MLIR自适应模糊测试框架，通过扰动采样和反馈驱动的增强循环，能够自主生成高质量测试用例，显著提升MLIR编译器的bug检测能力。


<details>
  <summary>Details</summary>
Motivation: MLIR作为现代编译器框架的基础技术，其正确性和鲁棒性验证面临挑战。现有模糊测试方法难以生成足够多样化和语义有效的测试用例，无法发现MLIR复杂代码空间中的深层bug。

Method: FLEX采用神经网络进行程序生成，结合扰动采样策略促进多样性，并通过反馈驱动的增强循环迭代改进模型，利用崩溃和非崩溃测试用例持续学习。

Result: 在30天测试中发现了80个未知bug，包括多个新的根本原因和解析器bug；在24小时固定版本比较中检测到53个bug（超过最佳基线的3.5倍），代码覆盖率达到28.2%，比次优工具高42%。

Conclusion: FLEX通过神经程序生成和自适应学习机制，显著提升了MLIR模糊测试的效果，消融研究证实了扰动生成和多样性增强在框架有效性中的关键作用。

Abstract: MLIR (Multi-Level Intermediate Representation) has rapidly become a
foundational technology for modern compiler frameworks, enabling extensibility
across diverse domains. However, ensuring the correctness and robustness of
MLIR itself remains challenging. Existing fuzzing approaches-based on manually
crafted templates or rule-based mutations-struggle to generate sufficiently
diverse and semantically valid test cases, making it difficult to expose subtle
or deep-seated bugs within MLIR's complex and evolving code space. In this
paper, we present FLEX, a novel self-adaptive fuzzing framework for MLIR. FLEX
leverages neural networks for program generation, a perturbed sampling strategy
to encourage diversity, and a feedback-driven augmentation loop that
iteratively improves its model using both crashing and non-crashing test cases.
Starting from a limited seed corpus, FLEX progressively learns valid syntax and
semantics and autonomously produces high-quality test inputs. We evaluate FLEX
on the upstream MLIR compiler against four state-of-the-art fuzzers. In a
30-day campaign, FLEX discovers 80 previously unknown bugs-including multiple
new root causes and parser bugs-while in 24-hour fixed-revision comparisons, it
detects 53 bugs (over 3.5x as many as the best baseline) and achieves 28.2%
code coverage, outperforming the next-best tool by 42%. Ablation studies
further confirm the critical role of both perturbed generation and diversity
augmentation in FLEX's effectiveness.

</details>


### [13] [Bug Histories as Sources of Compiler Fuzzing Mutators](https://arxiv.org/abs/2510.07834)
*Lingjun Liu,Feiran Qin,Owolabi Legunsen,Marcelo d'Amorim*

Main category: cs.SE

TL;DR: IssueMut从编译器bug历史中提取变异器，用于改进编译器模糊测试，在GCC和LLVM中发现了65个新bug。


<details>
  <summary>Details</summary>
Motivation: 编译器bug影响重大，现有变异模糊测试器效果依赖于变异器质量，但从未利用bug历史作为变异器来源。

Method: 从bug报告中自动挖掘变异器，并将其集成到现有变异编译器模糊测试器中。

Result: 从1760个GCC和LLVM bug报告中挖掘587个变异器，在GCC中发现28个新bug，LLVM中发现37个新bug，其中60个被确认或修复。

Conclusion: bug历史包含丰富信息，编译器模糊测试器应充分利用这些信息来发现更多bug。

Abstract: Bugs in compilers, which are critical infrastructure today, can have outsized
negative impacts. Mutational fuzzers aid compiler bug detection by
systematically mutating compiler inputs, i.e., programs. Their effectiveness
depends on the quality of the mutators used. Yet, no prior work used compiler
bug histories as a source of mutators. We propose IssueMut, the first approach
for extracting compiler fuzzing mutators from bug histories. Our insight is
that bug reports contain hints about program elements that induced compiler
bugs; they can guide fuzzers towards similar bugs. IssueMut uses an automated
method to mine mutators from bug reports and retrofit such mutators into
existing mutational compiler fuzzers. Using IssueMut, we mine 587 mutators from
1760 GCC and LLVM bug reports. Then, we run IssueMut on these compilers, with
all their test inputs as seed corpora. We find that "bug history" mutators are
effective: they find new bugs that a state-of-the-art mutational compiler
fuzzer misses-28 in GCC and 37 in LLVM. Of these, 60 were confirmed or fixed,
validating our idea that bug histories have rich information that compiler
fuzzers should leverage.

</details>


### [14] [An AUTOSAR-Aligned Architectural Study of Vulnerabilities in Automotive SoC Software](https://arxiv.org/abs/2510.07941)
*Srijita Basu,Haraldsson Bengt,Miroslaw Staron,Christian Berger,Jennifer Horkoff,Magnus Almgren*

Main category: cs.SE

TL;DR: 该研究分析了180个公开报告的汽车SoC漏洞，识别出16个根本原因和56个受影响的软件模块，揭示了AUTOSAR架构中的主要漏洞模式和关键模块的补丁延迟问题。


<details>
  <summary>Details</summary>
Motivation: 随着汽车SoC相关漏洞激增，但在AUTOSAR对齐架构中缺乏对其根本原因和影响的系统性分析，本研究填补了这一空白。

Method: 分析180个公开报告的汽车SoC漏洞，将其映射到符合AUTOSAR原则的代表性SoC软件架构模型中，识别根本原因和受影响模块。

Result: 识别出16个根本原因和56个受影响软件模块，发现主要漏洞模式和关键模块的补丁延迟问题，分析了CWE类别和架构层的缓解延迟。

Conclusion: 为保护汽车CPS平台提供了可行的见解，包括改进检测、优先级排序和定位策略的指南，以增强SoC软件架构的安全性。

Abstract: Cooperative, Connected and Automated Mobility (CCAM) are complex
cyber-physical systems (CPS) that integrate computation, communication, and
control in safety-critical environments. At their core, System-on-Chip (SoC)
platforms consolidate processing units, communication interfaces, AI
accelerators, and security modules into a single chip. AUTOSAR (AUTomotive Open
System ARchitecture) standard was developed in the automotive domain to better
manage this complexity, defining layered software structures and interfaces to
facilitate reuse of HW/SW components. However, in practice, this integrated SoC
software architecture still poses security challenges, particularly in
real-time, safety-critical environments. Recent reports highlight a surge in
SoC-related vulnerabilities, yet systematic analysis of their root causes and
impact within AUTOSAR-aligned architectures is lacking. This study fills that
gap by analyzing 180 publicly reported automotive SoC vulnerabilities, mapped
to a representative SoC software architecture model that is aligned with
AUTOSAR principles for layered abstraction and service orientation. We identify
16 root causes and 56 affected software modules, and examine mitigation delays
across Common Weakness Enumeration (CWE) categories and architectural layers.
We uncover dominant vulnerability patterns and critical modules with prolonged
patch delays, and provide actionable insights for securing automotive CPS
platforms, including guides for improved detection, prioritization, and
localization strategies for SoC software architectures in SoC-based vehicle
platforms.

</details>


### [15] [Past, Present, and Future of Bug Tracking in the Generative AI Era](https://arxiv.org/abs/2510.08005)
*Utku Boran Torun,Mehmet Taha Demircan,Mahmut Furkan Gön,Eray Tüzün*

Main category: cs.SE

TL;DR: 提出基于大语言模型的AI驱动bug追踪框架，通过智能自动化减少人工参与，缩短bug修复时间


<details>
  <summary>Details</summary>
Motivation: 传统bug追踪系统依赖人工报告、复现和解决，存在沟通成本高、响应延迟等问题，需要更高效的自动化解决方案

Method: 构建AI驱动的bug追踪框架，使用LLM自动化处理自然语言报告、精炼报告内容、尝试复现、分类报告、生成候选补丁等任务

Result: 该框架能够加速响应时间，改善协作效率，减少人工开销，提升软件维护实践

Conclusion: 通过在每个阶段集成自动化，该AI驱动的bug追踪框架为更高效、以用户为中心的未来提供了解决方案

Abstract: Traditional bug tracking systems rely heavily on manual reporting,
reproduction, triaging, and resolution, each carried out by different
stakeholders such as end users, customer support, developers, and testers. This
division of responsibilities requires significant coordination and widens the
communication gap between non-technical users and technical teams, slowing the
process from bug discovery to resolution. Moreover, current systems are highly
asynchronous; users often wait hours or days for a first response, delaying
fixes and contributing to frustration. This paper examines the evolution of bug
tracking, from early paper-based reporting to today's web-based and SaaS
platforms. Building on this trajectory, we propose an AI-powered bug tracking
framework that augments existing tools with intelligent, large language model
(LLM)-driven automation. Our framework addresses two main challenges: reducing
time-to-fix and minimizing human overhead. Users report issues in natural
language, while AI agents refine reports, attempt reproduction, and request
missing details. Reports are then classified, invalid ones resolved through
no-code fixes, and valid ones localized and assigned to developers. LLMs also
generate candidate patches, with human oversight ensuring correctness. By
integrating automation into each phase, our framework accelerates response
times, improves collaboration, and strengthens software maintenance practices
for a more efficient, user-centric future.

</details>


### [16] [Building Whitespace-Sensitive Languages Using Whitespace-Insensitive Components](https://arxiv.org/abs/2510.08200)
*Alexander Hellwig,Nico Jansen,Bernhard Rumpe*

Main category: cs.SE

TL;DR: 提出了一种通过预处理语言构件来构建空格敏感语言的技术，使用模块化的空格不敏感语言模块，从而提高语言组件的可重用性。


<details>
  <summary>Details</summary>
Motivation: 软件语言工程中，模块化语言组件的可重用性受到空格敏感和空格不敏感语言集成差距的严重限制，导致库无法重用，空格敏感语言需要从头开发。

Method: 在解析前对语言构件进行预处理，使用模块化的空格不敏感语言模块来构建空格敏感语言。

Result: 通过重构简化版Python语言来评估该方法，验证了技术的可行性。

Conclusion: 该解决方案旨在提高现有语言组件的可重用性，减少开发时间并提高软件语言的总体质量。

Abstract: In Software Language Engineering, there is a trend towards reusability by
composing modular language components. However, this reusability is severely
inhibited by a gap in integrating whitespace-sensitive and
whitespace-insensitive languages. There is currently no consistent procedure
for seamlessly reusing such language components in both cases, such that
libraries often cannot be reused, and whitespacesensitive languages are
developed from scratch. This paper presents a technique for using modular,
whitespaceinsensitive language modules to construct whitespace sensitive
languages by pre-processing language artifacts before parsing. The approach is
evaluated by reconstructing a simplified version of the programming language
Python. Our solution aims to increase the reusability of existing language
components to reduce development time and increase the overall quality of
software languages.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [17] [Languages of Words of Low Automatic Complexity Are Hard to Compute](https://arxiv.org/abs/2510.07696)
*Joey Chen,Bjørn Kjos-Hanssen,Ivan Koswara,Linus Richter,Frank Stephan*

Main category: cs.FL

TL;DR: 本文研究了非确定性自动复杂度A_Ne，这是一种基于非确定性自动机的字符串复杂度度量。作者定义了低复杂度语言类L_q，并证明了对于任意q∈(0,1/2)，该类既不是上下文无关语言，也不能被某些布尔电路识别。


<details>
  <summary>Details</summary>
Motivation: 研究非确定性自动复杂度的性质，特别是低复杂度语言类的计算复杂性，以扩展对自动机理论中复杂度度量的理解。

Method: 定义非确定性自动复杂度A_Ne，构建参数化的语言类L_q = {x∈Σ*: A_Ne(x) < q|x|}，通过理论分析证明这些语言类的性质。

Result: 对于所有q∈(0,1/2)，语言类L_q既不是上下文无关语言，也不能被某些布尔电路识别。同时解决了Kjos-Hanssen关于L_{1/3}布尔电路复杂度的开放问题，并证明了A_Ne的香农效应。

Conclusion: 非确定性自动复杂度定义的语言类具有高度的计算复杂性，这些结果深化了我们对自动机复杂度和计算复杂性理论之间联系的理解。

Abstract: The automatic complexity of a finite word (string) is an analogue for finite
automata of Sipser's distinguishing complexity (1983) and was introduced by
Shallit and Wang (2001). For a finite alphabet $\Sigma$ of at least two
elements, we consider the non-deterministic automatic complexity given by
exactly - yet not necessarily uniquely - accepting automata: a word $x \in
\Sigma^*$ has exact non-deterministic automatic complexity $k \in \mathbb{N}$
if there exists a non-deterministic automaton of $k$ states which accepts $x$
while rejecting every other word of the same length as $x$, and no automaton of
fewer states has this property. Importantly, and in contrast to the classical
notion, the witnessing automaton may have multiple paths of computation
accepting $x$. We denote this measure of complexity by $A_{Ne}$, and study a
class of languages of low $A_{Ne}$-complexity defined as $L_q = \{ \, x \in
\Sigma^* : A_{Ne}(x) < q|x| \, \}$, which is parameterised by rationals $q \in
(0,1/2)$ (generalising a class of sets first studied by Kjos-Hanssen). We show
that for every $q \in (0,1/2)$, this class is neither context-free nor
recognisable by certain Boolean circuits. In the process, we answer an open
question of Kjos-Hanssen quantifying the complexity of $L_{1/3}$ in terms of
Boolean circuits, and also prove the Shannon effect for $A_{Ne}$.

</details>


### [18] [On the Complexity of Language Membership for Probabilistic Words](https://arxiv.org/abs/2510.08127)
*Antoine Amarilli,Mikaël Monet,Paul Raphaël,Sylvain Salvati*

Main category: cs.FL

TL;DR: 本文研究了上下文无关语言（CFL）在概率词上的成员问题，即计算从概率分布中抽取的词属于给定CFL的概率。该问题推广了计数问题，对某些语言类别是多项式时间可解的，但对其他类别可能是#P难的。


<details>
  <summary>Details</summary>
Motivation: 研究上下文无关语言在概率词上的成员问题，这推广了传统的计数问题，具有理论和实际意义，特别是在处理不确定性和概率性数据时。

Method: 通过分析不同语言类别的复杂性，包括无歧义上下文无关语言（uCFL）、多片段无歧义语言等，并使用知识编译中的电路类进行可追踪计数。

Result: 证明该问题对无歧义上下文无关语言是多项式时间可解的，但对某些语言类别（如两个线性uCFL的并集）可能是#P难的。引入的电路类可以处理一些非多片段无歧义的CFL。

Conclusion: 该问题的复杂性高度依赖于具体的语言类别，存在条件不可判定的元问题，即给定CFG时无法确定其概率成员问题是可追踪还是#P难的。

Abstract: We study the membership problem to context-free languages L (CFLs) on
probabilistic words, that specify for each position a probability distribution
on the letters (assuming independence across positions). Our task is to
compute, given a probabilistic word, what is the probability that a word drawn
according to the distribution belongs to L. This problem generalizes the
problem of counting how many words of length n belong to L, or of counting how
many completions of a partial word belong to L.
  We show that this problem is in polynomial time for unambiguous context-free
languages (uCFLs), but can be #P-hard already for unions of two linear uCFLs.
More generally, we show that the problem is in polynomial time for so-called
poly-slicewise-unambiguous languages, where given a length n we can tractably
compute an uCFL for the words of length n in the language. This class includes
some inherently ambiguous languages, and implies the tractability of bounded
CFLs and of languages recognized by unambiguous polynomial-time counter
automata; but we show that the problem can be #P-hard for nondeterministic
counter automata, even for Parikh automata with a single counter. We then
introduce classes of circuits from knowledge compilation which we use for
tractable counting, and show that this covers the tractability of
poly-slicewise-unambiguous languages and of some CFLs that are not
poly-slicewise-unambiguous. Extending these circuits with negation further
allows us to show tractability for the language of primitive words, and for the
language of concatenations of two palindromes. We finally show the conditional
undecidability of the meta-problem that asks, given a CFG, whether the
probabilistic membership problem for that CFG is tractable or #P-hard.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [19] [Type, Ability, and Effect Systems: Perspectives on Purity, Semantics, and Expressiveness](https://arxiv.org/abs/2510.07582)
*Yuyan Bao,Tiark Rompf*

Main category: cs.PL

TL;DR: 该论文提出了评估编程语言中纯度和效应系统的语义基准，比较了不同类型系统的表达能力，并展示了类型、能力和效应系统的组合优势。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如单子、类型效应系统和能力系统）在精确性和可用性之间存在张力，各有优缺点，需要更好的评估标准。

Method: 首先基于上下文等价提出纯度的语义定义，然后通过完整性度量表达能力，比较最小效应和能力系统的表达力，最后提出类型、能力和效应系统的综合方法。

Result: 发现最小效应系统与能力系统在表达能力上不可比较，即彼此不能完全包含对方，证明了组合方法的必要性。

Conclusion: 类型、能力和效应系统的组合能够结合各自优势并避免弱点，为各种效应类型系统提供了支持纯度证明的逻辑关系模型。

Abstract: Programming benefits from a clear separation between pure, mathematical
computation and impure, effectful interaction with the world. Existing
approaches to enforce this separation include monads, type-and-effect systems,
and capability systems. All share a tension between precision and usability,
and each one has non-obvious strengths and weaknesses.
  This paper aims to raise the bar in assessing such systems. First, we propose
a semantic definition of purity, inspired by contextual equivalence, as a
baseline independent of any specific typing discipline. Second, we propose that
expressiveness should be measured by the degree of completeness, i.e., how many
semantically pure terms can be typed as pure. Using this measure, we focus on
minimal meaningful effect and capability systems and show that they are
incomparable, i.e., neither subsumes the other in terms of expressiveness.
  Based on this result, we propose a synthesis and show that type, ability, and
effect systems combine their respective strengths while avoiding their
weaknesses. As part of our formal model, we provide a logical relation to
facilitate proofs of purity and other properties for a variety of effect typing
disciplines.

</details>


### [20] [The Functional Machine Calculus III: Control](https://arxiv.org/abs/2510.07851)
*Willem Heijltjes*

Main category: cs.PL

TL;DR: Functional Machine Calculus扩展了lambda演算，统一了函数式和命令式编程范式，支持计算效果、控制流操作，并保持了汇合归约和类型终止特性。


<details>
  <summary>Details</summary>
Motivation: 统一函数式和命令式编程范式，在保持lambda演算核心特性的同时嵌入计算效果、评估策略和控制流操作。

Method: 扩展Krivine机器，添加多个操作数栈来建模效果，以及继续栈来建模顺序、分支和循环计算。定义了简单的操作语义和类型系统。

Result: 成功嵌入了一个完整的最小命令式语言，包括条件语句、异常处理、迭代、常量和代数数据类型。保持了汇合归约关系和类型保证的终止性。

Conclusion: 提供了一个统一的功能-命令式计算模型，支持简单类型、直观的操作语义和汇合归约语义。

Abstract: The Functional Machine Calculus (Heijltjes 2022) is a new approach to
unifying the imperative and functional programming paradigms. It extends the
lambda-calculus, preserving the key features of confluent reduction and typed
termination, to embed computational effects, evaluation strategies, and control
flow operations. The first instalment modelled sequential higher-order
computation with global store, input/output, probabilities, and
non-determinism, and embedded both the call-by-name and call-by-value
lambda-calculus, as well as Moggi's computational metalanguage and Levy's
call-by-push-value. The present paper extends the calculus from sequential to
branching and looping control flow. This allows the faithful embedding of a
minimal but complete imperative language, including conditionals, exception
handling, and iteration, as well as constants and algebraic data types.
  The calculus is defined through a simple operational semantics, extending the
(simplified) Krivine machine for the lambda-calculus with multiple operand
stacks to model effects and a continuation stack to model sequential,
branching, and looping computation. It features a confluent reduction relation
and a system of simple types that guarantees termination of the machine and
strong normalization of reduction (in the absence of iteration). These
properties carry over to the embedded imperative language, providing a unified
functional-imperative model of computation that supports simple types, a direct
and intuitive operational semantics, and a confluent reduction semantics.

</details>
