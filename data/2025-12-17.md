<div id=toc></div>

# Table of Contents

- [cs.LO](#cs.LO) [Total: 3]
- [cs.SE](#cs.SE) [Total: 10]


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [1] [TAPAAL HyperLTL: A Tool for Checking Hyperproperties of Petri Nets](https://arxiv.org/abs/2512.14265)
*Bruno Maria René Gonzalez,Peter Gjøl Jensen,Stefan Schmid,Jiří Srba,Martin Zimmermann*

Main category: cs.LO

TL;DR: 首个用于Petri网的HyperLTL模型检查器，集成到TAPAAL框架中，支持超属性验证


<details>
  <summary>Details</summary>
Motivation: 传统LTL和CTL逻辑无法描述需要同时推理多个模型轨迹的超属性（如非干扰性和观测确定性），而Petri网作为复杂分布式系统的建模形式化方法，需要支持超属性验证的工具

Method: 开发了首个Petri网的HyperLTL模型检查器，完全集成到TAPAAL验证框架中，定义了超逻辑语义，设计了工具架构和图形用户界面

Result: 工具成功实现并集成到TAPAAL中，在计算机网络领域的两个基准问题上评估了HyperLTL验证引擎的性能

Conclusion: 该研究填补了Petri网超属性验证的空白，为复杂分布式系统的安全性和隐私性验证提供了新工具

Abstract: Petri nets are a modeling formalism capable of describing complex distributed systems and there exists a large number of both academic and industrial tools that enable automatic verification of model properties. Typical questions include reachability analysis and model checking against logics like LTL and CTL. However, these logics fall short when describing properties like non-interference and observational determinism that require simultaneous reasoning about multiple traces of the model and can thus only be expressed as hyperproperties. We introduce, to the best of our knowledge, the first HyperLTL model checker for Petri nets. The tool is fully integrated into the verification framework TAPAAL and we describe the semantics of the hyperlogic, present the tool's architecture and GUI, and evaluate the performance of the HyperLTL verification engine on two benchmarks of problems originating from the computer networking domain.

</details>


### [2] [Relevant HAL Interface Requirements for Embedded Systems](https://arxiv.org/abs/2512.14514)
*Manuel Bentele,Andreas Podelski,Axel Sikora,Bernd Westphal*

Main category: cs.LO

TL;DR: 本文提出了一种形式化方法来提取硬件抽象层(HAL)接口需求中与防止系统故障相关的核心需求，通过软件模型检查生成数学证明，确保需求的相关性。


<details>
  <summary>Details</summary>
Motivation: 嵌入式应用中硬件抽象层(HAL)的不当使用可能导致硬件操作错误，引发系统故障甚至硬件损坏。如何从大量HAL接口需求中优先提取那些无可争议地相关于防止此类故障的需求是一个关键问题。

Method: 引入形式化的相关性概念，利用软件模型检查方法生成数学证明，确保需求的相关性。提出从系统故障问题报告中提取可证明相关需求的方法，并通过使用spidev HAL的SPI总线嵌入式应用案例研究验证可行性。

Result: 案例研究使用三个嵌入式应用问题报告示例，证明了该方法在原理上是可行的，能够从系统故障报告中提取出可证明相关的HAL接口需求。

Conclusion: 本文为研究新型需求优先级排序方法铺平了道路，这种排序方法专门针对预防特定类型的系统故障，通过形式化方法确保需求的相关性得到数学证明。

Abstract: Embedded applications often use a Hardware Abstraction Layer (HAL) to access hardware. Improper use of the HAL can lead to incorrect hardware operations, resulting in system failure and potentially serious damage to the hardware. The question is how one can obtain prioritize, among a possibly large set of HAL interface requirements, those that are indisputably relevant for preventing this kind of system failure. In this paper, we introduce a formal notion of relevance. This allows us to leverage a formal method, i.e., software model checking, to produce a mathematical proof that a requirement is indisputably relevant. We propose an approach to extract provably relevant requirements from issue reports on system failures. We present a case study to demonstrate that the approach is feasible in principle. The case study uses three examples of issue reports on embedded applications that use the SPI bus via the spidev HAL. The overall contribution of this paper is to pave the way for the study of approaches to a new kind of prioritization aimed at preventing a specific kind of system failure.

</details>


### [3] [Belief in Simplicial Complexes](https://arxiv.org/abs/2512.14647)
*Philip Sink,Adam Bjorndahl*

Main category: cs.LO

TL;DR: 提出基于单纯复形的新信念语义，满足KD45公理和"知识蕴含信念"公理，同时保持每个面片包含所有颜色的顶点，避免信念与知识等同的平凡化问题。


<details>
  <summary>Details</summary>
Motivation: 现有单纯复形信念模型要么无法同时满足KD45公理和"知识蕴含信念"公理，要么会导致信念与知识等同的平凡化问题。同时，现有文献中的"适当性"技术假设在某些概念合理的信念框架中可能被违反。

Method: 使用单纯复形框架建模信念，确保每个面片恰好包含每个颜色的一个顶点。利用"关于适当关系结构的注记"的结果绕过"适当性"限制，并探讨单纯集框架作为替代方案。

Result: 提出了能够同时满足KD45公理、"知识蕴含信念"公理，且不将信念平凡化为知识的单纯复形信念语义。展示了如何绕过"适当性"假设的限制。

Conclusion: 新框架成功解决了单纯复形中信念建模的关键问题。单纯集框架可能提供更简洁的信念表示方法，并能完全绕过"适当性"假设。

Abstract: We provide a novel semantics for belief using simplicial complexes. In our framework, belief satisfies the \textsf{KD45} axioms and rules as well as the ``knowledge implies belief'' axiom ($Kφ\lthen Bφ$); in addition, we adopt the (standard) assumption that each facet in our simplicial models has exactly one vertex of every color. No existing model of belief in simplicial complexes that we are aware of is able to satisfy all of these conditions without trivializing belief to coincide with knowledge. We also address the common technical assumption of ``properness'' for relational structures made in the simplicial semantics literature, namely, that no two worlds fall into the same knowledge cell for all agents; we argue that there are conceptually sensible belief frames in which this assumption is violated, and use the result of ``A Note on Proper Relational Structures'' to bypass this restriction. We conclude with a discussion of how an alternative ``simplicial sets'' framework could allow us to bypass properness altogether and perhaps provide a more streamlined simplicial framework for representing belief.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [4] [Practitioner Insights on Fairness Requirements in the AI Development Life Cycle: An Interview Study](https://arxiv.org/abs/2512.13830)
*Chaima Boufaied,Thanh Nguyen,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 研究通过26个半结构化访谈调查AI公平性在软件工程中的实践现状，发现虽然从业者认识到公平性维度，但实践不一致且常被降级处理，存在知识缺口。


<details>
  <summary>Details</summary>
Motivation: AI/ML和LLMs广泛应用但常作为黑箱运行，可能对不同人口群体产生不公平影响。传统上AI模型关注有效性，现在公平性问题日益受到重视，需要从软件工程角度研究公平性要求。

Method: 对来自23个国家、不同应用领域和背景的从业者进行26个半结构化访谈，采用主题定性分析方法，研究公平性在软件开发生命周期中的实践情况。

Result: 参与者认识到AI公平性的关键维度（实施、验证、评估、权衡），但实践不一致，公平性常被降级处理，存在明显知识缺口。需要与利益相关者就明确定义的公平性概念、评估指标和正式流程达成共识。

Conclusion: AI公平性实践存在显著差距，需要建立上下文相关的明确定义、相应评估指标和正式化流程，以更好地将公平性整合到AI/ML项目中。

Abstract: Nowadays, Artificial Intelligence (AI), particularly Machine Learning (ML) and Large Language Models (LLMs), is widely applied across various contexts. However, the corresponding models often operate as black boxes, leading them to unintentionally act unfairly towards different demographic groups. This has led to a growing focus on fairness in AI software recently, alongside the traditional focus on the effectiveness of AI models. Through 26 semi-structured interviews with practitioners from different application domains and with varied backgrounds across 23 countries, we conducted research on fairness requirements in AI from software engineering perspective. Our study assesses the participants' awareness of fairness in AI / ML software and its application within the Software Development Life Cycle (SDLC), from translating fairness concerns into requirements to assessing their arising early in the SDLC. It also examines fairness through the key assessment dimensions of implementation, validation, evaluation, and how it is balanced with trade-offs involving other priorities, such as addressing all the software functionalities and meeting critical delivery deadlines. Findings of our thematic qualitative analysis show that while our participants recognize the aforementioned AI fairness dimensions, practices are inconsistent, and fairness is often deprioritized with noticeable knowledge gaps. This highlights the need for agreement with relevant stakeholders on well-defined, contextually appropriate fairness definitions, the corresponding evaluation metrics, and formalized processes to better integrate fairness into AI/ML projects.

</details>


### [5] [Verification-Guided Context Optimization for Tool Calling via Hierarchical LLMs-as-Editors](https://arxiv.org/abs/2512.13860)
*Henger Li,Shuangjie You,Flavio Di Palo,Yiyue Qian,Ayush Jain*

Main category: cs.SE

TL;DR: VGCO框架使用LLM作为编辑器自动优化工具文档和知识库上下文，解决工业环境中大规模工具调用时文档与LLM理解不匹配的问题，显著提升准确率、鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前工具调用依赖的文档和知识库通常为人类用户设计，与LLM的信息处理方式不匹配。在工业环境中，数百个功能重叠的工具带来了可扩展性、可变性和模糊性挑战，导致工具使用效果不佳。

Method: 提出验证引导的上下文优化（VGCO）框架，包含两个阶段：评估阶段收集实际失败案例并识别工具与上下文的不匹配；优化阶段通过离线学习进行分层编辑，采用结构感知的上下文内优化。LLM编辑器具有分层结构、状态感知、动作特定和验证引导的特点。

Result: VGCO在单轮大规模工具调用问题上取得了显著改进，提高了准确率、鲁棒性和泛化能力，相比之前强调多轮推理的工作有更好的效果。

Conclusion: VGCO框架通过使用LLM作为编辑器自动优化工具相关文档和知识库上下文，有效解决了工业环境中大规模工具调用的文档对齐问题，为实际应用提供了实用解决方案。

Abstract: Tool calling enables large language models (LLMs) to interact with external environments through tool invocation, providing a practical way to overcome the limitations of pretraining. However, the effectiveness of tool use depends heavily on the quality of the associated documentation and knowledge base context. These materials are usually written for human users and are often misaligned with how LLMs interpret information. This problem is even more pronounced in industrial settings, where hundreds of tools with overlapping functionality create challenges in scalability, variability, and ambiguity. We propose Verification-Guided Context Optimization (VGCO), a framework that uses LLMs as editors to automatically refine tool-related documentation and knowledge base context. VGCO works in two stages. First, Evaluation collects real-world failure cases and identifies mismatches between tools and their context. Second, Optimization performs hierarchical editing through offline learning with structure-aware, in-context optimization. The novelty of our LLM editors has three main aspects. First, they use a hierarchical structure that naturally integrates into the tool-calling workflow. Second, they are state-aware, action-specific, and verification-guided, which constrains the search space and enables efficient, targeted improvements. Third, they enable cost-efficient sub-task specialization, either by prompt engineering large editor models or by post-training smaller editor models. Unlike prior work that emphasizes multi-turn reasoning, VGCO focuses on the single-turn, large-scale tool-calling problem and achieves significant improvements in accuracy, robustness, and generalization across LLMs.

</details>


### [6] [Context Branching for LLM Conversations: A Version Control Approach to Exploratory Programming](https://arxiv.org/abs/2512.13914)
*Bhargav Chickmagalur Nanjundappa,Spandan Maaheshwari*

Main category: cs.SE

TL;DR: ContextBranch是一个对话管理系统，将版本控制语义应用于LLM交互，通过检查点、分支、切换和注入四个核心原语，解决多轮对话中性能下降和上下文污染问题。


<details>
  <summary>Details</summary>
Motivation: LLM在多轮对话中性能显著下降（平均下降39%），特别是在探索性编程任务中，模型会做出过早假设且难以纠正。现有解决方案迫使用户在上下文污染的对话中继续（导致LLM越来越困惑）或重新开始（丢失所有累积上下文）之间做出选择。

Method: ContextBranch应用版本控制语义到LLM交互，提供四个核心原语：检查点（捕获对话状态）、分支（在隔离中探索替代方案）、切换（在不同分支间移动）和注入（选择性合并见解）。

Result: 在30个软件工程场景的实验中，分支对话相比线性对话获得更高的响应质量，在焦点和上下文意识方面有显著改进。分支将上下文大小减少58.1%（从31.0条消息减少到13.0条），消除了不相关的探索内容，特别是在涉及概念上距离较远的探索的复杂场景中效果最明显。

Conclusion: 对话分支是AI辅助探索性工作的基本原语，隔离可以防止在探索替代方案时发生上下文污染。ContextBranch为LLM交互中的对话管理提供了有效的解决方案。

Abstract: Large Language Models (LLMs) have become integral to software engineering workflows, yet their effectiveness degrades significantly in multi-turn conversations. Recent studies demonstrate an average 39% performance drop when instructions are delivered across multiple turns, with models making premature assumptions and failing to course correct (Laban et al., 2025). This degradation is particularly problematic in exploratory programming tasks where developers need to investigate alternative approaches without committing to a single path. Current solutions force users into a false dichotomy: continue in a context-polluted conversation where the LLM becomes increasingly confused, or start fresh and lose all accumulated context.
  We present ContextBranch, a conversation management system that applies version control semantics to LLM interactions. ContextBranch provides four core primitives--checkpoint, branch, switch, and inject--enabling users to capture conversation state, explore alternatives in isolation, and selectively merge insights. We evaluate ContextBranch through a controlled experiment with 30 software engineering scenarios featuring intentionally polluting explorations. Branched conversations achieved higher response quality compared to linear conversations, with large improvements in focus and context awareness. Benefits were concentrated in complex scenarios involving conceptually distant explorations. Branching reduced context size by 58.1% (31.0 to 13.0 messages), eliminating irrelevant exploratory content. Our work establishes conversation branching as a fundamental primitive for AI-assisted exploratory work, demonstrating that isolation prevents context pollution when exploring alternatives.

</details>


### [7] [Professional Software Developers Don't Vibe, They Control: AI Agent Use for Coding in 2025](https://arxiv.org/abs/2512.14012)
*Ruanqianqian Huang,Avery Reyna,Sorin Lerner,Haijun Xia,Brian Hempel*

Main category: cs.SE

TL;DR: 经验丰富的开发者将AI代理视为生产力工具，但保留对软件设计和实现的控制权，通过专业知识控制代理行为，对将代理融入开发持积极态度。


<details>
  <summary>Details</summary>
Motivation: 研究AI代理在专业软件开发中的实际角色，了解经验丰富的开发者如何使用代理，包括他们的动机、策略、任务适用性和情感态度。

Method: 通过实地观察（N=13）和定性调查（N=99）收集数据，分析开发者使用AI代理的实践和体验。

Result: 经验丰富的开发者重视代理作为生产力提升工具，但坚持控制软件设计和实现，以确保基本的软件质量属性；他们使用专业知识控制代理行为，并对将代理融入开发持积极态度，因为他们有信心弥补代理的局限性。

Conclusion: 研究揭示了软件开发最佳实践在有效使用代理中的价值，指出了代理可能适用的任务类型，并为未来更好的代理界面和使用指南提供了方向。

Abstract: The rise of AI agents is transforming how software can be built. The promise of agents is that developers might write code quicker, delegate multiple tasks to different agents, and even write a full piece of software purely out of natural language. In reality, what roles agents play in professional software development remains in question. This paper investigates how experienced developers use agents in building software, including their motivations, strategies, task suitability, and sentiments. Through field observations (N=13) and qualitative surveys (N=99), we find that while experienced developers value agents as a productivity boost, they retain their agency in software design and implementation out of insistence on fundamental software quality attributes, employing strategies for controlling agent behavior leveraging their expertise. In addition, experienced developers feel overall positive about incorporating agents into software development given their confidence in complementing the agents' limitations. Our results shed light on the value of software development best practices in effective use of agents, suggest the kinds of tasks for which agents may be suitable, and point towards future opportunities for better agentic interfaces and agentic use guidelines.

</details>


### [8] [PerfCoder: Large Language Models for Interpretable Code Performance Optimization](https://arxiv.org/abs/2512.14018)
*Jiuding Yang,Shengyao Lu,Hongxuan Liu,Shayan Shirahmad Gale Bagi,Zahra Fazel,Tomasz Czajkowski,Di Niu*

Main category: cs.SE

TL;DR: PerfCoder是一个专门为生成高性能代码设计的LLM家族，通过可解释的定制化优化，在代码性能优化基准测试中超越了现有所有模型。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在自动代码生成方面取得了显著进展，但在生成高性能代码方面仍然有限，这是现实世界软件系统的关键需求。作者认为当前LLM不仅因为数据稀缺，更重要的是缺乏指导可解释和有效性能改进的监督。

Method: PerfCoder在精心策划的真实世界优化轨迹数据集上进行微调，这些数据包含人类可读的注释，并通过使用运行时测量的强化微调进行偏好对齐。这使得模型能够提出输入特定的改进策略并直接应用，而不依赖迭代优化。

Result: 在PIE代码性能基准测试中，PerfCoder在运行时加速和有效优化率方面都超越了所有现有模型。此外，PerfCoder能够生成关于源代码的可解释反馈，当这些反馈作为输入提供给更大的LLM时，在规划者和优化器协作的工作流程中，可以进一步提升结果。

Conclusion: 性能优化不能仅通过扩大模型规模实现，而是需要优化策略意识。PerfCoder能够将32B模型和GPT-5的性能提升到新水平，显著超越其原始性能，证明了专门设计的性能优化模型的重要性。

Abstract: Large language models (LLMs) have achieved remarkable progress in automatic code generation, yet their ability to produce high-performance code remains limited--a critical requirement in real-world software systems. We argue that current LLMs struggle not only due to data scarcity but, more importantly, because they lack supervision that guides interpretable and effective performance improvements. In this work, we introduce PerfCoder, a family of LLMs specifically designed to generate performance-enhanced code from source code via interpretable, customized optimizations. PerfCoder is fine-tuned on a curated collection of real-world optimization trajectories with human-readable annotations, and preference-aligned by reinforcement fine-tuning using runtime measurements, enabling it to propose input-specific improvement strategies and apply them directly without relying on iterative refinement. On the PIE code performance benchmark, PerfCoder surpasses all existing models in both runtime speedup and effective optimization rate, demonstrating that performance optimization cannot be achieved by scale alone but requires optimization stratetgy awareness. In addition, PerfCoder can generate interpretable feedback about the source code, which, when provided as input to a larger LLM in a planner-and-optimizer cooperative workflow, can further improve outcomes. Specifically, we elevate the performance of 32B models and GPT-5 to new levels on code optimization, substantially surpassing their original performance.

</details>


### [9] [PentestEval: Benchmarking LLM-based Penetration Testing with Modular and Stage-Level Design](https://arxiv.org/abs/2512.14233)
*Ruozhao Yang,Mingfei Cheng,Gelei Deng,Tianwei Zhang,Junjie Wang,Xiaofei Xie*

Main category: cs.SE

TL;DR: PentestEval是首个全面的LLM在渗透测试六阶段评估基准，包含346个任务，评估9个主流LLM，发现整体性能较弱（端到端成功率仅31%），揭示现有LLM系统在结构化推理方面的不足。


<details>
  <summary>Details</summary>
Motivation: 传统渗透测试工作流高度依赖人工、难以扩展，现有LLM应用仅使用简单提示而缺乏任务分解和领域适应，导致不可靠的黑盒行为和有限的能力洞察，需要建立细粒度评估基准。

Method: 提出PentestEval基准，将渗透测试分解为六个阶段：信息收集、弱点收集与筛选、攻击决策、漏洞利用生成与修订；包含专家标注的真实数据和全自动评估流水线，涵盖12个真实漏洞场景中的346个任务。

Result: 评估9个主流LLM显示整体性能较弱，端到端流水线成功率仅31%；现有LLM系统（PentestGPT、PentestAgent、VulnBot）存在类似限制，自主代理几乎完全失败；模块化能提升各阶段性能并改善整体表现。

Conclusion: 自主渗透测试需要更强的结构化推理能力，模块化能增强各阶段性能；PentestEval为未来细粒度、阶段级评估研究提供了基础基准，推动更可靠的LLM自动化发展。

Abstract: Penetration testing is essential for assessing and strengthening system security against real-world threats, yet traditional workflows remain highly manual, expertise-intensive, and difficult to scale. Although recent advances in Large Language Models (LLMs) offer promising opportunities for automation, existing applications rely on simplistic prompting without task decomposition or domain adaptation, resulting in unreliable black-box behavior and limited insight into model capabilities across penetration testing stages. To address this gap, we introduce PentestEval, the first comprehensive benchmark for evaluating LLMs across six decomposed penetration testing stages: Information Collection, Weakness Gathering and Filtering, Attack Decision-Making, Exploit Generation and Revision. PentestEval integrates expert-annotated ground truth with a fully automated evaluation pipeline across 346 tasks covering all stages in 12 realistic vulnerable scenarios. Our stage-level evaluation of 9 widely used LLMs reveals generally weak performance and distinct limitations across the stages of penetration-testing workflow. End-to-end pipelines reach only 31% success rate, and existing LLM-powered systems such as PentestGPT, PentestAgent, and VulnBot exhibit similar limitations, with autonomous agents failing almost entirely. These findings highlight that autonomous penetration testing demands stronger structured reasoning, where modularization enhances each individual stage and improves overall performance. PentestEval provides the foundational benchmark needed for future research on fine-grained, stage-level evaluation, paving the way toward more reliable LLM-based automation.

</details>


### [10] [Aligning Security Compliance and DevOps: A Longitudinal Study](https://arxiv.org/abs/2512.14453)
*Fabiola Moyón,Florian Angermeir,Daniel Mendez,Tony Gorschek,Markus Voggenreiter,Pierre-Louis Bonvin*

Main category: cs.SE

TL;DR: 本文提出RefA框架，将IEC 62443-4-1安全标准整合到DevOps生命周期中，帮助企业在敏捷开发中保持安全合规


<details>
  <summary>Details</summary>
Motivation: 企业采用敏捷和DevOps方法开发软件密集型产品时，传统线性安全标准合规流程面临挑战，特别是对于关键基础设施产品。需要支持企业向DevOps转型同时保持安全合规。

Method: 基于IEC 62443-4-1标准开发了RefA框架，这是一个安全合规的DevOps生命周期规范性模型。在西门子AG进行了纵向研究，包含多个子研究，涵盖框架的构思、验证和初步采用阶段。

Result: RefA框架能够将安全合规知识有效传递给产品开发团队，支持跨职能团队具备交付合规产品所需的所有技能，实现了敏捷开发与安全标准的融合。

Conclusion: RefA框架为专业人员（不仅是安全专家）提供了在实施DevOps流程时保持安全合规的实用工具，特别适用于关键基础设施领域，解决了敏捷开发与安全标准合规之间的矛盾。

Abstract: Companies adopt agile methodologies and DevOps to facilitate efficient development and deployment of software-intensive products. This, in turn, introduces challenges in relation to security standard compliance traditionally following a more linear workflow. This is especially a challenge for the engineering of products and services associated with critical infrastructures. To support companies in their transition towards DevOps, this paper presents an adaptation of DevOps according to security regulations and standards. We report on our longitudinal study at Siemens AG, consisting of several individual sub-studies in the inception, validation, and initial adoption of our framework based on RefA as well as the implications for practice. RefA is a prescriptive model of a security compliant DevOps lifecycle based on the IEC 62443-4-1 standard. The overall framework is aimed at professionals, not only security experts, being able to use it on implementing DevOps processes while remaining compliant with security norms. We demonstrate how RefA facilitates the transfer of security compliance knowledge to product development teams. This knowledge transfer supports the agility aim of ensuring that cross-functional teams have all the skills needed to deliver the compliant products.

</details>


### [11] [Teralizer: Semantics-Based Test Generalization from Conventional Unit Tests to Property-Based Tests](https://arxiv.org/abs/2512.14475)
*Johann Glock,Clemens Bauer,Martin Pinzger*

Main category: cs.SE

TL;DR: Teralizer：将单元测试自动转换为基于属性的测试的工具，通过单路径符号分析从实现中提取规范，提高测试覆盖率


<details>
  <summary>Details</summary>
Motivation: 传统单元测试只验证单个输入输出对，而基于属性的测试需要大量手动工作定义属性和约束。需要自动化的方法来提升测试效果

Method: 提出语义驱动方法，通过单路径符号分析从实现中自动提取规范，将JUnit测试转换为基于属性的jqwik测试

Result: 在EvoSuite生成的测试上提升突变分数1-4个百分点；在成熟开发者编写的测试上提升有限（0.05-0.07个百分点）；实际项目中只有1.7%能完成转换，主要受符号分析类型支持和静态分析限制

Conclusion: Teralizer展示了从单元测试自动生成基于属性测试的潜力，但面临符号分析和静态分析的技术挑战，需要进一步研究解决实际应用障碍

Abstract: Conventional unit tests validate single input-output pairs, leaving most inputs of an execution path untested. Property-based testing addresses this shortcoming by generating multiple inputs satisfying properties but requires significant manual effort to define properties and their constraints. We propose a semantics-based approach that automatically transforms unit tests into property-based tests by extracting specifications from implementations via single-path symbolic analysis. We demonstrate this approach through Teralizer, a prototype for Java that transforms JUnit tests into property-based jqwik tests. Unlike prior work that generalizes from input-output examples, Teralizer derives specifications from program semantics.
  We evaluated Teralizer on three progressively challenging datasets. On EvoSuite-generated tests for EqBench and Apache Commons utilities, Teralizer improved mutation scores by 1-4 percentage points. Generalization of mature developer-written tests from Apache Commons utilities showed only 0.05-0.07 percentage points improvement. Analysis of 632 real-world Java projects from RepoReapers highlights applicability barriers: only 1.7% of projects completed the generalization pipeline, with failures primarily due to type support limitations in symbolic analysis and static analysis limitations in our prototype. Based on the results, we provide a roadmap for future work, identifying research and engineering challenges that need to be tackled to advance the field of test generalization.
  Artifacts available at: https://doi.org/10.5281/zenodo.17950381

</details>


### [12] [MoT: A Model-Driven Low-Code Approach for Simplifying Cloud-of-Things Application Development](https://arxiv.org/abs/2512.14613)
*Cristiano Welter,Kleinner Farias*

Main category: cs.SE

TL;DR: 本文提出了一种名为"物模型"(MoT)的基于模型的低代码方法，用于简化云物(CoT)应用开发，通过自定义UML配置文件降低技术门槛，并通过案例研究和TAM问卷验证了其可行性和易用性。


<details>
  <summary>Details</summary>
Motivation: 云计算与物联网(IoT)的集成对于构建可扩展的智能系统至关重要，但当前开发云物(CoT)应用面临技术门槛高、缺乏标准化模型驱动方法、现有方法无法确保互操作性、自动化和效率等问题。

Method: 提出"物模型"(MoT)方法，这是一种基于模型的方法，融合低代码原则，为IoT和云服务提供自定义UML配置文件。通过案例研究和技术接受模型(TAM)问卷进行评估。

Result: MoT被证实可行，能够简化CoT应用开发和部署。用户即使IoT经验有限也能轻松使用，报告了高感知易用性和有用性。定性反馈强调了MoT降低复杂性和加速开发的能力。

Conclusion: MoT为CoT应用开发提供了一个有前景的模型驱动解决方案，通过降低入门门槛和促进自动化，提高了效率和灵活性。这项研究朝着更用户友好的框架迈出了一步，有助于更广泛地采用CoT技术。

Abstract: The integration of cloud computing and the Internet of Things (IoT) is essential for scalable, intelligent systems. However, developing cloud-of-things (CoT) applications remains challenging. It requires significant technical expertise and lacks standardized, model-driven methodologies. Current approaches fail to ensure interoperability, automation, and efficiency. This study introduces the Model of Things (MoT), a model-based approach that incorporates low-code principles to simplify CoT development. MoT reduces technical barriers by providing a custom UML profile designed for IoT and cloud services. To evaluate MoT, we conducted a case study and a Technology Acceptance Model (TAM) questionnaire. The results confirmed MoT's feasibility, demonstrating that it streamlines CoT application development and deployment. Users found MoT accessible, even with limited IoT experience, and reported high perceived ease of use and usefulness. Qualitative feedback highlighted MoT's ability to reduce complexity and speed up development. MoT offers a promising, model-driven solution for CoT application development. By lowering entry barriers and promoting automation, it enhances both efficiency and flexibility. This study represents a step toward a more user-friendly framework, enabling broader adoption of CoT technologies.

</details>


### [13] [Reconsidering Conversational Norms in LLM Chatbots for Sustainable AI](https://arxiv.org/abs/2512.14673)
*Ronnie de Souza Santos,Cleyton Magalhães,Italo Santos*

Main category: cs.SE

TL;DR: 该论文认为用户交互行为是影响LLM系统环境可持续性的关键因素，提出了四个维度分析交互模式如何增加能耗，并呼吁重新设计对话系统以提升可持续性。


<details>
  <summary>Details</summary>
Motivation: 当前对LLM系统可持续性的研究主要集中在模型架构、硬件效率和部署基础设施上，但忽视了用户交互行为本身对能源消耗的影响。作者认为交互层面的行为是影响LLM系统环境影响的未充分研究因素。

Method: 作为一篇愿景论文，作者通过四个维度分析用户交互行为如何影响LLM系统的能源消耗：1）扩展对话模式增加token生成和推理计算成本；2）即时响应期望限制能源感知调度和工作负载整合机会；3）日常用户习惯累积操作需求；4）上下文积累影响内存需求和长对话效率。

Result: 论文识别了用户交互行为在LLM系统可持续性中的关键作用，揭示了四个具体的影响维度，为未来的研究和设计提供了分析框架。

Conclusion: 解决LLM系统的可持续性挑战需要重新思考聊天机器人交互的设计和概念化，采用新的视角，认识到可持续性部分取决于用户与LLM系统交互的对话规范。需要将可持续性考虑纳入交互设计，开发更环保的对话模式。

Abstract: LLM based chatbots have become central interfaces in technical, educational, and analytical domains, supporting tasks such as code reasoning, problem solving, and information exploration. As these systems scale, sustainability concerns have intensified, with most assessments focusing on model architecture, hardware efficiency, and deployment infrastructure. However, existing mitigation efforts largely overlook how user interaction practices themselves shape the energy profile of LLM based systems. In this vision paper, we argue that interaction level behavior appears to be an underexamined factor shaping the environmental impact of LLM based systems, and we present this issue across four dimensions. First, extended conversational patterns increase token production and raise the computational cost of inference. Second, expectations of instant responses limit opportunities for energy aware scheduling and workload consolidation. Third, everyday user habits contribute to cumulative operational demand in ways that are rarely quantified. Fourth, the accumulation of context affects memory requirements and reduces the efficiency of long running dialogues. Addressing these challenges requires rethinking how chatbot interactions are designed and conceptualized, and adopting perspectives that recognize sustainability as partly dependent on the conversational norms through which users engage with LLM based systems.

</details>
