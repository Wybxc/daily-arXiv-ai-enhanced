<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.LO](#cs.LO) [Total: 3]
- [cs.SE](#cs.SE) [Total: 16]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [RightTyper: Effective and Efficient Type Annotation for Python](https://arxiv.org/abs/2507.16051)
*Juan Altmayer Pizzorno,Emery D. Berger*

Main category: cs.PL

TL;DR: RightTyper是一种新的Python类型注释方法，通过基于采样的动态分析生成精确的类型注释，同时将类型检查转化为异常检测，仅产生30%的性能开销，克服了现有静态、AI和动态方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有Python类型注释方法存在诸多问题：静态方法难以处理动态特性且推断类型过于宽泛；AI方法本质上不可靠且可能遗漏罕见或用户定义类型；动态方法会带来高达270倍的运行时开销，甚至可能推断错误类型导致运行时错误。所有先前工作都隐含假设待注释代码是正确的，这在大型未类型化代码库中通常不成立。

Method: RightTyper采用基于采样的原则性方法，通过自我性能分析指导采样过程，结合统计过滤以及类型信息的仔细解析和聚合。该方法基于实际程序行为生成精确的类型注释，并将类型检查转化为异常检测机制。

Result: RightTyper在类型检查方面相对于先前方法提高了召回率，能够识别程序员可以审计的意外行为角落案例，同时保持快速和空间高效的特性，平均仅产生30%的性能开销。

Conclusion: RightTyper成功克服了现有Python类型注释方法的主要缺陷，通过创新的采样驱动动态分析方法，实现了精确、高效且具有异常检测能力的类型注释生成，为Python代码的类型安全提供了更好的解决方案。

Abstract: Python type annotations bring the benefits of static type checking to the
language. However, manually writing annotations can be time-consuming and
tedious. The result is that most real-world Python code remains largely
untyped. Past approaches to annotating types in Python code fall short in a
number of ways. Static approaches struggle with dynamic features and infer
overly broad types. AI-based methods are inherently unsound and can miss rare
or user-defined types. Dynamic methods can impose extreme runtime overheads,
degrading performance by up to 270x, abort execution as they exhaust resources,
and even infer incorrect types that lead to runtime errors. Crucially, all
prior work assumes implicitly that the code to be annotated is already correct.
This assumption is generally unwarranted, especially for large codebases that
have been untyped.
  This paper presents RightTyper, a novel approach for Python that overcomes
these disadvantages. RightTyper not only generates precise type annotations
based on actual program behavior, improving recall in type checking relative to
prior approaches. It also turns type checking into anomaly detection, allowing
the type checker to identify corner cases that the programmer can audit for
unintended behavior. RightTyper is also fast and space-efficient, imposing just
30% performance overhead on average. RightTyper achieves these characteristics
by a principled yet pervasive use of sampling--guided by self-profiling--along
with statistical filtering and careful resolution and aggregation of type
information.

</details>


### [2] [Understanding Haskell-style Overloading via Open Data and Open Functions](https://arxiv.org/abs/2507.16086)
*Andrew Marmaduke,Apoorv Ingle,J. Garrett Morris*

Main category: cs.PL

TL;DR: 本文提出了一种新的核心语言System F_D，为Haskell风格的重载提供统一语义，通过开放数据类型和开放函数实现，并在Lean4中机械化验证了其元理论


<details>
  <summary>Details</summary>
Motivation: 现有的Haskell类型类系统语义存在表达能力不足的问题，需要额外的类型等式公理，缺乏统一且表达力强的语义框架来处理Haskell风格的重载机制

Method: 设计了一种新的核心语言System F_D，其特点是采用开放数据类型和开放函数，这些都通过实例集合而非单一定义来给出；并在Lean4交互式定理证明器中机械化验证了该系统的元理论

Result: System F_D能够编码Haskell类型类系统的高级特性，比现有的这些特性语义更具表达力，且无需假设额外的类型等式公理

Conclusion: System F_D为Haskell风格重载提供了一个新的、统一的语义框架，通过开放数据类型和开放函数的设计，实现了比现有方法更强的表达能力，同时避免了对额外类型等式公理的依赖

Abstract: We present a new, uniform semantics for Haskell-style overloading. We realize
our approach in a new core language, System F$_\mathrm{D}$, whose metatheory we
mechanize in the Lean4 interactive theorem prover. System F$_\mathrm{D}$ is
distinguished by its open data types and open functions, each given by a
collection of instances rather than by a single definition. We show that System
F$_\mathrm{D}$ can encode advanced features of Haskell's of type class systems,
more expressively than current semantics of these features, and without
assuming additional type equality axioms.

</details>


### [3] [Querying Graph-Relational Data](https://arxiv.org/abs/2507.16089)
*Michael J. Sullivan,Zhibo Chen,Elvis Pranskevichus,Robert J. Simmons,Victor Petrovykh,Aljaž Mur Eržen,Yury Selivanov*

Main category: cs.PL

TL;DR: 本文提出了图关系数据库模型，通过EdgeQL查询语言和Gel系统解决了关系数据库中平面数据表示与应用程序期望的嵌套对象数据之间的阻抗不匹配问题


<details>
  <summary>Details</summary>
Motivation: 关系数据库的平面数据模型与应用程序期望接收的深度嵌套信息之间存在阻抗不匹配，传统的对象关系映射(ORM)技术在处理对象形状数据操作时效率低下

Method: 提出图关系数据库模型，设计EdgeQL查询语言（类SQL风格的通用查询语言），开发Gel系统将EdgeQL模式和查询编译为PostgreSQL查询，并为查询提供静态和动态语义定义

Result: Gel系统能够高效地处理对象形状的数据操作，同时保持接近直接编写复杂PostgreSQL查询的效率，提供了灵活、组合性强且强类型的解决方案

Conclusion: 图关系数据库模型通过EdgeQL和Gel系统成功解决了对象关系不匹配问题，在保持高效性的同时提供了更好的数据操作体验，为结构化数据存储提供了新的解决方案

Abstract: For applications that store structured data in relational databases, there is
an impedance mismatch between the flat representations encouraged by relational
data models and the deeply nested information that applications expect to
receive. In this work, we present the graph-relational database model, which
provides a flexible, compositional, and strongly-typed solution to this
"object-relational mismatch." We formally define the graph-relational database
model and present a static and dynamic semantics for queries. In addition, we
discuss the realization of the graph-relational database model in EdgeQL, a
general-purpose SQL-style query language, and the Gel system, which compiles
EdgeQL schemas and queries into PostgreSQL queries. Gel facilitates the kind of
object-shaped data manipulation that is frequently provided inefficiently by
object-relational mapping (ORM) technologies, while achieving most of the
efficiency that comes from require writing complex PostgreSQL queries directly.

</details>


### [4] [Enhancing Compiler Optimization Efficiency through Grammatical Decompositions of Control-Flow Graphs](https://arxiv.org/abs/2507.16660)
*Xuran Cai*

Main category: cs.PL

TL;DR: 本文提出了SPL（串行-并行-循环）分解框架，用于解决编译器优化中的寄存器分配、LOSPRE和银行选择等问题，相比传统树分解方法显著提升了性能并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统的编译器优化方法如树分解算法在处理寄存器分配和LOSPRE等问题时，经常忽略控制流图的重要稀疏性特征，导致计算成本过高，需要一种更有效的优化框架。

Method: 提出SPL（串行-并行-循环）分解框架，为图结构中的部分约束满足问题（PCSPs）制定通用解决方案，并将其应用于三个优化问题：寄存器分配、LOSPRE优化和银行选择指令放置优化。

Result: SPL分解在寄存器分配中通过准确建模变量干扰图实现高效寄存器分配；在LOSPRE中有效识别和消除程序执行中的冗余；在银行选择优化中提升数据检索效率并减少延迟。实验表明相比现有方法有显著性能提升。

Conclusion: SPL分解被确立为处理复杂编译器优化的强大工具，包括寄存器分配、LOSPRE和银行选择等问题，为编译器优化领域提供了一个高效且通用的解决方案框架。

Abstract: This thesis addresses the complexities of compiler optimizations, such as
register allocation and Lifetime-optimal Speculative Partial Redundancy
Elimination (LOSPRE), which are often handled using tree decomposition
algorithms. However, these methods frequently overlook important sparsity
aspects of Control Flow Graphs (CFGs) and result in high computational costs.
We introduce the SPL (Series-Parallel-Loop) decomposition, a novel framework
that offers optimal solutions to these challenges. A key contribution is the
formulation of a general solution for Partial Constraint Satisfaction Problems
(PCSPs) within graph structures, applied to three optimization problems. First,
SPL decomposition enhances register allocation by accurately modeling variable
interference graphs, leading to efficient register assignments and improved
performance across benchmarks. Second, it optimizes LOSPRE by effectively
identifying and eliminating redundancies in program execution. Finally, the
thesis focuses on optimizing the placement of bank selection instructions to
enhance data retrieval efficiency and reduce latency. Extensive experimentation
demonstrates significant performance improvements over existing methods,
establishing SPL decomposition as a powerful tool for complex compiler
optimizations, including register allocation, LOSPRE, and bank selection.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [5] [An Adequate While-Language for Stochastic Hybrid Computation](https://arxiv.org/abs/2507.15913)
*Renato Neves,José Proença,Juliana Souza*

Main category: cs.LO

TL;DR: 本文提出了一种形式化推理语言，用于分析结合微分构造和概率构造的程序，并为该语言提供了操作语义和指称语义，证明了两者之间的充分性定理


<details>
  <summary>Details</summary>
Motivation: 现有的形式化推理方法难以处理同时包含微分构造（如连续时间动态）和概率构造（如随机性）的复杂系统，需要一种统一的语言框架来描述和分析此类混合系统

Method: 设计了一种新的形式化语言，该语言能够表达微分和概率构造的组合；为该语言构建了操作语义（operational semantics）和指称语义（denotational semantics）；实现了相应的解释器；建立了两种语义之间的充分性定理

Result: 成功构建了能够形式化推理混合微分-概率程序的语言框架；证明了操作语义和指称语义之间的充分性定理，确保了语义的一致性；该语言能够描述自适应巡航控制器、连续时间随机游走、爱因斯坦布朗运动等复杂系统

Conclusion: 本文成功建立了一个统一的形式化框架，能够有效地推理和分析同时包含微分和概率特性的程序系统，为此类混合系统的验证和分析提供了理论基础

Abstract: We introduce a language for formally reasoning about programs that combine
differential constructs with probabilistic ones. The language harbours, for
example, such systems as adaptive cruise controllers, continuous-time random
walks, and physical processes involving multiple collisions, like in Einstein's
Brownian motion.
  We furnish the language with an operational semantics and use it to implement
a corresponding interpreter. We also present a complementary, denotational
semantics and establish an adequacy theorem between both cases.

</details>


### [6] [On Expansions of Monadic Second-Order Logic with Dynamical Predicates](https://arxiv.org/abs/2507.16581)
*Joris Nieuwveld,Joël Ouaknine*

Main category: cs.LO

TL;DR: 本文证明了带有一大类"动态"一元谓词的自然数结构的二阶单子逻辑理论的可判定性，这些谓词是整数线性递推序列取值的集合。


<details>
  <summary>Details</summary>
Motivation: 自1960年代Büchi和Elgot & Rabin的开创性工作以来，自然数结构⟨ℕ; <⟩的二阶单子逻辑(MSO)理论扩展一直是一个活跃的研究领域。本文旨在扩展这一理论到包含动态谓词的更复杂结构。

Method: 引入了"(有效)前析取性"这一新颖概念作为关键技术工具，用于分析包含动态一元谓词P的结构⟨ℕ; <,P⟩的MSO理论，其中P是由整数线性递推序列的非负值组成的集合。

Result: 成功建立了结构⟨ℕ; <,P⟩的MSO理论的可判定性，其中P涵盖了一大类"动态"一元谓词（即某些整数线性递推序列取的非负值集合）。

Conclusion: 证明了扩展的MSO理论在包含动态谓词的情况下仍然保持可判定性，并且提出的前析取性概念预期在更广泛的领域中具有独立的应用价值。

Abstract: Expansions of the monadic second-order (MSO) theory of the structure $\langle
\mathbb{N} ; < \rangle$ have been a fertile and active area of research ever
since the publication of the seminal papers of B\"uchi and Elgot & Rabin on the
subject in the 1960s. In the present paper, we establish decidability of the
MSO theory of $\langle \mathbb{N} ; <,P \rangle$, where $P$ ranges over a large
class of unary ''dynamical'' predicates, i.e., sets of non-negative values
assumed by certain integer linear recurrence sequences. One of our key
technical tools is the novel concept of (effective) prodisjunctivity, which we
expect may also find independent applications further afield.

</details>


### [7] [Transordinal Fixed-Point Operators and Self-Referential Games: A Categorical Framework for Reflective Semantic Convergence](https://arxiv.org/abs/2507.16620)
*Faruk Alpay,Hamdi Al Alakkad*

Main category: cs.LO

TL;DR: 该论文提出了一个统一范畴论不动点构造、超限递归和博弈语义学的理论框架，通过无限自指来建模语言解释的稳定化过程，证明了语义收敛的数学严格性账户


<details>
  <summary>Details</summary>
Motivation: 现有的语义收敛研究依赖统计训练或经验基准，缺乏数学严格性。需要一个理论框架来解释语言解释如何通过无限自指达到稳定，并为形式语言学提供精确保证

Method: 构建统一范畴论不动点构造、超限递归和博弈语义学的理论框架。通过在所有序数阶段迭代意义精化算子来分离唯一的"超序数"不动点，并通过反思博弈层次证明该对象是文本与解释者之间无限对话的唯一均衡

Result: 成功分离出唯一的"超序数"不动点，证明了该不动点是文本与解释者无限对话的唯一均衡。构造完全是符号化的，为形式语言学提供了精确保证，并为设计能够推理自身输出的语言感知系统提供了蓝图

Conclusion: 该研究提供了语义收敛的数学严格账户，无需依赖统计训练或经验基准。通过超序数机制的存在性和唯一性定理，连接了形式系统和语义学中关于反思、真理和均衡的长期问题，为语言理论和系统设计提供了新的理论基础

Abstract: We present a new theoretical framework that unifies category-theoretic
fixed-point constructions, transfinite recursion, and game-based semantics to
model how interpretations of language can stabilize through unlimited
self-reference. By iterating a meaning-refinement operator across all ordinal
stages, we isolate a unique "transordinal" fixed point and show, via a
hierarchy of reflective games, that this same object is the sole equilibrium of
an infinite dialogue between a text and its interpreter. The result delivers a
mathematically rigorous account of semantic convergence without resorting to
statistical training or empirical benchmarks, yet remains simple to explain:
start with a rough meaning, let speaker and listener correct each other
forever, and the process provably settles on a single, self-consistent
interpretation. Because the construction is entirely symbolic, it offers both
precise guarantees for formal linguistics and a blueprint for designing
language-aware systems that can reason about their own outputs. The paper
details the requisite transordinal machinery, proves existence and uniqueness
theorems, and connects them to long-standing questions about reflection, truth,
and equilibrium in formal systems and semantics.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [8] [AlgoTune: Can Language Models Speed Up General-Purpose Numerical Programs?](https://arxiv.org/abs/2507.15887)
*Ori Press,Brandon Amos,Haoyu Zhao,Yikai Wu,Samuel K. Ainsworth,Dominik Krupke,Patrick Kidger,Touqir Sajed,Bartolomeo Stellato,Jisun Park,Nathanael Bosch,Eli Meril,Albert Steppi,Arman Zharmagambetov,Fangzhao Zhang,David Perez-Pineiro,Alberto Mercurio,Ni Zhan,Talor Abramovich,Kilian Lieret,Hanlin Zhang,Shirley Huang,Matthias Bethge,Ofir Press*

Main category: cs.SE

TL;DR: 本研究提出了AlgoTune基准测试，评估语言模型在计算机科学、物理学和数学领域设计和实现高效算法的能力，发现当前模型主要依赖表面优化而非算法创新。


<details>
  <summary>Details</summary>
Motivation: 现有的语言模型评估主要集中在人类已经解决的任务上，缺乏对模型在开放性算法设计和实现方面创造性问题解决能力的评估。

Method: 构建了包含155个编程任务的AlgoTune基准测试，任务由领域专家收集，涵盖计算机科学、物理学和数学领域的计算挑战性问题；开发了验证和计时框架来评估LM生成的解决方案代码；创建了基线LM智能体AlgoTuner并在多个前沿模型上进行评估。

Result: AlgoTuner相比参考求解器（使用SciPy、sk-learn和CVXPY等库）实现了平均1.72倍的加速；但发现当前模型无法发现算法创新，更倾向于进行表面层次的优化。

Conclusion: 当前语言模型在算法设计方面仍存在局限性，主要依赖表面优化而非真正的算法创新，希望AlgoTune能够催化开发出具有超越人类最新性能的创造性问题解决能力的LM智能体。

Abstract: Despite progress in language model (LM) capabilities, evaluations have thus
far focused on models' performance on tasks that humans have previously solved,
including in programming (Jimenez et al., 2024) and mathematics (Glazer et al.,
2024). We therefore propose testing models' ability to design and implement
algorithms in an open-ended benchmark: We task LMs with writing code that
efficiently solves computationally challenging problems in computer science,
physics, and mathematics. Our AlgoTune benchmark consists of 155 coding tasks
collected from domain experts and a framework for validating and timing
LM-synthesized solution code, which is compared to reference implementations
from popular open-source packages. In addition, we develop a baseline LM agent,
AlgoTuner, and evaluate its performance across a suite of frontier models.
AlgoTuner achieves an average 1.72x speedup against our reference solvers,
which use libraries such as SciPy, sk-learn and CVXPY. However, we find that
current models fail to discover algorithmic innovations, instead preferring
surface-level optimizations. We hope that AlgoTune catalyzes the development of
LM agents exhibiting creative problem solving beyond state-of-the-art human
performance.

</details>


### [9] [Dr. Boot: Bootstrapping Program Synthesis Language Models to Perform Repairing](https://arxiv.org/abs/2507.15889)
*Noah van der Vleuten*

Main category: cs.SE

TL;DR: 该论文提出了一种用于程序合成的自举算法，通过教会模型如何修复代码来解决现有语言模型数据饥饿和与人类编程过程不匹配的问题，实验表明该方法优于常规微调且能以更小的模型达到更大模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的程序合成语言模型面临两个主要问题：1）训练数据集规模有限且质量不高，而模型极度依赖大量数据；2）模型的程序合成过程与人类不匹配，人类会借助编译器迭代开发代码，而大多数模型是一次性生成代码。

Method: 提出了一种用于程序合成的自举算法，该算法支持教会模型如何修复代码。通过这种方式，模型可以学习更贴近人类编程习惯的迭代修复过程。

Result: 自举算法在性能上持续优于常规微调；与其他工作相比，使用自举的模型能够达到比其大68%的微调模型相当的性能；带修复的自举方法在推理时也能提升非修复性能；但在该模型上，推理时的修复可能不如简单地采样相同数量的解决方案；发现APPS数据集训练部分的示例测试用例存在问题。

Conclusion: 自举算法是一种有效的程序合成训练方法，能够提升模型性能并减少对大模型的依赖，同时揭示了现有数据集的质量问题，为基于修复和强化学习的方法提供了有价值的发现。

Abstract: Language models for program synthesis are usually trained and evaluated on
programming competition datasets (MBPP, APPS). However, these datasets are
limited in size and quality, while these language models are extremely data
hungry. Additionally, the language models have a misaligned program synthesis
process compared to humans. While humans iteratively develop code with the help
of a compiler, most program synthesis models currently produce code in one go.
To solve these issues, we introduce a bootstrapping algorithm for program
synthesis, that supports teaching models how to repair. We show that
bootstrapping consistently outperforms regular fine-tuning. Compared to other
work, our bootstrapped model performs on par with fine-tuned models that are
68\% larger. Notably, bootstrapping with repairing also improves non-repairing
performance compared to regular bootstrapping during inference. However, on our
models, repairing during inference is likely inferior to simply sampling the
same number of solutions. Furthermore, we find that there are issues with the
example test cases in the training portion of the APPS dataset that are
valuable to the community, as many repairing and reinforcement learning methods
rely on them.

</details>


### [10] [StaAgent: An Agentic Framework for Testing Static Analyzers](https://arxiv.org/abs/2507.15892)
*Elijah Nnorom,Md Basim Uddin Ahmed,Jiho Shin,Hung Viet Pham,Song Wang*

Main category: cs.SE

TL;DR: 本文提出了StaAgent，一个基于大语言模型的多智能体框架，用于系统性评估静态分析器规则的正确性。该框架通过四个专门的智能体协作，发现了64个现有静态分析器中的问题规则。


<details>
  <summary>Details</summary>
Motivation: 静态分析器在软件开发生命周期中发挥关键作用，但其规则实现往往测试不足且容易出现不一致性问题。现有方法难以系统性地评估静态分析器规则的正确性，需要一种可扩展且适应性强的解决方案。

Method: 提出StaAgent框架，包含四个专门智能体：1）种子生成智能体：将错误检测规则转换为具体的错误诱导种子程序；2）代码验证智能体：确保种子程序的正确性；3）变异生成智能体：产生语义等价的变异体；4）分析器评估智能体：通过比较静态分析器对种子程序及其变异体的行为进行变形测试。

Result: 在5个最先进的大语言模型和5个广泛使用的静态分析器上进行评估，发现了64个问题规则（SpotBugs 28个、SonarQube 18个、ErrorProne 6个、Infer 4个、PMD 8个）。其中53个错误无法被现有最先进基线方法检测到。已向开发者报告所有错误，其中2个已修复，3个已确认。

Conclusion: 实验结果证明了该方法的有效性，展示了基于智能体和大语言模型驱动的数据合成方法在推进软件工程领域的巨大潜力。StaAgent为提高静态分析器可靠性提供了可扩展且适应性强的解决方案。

Abstract: Static analyzers play a critical role in identifying bugs early in the
software development lifecycle, but their rule implementations are often
under-tested and prone to inconsistencies. To address this, we propose
StaAgent, an agentic framework that harnesses the generative capabilities of
Large Language Models (LLMs) to systematically evaluate static analyzer rules.
StaAgent comprises four specialized agents: a Seed Generation Agent that
translates bug detection rules into concrete, bug-inducing seed programs; a
Code Validation Agent that ensures the correctness of these seeds; a Mutation
Generation Agent that produces semantically equivalent mutants; and an Analyzer
Evaluation Agent that performs metamorphic testing by comparing the static
analyzer's behavior on seeds and their corresponding mutants. By revealing
inconsistent behaviors, StaAgent helps uncover flaws in rule implementations.
This LLM-driven, multi-agent framework offers a scalable and adaptable solution
to improve the reliability of static analyzers. We evaluated StaAgent with five
state-of-the-art LLMs (CodeL-lama, DeepSeek, Codestral, Qwen, and GPT-4o)
across five widely used static analyzers (SpotBugs, SonarQube, ErrorProne,
Infer, and PMD). The experimental results show that our approach can help
reveal 64 problematic rules in the latest versions of these five static
analyzers (i.e., 28 in SpotBugs, 18 in SonarQube, 6 in ErrorProne, 4 in Infer,
and 8 in PMD). In addition, 53 out of the 64 bugs cannot be detected by the
SOTA baseline. We have reported all the bugs to developers, with two of them
already fixed. Three more have been confirmed by developers, while the rest are
awaiting response. These results demonstrate the effectiveness of our approach
and underscore the promise of agentic, LLM-driven data synthesis to advance
software engineering.

</details>


### [11] [A Pilot Study on LLM-Based Agentic Translation from Android to iOS: Pitfalls and Insights](https://arxiv.org/abs/2507.16037)
*Zhili Zeng,Kimya Khakzad Shahandashti,Alvine Boaye Belle,Song Wang,Zhen Ming,Jiang*

Main category: cs.SE

TL;DR: 本研究评估了基于大语言模型(LLM)的智能体方法在移动应用跨平台翻译(Android到iOS)中的表现，通过开发智能体链来处理依赖关系、规范、程序结构和控制流，并分析翻译失败的根本原因以提出改进指导方案。


<details>
  <summary>Details</summary>
Motivation: 传统的移动应用翻译方法依赖人工干预或基于规则的系统，效率低下且耗时；现有的机器学习自动化方法缺乏上下文理解和适应性，导致翻译效果不佳；虽然大语言模型在代码翻译方面取得进展，但在跨平台应用翻译(如Android到iOS迁移)方面仍缺乏深入研究，需要了解LLM在此领域的性能、优势和局限性。

Method: 开发了一个智能体链(chain of agents)来处理从Android到iOS的应用翻译，该方法考虑了依赖关系、规范、程序结构和程序控制流等因素。通过人工检查翻译代码的语法正确性、语义准确性和功能完整性来评估性能，并对翻译失败案例进行详细的根本原因分析。

Result: 通过评估LLM智能体方法在移动应用翻译中的表现，识别了关键的失败点，并对翻译过程的潜在局限性进行了深入分析，为理解智能体翻译过程的不足之处和改进机会提供了见解。

Conclusion: 该研究填补了LLM在跨平台应用翻译领域的研究空白，通过系统性评估智能体方法的性能和局限性，为推进软件工程自动化提供了重要贡献，并提出了改进翻译性能的指导方案，为未来的跨平台应用迁移研究奠定了基础。

Abstract: The rapid advancement of mobile applications has led to a significant demand
for cross-platform compatibility, particularly between the Android and iOS
platforms. Traditional approaches to mobile application translation often rely
on manual intervention or rule-based systems, which are labor-intensive and
time-consuming. While recent advancements in machine learning have introduced
automated methods, they often lack contextual understanding and adaptability,
resulting in suboptimal translations. Large Language Models (LLMs) were
recently leveraged to enhance code translation at different granularities,
including the method, class, and repository levels. Researchers have
investigated common errors, limitations, and potential strategies to improve
these tasks. However, LLM-based application translation across different
platforms, such as migrating mobile applications between Android and iOS or
adapting software across diverse frameworks, remains underexplored.
Understanding the performance, strengths, and limitations of LLMs in
cross-platform application translation is critical for advancing software
engineering automation. This study aims to fill this gap by evaluating
LLM-based agentic approaches for mobile application translation, identifying
key failure points, and proposing guidelines to improve translation
performance. We developed a chain of agents that account for dependencies,
specifications, program structure, and program control flow when translating
applications from Android to iOS. To evaluate the performance, we manually
examined the translated code for syntactic correctness, semantic accuracy, and
functional completeness. For translation failures, we further conducted a
detailed root cause analysis to understand the underlying limitations of the
agentic translation process and identify opportunities for improvement.

</details>


### [12] [Making REST APIs Agent-Ready: From OpenAPI to Model Context Protocol Servers for Tool-Augmented LLMs](https://arxiv.org/abs/2507.16044)
*Meriem Mastouri,Emna Ksontini,Wael Kessentini*

Main category: cs.SE

TL;DR: 该论文提出了AutoMCP，一个能够从OpenAPI规范自动生成MCP服务器的编译器，解决了手动构建MCP服务器繁琐重复的问题，在50个真实API的测试中实现了99.9%的成功率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型正从被动文本生成器演变为调用外部工具的主动代理，需要可扩展的工具集成协议。虽然Anthropic推出的模型上下文协议(MCP)提供了动态工具发现和调用的标准，但构建MCP服务器仍然需要手动编写大量重复代码，包括处理认证、配置模式等，这与MCP旨在消除集成工作的目标相矛盾。

Method: 研究者开发了AutoMCP编译器，它能够从OpenAPI 2.0/3.0规范自动生成完整的MCP服务器实现。AutoMCP解析REST API定义并生成包括模式注册和认证处理在内的完整服务器代码。研究团队在50个真实世界API上进行评估，涵盖超过10个领域的5,066个端点。

Result: 在1,023个分层抽样的工具调用中，76.5%开箱即用成功。失败分析显示五个反复出现的问题，都归因于OpenAPI契约中的不一致或遗漏。经过轻微修复（平均每个API修改19行规范），AutoMCP达到了99.9%的成功率。研究还分析了MCP采用趋势，发现在22,000+个MCP标签的GitHub仓库中，不到5%包含服务器。

Conclusion: 研究证明了尽管OpenAPI规范存在质量问题，但仍能实现近乎完全的MCP服务器自动化。该工作量化了手动MCP服务器开发的成本，贡献了包含5,066个可调用工具的语料库，并提供了修复常见规范缺陷的见解，为大语言模型工具集成的自动化提供了有效解决方案。

Abstract: Large Language Models (LLMs) are evolving from passive text generators into
active agents that invoke external tools. To support this shift, scalable
protocols for tool integration are essential. The Model Context Protocol (MCP),
introduced by Anthropic in 2024, offers a schema-driven standard for dynamic
tool discovery and invocation. Yet, building MCP servers remains manual and
repetitive, requiring developers to write glue code, handle authentication, and
configure schemas by hand-replicating much of the integration effort MCP aims
to eliminate.
  This paper investigates whether MCP server construction can be meaningfully
automated. We begin by analyzing adoption trends: among 22,000+ MCP-tagged
GitHub repositories created within six months of release, fewer than 5% include
servers, typically small, single-maintainer projects dominated by repetitive
scaffolding. To address this gap, we present AutoMCP, a compiler that generates
MCP servers from OpenAPI 2.0/3.0 specifications. AutoMCP parses REST API
definitions and produces complete server implementations, including schema
registration and authentication handling.
  We evaluate AutoMCP on 50 real-world APIs spanning 5,066 endpoints across
over 10 domains. From a stratified sample of 1,023 tool calls, 76.5% succeeded
out of the box. Manual failure analysis revealed five recurring issues, all
attributable to inconsistencies or omissions in the OpenAPI contracts. After
minor fixes, averaging 19 lines of spec changes per API, AutoMCP achieved 99.9%
success.
  Our findings (i) analyze MCP adoption and quantify the cost of manual server
development, (ii) demonstrate that OpenAPI specifications, despite quality
issues, enable near-complete MCP server automation, and (iii) contribute a
corpus of 5,066 callable tools along with insights on repairing common
specification flaws.

</details>


### [13] [AI-Powered Commit Explorer (APCE)](https://arxiv.org/abs/2507.16063)
*Yousab Grees,Polina Iaremchuk,Ramtin Ehsani,Esteban Parra,Preetha Chatterjee,Sonia Haiduc*

Main category: cs.SE

TL;DR: 本文介绍了AI-Powered Commit Explorer (APCE)工具，用于支持开发者和研究人员使用和研究大语言模型生成的代码提交信息，该工具提供了提示词存储、评估增强和自动化评估功能。


<details>
  <summary>Details</summary>
Motivation: 代码提交信息对开发者了解代码变更非常重要，但在实践中编写高质量提交信息经常被忽视。虽然大语言模型可以生成提交信息来缓解这个问题，但需要专门的工具来支持开发者和研究人员更好地使用和研究LLM生成的提交信息。

Method: 开发了APCE工具，该工具具有以下功能：1）为大语言模型存储不同的提示词；2）提供额外的评估提示来进一步增强LLM生成的提交信息；3）为研究人员提供对LLM生成信息进行自动化和人工评估的直接机制。

Result: 成功开发了APCE工具并提供了演示链接，该工具能够有效支持开发者和研究人员在LLM生成提交信息方面的工作和研究。

Conclusion: APCE工具为解决代码提交信息质量问题提供了有效的解决方案，通过集成大语言模型技术和评估机制，能够帮助开发者生成更高质量的提交信息，同时为研究人员提供了研究LLM在此领域应用的便利工具。

Abstract: Commit messages in a version control system provide valuable information for
developers regarding code changes in software systems. Commit messages can be
the only source of information left for future developers describing what was
changed and why. However, writing high-quality commit messages is often
neglected in practice. Large Language Model (LLM) generated commit messages
have emerged as a way to mitigate this issue. We introduce the AI-Powered
Commit Explorer (APCE), a tool to support developers and researchers in the use
and study of LLM-generated commit messages. APCE gives researchers the option
to store different prompts for LLMs and provides an additional evaluation
prompt that can further enhance the commit message provided by LLMs. APCE also
provides researchers with a straightforward mechanism for automated and human
evaluation of LLM-generated messages. Demo link https://youtu.be/zYrJ9s6sZvo

</details>


### [14] [Ten Essential Guidelines for Building High-Quality Research Software](https://arxiv.org/abs/2507.16166)
*Nasir U. Eisty,David E. Bernholdt,Alex Koufos,David J. Luet,Miranda Mundt*

Main category: cs.SE

TL;DR: 本文提出了十项高质量科研软件开发指南，涵盖软件开发生命周期的各个阶段，旨在帮助研究人员创建健壮、可用且可持续的科研软件工具。


<details>
  <summary>Details</summary>
Motivation: 现代科学研究严重依赖高质量的研究软件来分析复杂数据、模拟现象和分享可重现的结果，但创建此类软件需要遵循最佳实践以确保其健壮性、可用性和可持续性。

Method: 提出了涵盖软件开发生命周期各个阶段的十项指南，包括规划、编写清洁可读的代码、使用版本控制、实施全面的测试策略、模块化设计、可重现性、性能优化、长期维护、文档编写和社区参与等关键原则。

Result: 该指南为研究人员和开发者提供了一个实用的资源框架，能够帮助他们创建既能推进科学目标又能为更广泛的可靠可重用研究工具生态系统做出贡献的软件。

Conclusion: 通过遵循这些指南，研究人员可以创建高质量的科研软件，不仅能推进其科学目标，还能为构建可靠且可重用的研究工具生态系统做出贡献，从而提升研究软件的质量和影响力。

Abstract: High-quality research software is a cornerstone of modern scientific
progress, enabling researchers to analyze complex data, simulate phenomena, and
share reproducible results. However, creating such software requires adherence
to best practices that ensure robustness, usability, and sustainability. This
paper presents ten guidelines for producing high-quality research software,
covering every stage of the development lifecycle. These guidelines emphasize
the importance of planning, writing clean and readable code, using version
control, and implementing thorough testing strategies. Additionally, they
address key principles such as modular design, reproducibility, performance
optimization, and long-term maintenance. The paper also highlights the role of
documentation and community engagement in enhancing software usability and
impact. By following these guidelines, researchers can create software that
advances their scientific objectives and contributes to a broader ecosystem of
reliable and reusable research tools. This work serves as a practical resource
for researchers and developers aiming to elevate the quality and impact of
their research software.

</details>


### [15] [LOCOFY Large Design Models -- Design to code conversion solution](https://arxiv.org/abs/2507.16208)
*Sohaib Muhammad,Ashwati Vipin,Karan Shetti,Honey Mittal*

Main category: cs.SE

TL;DR: 本文提出了大型设计模型(LDMs)范式，专门针对设计和网页进行训练，实现从设计到代码的无缝转换，解决了现有大语言模型在设计到代码转换中的可解释性、可扩展性和资源需求等挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型和多模态大语言模型在设计到代码转换应用中面临可解释性、可扩展性、资源需求和可重复性等诸多挑战，需要一个专门针对设计领域训练的解决方案。

Method: 开发了包含三个核心组件的训练和推理管道：1)设计优化器：使用专有真实数据集处理次优设计；2)标记和特征检测：使用预训练和微调模型准确检测和分类UI元素；3)自动组件：提取重复的UI结构为可重用组件，创建模块化代码。此外还有专门的推理管道处理真实设计以生成精确可解释的代码生成指令。

Result: LDMs在端到端设计到代码转换准确性方面表现卓越，使用新颖的预览匹配得分指标进行评估。与LLMs相比，在节点定位准确性、响应性和可重现性方面表现优异。定制训练的标记和特征检测模型在识别UI元素方面展现了高精度和一致性。

Conclusion: 提出的LDMs是理解设计并生成高效可靠的生产就绪代码的可靠且优越的解决方案，为设计到代码转换领域提供了专业化的模型范式。

Abstract: Despite rapid advances in Large Language Models and Multimodal Large Language
Models (LLMs), numerous challenges related to interpretability, scalability,
resource requirements and repeatability remain, related to their application in
the design-to-code space. To address this, we introduce the Large Design Models
(LDMs) paradigm specifically trained on designs and webpages to enable seamless
conversion from design-to-code. We have developed a training and inference
pipeline by incorporating data engineering and appropriate model architecture
modification. The training pipeline consists of the following: 1)Design
Optimiser: developed using a proprietary ground truth dataset and addresses
sub-optimal designs; 2)Tagging and feature detection: using pre-trained and
fine-tuned models, this enables the accurate detection and classification of UI
elements; and 3)Auto Components: extracts repeated UI structures into reusable
components to enable creation of modular code, thus reducing redundancy while
enhancing code reusability. In this manner, each model addresses distinct but
key issues for design-to-code conversion. Separately, our inference pipeline
processes real-world designs to produce precise and interpretable instructions
for code generation and ensures reliability. Additionally, our models
illustrated exceptional end-to-end design-to-code conversion accuracy using a
novel preview match score metric. Comparative experiments indicated superior
performance of LDMs against LLMs on accuracy of node positioning,
responsiveness and reproducibility. Moreover, our custom-trained tagging and
feature detection model demonstrated high precision and consistency in
identifying UI elements across a wide sample of test designs. Thus, our
proposed LDMs are a reliable and superior solution to understanding designs
that subsequently enable the generation of efficient and reliable
production-ready code.

</details>


### [16] [Search-based Generation of Waypoints for Triggering Self-Adaptations in Maritime Autonomous Vessels](https://arxiv.org/abs/2507.16327)
*Karoline Nylænder,Aitor Arrieta,Shaukat Ali,Paolo Arcaini*

Main category: cs.SE

TL;DR: 本文提出了WPgen方法，通过多目标搜索算法生成轻微修改的航路点来测试海上自主船只的导航适应性，帮助验证自适应系统的实现。


<details>
  <summary>Details</summary>
Motivation: 海上自主船只需要自适应能力来应对意外情况并保持可靠性要求。在设计此类船只时，需要理解和识别触发适应的设置条件，以验证其实现的有效性。现有方法缺乏有效的测试手段来验证导航软件的自适应行为。

Method: 提出了基于多目标搜索的WPgen方法，使用NSGA-II算法生成对预定义航路点的轻微修改。该方法保持生成的航路点尽可能接近原始航路点，同时导致自主船只在使用生成航路点时出现不当导航行为。采用三种不同的种子策略进行初始种群设置，形成WPgen的三个变体。

Result: 在三艘自主船只（一艘水面油轮和两艘水下船只）上评估了WPgen的三个变体。与随机搜索基线方法和各变体之间进行了比较。实验结果显示不同变体的有效性因船只类型而异，证明了方法的实用性。

Conclusion: WPgen方法能够有效生成测试用例来验证海上自主船只导航软件的自适应能力。不同的种子策略适用于不同类型的自主船只。研究为自主船只自适应系统的设计验证提供了实用的解决方案和研究启示。

Abstract: Self-adaptation in maritime autonomous vessels (AVs) enables them to adapt
their behaviors to address unexpected situations while maintaining
dependability requirements. During the design of such AVs, it is crucial to
understand and identify the settings that should trigger adaptations, enabling
validation of their implementation. To this end, we focus on the navigation
software of AVs, which must adapt their behavior during operation through
adaptations. AVs often rely on predefined waypoints to guide them along
designated routes, ensuring safe navigation. We propose a multiobjective
search-based approach, called WPgen, to generate minor modifications to the
predefined set of waypoints, keeping them as close as possible to the original
waypoints, while causing the AV to navigate inappropriately when navigating
with the generated waypoints. WPgen uses NSGA-II as the multi-objective search
algorithm with three seeding strategies for its initial population, resulting
in three variations of WPgen. We evaluated these variations on three AVs (one
overwater tanker and two underwater). We compared the three variations of WPgen
with Random Search as the baseline and with each other. Experimental results
showed that the effectiveness of these variations varied depending on the AV.
Based on the results, we present the research and practical implications of
WPgen.

</details>


### [17] [Improving Code LLM Robustness to Prompt Perturbations via Layer-Aware Model Editing](https://arxiv.org/abs/2507.16407)
*Shuhan Liu,Xing Hu,Kerui Huang,Xiaohu Yang,David Lo,Xin Xia*

Main category: cs.SE

TL;DR: 本文提出CREME方法，通过模型编辑技术提升大语言模型在代码生成任务中对提示扰动的鲁棒性，在扰动提示上将Pass@1准确率提升63%，同时保持干净输入的稳定性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码生成中对提示扰动高度敏感，措辞、语法或格式的微小变化会显著降低生成代码的功能正确性。由于扰动在现实场景中频繁发生，提升LLM对提示扰动的鲁棒性对确保实际代码生成的可靠性能至关重要。

Method: CREME（通过模型编辑增强代码鲁棒性）是一种通过目标参数更新来增强LLM鲁棒性的新方法。首先通过比较原始提示和扰动变体之间的隐藏状态来识别鲁棒性敏感层，然后在识别的层进行轻量级参数编辑以减少性能下降。

Result: 在HumanEval和MBPP两个代码生成基准及其扰动对应版本上的实验结果显示，CREME在扰动提示上将Pass@1准确率提升了63%，同时在干净输入上保持稳定性能，准确率偏差在1%以内。分析发现鲁棒性敏感层主要集中在网络的中层和深层，且其位置因不同模型架构而异。

Conclusion: CREME有效提升了大语言模型对提示扰动的鲁棒性，实验证明了该方法的有效性。对鲁棒性敏感层分布的深入分析为未来开发面向鲁棒性的编辑策略提供了宝贵基础。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
code generation, where the natural language prompt plays a crucial role in
conveying user intent to the model. However, prior studies have shown that LLMs
are highly sensitive to prompt perturbations. Minor modifications in wording,
syntax, or formatting can significantly reduce the functional correctness of
generated code. As perturbations frequently occur in real-world scenarios,
improving the robustness of LLMs to prompt perturbations is essential for
ensuring reliable performance in practical code generation. In this paper, we
introduce CREME (Code Robustness Enhancement via Model Editing), a novel
approach that enhances LLM robustness through targeted parameter updates. CREME
first identifies robustness-sensitive layers by comparing hidden states between
an original prompt and its perturbed variant. Then, it performs lightweight
parameter editing at the identified layer to reduce performance degradation. We
evaluate CREME on two widely used code generation benchmarks (HumanEval and
MBPP) along with their perturbed counterparts. Experimental results show that
CREME improves Pass@1 accuracy by 63% on perturbed prompts while maintaining
stable performance on clean inputs, with accuracy deviations within 1%. Further
analysis reveals that robustness-sensitive layers are primarily concentrated in
the middle and deeper layers of the network, and their locations vary across
different model architectures. These insights provide a valuable foundation for
developing future robustness-oriented editing strategies.

</details>


### [18] [Exploring Large Language Models for Analyzing and Improving Method Names in Scientific Code](https://arxiv.org/abs/2507.16439)
*Gunnar Larsen,Carol Wong,Anthony Peruma*

Main category: cs.SE

TL;DR: 研究评估了四个大型语言模型在分析科学软件中方法名称质量方面的能力，发现LLMs在分析方法名称方面有一定效果但需要人工评估


<details>
  <summary>Details</summary>
Motivation: 科学研究人员越来越依赖软件支持研究工作，但在科学软件中标识符命名对程序理解的影响研究有限，特别是方法名称质量方面。大型语言模型的发展为自动化代码分析任务提供了新机会

Method: 评估四个流行的大型语言模型分析从Python Jupyter Notebooks中提取的496个方法名称的语法模式并提出改进建议的能力

Result: LLMs在分析方法名称方面有一定效果，通常遵循良好的命名实践（如方法名以动词开头），但在处理领域特定术语方面不一致，与人工标注的一致性仅为中等水平

Conclusion: 自动化建议需要人工评估，该工作为通过AI自动化改善科学代码质量提供了基础见解

Abstract: Research scientists increasingly rely on implementing software to support
their research. While previous research has examined the impact of identifier
names on program comprehension in traditional programming environments, limited
work has explored this area in scientific software, especially regarding the
quality of method names in the code. The recent advances in Large Language
Models (LLMs) present new opportunities for automating code analysis tasks,
such as identifier name appraisals and recommendations. Our study evaluates
four popular LLMs on their ability to analyze grammatical patterns and suggest
improvements for 496 method names extracted from Python-based Jupyter
Notebooks. Our findings show that the LLMs are somewhat effective in analyzing
these method names and generally follow good naming practices, like starting
method names with verbs. However, their inconsistent handling of
domain-specific terminology and only moderate agreement with human annotations
indicate that automated suggestions require human evaluation. This work
provides foundational insights for improving the quality of scientific code
through AI automation.

</details>


### [19] [On the Effectiveness of LLM-as-a-judge for Code Generation and Summarization](https://arxiv.org/abs/2507.16587)
*Giuseppe Crupi,Rosalia Tufano,Alejandro Velasco,Antonio Mastropaolo,Denys Poshyvanyk,Gabriele Bavota*

Main category: cs.SE

TL;DR: 本研究评估了大语言模型作为判断者在代码生成和代码摘要任务中的有效性，发现GPT-4-turbo表现最佳，但即使是最好的模型也经常出现误判。


<details>
  <summary>Details</summary>
Motivation: 针对代码生成和代码摘要等复杂任务，传统定量指标（如BLEU）无法充分评估质量，而大规模人工评估成本过高，因此需要探索使用大语言模型作为判断者的可行性，以实现自动化评估和多模型协作。

Method: 选择代码生成和代码摘要两个任务进行研究。对于代码生成，使用8个大语言模型判断1,405个Java方法和1,281个Python函数的正确性；对于代码摘要，将5个大语言模型的判断与9个人类评估者对约1,200个Java和Python函数摘要的判断进行比较。

Result: GPT-4-turbo在两个任务的判断能力方面表现最佳，而参数规模为数百亿的"较小"大语言模型无法胜任判断任务。然而，即使是表现最好的大语言模型也经常对代码正确性和摘要质量做出错误判断。

Conclusion: 虽然GPT-4-turbo在大语言模型判断者中表现最优，但当前大语言模型作为判断者的可靠性仍有限，在代码相关任务的自动化评估中存在显著局限性，需要进一步改进才能实现可靠的自动化判断。

Abstract: Large Language Models have been recently exploited as judges for complex
natural language processing tasks, such as Q&A. The basic idea is to delegate
to an LLM the assessment of the "quality" of the output provided by an
automated technique for tasks for which: (i) quantitative metrics would only
tell part of the story, and; (ii) a large-scale human-based evaluation would be
too expensive. LLMs-as-a-judge, if proven effective for a specific task, can
also unlock new possibilities for automation, with several LLMs proposing a
solution for a given instance of the task and others judging and deciding what
is the best output to show the user. We study the effectiveness of
LLMs-as-a-judge for two code-related tasks, namely code generation and code
summarization. The rationale for choosing these tasks is two-fold. First,
quantitative metrics are usually not enough for the assessment of code
summarizers/generators. For example, it is well documented that metrics such as
BLEU are quite weak proxies for the quality of the generated summaries. Second,
even state-of-the-art techniques still struggle with handling complex instances
of these tasks, making them good candidates for benefiting from more advanced
solutions envisioning collaboration among LLMs. For code generation, we check
whether eight LLMs are able to judge the correctness of 1,405 Java methods and
1,281 Python functions generated by the same LLMs or implemented by humans. For
code summarization, we compare the judgment of five LLMs to those provided by
nine humans for ~1.2k summaries, related to both Java and Python functions. Our
findings show that GPT-4-turbo is the best LLM in terms of judging capabilities
for both tasks, with "smaller" LLMs featuring tens of billions parameters not
being able to cope with judging tasks. However, even the best-performing LLM
frequently misjudges the correctness of the code and summary quality.

</details>


### [20] [VulCoCo: A Simple Yet Effective Method for Detecting Vulnerable Code Clones](https://arxiv.org/abs/2507.16661)
*Tan Bui,Yan Naing Tun,Thanh Phuc Nguyen,Yindu Su,Ferdian Thung,Yikun Li,Han Wei Ang,Yide Yin,Frank Liauw,Lwin Khin Shar,Eng Lieh Ouh,Ting Zhang,David Lo*

Main category: cs.SE

TL;DR: 本文提出VulCoCo，一种结合嵌入式检索和大语言模型验证的轻量级方法，用于检测易受攻击的代码克隆(VCCs)。该方法在合成基准测试中表现优异，并在实际开源项目中成功检测到漏洞，获得75个合并的拉取请求和15个新发布的CVE。


<details>
  <summary>Details</summary>
Motivation: 现代软件开发中代码复用很常见，但开发者无意中复制有风险的代码时会传播漏洞。现有的易受攻击代码克隆检测工具往往依赖语法相似性或产生粗糙的漏洞预测且缺乏清晰解释，限制了实用性。同时缺乏可复现的易受攻击代码克隆基准测试。

Method: 提出VulCoCo方法，结合嵌入式检索和大语言模型验证。从已知的易受攻击函数集合出发，从大型语料库中检索语法或语义相似的候选函数，然后使用LLM评估候选函数是否保留了漏洞。此外构建了一个跨越各种克隆类型的合成基准测试。

Result: 在基准测试中，VulCoCo在Precision@k和平均精度均值(MAP)方面优于现有最先进方法。在实际应用中，向284个开源项目提交了400个拉取请求，其中75个被合并，15个导致新发布的CVE，证明了方法在真实世界项目中的有效性。

Conclusion: VulCoCo是一种有效的易受攻击代码克隆检测方法，在合成基准和实际项目中都表现出色。研究还为未来进一步提高易受攻击代码克隆检测精度的工作提供了见解和启发。

Abstract: Code reuse is common in modern software development, but it can also spread
vulnerabilities when developers unknowingly copy risky code. The code fragments
that preserve the logic of known vulnerabilities are known as vulnerable code
clones (VCCs). Detecting those VCCs is a critical but challenging task.
Existing VCC detection tools often rely on syntactic similarity or produce
coarse vulnerability predictions without clear explanations, limiting their
practical utility. In this paper, we propose VulCoCo, a lightweight and
scalable approach that combines embedding-based retrieval with large language
model (LLM) validation. Starting from a set of known vulnerable functions, we
retrieve syntactically or semantically similar candidate functions from a large
corpus and use an LLM to assess whether the candidates retain the
vulnerability. Given that there is a lack of reproducible vulnerable code clone
benchmarks, we first construct a synthetic benchmark that spans various clone
types.
  Our experiments on the benchmark show that VulCoCo outperforms prior
state-of-the-art methods in terms of Precision@k and mean average precision
(MAP). In addition, we also demonstrate VulCoCo's effectiveness in real-world
projects by submitting 400 pull requests (PRs) to 284 open-source projects.
Among them, 75 PRs were merged, and 15 resulted in newly published CVEs. We
also provide insights to inspire future work to further improve the precision
of vulnerable code clone detection.

</details>


### [21] [VulGuard: An Unified Tool for Evaluating Just-In-Time Vulnerability Prediction Models](https://arxiv.org/abs/2507.16685)
*Duong Nguyen,Manh Tran-Duc,Thanh Le-Cong,Triet Huynh Minh Le,M. Ali Babar,Quyet-Thang Huynh*

Main category: cs.SE

TL;DR: VulGuard是一个自动化工具，用于从GitHub仓库中提取、处理和分析提交数据，以支持即时漏洞预测(JIT-VP)研究，集成了多种最先进的漏洞预测模型，并可轻松集成到CI/CD流水线中。


<details>
  <summary>Details</summary>
Motivation: 解决软件安全研究中即时漏洞预测(JIT-VP)领域的关键挑战，包括可重现性和可扩展性问题，简化从GitHub仓库中提取和分析提交数据的复杂流程。

Method: 设计了一个统一框架，自动挖掘提交历史，提取细粒度代码变更、提交消息和软件工程指标，并将其格式化用于下游分析；集成多个最先进的漏洞预测模型，支持仓库级挖掘和模型级实验。

Result: 在FFmpeg和Linux内核两个有影响力的开源项目中验证了工具的有效性，展示了其在真实世界JIT-VP研究中的应用潜力，并能促进标准化基准测试。

Conclusion: VulGuard为软件安全研究提供了一个有效的自动化解决方案，能够加速即时漏洞预测研究，提高研究的可重现性和可扩展性，同时支持CI/CD集成，具有重要的实用价值。

Abstract: We present VulGuard, an automated tool designed to streamline the extraction,
processing, and analysis of commits from GitHub repositories for Just-In-Time
vulnerability prediction (JIT-VP) research. VulGuard automatically mines commit
histories, extracts fine-grained code changes, commit messages, and software
engineering metrics, and formats them for downstream analysis. In addition, it
integrates several state-of-the-art vulnerability prediction models, allowing
researchers to train, evaluate, and compare models with minimal setup. By
supporting both repository-scale mining and model-level experimentation within
a unified framework, VulGuard addresses key challenges in reproducibility and
scalability in software security research. VulGuard can also be easily
integrated into the CI/CD pipeline. We demonstrate the effectiveness of the
tool in two influential open-source projects, FFmpeg and the Linux kernel,
highlighting its potential to accelerate real-world JIT-VP research and promote
standardized benchmarking. A demo video is available at:
https://youtu.be/j96096-pxbs

</details>


### [22] [Never Come Up Empty: Adaptive HyDE Retrieval for Improving LLM Developer Support](https://arxiv.org/abs/2507.16754)
*Fangjian Lei,Mariam El Mezouar,Shayan Noei,Ying Zou*

Main category: cs.SE

TL;DR: 研究者构建了包含300万个Java和Python相关Stack Overflow帖子的检索语料库，设计并评估了7种不同的RAG管道和63个管道变体，以提高大语言模型回答开发者问题的准确性和可靠性，发现结合HyDE和完整答案上下文的RAG管道表现最佳。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在协助开发者回答代码相关问题方面显示出潜力，但存在生成不可靠答案（幻觉）的风险。虽然检索增强生成（RAG）被提出来减少这种不可靠性，但由于设计选择众多，设计有效的管道仍然具有挑战性。

Method: 构建了包含超过300万个Java和Python相关Stack Overflow帖子的检索语料库，设计并评估7种不同的RAG管道和63个管道变体。对于历史上有相似匹配的问题进行评估，对于没有相近匹配的新问题，通过自动降低检索时的相似度阈值来增加找到部分相关上下文的机会。

Result: 发现结合假设文档嵌入（HyDE）和完整答案上下文的RAG管道在检索和回答Stack Overflow相似问题方面表现最佳。将最优RAG管道应用于4个开源LLM并与零样本性能比较，结果显示RAG始终优于零样本基线，在有用性、正确性和详细性方面获得更高分数。

Conclusion: 最优RAG管道能够稳健地提升各种开发者查询的答案质量，包括之前见过的和新颖的问题，并且适用于不同的大语言模型，证明了RAG在改善代码相关问答任务中的有效性。

Abstract: Large Language Models (LLMs) have shown promise in assisting developers with
code-related questions; however, LLMs carry the risk of generating unreliable
answers. To address this, Retrieval-Augmented Generation (RAG) has been
proposed to reduce the unreliability (i.e., hallucinations) of LLMs. However,
designing effective pipelines remains challenging due to numerous design
choices. In this paper, we construct a retrieval corpus of over 3 million Java
and Python related Stack Overflow posts with accepted answers, and explore
various RAG pipeline designs to answer developer questions, evaluating their
effectiveness in generating accurate and reliable responses. More specifically,
we (1) design and evaluate 7 different RAG pipelines and 63 pipeline variants
to answer questions that have historically similar matches, and (2) address new
questions without any close prior matches by automatically lowering the
similarity threshold during retrieval, thereby increasing the chance of finding
partially relevant context and improving coverage for unseen cases. We find
that implementing a RAG pipeline combining hypothetical-documentation-embedding
(HyDE) with the full-answer context performs best in retrieving and answering
similarcontent for Stack Overflow questions. Finally, we apply our optimal RAG
pipeline to 4 open-source LLMs and compare the results to their zero-shot
performance. Our findings show that RAG with our optimal RAG pipeline
consistently outperforms zero-shot baselines across models, achieving higher
scores for helpfulness, correctness, and detail with LLM-as-a-judge. These
findings demonstrate that our optimal RAG pipelines robustly enhance answer
quality for a wide range of developer queries including both previously seen
and novel questions across different LLMs

</details>


### [23] [Rethinking LLM-Based RTL Code Optimization Via Timing Logic Metamorphosis](https://arxiv.org/abs/2507.16808)
*Zhihao Xu,Bixin Li,Lulu Wang*

Main category: cs.SE

TL;DR: 本研究评估了基于大语言模型(LLM)的寄存器传输级(RTL)代码优化方法在处理复杂时序逻辑方面的有效性，发现LLM在逻辑操作优化方面表现良好，但在复杂时序逻辑优化方面不如传统编译器方法。


<details>
  <summary>Details</summary>
Motivation: 传统RTL代码优化方法依赖人工调整和启发式算法，耗时且容易出错。虽然近期有研究提出使用LLM辅助RTL代码优化，但缺乏对LLM在处理复杂时序逻辑RTL代码方面有效性的全面评估。

Method: 提出了一个包含四个子集的RTL优化评估基准，每个子集对应RTL代码优化的特定领域。引入基于变形测试的方法来系统评估LLM的RTL代码优化方法的有效性，核心思想是对于语义等价但更复杂的代码，优化效果应保持一致。

Result: 通过密集实验发现：(1)基于LLM的RTL优化方法能够有效优化逻辑操作，性能超过现有的基于编译器的方法；(2)在处理复杂时序逻辑的RTL代码时，特别是在时序控制流优化和时钟域优化方面，LLM方法表现不如现有的编译器方法。

Conclusion: LLM在理解RTL代码中的时序逻辑方面面临挑战，这是其在复杂时序逻辑优化方面表现不佳的主要原因。研究为进一步利用LLM进行RTL代码优化提供了重要洞察和指导方向。

Abstract: Register Transfer Level(RTL) code optimization is crucial for achieving high
performance and low power consumption in digital circuit design. However,
traditional optimization methods often rely on manual tuning and heuristics,
which can be time-consuming and error-prone. Recent studies proposed to
leverage Large Language Models(LLMs) to assist in RTL code optimization. LLMs
can generate optimized code snippets based on natural language descriptions,
potentially speeding up the optimization process. However, existing approaches
have not thoroughly evaluated the effectiveness of LLM-Based code optimization
methods for RTL code with complex timing logic. To address this gap, we
conducted a comprehensive empirical investigation to assess the capability of
LLM-Based RTL code optimization methods in handling RTL code with complex
timing logic. In this study, we first propose a new benchmark for RTL
optimization evaluation. It comprises four subsets, each corresponding to a
specific area of RTL code optimization. Then we introduce a method based on
metamorphosis to systematically evaluate the effectiveness of LLM-Based RTL
code optimization methods.Our key insight is that the optimization
effectiveness should remain consistent for semantically equivalent but more
complex code. After intensive experiments, we revealed several key findings.
(1) LLM-Based RTL optimization methods can effectively optimize logic
operations and outperform existing compiler-based methods. (2) LLM-Based RTL
optimization methods do not perform better than existing compiler-based methods
on RTL code with complex timing logic, particularly in timing control flow
optimization and clock domain optimization. This is primarily attributed to the
challenges LLMs face in understanding timing logic in RTL code. Based on these
findings, we provide insights for further research in leveraging LLMs for RTL
code optimization.

</details>
