<div id=toc></div>

# Table of Contents

- [cs.LO](#cs.LO) [Total: 6]
- [cs.SE](#cs.SE) [Total: 31]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [1] [Graded Quantitative Narrowing](https://arxiv.org/abs/2507.19630)
*Mauricio Ayala-Rincón,Thaynara Arielly de Lima,Georg Ehling,Temur Kutsia*

Main category: cs.LO

TL;DR: 论文提出了定量窄化（quantitative narrowing）方法，扩展了传统窄化技术，用于解决Lawverean quantales上的定量方程问题。


<details>
  <summary>Details</summary>
Motivation: 传统窄化技术在非定量环境中应用广泛，但缺乏对定量方程的支持。本文旨在将窄化技术扩展到定量框架中，以解决更丰富的定量方程问题。

Method: 通过将匹配替换为统一化，引入定量窄化方法，结合同时实例化和重写步骤，处理含变量的项。

Result: 证明了定量窄化的正确性，并讨论了确保完备性的条件，能够解决更复杂的定量方程问题。

Conclusion: 定量窄化为定量方程求解提供了更通用的方法，扩展了传统窄化技术的应用范围。

Abstract: The recently introduced framework of Graded Quantitative Rewriting is an
innovative extension of traditional rewriting systems, in which rules are
annotated with degrees drawn from a quantale. This framework provides a robust
foundation for equational reasoning that incorporates metric aspects, such as
the proximity between terms and the complexity of rewriting-based computations.
Quantitative narrowing, introduced in this paper, generalizes quantitative
rewriting by replacing matching with unification in reduction steps, enabling
the reduction of terms even when they contain variables, through simultaneous
instantiation and rewriting. In the standard (non-quantitative) setting,
narrowing has been successfully applied in various domains, including
functional logic programming, theorem proving, and equational unification.
Here, we focus on quantitative narrowing to solve unification problems in
quantitative equational theories over Lawverean quantales. We establish its
soundness and discuss conditions under which completeness can be ensured. This
approach allows us to solve quantitative equations in richer theories than
those addressed by previous methods.

</details>


### [2] [Scroll nets](https://arxiv.org/abs/2507.19689)
*Pablo Donato*

Main category: cs.LO

TL;DR: 提出了一种名为“scroll nets”的新形式化方法，用于表示命题逻辑中的证明，基于Peirce的“scroll”概念，并结合Curry-Howard方法。


<details>
  <summary>Details</summary>
Motivation: 旨在通过拓扑和图形化的方式改进命题逻辑的证明表示，同时结合计算表达性。

Method: 从Peirce的existential graphs出发，通过Curry-Howard方法内化推理规则，定义scroll nets的图论本质，并提出detour-elimination过程。

Result: 展示了scroll nets在逻辑和计算表达性上的能力，能够模拟简单类型λ演算的归一化。

Conclusion: scroll nets为命题逻辑提供了一种新颖且表达力强的证明表示方法，兼具逻辑和计算特性。

Abstract: We introduce a new formalism for representing proofs in propositional logic
called "scroll nets". Its fundamental construct is the "scroll", a topological
notation for implication proposed by C. S. Peirce at the end of the 19th
century as the basis for his diagrammatic system of existential graphs (EGs).
Scroll nets are derived from EGs by following the Curry-Howard methodology of
internalizing inference rules inside judgments, just as terms in type theory
internalize natural deduction rules. We focus on the intuitionistic implicative
fragment of EGs, starting from a natural diagrammatic representation of scroll
nets, and then distilling their combinatorial essence into a purely
graph-theoretic definition. We also identify a notion of detour, that we use to
sketch a detour-elimination procedure akin to cut-elimination. We illustrate
how to simulate normalization in the simply typed $\lambda$-calculus,
demonstrating both the logical and computational expressivity of our framework.

</details>


### [3] [Synthesis Benchmarks for Automated Reasoning](https://arxiv.org/abs/2507.19827)
*Márton Hajdu,Petra Hozzová,Laura Kovács,Andrei Voronkov,Eva Maria Wagner,Richard Steven Žilinčík*

Main category: cs.LO

TL;DR: 本文提出了一个动态增长的基准数据集，用于支持基于∀∃公式的演绎程序合成，填补了该领域缺乏标准测试集的空白。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏支持∀∃格式和不可计算符号限制的演绎合成基准数据集，阻碍了理论研究和实践发展。

Method: 通过补充现有基准和新设计的数据集，构建了一个动态增长的数据集。

Result: 提供了一个标准化的基准数据集，支持未来自动化合成的研究和实践。

Conclusion: 该数据集将推动演绎程序合成的理论和实践发展。

Abstract: Program synthesis is the task of constructing a program conforming to a given
specification. We focus on deductive synthesis, and in particular on synthesis
problems with specifications given as $\forall\exists$-formulas, expressing the
existence of an output corresponding to any input. So far there has been no
canonical benchmark set for deductive synthesis using the
$\forall\exists$-format and supporting the so-called uncomputable symbol
restriction. This work presents such a data set, composed by complementing
existing benchmarks by new ones. Our data set is dynamically growing and should
motivate future developments in the theory and practice of automating
synthesis.

</details>


### [4] [A Model-Independent Theory of Probabilistic Testing](https://arxiv.org/abs/2507.19886)
*Weijun Chen,Yuxi Fu,Huan Long,Hao Wu*

Main category: cs.LO

TL;DR: 提出了一种独立于模型的概率测试方法，研究了测试等价性的模型独立和外部表征，并比较了这些等价性与概率双相似性。


<details>
  <summary>Details</summary>
Motivation: 概率并发系统是现代移动计算的基础模型，需要一种通用的测试方法。

Method: 基于新的分布语义和概率测试框架，研究测试等价性的表征。

Result: 证明了这些等价性是同余的，并与概率双相似性进行了比较。

Conclusion: 所提出的技术可扩展到其他概率并发模型。

Abstract: Probabilistic concurrent systems are foundational models for modern mobile
computing. In this paper, a general model-independent approach to probabilistic
testing is proposed. With the help of a new distribution-based semantics for
probabilistic models and a probabilistic testing framework with respect to
process predicates, the model-independent characterization and the external
characterization for testing equivalences are studied. The latter
characterization can be viewed as the generalization of the classical
fair/should equivalence and may equivalence. These equivalences are shown to be
congruent. A thorough comparison between these equivalences and probabilistic
bisimilarities is carried out. The techniques introduced in this paper can be
easily extended to other probabilistic concurrent models.

</details>


### [5] [Active Monitoring with RTLola: A Specification-Guided Scheduling Approach](https://arxiv.org/abs/2507.20615)
*Jan Baumeister,Bernd Finkbeiner,Frederik Scheerer*

Main category: cs.LO

TL;DR: 论文提出了一种主动监控方法，通过动态调度传感器查询以提高资源受限环境下的监控效率。


<details>
  <summary>Details</summary>
Motivation: 传统流监控方法中，监控器是被动的，由系统决定数据传输，可能导致资源浪费。在资源受限环境中（如自主飞行器），这种低效尤为突出。

Method: 将监控器设计为主动组件，根据内部状态动态决定查询哪些传感器及频率，通过规范中的调度注释指导带宽分配。

Result: 在相同带宽下，该方法比固定频率采样更快检测到规范违规。

Conclusion: 主动监控方法显著提高了监控效率，适用于资源受限环境。

Abstract: Stream-based monitoring is a well-established runtime verification approach
which relates input streams, representing sensor readings from the monitored
system, with output streams that capture filtered or aggregated results. In
such approaches, the monitor is a passive external component that continuously
receives sensor data from the system under observation. This setup assumes that
the system dictates what data is sent and when, regardless of the monitor's
current needs. However, in many applications -- particularly in
resource-constrained environments like autonomous aircraft, where energy, size,
or weight are limited -- this can lead to inefficient use of communication
resources. We propose making the monitor an active component that decides,
based on its current internal state, which sensors to query and how often. This
behavior is driven by scheduling annotations in the specification, which guide
the dynamic allocation of bandwidth towards the most relevant data, thereby
improving monitoring efficiency. We demonstrate our approach using the
stream-based specification language RTLola and assess the performance by
monitoring a specification from the aerospace domain. With equal bandwidth
usage, our approach detects specification violations significantly sooner than
monitors sampling all inputs at a fixed frequency.

</details>


### [6] [Automated Catamorphism Synthesis for Solving Constrained Horn Clauses over Algebraic Data Types](https://arxiv.org/abs/2507.20726)
*Hiroyuki Katsura,Naoki Kobayashi,Ken Sakayori,Ryosuke Sato*

Main category: cs.LO

TL;DR: 提出了一种基于Catamorphisms的新方法，用于解决ADTs上的CHC可满足性问题，并开发了Catalia求解器，性能优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 现有ADTs上的CHC求解器在表达归纳定义的函数/谓词模型时能力有限，需要改进。

Method: 利用Catamorphisms（广义折叠函数）自动发现并表达CHC模型。

Result: Catalia在CHC-COMP 2024基准测试中表现优于现有求解器，并在CHC-COMP 2025中获奖。

Conclusion: 该方法显著提升了ADTs上CHC求解的能力，Catalia成为高效工具。

Abstract: We propose a novel approach to satisfiability checking of Constrained Horn
Clauses (CHCs) over Algebraic Data Types (ADTs). CHC-based automated
verification has gained considerable attention in recent years, leading to the
development of various CHC solvers. However, existing solvers for CHCs over
ADTs are not fully satisfactory, due to their limited ability to find and
express models involving inductively defined functions/predicates (e.g., those
about the sum of list elements). To address this limitation, we consider
catamorphisms (generalized fold functions), and present a framework for
automatically discovering appropriate catamorphisms on demand and using them to
express a model of given CHCs. We have implemented a new CHC solver called
Catalia based on the proposed method. Our experimental results for the CHC-COMP
2024 benchmark show that Catalia outperforms state-of-the-art solvers in
solving satisfiable CHCs over ADTs. Catalia was also used as a core part of the
tool called ChocoCatalia, which won the ADT-LIA category of CHC-COMP 2025.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [7] [AccessGuru: Leveraging LLMs to Detect and Correct Web Accessibility Violations in HTML Code](https://arxiv.org/abs/2507.19549)
*Nadeen Fathallah,Daniel Hernández,Steffen Staab*

Main category: cs.SE

TL;DR: 论文提出了一种自动检测和纠正网页可访问性违规的方法AccessGuru，结合现有工具和大语言模型，显著提升了效果。


<details>
  <summary>Details</summary>
Motivation: 大多数网页不符合可访问性指南，阻碍了多样用户群体的使用，需降低手动修复成本。

Method: 提出分类法将违规分为三类，结合工具和LLMs检测与纠正，并开发基准评估。

Result: AccessGuru平均违规分数降低84%，远超之前方法的50%。

Conclusion: AccessGuru有效解决了自动检测和纠正网页可访问性违规的挑战。

Abstract: The vast majority of Web pages fail to comply with established Web
accessibility guidelines, excluding a range of users with diverse abilities
from interacting with their content. Making Web pages accessible to all users
requires dedicated expertise and additional manual efforts from Web page
providers. To lower their efforts and promote inclusiveness, we aim to
automatically detect and correct Web accessibility violations in HTML code.
While previous work has made progress in detecting certain types of
accessibility violations, the problem of automatically detecting and correcting
accessibility violations remains an open challenge that we address. We
introduce a novel taxonomy classifying Web accessibility violations into three
key categories - Syntactic, Semantic, and Layout. This taxonomy provides a
structured foundation for developing our detection and correction method and
redefining evaluation metrics. We propose a novel method, AccessGuru, which
combines existing accessibility testing tools and Large Language Models (LLMs)
to detect violations and applies taxonomy-driven prompting strategies to
correct all three categories. To evaluate these capabilities, we develop a
benchmark of real-world Web accessibility violations. Our benchmark quantifies
syntactic and layout compliance and judges semantic accuracy through
comparative analysis with human expert corrections. Evaluation against our
benchmark shows that AccessGuru achieves up to 84% average violation score
decrease, significantly outperforming prior methods that achieve at most 50%.

</details>


### [8] [LastMerge: A language-agnostic structured tool for code integration](https://arxiv.org/abs/2507.19687)
*Joao Pedro Duarte,Paulo Borba,Guilherme Cavalcanti*

Main category: cs.SE

TL;DR: LastMerge是一种通用的结构化合并工具，通过简化配置接口支持多种语言，实验表明其准确性不逊于语言特定工具。


<details>
  <summary>Details</summary>
Motivation: 解决结构化合并工具因语言特定性和高成本而难以广泛应用的问题。

Method: 提出LastMerge，并通过实验比较其与语言特定工具（jDime和Spork）及其通用版本（Mergiraf）的性能和准确性。

Result: 通用工具在准确性上与语言特定工具相当，LastMerge减少15%假阳性，Mergiraf减少42%假阴性，且运行时性能相近。

Conclusion: 通用结构化合并工具可替代语言特定工具，推动其在工业中的广泛应用。

Abstract: Unstructured line-based merge tools are widely used in practice. Structured
AST-based merge tools show significantly improved merge accuracy, but are
rarely used in practice because they are language specific and costly,
consequently not being available for many programming languages. To improve
merge accuracy for a wide range of languages, we propose LastMerge, a generic
structured merge tool that can be configured through a thin interface that
significantly reduces the effort of supporting structured merge. To understand
the impact that generic structured merge might have on merge accuracy and
performance, we run an experiment with four structured merge tools: two Java
specific tools, jDime and Spork, and their generic counterparts, respectively
LastMerge and Mergiraf. Using each tool, we replay merge scenarios from a
significant dataset, and collect data on runtime, behavioral divergences, and
merge accuracy. Our results show no evidence that generic structured merge
significantly impacts merge accuracy. Although we observe a difference rate of
approximately 10% between the Java specific tools and their generic
counterparts, most of the differences stem from implementation details and
could be avoided. We find that LastMerge reports 15% fewer false positives than
jDime while Mergiraf misses 42% fewer false negatives than Spork. Both generic
tools exhibit comparable runtime performance to the state of the art language
specific implementations. These results suggest that generic structured merge
tools can effectively replace language-specific ones, paving the way for
broader adoption of structured merge in industry.

</details>


### [9] [LLM-Based Repair of Static Nullability Errors](https://arxiv.org/abs/2507.20674)
*Nima Karimipour,Michael Pradel,Martin Kellogg,Manu Sridharan*

Main category: cs.SE

TL;DR: NullRepair是一个结合LLMs和静态分析的系统，用于自动修复Java代码中的nullability错误，解决了现有工具无法完全解决的问题。


<details>
  <summary>Details</summary>
Motivation: 现有静态分析工具在大型代码库中难以完全消除nullability错误，手动修复繁琐且易出错。

Method: NullRepair通过流程图决策、静态分析识别符号使用区域，并利用LLMs迭代生成修复补丁。

Result: 在12个Java项目中，NullRepair平均修复了72%的剩余错误，且保持了程序语义完整性。

Conclusion: NullRepair有效解决了nullability错误的自动化修复问题，优于直接使用LLMs的方法。

Abstract: Modern Java projects increasingly adopt static analysis tools that prevent
null-pointer exceptions by treating nullness as a type property. However,
integrating such tools into large, existing codebases remains a significant
challenge. While annotation inference can eliminate many errors automatically,
a subset of residual errors -- typically a mix of real bugs and false positives
-- often persist and can only be resolved via code changes. Manually addressing
these errors is tedious and error-prone. Large language models (LLMs) offer a
promising path toward automating these repairs, but naively-prompted LLMs often
generate incorrect, contextually-inappropriate edits. Resolving a nullability
error demands a deep understanding of how a symbol is used across the codebase,
often spanning methods, classes, and packages. We present NullRepair, a system
that integrates LLMs into a structured workflow for resolving the errors from a
nullability checker. NullRepair's decision process follows a flowchart derived
from manual analysis of 200 real-world errors. It leverages static analysis to
identify safe and unsafe usage regions of symbols, using error-free usage
examples to contextualize model prompts. Patches are generated through an
iterative interaction with the LLM that incorporates project-wide context and
decision logic. Our evaluation on 12 real-world Java projects shows that
NullRepair resolves an average of 72% of the errors that remain after applying
a state-of-the-art annotation inference technique. Unlike a naively-prompted
LLM, NullRepair also largely preserves program semantics, with all unit tests
passing in 10/12 projects after applying every edit proposed by NullRepair, and
98% or more tests passing in the remaining two projects.

</details>


### [10] [Refactoring $\neq$ Bug-Inducing: Improving Defect Prediction with Code Change Tactics Analysis](https://arxiv.org/abs/2507.19714)
*Feifei Niu,Junqian Shao,Christoph Mayr-Dorn,Liguo Huang,Wesley K. G. Assunção,Chuanyi Li,Jidong Ge,Alexander Egyed*

Main category: cs.SE

TL;DR: 论文研究了代码重构及其传播对即时缺陷预测（JIT-DP）的影响，提出CAT分析方法，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有JIT-DP研究忽视了代码重构的影响，导致模型学习和评估存在偏差。

Method: 提出Code chAnge Tactics (CAT)分析方法，用于分类代码重构及其传播，并整合重构信息改进基线模型。

Result: CAT方法在JIT-Defects4J数据集上提升标签准确率13.7%，忽略重构信息会降低模型性能（F1-score下降18.6%-37.3%）。整合重构信息后，基线模型的召回率和F1-score分别提升43.2%和32.5%。

Conclusion: 研究强调了在JIT-DP方法和评估中纳入重构信息的重要性，CAT方法在软件维护中具有广泛应用潜力。

Abstract: Just-in-time defect prediction (JIT-DP) aims to predict the likelihood of
code changes resulting in software defects at an early stage. Although code
change metrics and semantic features have enhanced prediction accuracy, prior
research has largely ignored code refactoring during both the evaluation and
methodology phases, despite its prevalence. Refactoring and its propagation
often tangle with bug-fixing and bug-inducing changes within the same commit
and statement. Neglecting refactoring can introduce bias into the learning and
evaluation of JIT-DP models. To address this gap, we investigate the impact of
refactoring and its propagation on six state-of-the-art JIT-DP approaches. We
propose Code chAnge Tactics (CAT) analysis to categorize code refactoring and
its propagation, which improves labeling accuracy in the JIT-Defects4J dataset
by 13.7%. Our experiments reveal that failing to consider refactoring
information in the dataset can diminish the performance of models, particularly
semantic-based models, by 18.6% and 37.3% in F1-score. Additionally, we propose
integrating refactoring information to enhance six baseline approaches,
resulting in overall improvements in recall and F1-score, with increases of up
to 43.2% and 32.5%, respectively. Our research underscores the importance of
incorporating refactoring information in the methodology and evaluation of
JIT-DP. Furthermore, our CAT has broad applicability in analyzing refactoring
and its propagation for software maintenance.

</details>


### [11] [Clean Code In Practice: Challenges and Opportunities](https://arxiv.org/abs/2507.19721)
*Dapeng Yan,Wenjie Yang,Kui Liu,Zhiming Liu,Zhikuang Cai*

Main category: cs.SE

TL;DR: 论文探讨了软件可靠性、安全性和安全性之间的相互作用，提出了一个结合安全性和安全性考虑的可靠性预测框架，并提供了改进模型的实用指南。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统的复杂性要求更深入地理解可靠性指标与安全性和安全性问题的关系，以确保系统的稳健性。

Method: 分析了行业中的关键指标和测量技术，提出了一个威胁估计框架，并结合安全性和安全性考虑。

Result: 研究发现，将可靠性指标与安全性和安全性考虑结合可以增强软件系统的稳健性。

Conclusion: 论文提出了一套实用指南，帮助从业者在改进可靠性预测模型的同时解决安全性和安全性挑战。

Abstract: Reliability prediction is crucial for ensuring the safety and security of
software systems, especially in the context of industry practices. While
various metrics and measurements are employed to assess software reliability,
the complexity of modern systems necessitates a deeper understanding of how
these metrics interact with security and safety concerns. This paper explores
the interplay between software reliability, safety, and security, offering a
comprehensive analysis of key metrics and measurement techniques used in the
industry for reliability prediction. We identify critical threats to software
reliability and provide a threat estimation framework that incorporates both
safety and security aspects. Our findings suggest that integrating reliability
metrics with safety and security considerations can enhance the robustness of
software systems. Furthermore, we propose a set of actionable guidelines for
practitioners to improve their reliability prediction models while
simultaneously addressing the security and safety challenges of contemporary
software applications.

</details>


### [12] [Defining ethically sourced code generation](https://arxiv.org/abs/2507.19743)
*Zhuolin Xu,Chenglin Li,Qiushi Li,Shin Hwei Tan*

Main category: cs.SE

TL;DR: 论文提出了一种新的概念——Ethically Sourced Code Generation（ES-CodeGen），旨在通过伦理和可持续的实践管理代码生成模型的开发过程。通过文献综述和从业者调查，确定了11个ES-CodeGen的维度及其相关后果、工件和阶段。


<details>
  <summary>Details</summary>
Motivation: 随着代码生成模型的广泛应用，确保其开发过程符合伦理和可持续性变得越来越重要。研究旨在填补这一领域的空白，推动伦理代码生成的实践。

Method: 采用两阶段文献综述（803篇论文）和从业者调查（32人），识别并提炼ES-CodeGen的维度和后果。

Result: 确定了11个ES-CodeGen维度，包括新增的代码质量维度，并揭示了相关后果和阶段。调查显示从业者对社交相关维度关注不足。

Conclusion: 研究呼吁关注代码生成中的伦理问题，并提供了ES-CodeGen的框架，帮助从业者更好地理解和实践伦理代码生成。

Abstract: Several code generation models have been proposed to help reduce time and
effort in solving software-related tasks. To ensure responsible AI, there are
growing interests over various ethical issues (e.g., unclear licensing,
privacy, fairness, and environment impact). These studies have the overarching
goal of ensuring ethically sourced generation, which has gained growing
attentions in speech synthesis and image generation. In this paper, we
introduce the novel notion of Ethically Sourced Code Generation (ES-CodeGen) to
refer to managing all processes involved in code generation model development
from data collection to post-deployment via ethical and sustainable practices.
To build a taxonomy of ES-CodeGen, we perform a two-phase literature review
where we read 803 papers across various domains and specific to AI-based code
generation. We identified 71 relevant papers with 10 initial dimensions of
ES-CodeGen. To refine our dimensions and gain insights on consequences of
ES-CodeGen, we surveyed 32 practitioners, which include six developers who
submitted GitHub issues to opt-out from the Stack dataset (these impacted users
have real-world experience of ethically sourcing issues in code generation
models). The results lead to 11 dimensions of ES-CodeGen with a new dimension
on code quality as practitioners have noted its importance. We also identified
consequences, artifacts, and stages relevant to ES-CodeGen. Our post-survey
reflection showed that most practitioners tend to ignore social-related
dimensions despite their importance. Most practitioners either agreed or
strongly agreed that our survey help improve their understanding of ES-CodeGen.
Our study calls for attentions of various ethical issues towards ES-CodeGen.

</details>


### [13] [From Few-Label to Zero-Label: An Approach for Cross-System Log-Based Anomaly Detection with Meta-Learning](https://arxiv.org/abs/2507.19806)
*Xinlong Zhao,Tong Jia,Minghua He,Yihan Wu,Ying Li,Gang Huang*

Main category: cs.SE

TL;DR: FreeLog提出了一种零标签跨系统日志异常检测方法，无需目标系统的标记日志，解决了冷启动问题。


<details>
  <summary>Details</summary>
Motivation: 现有跨系统日志异常检测方法依赖少量目标系统标记日志，难以应对冷启动问题。

Method: 采用系统无关的表示元学习方法FreeLog，无需目标系统标记数据。

Result: 在三个公共日志数据集上，FreeLog性能媲美依赖少量标记数据的最先进方法。

Conclusion: FreeLog为零标签跨系统日志异常检测提供了有效解决方案。

Abstract: Log anomaly detection plays a critical role in ensuring the stability and
reliability of software systems. However, existing approaches rely on large
amounts of labeled log data, which poses significant challenges in real-world
applications. To address this issue, cross-system transfer has been identified
as a key research direction. State-of-the-art cross-system approaches achieve
promising performance with only a few labels from the target system. However,
their reliance on labeled target logs makes them susceptible to the cold-start
problem when labeled logs are insufficient. To overcome this limitation, we
explore a novel yet underexplored setting: zero-label cross-system log anomaly
detection, where the target system logs are entirely unlabeled. To this end, we
propose FreeLog, a system-agnostic representation meta-learning method that
eliminates the need for labeled target system logs, enabling cross-system log
anomaly detection under zero-label conditions. Experimental results on three
public log datasets demonstrate that FreeLog achieves performance comparable to
state-of-the-art methods that rely on a small amount of labeled data from the
target system.

</details>


### [14] [A Cooperative Approach for Knowledge-based Business Process Design in a Public Authority](https://arxiv.org/abs/2507.19842)
*Mohammad Azarijafari,Luisa Mich,Michele Missikoff,Oleg Missikoff*

Main category: cs.SE

TL;DR: 本文提出了一种基于知识的方法，帮助业务专家设计业务流程，无需知识工程背景，通过结构化步骤生成流程图。


<details>
  <summary>Details</summary>
Motivation: 企业数字化转型迫在眉睫，中小型企业需调整组织结构以保持竞争力，流程导向的生产模式是关键创新领域。

Method: 从简单的文本知识逐步构建结构化知识库，通过结构化步骤生成目标流程的流程图。

Result: 方法支持所有利益相关者共同参与业务流程设计，无需专业知识。

Conclusion: 该方法为业务流程设计提供了一种共享且易用的解决方案。

Abstract: Enterprises are currently undergoing profound transformations due to the
unpostponable digital transformation. Then, to remain competitive, enterprises
must adapt their organisational structures and operations. This organisational
shift is also important for small and medium-sized enterprises. A key
innovation frontier is the adoption of process-oriented production models. This
paper presents a knowledge-based method to support business experts in
designing business processes. The method requires no prior expertise in
Knowledge Engineering and guides designers through a structured sequence of
steps to produce a diagrammatic workflow of the target process. The
construction of the knowledge base starts from simple, text-based, knowledge
artefacts and then progresses towards more structured, formal representations.
The approach has been conceived to allow a shared approach for all stakeholders
and actors who participate in the BP design.

</details>


### [15] [AgentMesh: A Cooperative Multi-Agent Generative AI Framework for Software Development Automation](https://arxiv.org/abs/2507.19902)
*Sourena Khanzadeh*

Main category: cs.SE

TL;DR: AgentMesh是一个基于Python的框架，利用多个合作的LLM驱动代理自动化软件开发任务。


<details>
  <summary>Details</summary>
Motivation: 传统软件开发需要多领域专家协作，复杂且耗时，AgentMesh旨在通过多代理协作自动化这一过程。

Method: 框架包含Planner、Coder、Debugger和Reviewer四个代理，分别负责任务分解、代码实现、调试和代码审查。

Result: 案例研究表明，AgentMesh能通过多代理协作处理复杂开发请求，生成高质量代码。

Conclusion: 多代理协作能发挥LLM优势并缓解单代理限制，但需解决错误传播和上下文扩展等问题。

Abstract: Software development is a complex, multi-phase process traditionally
requiring collaboration among individuals with diverse expertise. We propose
AgentMesh, a Python-based framework that uses multiple cooperating LLM-powered
agents to automate software development tasks. In AgentMesh, specialized agents
- a Planner, Coder, Debugger, and Reviewer - work in concert to transform a
high-level requirement into fully realized code. The Planner agent first
decomposes user requests into concrete subtasks; the Coder agent implements
each subtask in code; the Debugger agent tests and fixes the code; and the
Reviewer agent validates the final output for correctness and quality. We
describe the architecture and design of these agents and their communication,
and provide implementation details including prompt strategies and workflow
orchestration. A case study illustrates AgentMesh handling a non-trivial
development request via sequential task planning, code generation, iterative
debugging, and final code review. We discuss how dividing responsibilities
among cooperative agents leverages the strengths of large language models while
mitigating single-agent limitations. Finally, we examine current limitations -
such as error propagation and context scaling - and outline future work toward
more robust, scalable multi-agent AI systems for software engineering
automation.

</details>


### [16] [CrossPL: Evaluating Large Language Models on Cross Programming Language Code Generation](https://arxiv.org/abs/2507.19904)
*Zhanhang Xiong,Dongxia Wang,Yuekang Li,Xinyuan An,Wenhai Wang*

Main category: cs.SE

TL;DR: 论文提出CrossPL基准，首次系统评估大语言模型（LLM）生成跨编程语言（CPL）互操作代码的能力，发现现有模型表现不佳。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在软件工程中的应用增多，生成跨语言互操作代码的能力尚未充分探索，这对构建复杂系统至关重要。

Method: 通过分析GitHub多语言仓库，设计156个有限状态机（FSM），构建包含1,982个任务的CrossPL基准，并开发LLM自动提取和验证代码的流程。

Result: 评估14个通用LLM和6个代码专用LLM，发现即使最佳模型在CPL场景中表现仍不理想。

Conclusion: CrossPL填补了评估LLM跨语言互操作能力的空白，呼吁更多针对性研究。

Abstract: As large language models (LLMs) become increasingly embedded in software
engineering workflows, a critical capability remains underexplored: generating
correct code that enables cross-programming-language (CPL) interoperability.
This skill is essential for building complex systems that integrate components
written in multiple languages via mechanisms like inter-process communication
(IPC). To bridge this gap, we present CrossPL, the first benchmark designed to
systematically evaluate LLMs' ability to generate CPL-interoperating code.
CrossPL comprises 1,982 tasks centered around IPC, covering six widely-used
programming languages and seven representative CPL techniques. We construct
this benchmark by (i) analyzing 19,169 multi-language GitHub repositories using
156 hand-crafted finite state machines (FSMs), and (ii) developing an LLM-based
pipeline that automatically extracts CPL code snippets, generates task
instructions, and validates functional correctness. We evaluate 14
state-of-the-art general-purpose LLMs and 6 code-oriented LLMs released in the
past three years on CrossPL via FSM-based validation. Results reveal that even
the best-performing models struggle with CPL scenarios, underscoring the need
for more targeted research in this space. Our benchmark and code are available
at: https://anonymous.4open.science/r/crosspl-2814.

</details>


### [17] [The Impact of Fine-tuning Large Language Models on Automated Program Repair](https://arxiv.org/abs/2507.19909)
*Roman Macháček,Anastasiia Grishina,Max Hort,Leon Moonen*

Main category: cs.SE

TL;DR: 研究探讨了不同微调技术对用于自动程序修复（APR）的大型语言模型（LLMs）性能的影响，发现参数高效微调（PEFT）方法优于完全微调。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在自动程序修复（APR）中表现出色，但训练成本高。研究旨在通过微调技术提升性能并降低成本。

Method: 实验评估了六种预训练LLMs（如CodeGen、CodeT5等）在三种微调方案（无微调、完全微调、参数高效微调）下的表现，使用三个APR基准测试。

Result: 完全微调因数据分布差异和过拟合导致性能下降，而参数高效微调（如LoRA和IA3）通过限制可训练参数取得了更好结果。

Conclusion: 参数高效微调是提升LLMs在APR任务中性能的有效方法，同时降低了计算成本。

Abstract: Automated Program Repair (APR) uses various tools and techniques to help
developers achieve functional and error-free code faster. In recent years,
Large Language Models (LLMs) have gained popularity as components in APR tool
chains because of their performance and flexibility. However, training such
models requires a significant amount of resources. Fine-tuning techniques have
been developed to adapt pre-trained LLMs to specific tasks, such as APR, and
enhance their performance at far lower computational costs than training from
scratch. In this study, we empirically investigate the impact of various
fine-tuning techniques on the performance of LLMs used for APR. Our experiments
provide insights into the performance of a selection of state-of-the-art LLMs
pre-trained on code. The evaluation is done on three popular APR benchmarks
(i.e., QuixBugs, Defects4J and HumanEval-Java) and considers six different LLMs
with varying parameter sizes (resp. CodeGen, CodeT5, StarCoder, DeepSeekCoder,
Bloom, and CodeLlama-2). We consider three training regimens: no fine-tuning,
full fine-tuning, and parameter-efficient fine-tuning (PEFT) using LoRA and
IA3. We observe that full fine-tuning techniques decrease the benchmarking
performance of various models due to different data distributions and
overfitting. By using parameter-efficient fine-tuning methods, we restrict
models in the amount of trainable parameters and achieve better results.
  Keywords: large language models, automated program repair,
parameter-efficient fine-tuning, AI4Code, AI4SE, ML4SE.

</details>


### [18] [Prometheus: Unified Knowledge Graphs for Issue Resolution in Multilingual Codebases](https://arxiv.org/abs/2507.19942)
*Zimin Chen,Yue Pan,Siyu Lu,Jiayi Xu,Claire Le Goues,Martin Monperrus,He Ye*

Main category: cs.SE

TL;DR: Prometheus是一个多代理系统，通过将代码库转换为知识图谱来解决多语言问题，显著提升了问题解决率。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于Python问题和预构建容器，Prometheus旨在解决真实世界和多语言仓库的问题。

Method: 将代码库转换为统一知识图谱，支持多种编程语言，使用Neo4j存储图谱，集成DeepSeek-V3模型。

Result: 在SWE-bench Lite和Multilingual上分别解决28.67%和13.7%的问题，成本低，且首次支持七种语言。

Conclusion: Prometheus在多语言问题解决中表现优异，并成功应用于真实世界问题。

Abstract: Language model (LM) agents, such as SWE-agent and OpenHands, have made
progress toward automated issue resolution. However, existing approaches are
often limited to Python-only issues and rely on pre-constructed containers in
SWE-bench with reproduced issues, restricting their applicability to real-world
and work for multi-language repositories. We present Prometheus, designed to
resolve real-world issues beyond benchmark settings. Prometheus is a
multi-agent system that transforms an entire code repository into a unified
knowledge graph to guide context retrieval for issue resolution. Prometheus
encodes files, abstract syntax trees, and natural language text into a graph of
typed nodes and five general edge types to support multiple programming
languages. Prometheus uses Neo4j for graph persistence, enabling scalable and
structured reasoning over large codebases. Integrated by the DeepSeek-V3 model,
Prometheus resolves 28.67% and 13.7% of issues on SWE-bench Lite and SWE-bench
Multilingual, respectively, with an average API cost of $0.23 and $0.38 per
issue. Prometheus resolves 10 unique issues not addressed by prior work and is
the first to demonstrate effectiveness across seven programming languages.
Moreover, it shows the ability to resolve real-world GitHub issues in the
LangChain and OpenHands repositories. We have open-sourced Prometheus at:
https://github.com/Pantheon-temple/Prometheus

</details>


### [19] [PDLogger: Automated Logging Framework for Practical Software Development](https://arxiv.org/abs/2507.19951)
*Shengcheng Duan,Yihua Xu,Sheng Zhang,Shen Wang,Yue Duan*

Main category: cs.SE

TL;DR: PDLogger是一种端到端的日志生成技术，针对多日志场景设计，通过三阶段流程（位置预测、日志生成、日志优化）显著提升日志质量。


<details>
  <summary>Details</summary>
Motivation: 现有自动化日志技术仅关注单一子任务（如日志位置、级别或消息），无法生成完整高质量的日志，且忽略语义依赖和变量多样性。

Method: PDLogger分三阶段：1) 日志位置预测；2) 日志生成（利用程序切片和变量提取）；3) 日志优化（级别校正和去重）。

Result: 在3113条日志上测试，PDLogger显著优于现有系统，位置精度提升139.0%，F1提升69.2%，消息质量提升65.7%。

Conclusion: PDLogger是一种高效、通用的日志生成框架，支持主流LLM，开源以促进研究和应用。

Abstract: Logging is indispensable for maintaining the reliability and diagnosability
of modern software, yet developers still struggle to decide where and how to
log effectively. Existing automated logging techniques focus on isolated
sub-tasks - predicting a single log position, level, or message - and therefore
cannot produce complete, high-quality log statements that reflect real-world
practice in which multiple logs often appear inside one method. They also
neglect deeper semantic dependencies among methods and consider only a narrow
set of candidate variables, leading to superficial or incomplete logs. In this
paper, we present PDLogger, the first end-to-end log generation technique
expressly designed for practical, multi-log scenarios. PDLogger operates in
three phases. (1) Log position prediction: block-type-aware structured prompts
guide a large language model (LLM) to suggest candidate positions across all
control-flow blocks of a method. (2) Log generation: backward program slicing
supplies precise inter-procedural control and data-dependency context, while an
expanded variable extractor captures both member and external function
expressions; the enriched prompt enables the LLM to emit a full log statement
(position, level, message, variables). (3) Log refinement: level correction and
context-sensitive deduplication prune false positives and redundant logs. We
evaluate PDLogger on 3,113 log statements drawn from two widely used Java
projects. Compared with the strongest prior systems, PDLogger improves
log-position precision by 139.0 percent, F1 by 69.2 percent, level accuracy by
82.3 percent, variable precision by 131.8 percent, and message quality
(BERTScore) by 65.7 percent. The framework consistently performs well with
different mainstream LLMs, demonstrating robustness and generality. PDLogger's
implementation is available as open source to foster future research and
adoption.

</details>


### [20] [The Effect of Pointer Analysis on Semantic Conflict Detection](https://arxiv.org/abs/2507.20081)
*Matheus Barbosa,Paulo Borba,Rodrigo Bonifácio,Victor Lira,Galileu Santos*

Main category: cs.SE

TL;DR: 论文探讨了当前合并工具无法检测语义冲突的问题，并通过实证研究比较了使用指针分析与否对静态分析检测语义冲突的影响。


<details>
  <summary>Details</summary>
Motivation: 现有静态分析在检测语义冲突时存在高误报率，研究旨在验证指针分析是否能减少这些误报。

Method: 实现两种分析（带指针分析和不带指针分析），在两组数据集上运行，比较其准确性、计算性能及差异频率。

Result: 指针分析显著减少了超时和误报，但也大幅增加了漏报，导致召回率和F1分数下降。

Conclusion: 建议在语义冲突检测中探索结合两种分析优点的混合分析技术。

Abstract: Current merge tools don't detect semantic conflicts, which occur when changes
from different developers are textually integrated but semantically interfere
with each other. Although researchers have proposed static analyses for
detecting semantic conflicts, these analyses suffer from significant false
positive rates. To understand whether such false positives could be reduced by
using pointer analysis in the implementation of semantic conflict static
analyses, we conduct an empirical study. We implement the same analysis with
and without pointer analysis, run them on two datasets, observe how often they
differ, and compare their accuracy and computational performance. Although
pointer analysis is known to improve precision in static analysis, we find that
its effect on semantic conflict detection can be drastic: we observe a
significant reduction in timeouts and false positives, but also a significant
increase in false negatives, with prohibitive drops in recall and F1-score.
These results suggest that, in the context of semantic conflict detection, we
should explore hybrid analysis techniques, combining aspects of both
implementations we compare in our study.

</details>


### [21] [From First Use to Final Commit: Studying the Evolution of Multi-CI Service Adoption](https://arxiv.org/abs/2507.20095)
*Nitika Chopra,Taher A. Ghaleb*

Main category: cs.SE

TL;DR: 研究分析了18,924个Java项目在2008年至2024年间对8种CI服务的采用情况，发现多服务共用在项目中很常见，且常伴随迁移行为。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注单一CI服务，缺乏对多服务采用及迁移行为的理解。

Method: 通过分析GitHub上Java项目的CI服务历史数据，研究多服务共用的频率及维护活动的差异。

Result: 近五分之一项目采用多CI服务，且常表现为服务间的迁移。

Conclusion: 研究揭示了多CI服务采用的实践模式，为未来研究及工具开发提供了新方向。

Abstract: Continuous Integration (CI) services, such as GitHub Actions and Travis CI,
are widely adopted in open-source development to automate testing and
deployment. Though existing research often examines individual services in
isolation, it remains unclear how projects adopt and transition between
multiple services over time. To understand how CI adoption is evolving across
services, we present a preliminary study analyzing the historical CI adoption
of 18,924 Java projects hosted on GitHub between January 2008 and December
2024, adopting at least one of eight CI services, namely Travis CI, AppVeyor,
CircleCI, Azure Pipelines, GitHub Actions, Bitbucket, GitLab CI, and Cirrus CI.
Specifically, we investigate: (1) how frequently CI services are co-adopted or
replaced, and (2) how maintenance activity varies across different services.
Our analysis shows that the use of multiple CI services within the same project
is a recurring pattern observed in nearly one in five projects, often
reflecting migration across CI services. Our study is among the first to
examine multi-CI adoption in practice, offering new insights for future
research and highlighting the need for strategies and tools to support service
selection, coordination, and migration in evolving CI environments.

</details>


### [22] [Learning to Align Human Code Preferences](https://arxiv.org/abs/2507.20109)
*Xin Yin,Chao Ni,Liushan Chen,Xiaohu Yang*

Main category: cs.SE

TL;DR: 本文研究了SFT和DPO在代码偏好对齐中的作用，提出自适应偏好优化（APO），在六项任务中验证其优于现有策略。


<details>
  <summary>Details</summary>
Motivation: 探索SFT和DPO在不同代码偏好场景中的最佳训练策略，填补理论与实践的空白。

Method: 通过理论分析和实验验证，提出APO方法，动态整合SFT和DPO。

Result: APO在六项任务中表现优于或匹配现有策略。

Conclusion: APO为不同代码偏好对齐场景提供了理论和实践指导。

Abstract: Large Language Models (LLMs) have demonstrated remarkable potential in
automating software development tasks. While recent advances leverage
Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) to align
models with human preferences, the optimal training strategy remains unclear
across diverse code preference scenarios. This paper systematically
investigates the roles of SFT and DPO in aligning LLMs with different code
preferences. Through both theoretical analysis and empirical observation, we
hypothesize that SFT excels in scenarios with objectively verifiable optimal
solutions, while applying SFT followed by DPO (S&D) enables models to explore
superior solutions in scenarios without objectively verifiable optimal
solutions. Based on the analysis and experimental evidence, we propose Adaptive
Preference Optimization (APO), a dynamic integration approach that adaptively
amplifies preferred responses, suppresses dispreferred ones, and encourages
exploration of potentially superior solutions during training. Extensive
experiments across six representative code preference tasks validate our
theoretical hypotheses and demonstrate that APO consistently matches or
surpasses the performance of existing SFT and S&D strategies. Our work provides
both theoretical foundations and practical guidance for selecting appropriate
training strategies in different code preference alignment scenarios.

</details>


### [23] [From Prompt to Pipeline: Large Language Models for Scientific Workflow Development in Bioinformatics](https://arxiv.org/abs/2507.20122)
*Khairul Alam,Banani Roy*

Main category: cs.SE

TL;DR: 研究探讨了大型语言模型（如GPT-4o、Gemini 2.5 Flash和DeepSeek-V3）在生成生物信息学工作流中的表现，发现不同模型在不同平台（如Galaxy和Nextflow）上表现各异，提示策略对结果质量有显著影响。


<details>
  <summary>Details</summary>
Motivation: 生物信息学数据分析的复杂性使得科学工作流系统（SWSs）如Galaxy和Nextflow变得至关重要，但创建和理解这些工作流对非编程专家仍具挑战性。

Method: 研究评估了GPT-4o、Gemini 2.5 Flash和DeepSeek-V3在生成生物信息学工作流中的表现，涵盖了SNP分析、RNA-seq、DNA甲基化和数据检索等任务，并对比了社区基准。

Result: Gemini 2.5 Flash在Galaxy工作流生成中表现最佳，DeepSeek-V3在Nextflow中表现突出。提示策略（如角色扮演和思维链）显著提高了完整性和正确性。

Conclusion: 大型语言模型有望降低工作流开发门槛，提高可重复性，并通过提示工程促进生物信息学工具的普及。

Abstract: The increasing complexity of bioinformatics data analysis has made Scientific
Workflow Systems (SWSs) like Galaxy and Nextflow essential for enabling
scalable, reproducible, and automated workflows. However, creating and
understanding these workflows remains challenging, particularly for domain
experts without programming expertise. This study investigates whether modern
Large Language Models (LLMs), GPT-4o, Gemini 2.5 Flash, and DeepSeek-V3, can
support the generation of accurate, complete, and usable bioinformatics
workflows, and examines which prompting strategies most effectively guide this
process. We evaluate these models using diverse tasks such as SNP analysis,
RNA-seq, DNA methylation, and data retrieval, spanning both graphical (Galaxy)
and script-based (Nextflow) platforms. Expert reviewers assess the generated
workflows against community-curated baselines from the Galaxy Training Network
and nf-core repositories. The results show that Gemini 2.5 Flash excels in
generating Galaxy workflows, while DeepSeek-V3 performs strongly in Nextflow.
Prompting strategies significantly impact quality, with role-based and
chain-of-thought prompts improving completeness and correctness. While GPT-4o
benefits from structured inputs, DeepSeek-V3 offers rich technical detail,
albeit with some verbosity. Overall, the findings highlight the potential of
LLMs to lower the barrier for workflow development, improve reproducibility,
and democratize access to computational tools in bioinformatics, especially
when combined with thoughtful prompt engineering.

</details>


### [24] [Relating System Safety and Machine Learnt Model Performance](https://arxiv.org/abs/2507.20135)
*Ganesh Pai*

Main category: cs.SE

TL;DR: 论文探讨了在航空应用中如何将机器学习模型的性能指标与系统安全目标关联，提出了一种从安全评估中推导最小性能要求的方法。


<details>
  <summary>Details</summary>
Motivation: 在航空应用中，机器学习模型的性能指标与系统安全目标之间的关系不明确，需要一种方法确保模型满足安全要求。

Method: 通过抽象机器学习组件的行为，提出一种方法推导最小安全相关性能要求及其指标，并验证其有效性。

Result: 提供了一种从安全目标推导性能要求的方法，明确了假设、适用约束和验证意义。

Conclusion: 该方法为航空应用中机器学习组件的安全性能要求提供了初步解决方案。

Abstract: The prediction quality of machine learnt models and the functionality they
ultimately enable (e.g., object detection), is typically evaluated using a
variety of quantitative metrics that are specified in the associated model
performance requirements. When integrating such models into aeronautical
applications, a top-down safety assessment process must influence both the
model performance metrics selected, and their acceptable range of values.
Often, however, the relationship of system safety objectives to model
performance requirements and the associated metrics is unclear. Using an
example of an aircraft emergency braking system containing a machine learnt
component (MLC) responsible for object detection and alerting, this paper first
describes a simple abstraction of the required MLC behavior. Then, based on
that abstraction, an initial method is given to derive the minimum
safety-related performance requirements, the associated metrics, and their
targets for the both MLC and its underlying deep neural network, such that they
meet the quantitative safety objectives obtained from the safety assessment
process. We give rationale as to why the proposed method should be considered
valid, also clarifying the assumptions made, the constraints on applicability,
and the implications for verification.

</details>


### [25] [Strategic Motivators for Ethical AI System Development: An Empirical and Holistic Model](https://arxiv.org/abs/2507.20218)
*Muhammad Azeem Akbar,Arif Ali Khan,Saima Rafi,Damian Kedziora,Sami Hyrynsalmi*

Main category: cs.SE

TL;DR: 该研究通过多源文献综述和问卷调查，识别并优先排序了推动AI伦理发展的20个关键动机因素，分为8类。ISM和MICMAC分析揭示了‘人力资源’和‘协调’对其他因素的重要影响，而Fuzzy TOPSIS则确定了团队多样性、AI治理机构等为最关键的动机。


<details>
  <summary>Details</summary>
Motivation: 研究旨在识别和优先排序推动AI伦理发展的关键动机，以支持负责任AI的开发。

Method: 采用多源文献综述（MLR）、问卷调查、解释性结构建模（ISM）、MICMAC分析和模糊TOPSIS方法。

Result: 识别出20个关键动机因素，分为8类；ISM显示‘人力资源’和‘协调’影响显著；MICMAC分析将部分类别归为独立集群；Fuzzy TOPSIS确定了最关键的动机。

Conclusion: 组织应将这些动机因素纳入政策、治理模型和开发框架，以支持伦理AI的采用。

Abstract: Artificial Intelligence (AI) presents transformative opportunities for
industries and society, but its responsible development is essential to prevent
unintended consequences. Ethically sound AI systems demand strategic planning,
strong governance, and an understanding of the key drivers that promote
responsible practices. This study aims to identify and prioritize the
motivators that drive the ethical development of AI systems. A Multivocal
Literature Review (MLR) and a questionnaire-based survey were conducted to
capture current practices in ethical AI. We applied Interpretive Structure
Modeling (ISM) to explore the relationships between motivator categories,
followed by MICMAC analysis to classify them by their driving and dependence
power. Fuzzy TOPSIS was used to rank these motivators by importance. Twenty key
motivators were identified and grouped into eight categories: Human Resource,
Knowledge Integration, Coordination, Project Administration, Standards,
Technology Factor, Stakeholders, and Strategy & Matrices. ISM results showed
that 'Human Resource' and 'Coordination' heavily influence other factors.
MICMAC analysis placed categories like Human Resource (CA1), Coordination
(CA3), Stakeholders (CA7), and Strategy & Matrices (CA8) in the independent
cluster, indicating high driving but low dependence power. Fuzzy TOPSIS ranked
motivators such as promoting team diversity, establishing AI governance bodies,
appointing oversight leaders, and ensuring data privacy as most critical. To
support ethical AI adoption, organizations should align their strategies with
these motivators and integrate them into their policies, governance models, and
development frameworks.

</details>


### [26] [Beyond Binary Moderation: Identifying Fine-Grained Sexist and Misogynistic Behavior on GitHub with Large Language Models](https://arxiv.org/abs/2507.20358)
*Tanni Dev,Sayma Sultana,Amiangshu Bosu*

Main category: cs.SE

TL;DR: 研究提出了一种基于指令调优大语言模型的多类别分类框架，用于检测GitHub上的性别歧视和厌女言论，显著优于基线方法，但在处理微妙语境时仍有局限。


<details>
  <summary>Details</summary>
Motivation: 技术社区（如GitHub）中的性别歧视和厌女行为阻碍了包容性，现有工具难以有效检测这些微妙伤害。

Method: 采用指令调优的大语言模型框架，通过20次迭代优化提示，评估了1,440条标记评论，使用精确率、召回率、F1分数和MCC进行性能比较。

Result: 优化后的方法（GPT-4o与Prompt 19）MCC为0.501，显著优于基线，但对微妙语境的理解仍有不足。

Conclusion: 设计良好的提示能显著提升性别歧视检测的准确性和可解释性，为开发者平台提供实用工具。

Abstract: Background: Sexist and misogynistic behavior significantly hinders inclusion
in technical communities like GitHub, causing developers, especially
minorities, to leave due to subtle biases and microaggressions. Current
moderation tools primarily rely on keyword filtering or binary classifiers,
limiting their ability to detect nuanced harm effectively.
  Aims: This study introduces a fine-grained, multi-class classification
framework that leverages instruction-tuned Large Language Models (LLMs) to
identify twelve distinct categories of sexist and misogynistic comments on
GitHub.
  Method: We utilized an instruction-tuned LLM-based framework with systematic
prompt refinement across 20 iterations, evaluated on 1,440 labeled GitHub
comments across twelve sexism/misogyny categories. Model performances were
rigorously compared using precision, recall, F1-score, and the Matthews
Correlation Coefficient (MCC).
  Results: Our optimized approach (GPT-4o with Prompt 19) achieved an MCC of
0.501, significantly outperforming baseline approaches. While this model had
low false positives, it struggled to interpret nuanced, context-dependent
sexism and misogyny reliably.
  Conclusion: Well-designed prompts with clear definitions and structured
outputs significantly improve the accuracy and interpretability of sexism
detection, enabling precise and practical moderation on developer platforms
like GitHub.

</details>


### [27] [CIgrate: Automating CI Service Migration with Large Language Models](https://arxiv.org/abs/2507.20402)
*Md Nazmul Hossain,Taher A. Ghaleb*

Main category: cs.SE

TL;DR: 论文提出CIgrate，一个基于LLM的框架，用于自动化迁移CI配置，旨在提高准确性和可用性，并与现有规则方法CIMig进行比较。


<details>
  <summary>Details</summary>
Motivation: CI配置迁移在开源项目中需求频繁，但手动迁移耗时且易错，现有规则方法CIMig准确性不足，LLM在代码生成任务中表现优异，可能提升迁移效果。

Method: 提出CIgrate框架，通过零样本/少样本提示或微调LLM实现CI配置迁移，并与CIMig对比，评估准确性和开发者反馈。

Result: 预期贡献包括首个LLM驱动的CI迁移方法、与规则方法的比较评估，以及LLM在软件配置演化中的应用洞察。

Conclusion: LLM有望提升CI配置迁移的自动化水平和准确性，为软件配置管理提供新思路。

Abstract: Continuous Integration (CI) configurations often need to be migrated between
services (e.g., Travis CI to GitHub Actions) as projects evolve, due to changes
in service capabilities, usage limits, or service deprecation. Previous studies
reported that migration across CI services is a recurring need in open-source
development. However, manual migration can be time-consuming and error-prone.
The state-of-the-art approach, CIMig, addresses this challenge by analyzing
past migration examples to create service-specific rules and produce equivalent
configurations across CI services. However, its relatively low accuracy raises
concerns about the overall feasibility of automated CI migration using
rule-based techniques alone. Meanwhile, Large Language Models (LLMs) have
demonstrated strong capabilities in code generation and transformation tasks,
suggesting potential to improve the automation, usability, and generalizability
of CI configuration migration. This registered report presents a study in which
we aim to assess whether CI migration can be improved using LLMs. To this end,
we propose CIgrate, an LLM-based framework for automatically migrating CI
configurations. We plan to evaluate the performance of CIgrate compared to
CIMig as a baseline, in different setups (a) zero-shot/few-shot prompting of
LLMs for configuration migration and (b) fine-tuning an LLM on a dataset of
already established CI service migrations. We will also seek developer feedback
on the quality and usability of the generated configurations. We formulate
research questions focusing on the accuracy of LLM-generated migrations versus
ground truth and the output of CIMig. The expected contributions include the
first LLM-powered approach for CI service migration, a comparative evaluation
of its effectiveness compared to rule-based approaches, and insight into
leveraging LLMs to support software configuration evolution.

</details>


### [28] [Testing Is Not Boring: Characterizing Challenge in Software Testing Tasks](https://arxiv.org/abs/2507.20407)
*Davi Gama Hardman,Cesar França,Brody Stuart-Verner,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 论文探讨了软件测试的复杂性和挑战性，反驳了其被视为低技能活动的观点，并通过研究发现创造性、学习性和时间压力的任务对测试人员具有激励作用。


<details>
  <summary>Details</summary>
Motivation: 软件测试常被视为重复性低技能工作，但实际需要创造力、问题解决能力和适应性。研究旨在揭示测试工作的真实挑战及其对专业人员的影响。

Method: 对软件测试专业人员进行了研究，分析他们在测试任务中的体验和挑战。

Result: 研究发现，涉及创造力、持续学习和时间压力的任务能激励测试人员，而缺乏挑战或过度要求则会导致挫败感。

Conclusion: 研究强调了平衡任务复杂性的重要性，以维持测试人员的动力，并将软件测试视为一个动态且智力上富有吸引力的领域。

Abstract: As software systems continue to grow in complexity, testing has become a
fundamental part of ensuring the quality and reliability of software products.
Yet, software testing is still often perceived, both in industry and academia,
as a repetitive, low-skill activity. This perception fails to recognize the
creativity, problem-solving, and adaptability required in testing work. Tasks
such as designing complex test cases, automating testing processes, and
handling shifting requirements illustrate the challenges testing professionals
regularly face. To better understand these experiences, we conducted a study
with software testing professionals to explore the nature of challenging tasks
in software testing and how they affect these professionals. Our findings show
that tasks involving creativity, ongoing learning, and time pressure are often
seen as motivating and rewarding. On the other hand, a lack of challenge or
overwhelming demands can lead to frustration and disengagement. These findings
demonstrate the importance of balancing task complexity to sustain motivation
and present software testing as a dynamic and intellectually engaging field.

</details>


### [29] [When Prompts Go Wrong: Evaluating Code Model Robustness to Ambiguous, Contradictory, and Incomplete Task Descriptions](https://arxiv.org/abs/2507.20439)
*Maya Larbi,Amal Akli,Mike Papadakis,Rihab Bouyousfi,Maxime Cordy,Federica Sarro,Yves Le Traon*

Main category: cs.SE

TL;DR: 该论文研究了大型语言模型（LLMs）在代码生成任务中面对模糊、不完整或矛盾的任务描述时的鲁棒性，发现即使是小的问题也会显著降低性能。


<details>
  <summary>Details</summary>
Motivation: 实际开发中任务描述常不清晰，而现有研究多基于理想条件，因此需评估LLMs在非理想条件下的表现。

Method: 扩展HumanEval和MBPP基准，通过系统引入任务描述缺陷，评估不同规模和架构的LLMs。

Result: 任务描述的小问题会导致性能显著下降，矛盾描述易引发逻辑错误；大模型更稳健但仍受影响。

Conclusion: 需开发更鲁棒的LLMs，改进训练策略和评估基准，以应对实际开发中的不清晰需求。

Abstract: Large Language Models (LLMs) have demonstrated impressive performance in code
generation tasks under idealized conditions, where task descriptions are clear
and precise. However, in practice, task descriptions frequently exhibit
ambiguity, incompleteness, or internal contradictions. In this paper, we
present the first empirical study examining the robustness of state-of-the-art
code generation models when faced with such unclear task descriptions. We
extend the HumanEval and MBPP benchmarks by systematically introducing
realistic task descriptions flaws through guided mutation strategies, producing
a dataset that mirrors the messiness of informal developer instructions. We
evaluate multiple LLMs of varying sizes and architectures, analyzing their
functional correctness and failure modes across task descriptions categories.
Our findings reveal that even minor imperfections in task description phrasing
can cause significant performance degradation, with contradictory task
descriptions resulting in numerous logical errors. Moreover, while larger
models tend to be more resilient than smaller variants, they are not immune to
the challenges posed by unclear requirements. We further analyze semantic error
patterns and identify correlations between description clarity, model behavior,
and error types. Our results underscore the critical need for developing LLMs
that are not only powerful but also robust to the imperfections inherent in
natural user tasks, highlighting important considerations for improving model
training strategies, designing more realistic evaluation benchmarks, and
ensuring reliable deployment in practical software development environments.

</details>


### [30] [Distinguishing Quantum Software Bugs from Hardware Noise: A Statistical Approach](https://arxiv.org/abs/2507.20475)
*Ahmik Virani,Devraj,Anirudh Suresh,Lei Zhang,M V Panduranga Rao*

Main category: cs.SE

TL;DR: 提出了一种统计方法，通过概率指标区分量子软件错误和硬件噪声，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: NISQ时代的量子计算中，量子软件错误与硬件噪声难以区分，传统调试方法无法直接解决这一问题。

Method: 采用统计方法，利用概率指标区分软件错误和硬件噪声，并在Grover、Deutsch-Jozsa和Simon算法中进行了实验验证。

Result: 实验结果表明该方法有效且实用，为量子软件开发人员提供了可靠的分析工具。

Conclusion: 该方法能有效帮助开发者识别和分类量子程序中的异常行为。

Abstract: Quantum computing in the Noisy Intermediate-Scale Quantum (NISQ) era presents
significant challenges in differentiating quantum software bugs from hardware
noise. Traditional debugging techniques from classical software engineering
cannot directly resolve this issue due to the inherently stochastic nature of
quantum computation mixed with noises from NISQ computers. To address this gap,
we propose a statistical approach leveraging probabilistic metrics to
differentiate between quantum software bugs and hardware noise. We evaluate our
methodology empirically using well-known quantum algorithms, including Grover's
algorithm, Deutsch-Jozsa algorithm, and Simon's algorithm. Experimental results
demonstrate the efficacy and practical applicability of our approach, providing
quantum software developers with a reliable analytical tool to identify and
classify unexpected behavior in quantum programs.

</details>


### [31] [VDGraph: A Graph-Theoretic Approach to Unlock Insights from SBOM and SCA Data](https://arxiv.org/abs/2507.20502)
*Howell Xia,Jonah Gluck,Sevval Simsek,David Sastre Medina,David Starobinski*

Main category: cs.SE

TL;DR: VDGraph是一种基于知识图谱的方法，用于整合SBOM和SCA工具的数据，提供依赖和漏洞的统一视图，并通过实际项目验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现代软件供应链的复杂性需要工具（如SBOM和SCA）来管理依赖和漏洞，但目前缺乏统一的视图来整合这些数据。

Method: 提出VDGraph，将SBOM和SCA输出整合为依赖和漏洞的图谱表示，并解决数据冲突问题。通过实际工具（CycloneDX Maven插件和OSV-Scanner）实现概念验证。

Result: 在21个Java项目中应用VDGraph，发现高风险漏洞集中在深层依赖路径中，且漏洞多出现在三层或更深依赖层级。

Conclusion: VDGraph通过图谱理论方法提升了漏洞传播的可见性，结合开源工具和Neo4j为实际项目提供了可扩展的自动化分析基础。

Abstract: The high complexity of modern software supply chains necessitates tools such
as Software Bill of Materials (SBOMs) to manage component dependencies, and
Software Composition Analysis (SCA) tools to identify vulnerabilities. While
there exists limited integration between SBOMs and SCA tools, a unified view of
complex dependency-vulnerability relationships remains elusive. In this paper,
we introduce VDGraph, a novel knowledge graph-based methodology for integrating
vulnerability and dependency data into a holistic view. VDGraph consolidates
SBOM and SCA outputs into a graph representation of software projects'
dependencies and vulnerabilities. We provide a formal description and analysis
of the theoretical properties of VDGraph and present solutions to manage
possible conflicts between the SBOM and SCA data. We further introduce and
evaluate a practical, proof-of-concept implementation of VDGraph using two
popular SBOM and SCA tools, namely CycloneDX Maven plugin and Google's
OSV-Scanner. We apply VDGraph on 21 popular Java projects. Through the
formulation of appropriate queries on the graphs, we uncover the existence of
concentrated risk points (i.e., vulnerable components of high severity
reachable through numerous dependency paths). We further show that
vulnerabilities predominantly emerge at a depth of three dependency levels or
higher, indicating that direct or secondary dependencies exhibit lower
vulnerability density and tend to be more secure. Thus, VDGraph contributes a
graph-theoretic methodology that improves visibility into how vulnerabilities
propagate through complex, transitive dependencies. Moreover, our
implementation, which combines open SBOM and SCA standards with Neo4j, lays a
foundation for scalable and automated analysis across real-world projects.

</details>


### [32] [GeoJSEval: An Automated Evaluation Framework for Large Language Models on JavaScript-Based Geospatial Computation and Visualization Code Generation](https://arxiv.org/abs/2507.20553)
*Guanyu Chen,Haoyue Jiao,Shuyang Hou,Ziqi Liu,Lutong Xie,Shaowen Wu,Huayi Wu,Xuefeng Guan,Zhipeng Gui*

Main category: cs.SE

TL;DR: GeoJSEval是一个多模态、函数级的自动评估框架，用于评估大语言模型在JavaScript地理空间代码生成中的表现，涵盖432个任务和2071个测试用例。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在地理空间代码生成中的广泛应用，亟需系统化的评估方法来衡量其能力，尤其是在JavaScript环境中对地理空间计算和可视化的支持。

Method: 提出GeoJSEval框架，包括标准化测试套件（GeoJSEval-Bench）、代码提交引擎和评估模块，支持多维度的定量评估。

Result: 对18个先进大语言模型的评估揭示了其在空间语义理解、代码可靠性和函数调用准确性方面的显著性能差异。

Conclusion: GeoJSEval为地理空间代码生成模型的标准化评估和优化提供了方法论、资源和工具，具有强扩展性和实际应用价值。

Abstract: With the widespread adoption of large language models (LLMs) in code
generation tasks, geospatial code generation has emerged as a critical frontier
in the integration of artificial intelligence and geoscientific analysis. This
trend underscores the urgent need for systematic evaluation methodologies to
assess LLMs generation capabilities in geospatial contexts. In particular,
geospatial computation and visualization tasks in JavaScript environments rely
heavily on orchestrating diverse frontend libraries and ecosystems, placing
elevated demands on a model's semantic understanding and code synthesis
abilities. To address this challenge, we propose GeoJSEval--the first
multimodal, function-level automatic evaluation framework for LLMs in
JavaScript-based geospatial code generation. GeoJSEval comprises three core
components: a standardized test suite (GeoJSEval-Bench), a code submission
engine, and an evaluation module. It includes 432 function-level tasks and
2,071 structured test cases spanning five widely used JavaScript geospatial
libraries and 25 mainstream geospatial data types. GeoJSEval enables
multidimensional quantitative evaluation across metrics such as accuracy,
output stability, execution efficiency, resource consumption, and error type
distribution, and integrates boundary testing mechanisms to enhance robustness
and coverage. We conduct a comprehensive evaluation of 18 state-of-the-art LLMs
using GeoJSEval, revealing significant performance disparities and bottlenecks
in spatial semantic understanding, code reliability, and function invocation
accuracy. GeoJSEval provides a foundational methodology, evaluation resource,
and practical toolkit for the standardized assessment and optimization of
geospatial code generation models, with strong extensibility and applicability
in real-world scenarios.

</details>


### [33] [Intention-Driven Generation of Project-Specific Test Cases](https://arxiv.org/abs/2507.20619)
*Binhang Qi,Yun Lin,Xinyi Weng,Yuhuan Huang,Chenyan Liu,Hailong Sun,Jin Song Dong*

Main category: cs.SE

TL;DR: IntentionTest 是一种生成项目特定测试的方法，通过结构化描述验证意图，显著提升测试的语义正确性和通过率。


<details>
  <summary>Details</summary>
Motivation: 现有测试生成技术多基于代码覆盖率，但实际项目中测试需体现开发者对特定行为的验证意图和项目知识。

Method: IntentionTest 利用验证意图描述和项目内可重用测试代码，将测试生成问题转化为代码编辑问题。

Result: 在 4,146 个测试案例中，IntentionTest 生成的测试语义正确性提高 39.03%，通过率提高 21.30%。

Conclusion: IntentionTest 通过结合验证意图和项目知识，显著提升了测试生成的质量和实用性。

Abstract: Test cases are valuable assets for maintaining software quality. While
numerous automated techniques have been proposed for generating tests (either
by maximizing code coverage or by translating focal code into test code),
practical tests are seldom driven by coverage alone. In real projects, each
test reflects a developer's validation intention for a specific behaviour and
embodies rich, project-specific knowledge: which specific APIs to call and what
assertions truly matter. Without considering such knowledge, tests can hardly
pass code review and be integrated into the software product.
  In this work, we propose IntentionTest, which generates project-specific
tests with validation intention as a structured description. Our design is
motivated by two insights: (1) a description of validation intention, compared
to coverage and focal code, carries more crucial information about what to
test; and (2) practical tests exhibit high code duplication, indicating that
domain knowledge is highly reusable for writing new tests. Given a focal code
and a description of validation intention (in the form of either an informal
comment or a formal test plan), IntentionTest retrieves a referable test in the
project to guide test generation. Moreover, IntentionTest reduces the test
generation problem into an editing problem on the test code regarding the
validation intention. It generates a test including both test prefix and
oracle, which aims to be executable and semantically correct.
  We evaluate IntentionTest against state-of-the-art baselines on 4,146 test
cases from 13 open-source projects. Specifically, compared to ChatTester,
IntentionTest can (1) generate significantly more semantically correct tests,
improving common mutation scores by 39.03% and coverage overlap with
ground-truth tests by 40.14%; (2) generate 21.30% more successful passing
tests.

</details>


### [34] [Client--Library Compatibility Testing with API Interaction Snapshots](https://arxiv.org/abs/2507.20814)
*Gustave Monce,Thomas Degueule,Jean-Rémy Falleri,Romain Robbes*

Main category: cs.SE

TL;DR: 提出了一种新方法Gilesi，通过记录客户端测试中的API交互快照来检测库的行为破坏性变更（BBCs），解决了传统回归测试难以发现BBCs的问题。


<details>
  <summary>Details</summary>
Motivation: 现代软件开发依赖第三方库，但库的更新可能引入行为破坏性变更（BBCs），传统客户端测试难以检测。

Method: 记录客户端测试中的API交互快照（协议、输入输出值、异常等），通过比较快照变化识别BBCs。

Result: 原型工具Gilesi在实验中可靠地检测到了传统测试遗漏的BBCs。

Conclusion: Gilesi方法有效解决了BBCs检测问题，为客户端-库兼容性测试提供了新思路。

Abstract: Modern software development heavily relies on third-party libraries to speed
up development and enhance quality. As libraries evolve, they may break the
tacit contract established with their clients by introducing behavioral
breaking changes (BBCs) that alter run-time behavior and silently break client
applications without being detected at compile time. Traditional regression
tests on the client side often fail to detect such BBCs, either due to limited
library coverage or weak assertions that do not sufficiently exercise the
library's expected behavior. To address this issue, we propose a novel approach
to client--library compatibility testing that leverages existing client tests
in a novel way. Instead of relying on developer-written assertions, we propose
recording the actual interactions at the API boundary during the execution of
client tests (protocol, input and output values, exceptions, etc.). These
sequences of API interactions are stored as snapshots which capture the exact
contract expected by a client at a specific point in time. As the library
evolves, we compare the original and new snapshots to identify perturbations in
the contract, flag potential BBCs, and notify clients. We implement this
technique in our prototype tool Gilesi, a Java framework that automatically
instruments library APIs, records snapshots, and compares them. Through a
preliminary case study on several client--library pairs with artificially
seeded BBCs, we show that Gilesi reliably detects BBCs missed by client test
suites.

</details>


### [35] [Search-Based Fuzzing For RESTful APIs That Use MongoDB](https://arxiv.org/abs/2507.20848)
*Hernan Ghianni,Man Zhang,Juan P. Galeotti,Andrea Arcuri*

Main category: cs.SE

TL;DR: 本文提出了一种新颖的技术，通过动态分析NoSQL数据库状态和直接从测试用例插入数据，提升了RESTful API的白盒测试生成效果，显著提高了代码覆盖率。


<details>
  <summary>Details</summary>
Motivation: 在RESTful API测试中，数据库状态对提高代码覆盖率和发现隐藏故障至关重要。现有方法在生成测试时未充分考虑数据库状态，尤其是在NoSQL数据库（如MongoDB）中。

Method: 通过自动化代码插桩动态分析数据库状态，并直接从测试用例插入NoSQL数据，优化测试生成过程。该方法作为EvoMaster工具的扩展实现。

Result: 在六个RESTful API上的实验显示，代码覆盖率提高了18%，优于现有白盒方法和四种黑盒模糊测试工具。

Conclusion: 该方法显著提升了RESTful API测试的效果，尤其在处理NoSQL数据库状态时表现出色，适用于测试只读微服务。

Abstract: In RESTful APIs, interactions with a database are a common and crucial
aspect. When generating whitebox tests, it is essential to consider the
database's state (i.e., the data contained in the database) to achieve higher
code coverage and uncover more hidden faults. This article presents novel
techniques to enhance search-based software test generation for RESTful APIs
interacting with NoSQL databases. Specifically, we target the popular MongoDB
database, by dynamically analyzing (via automated code instrumentation) the
state of the database during the test generation process. Additionally, to
achieve better results, our novel approach allows inserting NoSQL data directly
from test cases. This is particularly beneficial when generating the correct
sequence of events to set the NoSQL database in an appropriate state is
challenging or time-consuming. This method is also advantageous for testing
read-only microservices. Our novel techniques are implemented as an extension
of EvoMaster, the only open-source tool for white-box fuzzing RESTful APIs.
Experiments conducted on six RESTful APIs demonstrated significant improvements
in code coverage, with increases of up to 18% compared to existing white-box
approaches. To better highlight the improvements of our novel techniques,
comparisons are also carried out with four state-of-the-art black-box fuzzers.

</details>


### [36] [Enhancing Project-Specific Code Completion by Inferring Internal API Information](https://arxiv.org/abs/2507.20888)
*Le Deng,Xiaoxue Ren,Chao Ni,Ming Liang,David Lo,Zhongxin Liu*

Main category: cs.SE

TL;DR: 提出了一种无需依赖导入即可推断内部API信息的方法，通过构建API使用示例和语义描述扩展表示，显著提升了代码补全的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在代码补全中难以有效利用未显式导入的内部API信息，影响了准确性。

Method: 构建API使用示例和语义描述，形成知识库供LLMs生成补全，并引入ProjBench基准测试。

Result: 在ProjBench和CrossCodeEval上，代码精确匹配提升22.72%，标识符匹配提升18.31%；与现有基线结合后，代码匹配提升47.80%，标识符匹配提升35.55%。

Conclusion: 该方法显著提升了代码补全的准确性，尤其在处理未显式导入的API时表现优异。

Abstract: Project-specific code completion is a critical task that leverages context
from a project to generate accurate code. State-of-the-art methods use
retrieval-augmented generation (RAG) with large language models (LLMs) and
project information for code completion. However, they often struggle to
incorporate internal API information, which is crucial for accuracy, especially
when APIs are not explicitly imported in the file.
  To address this, we propose a method to infer internal API information
without relying on imports. Our method extends the representation of APIs by
constructing usage examples and semantic descriptions, building a knowledge
base for LLMs to generate relevant completions. We also introduce ProjBench, a
benchmark that avoids leaked imports and consists of large-scale real-world
projects.
  Experiments on ProjBench and CrossCodeEval show that our approach
significantly outperforms existing methods, improving code exact match by
22.72% and identifier exact match by 18.31%. Additionally, integrating our
method with existing baselines boosts code match by 47.80% and identifier match
by 35.55%.

</details>


### [37] [Repairing vulnerabilities without invisible hands. A differentiated replication study on LLMs](https://arxiv.org/abs/2507.20977)
*Maria Camporese,Fabio Massacci*

Main category: cs.SE

TL;DR: 研究探讨了大型语言模型（LLMs）在自动漏洞修复（AVR）中的表现，质疑其优势是否源于训练数据泄漏或完美故障定位等隐藏因素。通过故意偏移漏洞位置进行实验，验证LLMs是否仅记忆修复方案。


<details>
  <summary>Details</summary>
Motivation: 验证LLMs在AVR中的优势是否由隐藏因素（如数据泄漏或完美定位）驱动，而非其实际能力。

Method: 在Vul4J和VJTrans基准测试中，故意偏移漏洞位置n行，使用LLM生成补丁并由另一LLM审核，最后通过回归和漏洞测试验证。

Result: 通过实验分析LLMs在不同偏移量下的修复表现，判断其是否依赖记忆。

Conclusion: 研究旨在揭示LLMs在AVR中的真实能力，避免高估其性能。

Abstract: Background: Automated Vulnerability Repair (AVR) is a fast-growing branch of
program repair. Recent studies show that large language models (LLMs)
outperform traditional techniques, extending their success beyond code
generation and fault detection.
  Hypothesis: These gains may be driven by hidden factors -- "invisible hands"
such as training-data leakage or perfect fault localization -- that let an LLM
reproduce human-authored fixes for the same code.
  Objective: We replicate prior AVR studies under controlled conditions by
deliberately adding errors to the reported vulnerability location in the
prompt. If LLMs merely regurgitate memorized fixes, both small and large
localization errors should yield the same number of correct patches, because
any offset should divert the model from the original fix.
  Method: Our pipeline repairs vulnerabilities from the Vul4J and VJTrans
benchmarks after shifting the fault location by n lines from the ground truth.
A first LLM generates a patch, a second LLM reviews it, and we validate the
result with regression and proof-of-vulnerability tests. Finally, we manually
audit a sample of patches and estimate the error rate with the
Agresti-Coull-Wilson method.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [38] [Self-Verifying Predicates in Büchi Arithmetic](https://arxiv.org/abs/2507.19717)
*Mazen Khodier,Luke Schaeffer,Jeffrey Shallit*

Main category: cs.FL

TL;DR: 基于Angluin算法，提出一种自动生成有限自动机的方法，用于处理Büchi算术中的一阶逻辑公式，比直接方法更快且节省空间。


<details>
  <summary>Details</summary>
Motivation: 直接方法在处理Büchi算术中的一阶逻辑公式时效率低且占用空间大，需要更高效的解决方案。

Method: 利用Angluin算法自动生成有限自动机，并通过免费软件Walnut进行实现。

Result: 实验数据显示，该方法比直接方法更快且占用空间更少。

Conclusion: 基于Angluin算法的自动生成方法在处理Büchi算术中的一阶逻辑公式时具有显著优势。

Abstract: We discuss a technique, based on Angluin's algorithm, for automatically
generating finite automata for various kinds of useful first-order logic
formulas in B\"uchi arithmetic. Construction in this way can be faster and use
much less space than more direct methods. We discuss the theory and we present
some empirical data for the free software Walnut.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [39] [Development and Evaluation of Adaptive LearningSupport System Based on Ontology of MultipleProgramming Languages](https://arxiv.org/abs/2507.19728)
*Lalita Na Nongkhai,Jingyun Wang,Takahiko Mendori*

Main category: cs.PL

TL;DR: 论文介绍了一种基于本体的自适应学习支持系统ADVENTURE，用于个性化编程练习，通过Elo评分系统动态调整难度，实验结果显示自适应模式优于随机模式。


<details>
  <summary>Details</summary>
Motivation: 为编程学习者提供个性化的练习支持，通过自适应机制提升学习效果。

Method: 利用本体CONTINUOUS跨语言概念，结合Elo评分系统动态调整练习难度，实验比较自适应与随机模式。

Result: 自适应模式在正确提交和通过概念数量上显著优于随机模式。

Conclusion: ADVENTURE系统能有效支持编程学习者，自适应机制具有实际应用价值。

Abstract: This paper introduces an ontology-based approach within an adaptive learning
support system for computer programming. This system (named ADVENTURE) is
designed to deliver personalized programming exercises that are tailored to
individual learners' skill levels. ADVENTURE utilizes an ontology, named
CONTINUOUS, which encompasses common concepts across multiple programming
languages. The system leverages this ontology not only to visualize programming
concepts but also to provide hints during practice programming exercises and
recommend subsequent programming concepts. The adaptive mechanism is driven by
the Elo Rating System, applied in an educational context to dynamically
estimate the most appropriate exercise difficulty for each learner. An
experimental study compared two instructional modes, adaptive and random, based
on six features derived from 1,186 code submissions across all the experimental
groups. The results indicate significant differences in four of six analyzed
features between these two modes. Notably, the adaptive mode demonstrates a
significant difference over the random mode in two features, the submission of
correct answers and the number of pass concepts. Therefore, these results
underscore that this adaptive learning support system may support learners in
practicing programming exercises.

</details>


### [40] [The Power of Negation in Higher-Order Datalog](https://arxiv.org/abs/2507.20251)
*Angelos Charalambidis,Babis Kostopoulos,Christos Nomikos,Panos Rondogiannis*

Main category: cs.PL

TL;DR: 研究了高阶Datalog$^\neg$在良基语义和稳定模型语义下的表达能力，发现其与复杂性类有紧密联系。


<details>
  <summary>Details</summary>
Motivation: 探索高阶逻辑编程中阶数与非确定性之间的权衡关系。

Method: 使用良基语义和稳定模型语义分析高阶Datalog$^\neg$的表达能力，并通过部分应用关系和关系枚举等方法证明结果。

Result: 在良基语义下，$(k+1)$-阶Datalog$^\neg$捕获k-EXP；在稳定模型语义下，分别捕获co-(k-NEXP)和k-NEXP。

Conclusion: 研究揭示了高阶逻辑编程中阶数与非确定性的表达能力层次结构。

Abstract: We investigate the expressive power of Higher-Order Datalog$^\neg$ under both
the well-founded and the stable model semantics, establishing tight connections
with complexity classes. We prove that under the well-founded semantics, for
all $k\geq 1$, $(k+1)$-Order Datalog$^\neg$ captures k-EXP, a result that holds
without explicit ordering of the input database. The proof of this fact can be
performed either by using the powerful existential predicate variables of the
language or by using partially applied relations and relation enumeration.
Furthermore, we demonstrate that this expressive power is retained within a
stratified fragment of the language. Under the stable model semantics, we show
that $(k+1)$-Order Datalog$^\neg$ captures co-(k-NEXP) using cautious reasoning
and k-NEXP using brave reasoning, again with analogous results for the
stratified fragment augmented with choice rules. Our results establish a
hierarchy of expressive power, highlighting an interesting trade-off between
order and non-determinism in the context of higher-order logic programming:
increasing the order of programs under the well-founded semantics can surpass
the expressive power of lower-order programs under the stable model semantics.

</details>
