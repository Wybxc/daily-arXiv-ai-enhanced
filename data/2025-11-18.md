<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 24]
- [cs.LO](#cs.LO) [Total: 4]
- [cs.PL](#cs.PL) [Total: 4]
- [cs.FL](#cs.FL) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [WITNESS: A lightweight and practical approach to fine-grained predictive mutation testing](https://arxiv.org/abs/2511.11999)
*Zeyu Lu,Peng Zhang,Chun Yong Chong,Shan Gao,Yibiao Yang,Yanhui Li,Lin Chen,Yuming Zhou*

Main category: cs.SE

TL;DR: WITNESS是一种新的细粒度预测性变异测试方法，采用轻量级经典机器学习模型，能够处理方法内和方法外的变异体，显著提高了效率和适用性。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的细粒度预测性变异测试方法存在两个关键限制：(1)计算成本过高，违背了预测性变异测试降低成本的初衷；(2)适用性受限，只能处理方法内变异体而无法预测方法外变异体。

Method: WITNESS采用双重设计：(1)收集方法内和方法外变异体的特征，适用于所有生成的变异体；(2)使用轻量级经典机器学习模型进行训练和预测，而非计算昂贵的深度学习。

Result: 在Defects4J项目上的评估显示，WITNESS在不同场景下始终达到最先进的预测性能，显著提高了杀灭矩阵预测效率。后分析表明包含变异前后信息的特征最为重要。

Conclusion: WITNESS通过轻量级机器学习方法实现了高效且全面的预测性变异测试，在测试用例优先级排序方面表现优于基线方法，更接近使用实际杀灭矩阵的结果。

Abstract: Existing fine-grained predictive mutation testing studies predominantly rely on deep learning, which faces two critical limitations in practice: (1) Exorbitant computational costs. The deep learning models adopted in these studies demand significant computational resources for training and inference acceleration. This introduces high costs and undermines the cost-reduction goal of predictive mutation testing. (2) Constrained applicability. Although modern mutation testing tools generate mutants both inside and outside methods, current fine-grained predictive mutation testing approaches handle only inside-method mutants. As a result, they cannot predict outside-method mutants, limiting their applicability in real-world scenarios. We propose WITNESS, a new fine-grained predictive mutation testing approach. WITNESS adopts a twofold design: (1) With collected features from both inside-method and outside-method mutants, WITNESS is suitable for all generated mutants. (2) Instead of using computationally expensive deep learning, WITNESS employs lightweight classical machine learning models for training and prediction. This makes it more cost-effective and enabling straightforward explanations of the decision-making processes behind the adopted models. Evaluations on Defects4J projects show that WITNESS consistently achieves state-of-the-art predictive performance across different scenarios. Additionally, WITNESS significantly enhances the efficiency of kill matrix prediction. Post-hoc analysis reveals that features incorporating information from before and after the mutation are the most important among those used in WITNESS. Test case prioritization based on the predicted kill matrix shows that WITNESS delivers results much closer to those obtained by using the actual kill matrix, outperforming baseline approaches.

</details>


### [2] [A Code Smell Refactoring Approach using GNNs](https://arxiv.org/abs/2511.12069)
*HanYu Zhang,Tomoji Kishi*

Main category: cs.SE

TL;DR: 提出了一种基于图深度学习的代码异味重构方法，使用图分类和节点分类任务来处理长方法、大类、特性嫉妒三种代码异味，在实验中表现出优越的重构性能。


<details>
  <summary>Details</summary>
Motivation: 现有代码异味重构方法存在局限性：基于指标和规则的方法依赖手动定义的启发式和阈值，而深度学习方法受限于数据集可用性和模型设计。

Method: 设计了类级和方法级两种输入图，采用图分类和节点分类任务，使用GCN、GraphSAGE和GAT三种经典GNN架构，并提出半自动化数据集生成方法。

Result: 实验结果表明，所提出的方法在重构性能上优于传统方法和最先进的深度学习方法。

Conclusion: 基于图深度学习的代码异味重构方法能够有效解决现有方法的局限性，并在重构任务中表现出优越性能。

Abstract: Code smell is a great challenge in software refactoring, which indicates latent design or implementation flaws that may degrade the software maintainability and evolution. Over the past decades, a variety of refactoring approaches have been proposed, which can be broadly classified into metrics-based, rule-based, and machine learning-based approaches. Recent years, deep learning-based approaches have also attracted widespread attention. However, existing techniques exhibit various limitations. Metrics- and rule-based approaches rely heavily on manually defined heuristics and thresholds, whereas deep learning-based approaches are often constrained by dataset availability and model design. In this study, we proposed a graph-based deep learning approach for code smell refactoring. Specifically, we designed two types of input graphs (class-level and method-level) and employed both graph classification and node classification tasks to address the refactoring of three representative code smells: long method, large class, and feature envy. In our experiment, we propose a semi-automated dataset generation approach that could generate a large-scale dataset with minimal manual effort. We implemented the proposed approach with three classical GNN (graph neural network) architectures: GCN, GraphSAGE, and GAT, and evaluated its performance against both traditional and state-of-the-art deep learning approaches. The results demonstrate that proposed approach achieves superior refactoring performance.

</details>


### [3] [Actionable Warning Is Not Enough: Recommending Valid Actionable Warnings with Weak Supervision](https://arxiv.org/abs/2511.12229)
*Zhipeng Xue,Zhipeng Gao,Tongtong Xu,Xing Hu,Xin Xia,Shanping Li*

Main category: cs.SE

TL;DR: 该研究构建了首个大规模可操作警告数据集，并提出ACWRecommender框架来推荐高概率为真实bug的可操作警告，显著提升了静态分析工具的实用性。


<details>
  <summary>Details</summary>
Motivation: 静态分析工具的高误报率阻碍了其广泛采用，现有方法对可操作警告的收集假设不准确，导致大量无效警告。

Method: 通过挖掘GitHub C仓库的68,274次代码回退构建数据集，提出两阶段框架：粗粒度检测阶段识别可操作警告，细粒度重排阶段使用弱监督学习将高概率bug警告排到顶部。

Result: 实验显示ACWRecommender在nDCG和MRR指标上大幅优于基线方法，在6个项目测试中，向开发者报告的top-10警告中有27个被确认为真实bug。

Conclusion: 该工具能帮助开发者从大量警告中快速找到真实bug，验证了其实际应用价值。

Abstract: The use of static analysis tools has gained increasing popularity among developers in the last few years. However, the widespread adoption of static analysis tools is hindered by their high false alarm rates. Previous studies have introduced the concept of actionable warnings and built a machine-learning method to distinguish actionable warnings from false alarms. However, according to our empirical observation, the current assumption used for actionable warning(s) collection is rather shaky and inaccurate, leading to a large number of invalid actionable warnings. To address this problem, in this study, we build the first large actionable warning dataset by mining 68,274 reversions from Top-500 GitHub C repositories, we then take one step further by assigning each actionable warning a weak label regarding its likelihood of being a real bug. Following that, we propose a two-stage framework called ACWRecommender to automatically recommend the actionable warnings with high probability to be real bugs (AWHB). Our approach warms up the pre-trained model UniXcoder by identifying actionable warnings task (coarse-grained detection stage) and rerank AWHB to the top by weakly supervised learning (fine-grained reranking stage). Experimental results show that our proposed model outperforms several baselines by a large margin in terms of nDCG and MRR for AWHB recommendation. Moreover, we ran our tool on 6 randomly selected projects and manually checked the top-ranked warnings from 2,197 reported warnings, we reported top-10 recommended warnings to developers, 27 of them were already confirmed by developers as real bugs. Developers can quickly find real bugs among the massive amount of reported warnings, which verifies the practical usage of our tool.

</details>


### [4] [Reflections on the design, applications and implementations of the normative specification language eFLINT](https://arxiv.org/abs/2511.12276)
*L. Thomas van Binsbergen,Christopher A. Esterhuyse,Tim Müller*

Main category: cs.SE

TL;DR: 本文介绍了eFLINT领域特定软件语言，该语言结合声明式和过程式元素，用于自动化法律合规检查，并讨论了其设计决策和实际应用经验。


<details>
  <summary>Details</summary>
Motivation: 随着软件在社会实践中的普及，检查软件是否符合法律、法规和合同的要求变得越来越重要和昂贵。政府和公司提供的数字化服务受到越来越多法律法规的约束，需要高度适应的合规实践。

Method: 开发了eFLINT领域特定软件语言，该语言结合声明式和过程式元素，分别用于推理情况和场景，明确并形式化法律概念与计算概念之间的联系，旨在在软件系统运行前、运行中和运行后自动化合规检查。

Result: 通过回顾各种应用案例、它们提出的要求以及后续的设计决策，反思了eFLINT语言的当前设计。

Conclusion: 本文报告了对自动化合规领域语言开发者有益的研究结果和见解，为自动化合规检查提供了实用的解决方案和经验分享。

Abstract: Checking the compliance of software against laws, regulations and contracts is increasingly important and costly as the embedding of software into societal practices is getting more pervasive. Moreover, the digitalised services provided by governmental organisations and companies are governed by an increasing amount of laws and regulations, requiring highly adaptable compliance practices. A potential solution is to automate compliance using software. However, automating compliance is difficult for various reasons. Legal practices involve subjective processes such as interpretation and qualification. New laws and regulations come into effect regularly and laws and regulations, as well as their interpretations, are subjected to constant revision. In addition, computational reasoning with laws requires a cross-disciplinary process involving both legal and software expertise.
  This paper reflects on the domain-specific software language eFLINT developed to experiment with novel solutions. The language combines declarative and procedural elements to reason about situations and scenarios respectively, explicates and formalises connections between legal concepts and computational concepts, and is designed to automate compliance checks both before, during and after a software system runs. The various goals and applications areas for the language give rise to (conflicting) requirements. This paper reflects on the current design of the language by recalling various applications, the requirements they imposed, and subsequent design decisions. As such, this paper reports on results and insights of an investigation that can benefit language developers within the field of automated compliance.

</details>


### [5] [Ontology-Driven Model-to-Model Transformation of Workflow Specifications](https://arxiv.org/abs/2511.13661)
*Francisco Abreu,Luís Cruz,Sérgio Guerreiro*

Main category: cs.SE

TL;DR: 开发了一个基于本体的模型转换管道，将专有工作流语言Smart Forms & Smart Flow转换为标准BPMN 2.0，解决厂商锁定问题。


<details>
  <summary>Details</summary>
Motivation: 专有工作流建模语言如Smart Forms & Smart Flow阻碍了互操作性和重用，将流程知识锁定在封闭格式中。

Method: 采用三阶段管道：RML将JSON语义提升为RDF/OWL、本体对齐和推理、通过Camunda Model API生成BPMN。通过本体和声明性规则外部化映射知识。

Result: 在69个真实工作流上评估，生成了92个BPMN图，成功率94.2%。失败率5.81%源于动态行为和时间转换。访谈显示结果图提供了自上而下的视图，改善了理解、诊断和入职。

Conclusion: 基于本体的M2M转换可以系统性地桥接领域特定工作流和标准符号，为利益相关者提供可量化的性能和定性收益，支持互操作性并减少厂商依赖。

Abstract: Proprietary workflow modeling languages such as Smart Forms & Smart Flow hamper interoperability and reuse because they lock process knowledge into closed formats. To address this vendor lock-in and ease migration to open standards, we introduce an ontology-driven model-to-model pipeline that systematically translates domain-specific workflow definitions to Business Process Model and Notation (BPMN) 2.0. The pipeline comprises three phases: RML-based semantic lifting of JSON to RDF/OWL, ontology alignment and reasoning, and BPMN generation via the Camunda Model API. By externalizing mapping knowledge into ontologies and declarative rules rather than code, the approach supports reusability across vendor-specific formats and preserves semantic traceability between source definitions and target BPMN models. We instantiated the pipeline for Instituto Superior Técnico (IST)'s Smart Forms & Smart Flow and implemented a converter that produces standard-compliant BPMN diagrams. Evaluation on a corpus of 69 real-world workflows produced 92 BPMN diagrams with a 94.2% success rate. Failures (5.81%) stemmed from dynamic behaviors and time-based transitions not explicit in the static JSON. Interviews with support and development teams indicated that the resulting diagrams provide a top-down view that improves comprehension, diagnosis and onboarding by exposing implicit control flow and linking tasks and forms back to their sources. The pipeline is generalizable to other proprietary workflow languages by adapting the ontology and mappings, enabling interoperability and reducing vendor dependency while supporting continuous integration and long-term maintainability. The presented case study demonstrates that ontology-driven M2M transformation can systematically bridge domain-specific workflows and standard notations, offering quantifiable performance and qualitative benefits for stakeholders.

</details>


### [6] [Reducing Hallucinations in LLM-Generated Code via Semantic Triangulation](https://arxiv.org/abs/2511.12288)
*Yihan Dai,Sijie Liang,Haotian Xu,Peichu Xie,Sergey Mechtaev*

Main category: cs.SE

TL;DR: 提出语义三角化方法，通过问题变换来验证代码生成的一致性，提高LLM生成代码的可靠性和弃权能力


<details>
  <summary>Details</summary>
Motivation: 现有样本共识技术在采样概率低或存在多个有效但不等价解时，难以正确选择解决方案或适时弃权

Method: 语义三角化：对编程问题进行语义非平凡变换，保持解之间的可验证映射，通过跨变换的一致性验证来提高置信度

Result: 在LiveCodeBench和CodeElo基准测试中，相比概率阈值0.5的高置信度选择方法，可靠性提高21%，能在低至0.14的采样概率下识别正确解

Conclusion: 语义三角化能更可靠地形成真实共识，特别是在存在多个有效但不等价解的任务中表现突出

Abstract: When generating code from natural language prompts, an LLM samples programs from a probability distribution, many of which might be incorrect. Sample consensus techniques - such as majority voting or validation against generated tests or specifications - aim to identify a correct program in the sample or abstain if none is valid. However, existing methods often fail to select a correct solution when its sampling probability is low, or when the problem permits multiple valid but non-equivalent solutions. Additionally, they often fail to abstain when no correct solution is present in the sample. To overcome these limitations, we introduce semantic triangulation, which transforms a programming problem in a way that non-trivially alters its semantics while preserving an exact, verifiable mapping between solutions before and after transformation. We theoretically establish that verifying consistency across such problem transformations increases confidence that generated programs reflect accurate generalization rather than spurious statistical correlations, enabling more reliable sample consensus and abstention. On the LiveCodeBench and CodeElo benchmarks, using GPT-4o and DeepSeek-V3 models, semantic triangulation increases reliability of generated code by 21% compared to the method that selects only high-confidence solutions with the probability threshold 0.5, while being able to pinpoint correct solutions at sampling probabilities as low as 0.14. Apart from that, it is also the only approach to consistently form true consensus on tasks with multiple valid but non-equivalent solutions.

</details>


### [7] [ProofWright: Towards Agentic Formal Verification of CUDA](https://arxiv.org/abs/2511.12294)
*Bodhisatwa Chatterjee,Drew Zagieboylo,Sana Damani,Siva Hari,Christos Kozyrakis*

Main category: cs.SE

TL;DR: ProofWright是一个集成自动形式验证与LLM代码生成的代理验证框架，为LLM生成的CUDA内核提供内存安全、线程安全和语义正确性的端到端保证。


<details>
  <summary>Details</summary>
Motivation: LLM自动生成的CUDA内核存在难以发现的正确性错误且缺乏形式安全保证，运行时测试不可靠而手动形式验证无法跟上LLM输出速度，造成验证瓶颈。

Method: 将自动形式验证与LLM代码生成相结合，构建代理验证框架，对生成的CUDA内核进行形式化验证。

Result: 在KernelBench L1上，ProofWright验证了74%生成内核的安全属性，发现了传统测试遗漏的细微正确性错误，并为一类逐元素内核建立了语义等价性。

Conclusion: ProofWright证明了对LLM生成的GPU代码进行可扩展的自动形式验证是可行的，为可信高性能代码生成提供了路径，且仅带来每内核3分钟的适度开销。

Abstract: Large Language Models (LLMs) are increasingly used to automatically generate optimized CUDA kernels, substantially improving developer productivity. However, despite rapid generation, these kernels often contain subtle correctness bugs and lack formal safety guarantees. Runtime testing is inherently unreliable - limited input coverage and reward hacking can mask incorrect behavior - while manual formal verification is reliable but cannot scale to match LLM output rates, creating a critical validation bottleneck.
  We present ProofWright, an agentic verification framework that bridges this gap by integrating automated formal verification with LLM-based code generation. ProofWright provides end-to-end guarantees of memory safety, thread safety, and semantic correctness for LLM-generated CUDA kernels. On KernelBench L1, ProofWright verifies safety properties for 74% of generated kernels, uncovers subtle correctness errors missed by conventional testing, and establishes semantic equivalence for a class of element-wise kernels. With a modest overhead of 3 minutes per kernel, ProofWright demonstrates that scalable, automated formal verification of LLM-generated GPU code is feasible - offering a path toward trustworthy high-performance code generation without sacrificing developer productivity.

</details>


### [8] [Enhancing LLM Code Generation Capabilities through Test-Driven Development and Code Interpreter](https://arxiv.org/abs/2511.12823)
*Sajed Jalil,Shuvo Saha,Hossain Mohammad Seym*

Main category: cs.SE

TL;DR: 提出了一种结合测试驱动开发和代码解释器的新方法，使用开源模型提升孟加拉语代码生成准确率至85%，无需微调即可让小型模型达到大型模型98%的性能。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语拥有2.42亿母语者但在LLM训练中关注不足，现有代码生成技术需要专业知识和资源，目标是让新兴市场用户能用母语使用强大的代码生成工具。

Method: 结合测试驱动开发(TDD)和代码解释器(CI)，使用开源权重模型，无需微调。

Result: 将孟加拉语提示的代码生成基线准确率提升至85%，小型模型能达到大型模型98%的准确率。

Conclusion: 该方法有效提升了孟加拉语代码生成性能，证明了无需微调即可让小型模型接近大型模型性能，所有结果已在GitHub公开。

Abstract: Over the past few years, improving LLM code generation capabilities has been a key focus in NLP research. Despite Bengali having 242 million native speakers worldwide, it receives little attention when it comes to training LLMs. More recently, various fine-tuning and augmented generation techniques have been employed to significantly enhance code generation performance. However, they require considerable expertise and resources to utilize effectively as an end user. The goal of our work is to democratize access to powerful code generation tools in resource-constrained emerging markets, enabling users to leverage them in their native language.
  We introduce a novel approach that combines Test-Driven Development (TDD) and Code Interpreter (CI), utilizing open-weight models, which improves the baseline accuracy for code generation with Bengali prompts and achieves an overall accuracy of 85%. Our approach requires no finetuning and proves that even the smallest models in the same family can attain up to 98% accuracy compared to the largest models. All of our results are publicly shared in GitHub for validation and reproducibility.

</details>


### [9] [High-level reasoning while low-level actuation in Cyber-Physical Systems: How efficient is it?](https://arxiv.org/abs/2511.12543)
*Burak Karaduman,Baris Tekin Tezel,Moharram Challenger*

Main category: cs.SE

TL;DR: 该研究比较了六种编程语言和框架（C++、Java、Jade、Jason以及松散和紧密耦合的模糊Jason BDI）在最坏情况执行时间和开发时间方面的表现，为工业信息化系统选择软件技术提供实证依据。


<details>
  <summary>Details</summary>
Motivation: 工业信息集成系统日益复杂，需要智能行为、实时响应和高效开发的软件技术，但工程师缺乏选择工具的实证证据。

Method: 采用以开发者为中心的方法，基于可衡量的结果，通过结构化比较分析抽象层次和推理能力如何影响开发工作和运行时行为。

Result: 研究发现抽象和推理机制影响系统性能和开发效率，揭示了工程工作量与执行效率之间的具体权衡关系。

Conclusion: 研究为工业信息化中选择软件技术提供了基于证据的指导，支持改进集成效率、可维护性和响应性，为未来研究奠定基础。

Abstract: The increasing complexity of industrial information-integration systems demands software technologies that enable intelligent behaviour, real-time response, and efficient development. Although many programming languages and frameworks exist, engineers still lack sufficient empirical evidence to guide the choice of tools for advanced industrial applications. This study addresses that need by measuring and comparing worst-case execution time (WCET) and development time across six languages and frameworks: C++, Java, Jade, Jason, and fuzzy Jason BDI with both loosely and tightly coupled integration. These technologies reflect a progression from procedural and object-oriented programming to agent-based frameworks capable of symbolic and fuzzy reasoning.
  Rather than relying on broad concepts such as paradigms or orientations, the study adopts a developer-centred approach grounded in measurable outcomes. The structured comparison examines how rising abstraction levels and reasoning capabilities affect both development effort and runtime behaviour. By analysing these dimensions, the study highlights concrete trade-offs between engineering workload and execution efficiency.
  The findings show how abstraction and reasoning mechanisms shape system performance and developer productivity, offering practical insight for designing intelligent, agent-based solutions that must operate under real-time constraints and complex decision-making requirements. Overall, the study contributes evidence-based guidance for selecting software technologies in industrial informatization, supporting improved integration efficiency, maintainability, and responsiveness, and laying groundwork for future research on the interplay between language features, development dynamics, and runtime behaviour in cyber-physical and smart manufacturing systems.

</details>


### [10] [Can Small GenAI Language Models Rival Large Language Models in Understanding Application Behavior?](https://arxiv.org/abs/2511.12576)
*Mohammad Meymani,Hamed Jelodar,Parisa Hamedi,Roozbeh Razavi-Far,Ali A. Ghorbani*

Main category: cs.SE

TL;DR: 本文系统评估了大小生成式AI模型在应用行为理解（特别是恶意软件检测）中的表现，发现小模型在保持竞争力的同时具有更高的计算效率和部署灵活性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型在代码分析和理解方面展现出强大能力，但大模型的计算资源需求限制了实际应用。研究旨在评估小模型是否能在大模型主导的领域提供可行的替代方案。

Method: 系统评估不同规模的生成式AI语言模型在应用行为理解任务中的表现，以恶意软件检测为代表任务，比较准确性、精确率、召回率和F1分数等指标。

Result: 大模型整体准确率更高，但小模型在精确率和召回率方面保持竞争力，并在计算效率、推理速度和资源受限环境部署方面具有显著优势。

Conclusion: 小生成式AI模型可以有效补充大模型，在实际应用行为分析中提供性能与资源效率之间的实用平衡。

Abstract: Generative AI (GenAI) models, particularly large language models (LLMs), have transformed multiple domains, including natural language processing, software analysis, and code understanding. Their ability to analyze and generate code has enabled applications such as source code summarization, behavior analysis, and malware detection. In this study, we systematically evaluate the capabilities of both small and large GenAI language models in understanding application behavior, with a particular focus on malware detection as a representative task. While larger models generally achieve higher overall accuracy, our experiments show that small GenAI models maintain competitive precision and recall, offering substantial advantages in computational efficiency, faster inference, and deployment in resource-constrained environments. We provide a detailed comparison across metrics such as accuracy, precision, recall, and F1-score, highlighting each model's strengths, limitations, and operational feasibility. Our findings demonstrate that small GenAI models can effectively complement large ones, providing a practical balance between performance and resource efficiency in real-world application behavior analysis.

</details>


### [11] [LLM4SCREENLIT: Recommendations on Assessing the Performance of Large Language Models for Screening Literature in Systematic Reviews](https://arxiv.org/abs/2511.12635)
*Lech Madeyski,Barbara Kitchenham,Martin Shepperd*

Main category: cs.SE

TL;DR: 评估LLM在系统综述文献筛选中的性能时，存在传统指标不适用、证据丢失问题、混淆矩阵报告不完整等问题，建议优先考虑召回率、加权MCC指标，并报告完整混淆矩阵。


<details>
  <summary>Details</summary>
Motivation: LLM发布速度过快导致评估不足，当LLM用于系统综述文献筛选时，需要稳健的实证评估来确保可靠性。

Method: 以大规模研究为例分析传统指标问题，分析27篇相关论文的性能指标使用情况，识别良好实践和普遍问题。

Result: 发现主要弱点：使用不平衡数据不稳健的指标、忽视证据丢失影响、混淆矩阵报告不完整；同时提取了良好评估实践。

Conclusion: 建议优先考虑召回率和加权MCC指标，报告完整混淆矩阵，采用防泄漏设计，基于成本效益分析得出结论。

Abstract: Context: Large language models (LLMs) are released faster than users' ability to evaluate them rigorously. When LLMs underpin research, such as identifying relevant literature for systematic reviews (SRs), robust empirical assessment is essential. Objective: We identify and discuss key challenges in assessing LLM performance for selecting relevant literature, identify good (evaluation) practices, and propose recommendations. Method: Using a recent large-scale study as an example, we identify problems with the use of traditional metrics for assessing the performance of Gen-AI tools for identifying relevant literature in SRs. We analyzed 27 additional papers investigating this issue, extracted the performance metrics, and found both good practices and widespread problems, especially with the use and reporting of performance metrics for SR screening. Results: Major weaknesses included: i) a failure to use metrics that are robust to imbalanced data and do not directly indicate whether results are better than chance, e.g., the use of Accuracy, ii) a failure to consider the impact of lost evidence when making claims concerning workload savings, and iii) pervasive failure to report the full confusion matrix (or performance metrics from which it can be reconstructed) which is essential for future meta-analyses. On the positive side, we extract good (evaluation) practices on which our recommendations for researchers and practitioners, as well as policymakers, are built. Conclusions: SR screening evaluations should prioritize lost evidence/recall alongside chance-anchored and cost-sensitive Weighted MCC (WMCC) metric, report complete confusion matrices, treat unclassifiable outputs as referred-back positives for assessment, adopt leakage-aware designs with non-LLM baselines and open artifacts, and ground conclusions in cost-benefit analysis where FNs carry higher penalties than FPs.

</details>


### [12] [Human-Centred Requirements Engineering for Critical Systems: Insights from Disaster Early Warning Applications](https://arxiv.org/abs/2511.12856)
*Anuradha Madugalla,Jixuan Dong,Kai Lyne Loi,Matthew Crossman,John Grundy*

Main category: cs.SE

TL;DR: 本文提出了一种以人为中心的软件需求工程流程，将社会责任融入关键系统开发，并通过早期预警系统原型验证了人类中心化需求对提升系统可用性和可访问性的重要性。


<details>
  <summary>Details</summary>
Motivation: 传统关键系统需求工程过于关注技术保证，忽视了系统运行的人类和社会背景。本文认为考虑人类中心化方面是系统可靠性的重要维度，需要将社会责任整合到关键系统开发中。

Method: 通过文献综述识别为脆弱社区设计软件的指导原则，转化为62个功能和非功能需求，设计自适应早期预警系统原型，并通过6次访谈和8次认知走查评估需求的相关性和适用性。

Result: 研究发现，早期处理人类中心化需求能够增强系统对所有用户的可用性和可访问性。原型验证表明这些需求具有实际相关性和应用价值。

Conclusion: 人类中心化不应被视为伦理附加项，而是安全和公平关键系统的定义性质量特征，应作为系统开发的核心要素。

Abstract: Critical systems, such as those used in healthcare, defence, and disaster management, demand rigorous requirements engineering to ensure safety and reliability. Yet, much of this rigour has traditionally focused on technical assurance, often overlooking the human and social contexts in which these systems operate. This paper argues that considering human-centric aspects is an essential dimension of dependability, and presents a human-centred RE process designed to integrate social responsibility into critical system development. Drawing from a literature review, we identified a set of guidelines for designing software for vulnerable communities and translated these into sixty-two functional and non-functional requirements. These requirements were operationalised through the design of an adaptive early warning system prototype, which was subsequently evaluated through six interviews and eight cognitive walkthroughs to validate their relevance and applicability. The findings demonstrate that human-centric requirements, when addressed early, enhance the usability and accessibility of systems for all users. The paper concludes by positioning human-centricity not as an ethical add-on but as a defining quality of safe and equitable critical systems.

</details>


### [13] [Agent READMEs: An Empirical Study of Context Files for Agentic Coding](https://arxiv.org/abs/2511.12884)
*Worawalan Chatlatanagulchai,Hao Li,Yutaro Kashiwa,Brittany Reid,Kundjanasith Thonglek,Pattara Leelaprute,Arnon Rungsawang,Bundit Manaskasemsak,Bram Adams,Ahmed E. Hassan,Hajimu Iida*

Main category: cs.SE

TL;DR: 对2,303个智能体上下文文件的大规模实证研究发现，这些文件是复杂、难以阅读的配置代码，主要关注功能需求而忽视安全性和性能等非功能需求。


<details>
  <summary>Details</summary>
Motivation: 智能体编码工具使用自然语言目标输入并自动生成代码，其核心是提供项目级指令的智能体上下文文件。本研究旨在首次大规模分析这些文件的结构、维护和内容特征。

Method: 对来自1,925个代码仓库的2,303个智能体上下文文件进行大规模实证研究，分析其结构、维护模式和内容特征，包括16种指令类型的分类分析。

Result: 发现上下文文件是动态演化的配置代码，维护频繁但改动小。内容上，开发者主要关注功能上下文：构建运行命令(62.3%)、实现细节(69.9%)和架构(67.7%)，而安全(14.5%)和性能(14.5%)等非功能需求很少被指定。

Conclusion: 开发者使用上下文文件主要确保智能体功能正常，但缺乏确保代码安全性和性能的防护措施，凸显了改进工具和实践的必要性。

Abstract: Agentic coding tools receive goals written in natural language as input, break them down into specific tasks, and write or execute the actual code with minimal human intervention. Central to this process are agent context files ("READMEs for agents") that provide persistent, project-level instructions. In this paper, we conduct the first large-scale empirical study of 2,303 agent context files from 1,925 repositories to characterize their structure, maintenance, and content. We find that these files are not static documentation but complex, difficult-to-read artifacts that evolve like configuration code, maintained through frequent, small additions. Our content analysis of 16 instruction types shows that developers prioritize functional context, such as build and run commands (62.3%), implementation details (69.9%), and architecture (67.7%). We also identify a significant gap: non-functional requirements like security (14.5%) and performance (14.5%) are rarely specified. These findings indicate that while developers use context files to make agents functional, they provide few guardrails to ensure that agent-written code is secure or performant, highlighting the need for improved tooling and practices.

</details>


### [14] [Diffploit: Facilitating Cross-Version Exploit Migration for Open Source Library Vulnerabilities](https://arxiv.org/abs/2511.12950)
*Zirui Chen,Zhipeng Xue,Jiayuan Zhou,Xing Hu,Xin Xia,Xiaohu Yang*

Main category: cs.SE

TL;DR: Diffploit是一种基于差异驱动的漏洞利用迁移方法，通过上下文模块和迁移模块的迭代反馈循环，有效解决跨版本漏洞利用失败问题。


<details>
  <summary>Details</summary>
Motivation: 现有技术在处理跨版本漏洞利用时，主要关注代码级追踪对齐，但这种方法耗时且无法有效处理环境级故障和复杂的触发条件变化。

Method: Diffploit采用迭代的差异驱动方法，包含上下文模块（动态构建行为差异上下文）和迁移模块（基于LLM的迭代适应），通过平衡差异候选探索和渐进优化来解决复制失败。

Result: 在包含102个Java CVE和689个版本迁移任务的大规模数据集上，Diffploit成功迁移了84.2%的漏洞利用，比TARGET工具高出52.0%，比IDEA工具高出61.6%。

Conclusion: Diffploit不仅能有效迁移漏洞利用，还能识别错误的CVE影响版本范围，并发现未报告的易受攻击版本，展示了其在漏洞分析领域的实用价值。

Abstract: Exploits are commonly used to demonstrate the presence of library vulnerabilities and validate their impact across different versions. However, their direct application to alternative versions often fails due to breaking changes introduced during evolution. These failures stem from both changes in triggering conditions (e.g., API refactorings) and broken dynamic environments (e.g., build or runtime errors), which are challenging to interpret and adapt manually. Existing techniques primarily focus on code-level trace alignment through fuzzing, which is both time-consuming and insufficient for handling environment-level failures. Moreover, they often fall short when dealing with complicated triggering condition changes across versions. To overcome this, we propose Diffploit, an iterative, diff-driven exploit migration method structured around two key modules: the Context Module and the Migration Module. The Context Module dynamically constructs contexts derived from analyzing behavioral discrepancies between the target and reference versions, which capture the failure symptom and its related diff hunks. Leveraging these contexts, the Migration Module guides an LLM-based adaptation through an iterative feedback loop, balancing exploration of diff candidates and gradual refinement to resolve reproduction failures effectively. We evaluate Diffploit on a large-scale dataset containing 102 Java CVEs and 689 version-migration tasks across 79 libraries. Diffploit successfully migrates 84.2% exploits, outperforming the change-aware test repair tool TARGET by 52.0% and the rule-based tool in IDEA by 61.6%. Beyond technical effectiveness, Diffploit identifies 5 CVE reports with incorrect affected version ranges, three of which have been confirmed. It also discovers 111 unreported vulnerable versions in the GitHub Advisory Database.

</details>


### [15] [SmartPoC: Generating Executable and Validated PoCs for Smart Contract Bug Reports](https://arxiv.org/abs/2511.12993)
*Longfei Chen,Ruibin Yan,Taiyu Wong,Yiyang Chen,Chao Zhang*

Main category: cs.SE

TL;DR: SmartPoC是一个自动化框架，能够将文本审计报告转换为可执行的验证测试用例，解决了现有审计工具在噪声输入、幻觉和缺失运行时预言方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 智能合约审计报告存在异构性且缺乏可复现的可执行PoC测试，导致需要昂贵的人工验证。利用LLM将审计报告转化为测试用例面临噪声输入、幻觉和缺失运行时预言三大挑战。

Method: 首先处理输入审计报告以减少噪声，仅提取与漏洞相关的函数作为LLM上下文；利用LLM合成PoC测试用例，采用特殊设计的执行前后修复机制来抑制幻觉并确保编译运行就绪；使用差分验证作为预言来确认PoC测试用例的可利用性。

Result: 在SmartBugs-Vul和FORGE-Vul基准测试上，SmartPoC分别成功为85.61%和86.45%的目标生成可执行的验证Foundry测试用例。应用于最新的Etherscan验证源代码语料库，SmartPoC以每个发现仅0.03美元的成本确认了545个审计发现中的236个真实漏洞。

Conclusion: SmartPoC框架能够高效地将文本审计报告转化为可执行的验证测试用例，显著降低了智能合约漏洞验证的成本和复杂性。

Abstract: Smart contracts are prone to vulnerabilities and are analyzed by experts as well as automated systems, such as static analysis and AI-assisted solutions. However, audit artifacts are heterogeneous and often lack reproducible, executable PoC tests suitable for automated validation, leading to costly, ad hoc manual verification. Large language models (LLMs) can be leveraged to turn audit reports into PoC test cases, but have three major challenges: noisy inputs, hallucinations, and missing runtime oracles. In this paper, we present SmartPoC, an automated framework that converts textual audit reports into executable, validated test cases. First, the input audit report is processed to reduce noise, and only bug-related functions are extracted and fed to LLMs as context. To curb hallucinations and ensure compile-and-run readiness, we leverage LLMs to synthesize PoC test cases with specially-designed pre-/post-execution repair. We further utilize differential verification as oracles to confirm exploitability of the PoC test cases. On the SmartBugs-Vul and FORGE-Vul benchmarks, SmartPoC generates executable, validated Foundry test cases for 85.61% and 86.45% of targets, respectively. Applied to the latest Etherscan verified-source corpus, SmartPoC confirms 236 real bugs out of 545 audit findings at a cost of only $0.03 per finding.

</details>


### [16] [Towards Requirements Engineering for GenAI-Enabled Software: Bridging Responsibility Gaps through Human Oversight Requirements](https://arxiv.org/abs/2511.13069)
*Zhenyu Mao,Jacky Keung,Yicheng Sun,Yifei Wang,Shuo Liu,Jialong Li*

Main category: cs.SE

TL;DR: 本文提出了一种三层分析方法来解决GenAI软件中的责任缺口问题，包括概念化、方法论和工件层，并通过用户研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: GenAI软件的生成性和适应性使得人类监督和责任分配变得复杂，现有需求工程方法在处理责任缺口方面存在概念、方法和工件层面的研究空白。

Method: 采用三层设计方法：概念化层定义责任要素和缺口形成机制；方法论层引入演绎管道识别责任缺口并推导监督需求；工件层用演绎骨干表形式化结果。

Result: 用户研究显示该方法在六个维度上明显优于基线目标导向需求工程方法，有效解决了三个研究空白。

Conclusion: 提出的三层分析方法能够系统性地识别和解决GenAI软件中的责任缺口问题，为人类监督需求提供了有效的工程框架。

Abstract: Context: Responsibility gaps, long-recognized challenges in socio-technical systems where accountability becomes diffuse or ambiguous, have become increasingly pronounced in GenAI-enabled software. The generative and adaptive nature complicates how human oversight and responsibility are specified, delegated, and traced. Existing requirements engineering (RE) approaches remain limited in addressing these phenomena, revealing conceptual, methodological, and artifact-level research gaps.. Objective: This study aims to analyze these research gaps in the context of GenAI-enabled software systems. It seeks to establish a coherent perspective for a systematic analysis of responsibility gaps from a human oversight requirements standpoint, encompassing how these responsibility gaps should be conceptualized, identified, and represented throughout the RE process. Methods: The proposed design methodology is structured across three analytical layers. At the conceptualization layer, it establishes a conceptual framing that defines the key elements of responsibility across the human and system dimensions and explains how potential responsibility gaps emerge from their interactions. At the methodological layer, it introduces a deductive pipeline for identifying responsibility gaps by analyzing interactions between these dimensions and deriving corresponding oversight requirements within established RE frameworks. At the artifact layer, it formalizes the results in a Deductive Backbone Table, a reusable representation that traces the reasoning path from responsibility gaps identification to human oversight requirements derivation. Results: A user study compared the proposed methodology with a baseline goal-oriented RE across two scenarios. Evaluation across six dimensions indicated clear improvements of the proposed methodology, confirming its effectiveness in addressing three research gaps.

</details>


### [17] [Examining the Usage of Generative AI Models in Student Learning Activities for Software Programming](https://arxiv.org/abs/2511.13271)
*Rufeng Chen,Shuaishuai Jiang,Jiyun Shen,AJung Moon,Lili Wei*

Main category: cs.SE

TL;DR: 研究比较了GenAI辅助与传统在线资源在编程学习中的知识获取效果，发现GenAI能显著提升任务完成度但对知识增益不一致，初学者容易过度依赖而缺乏知识收获。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注GenAI完成任务的能力和对学生表现的影响，但忽视了其对知识获取的影响，需要比较GenAI与传统资源在支持知识增益方面的差异。

Method: 对24名不同编程经验水平（初学者、中级）的本科生进行对照实验，分析他们在解决编程任务时与ChatGPT的交互行为、任务表现和概念理解。

Result: 使用GenAI生成完整解决方案显著提高了任务完成度（尤其是初学者），但知识增益不一致；初学者倾向于过度依赖GenAI完成任务而缺乏知识收获，中级学生采用更有选择性的方法；过度依赖和极少使用都会导致较弱的知识增益。

Conclusion: 呼吁学生和教育者将GenAI作为学习工具而非问题解决工具，强调在编程教育中整合GenAI时需要指导以促进更深层次的理解。

Abstract: The rise of Generative AI (GenAI) tools like ChatGPT has created new opportunities and challenges for computing education. Existing research has primarily focused on GenAI's ability to complete educational tasks and its impact on student performance, often overlooking its effects on knowledge gains. In this study, we investigate how GenAI assistance compares to conventional online resources in supporting knowledge gains across different proficiency levels. We conducted a controlled user experiment with 24 undergraduate students of two different levels of programming experience (beginner, intermediate) to examine how students interact with ChatGPT while solving programming tasks. We analyzed task performance, conceptual understanding, and interaction behaviors. Our findings reveal that generating complete solutions with GenAI significantly improves task performance, especially for beginners, but does not consistently result in knowledge gains. Importantly, usage strategies differ by experience: beginners tend to rely heavily on GenAI toward task completion often without knowledge gain in the process, while intermediates adopt more selective approaches. We find that both over-reliance and minimal use result in weaker knowledge gains overall. Based on our results, we call on students and educators to adopt GenAI as a learning rather than a problem solving tool. Our study highlights the urgent need for guidance when integrating GenAI into programming education to foster deeper understanding.

</details>


### [18] [SAINT: Service-level Integration Test Generation with Program Analysis and LLM-based Agents](https://arxiv.org/abs/2511.13305)
*Rangeet Pan,Raju Pavuluri,Ruikai Huang,Rahul Krishna,Tyler Stennett,Alessandro Orso,Saurabh SInha*

Main category: cs.SE

TL;DR: SAINT是一个用于企业Java应用服务级测试的白盒测试方法，结合静态分析、大语言模型和基于LLM的代理来自动生成端点和基于场景的测试。


<details>
  <summary>Details</summary>
Motivation: 现有服务级测试工具主要依赖模糊测试和OpenAPI规范，但这些规范在企业代码库中通常不可用，且现有工具在生成有效功能测试方面能力有限。

Method: SAINT构建两个关键模型：端点模型（捕获服务端点的语法和语义信息）和操作依赖图（捕获端点间顺序约束）。然后使用基于LLM的代理生成测试，包括端点聚焦测试（最大化代码和数据库交互覆盖率）和基于场景的测试（从代码中提取应用用例并通过代理循环的规划、行动和反思阶段精炼为可执行测试）。

Result: 在8个Java应用（包括一个专有企业应用）上的评估显示，SAINT在覆盖率、故障检测和场景生成方面表现有效。开发者调查对SAINT生成的基于场景的测试给予了强烈认可。

Conclusion: 将静态分析与基于代理的LLM工作流相结合，能够实现更有效、功能性和开发者对齐的服务级测试生成。

Abstract: Enterprise applications are typically tested at multiple levels, with service-level testing playing an important role in validating application functionality. Existing service-level testing tools, especially for RESTful APIs, often employ fuzzing and/or depend on OpenAPI specifications which are not readily available in real-world enterprise codebases. Moreover, these tools are limited in their ability to generate functional tests that effectively exercise meaningful scenarios. In this work, we present SAINT, a novel white-box testing approach for service-level testing of enterprise Java applications. SAINT combines static analysis, large language models (LLMs), and LLM-based agents to automatically generate endpoint and scenario-based tests. The approach builds two key models: an endpoint model, capturing syntactic and semantic information about service endpoints, and an operation dependency graph, capturing inter-endpoint ordering constraints. SAINT then employs LLM-based agents to generate tests. Endpoint-focused tests aim to maximize code and database interaction coverage. Scenario-based tests are synthesized by extracting application use cases from code and refining them into executable tests via planning, action, and reflection phases of the agentic loop. We evaluated SAINT on eight Java applications, including a proprietary enterprise application. Our results illustrate the effectiveness of SAINT in coverage, fault detection, and scenario generation. Moreover, a developer survey provides strong endorsement of the scenario-based tests generated by SAINT. Overall, our work shows that combining static analysis with agentic LLM workflows enables more effective, functional, and developer-aligned service-level test generation.

</details>


### [19] [LinkXplore: A Framework for Affordable High-Quality Blockchain Data](https://arxiv.org/abs/2511.13318)
*Peihao Li*

Main category: cs.SE

TL;DR: LinkXplore是一个开源的区块链数据收集和管理框架，通过直接分析RPC查询或流数据来绕过昂贵的数据提供商，为预算有限的研究和开发提供低成本、高质量的区块链数据。


<details>
  <summary>Details</summary>
Motivation: 大规模区块链数据收集成本过高，许多RPC提供商只提供高价API，这严重阻碍了学术研究和产品开发。同时缺乏能够灵活集成新模块的系统化框架来分析链上数据。

Method: 通过简单的API和后端处理逻辑，允许任何类型的链数据集成到框架中，直接分析来自RPC查询或流的原始数据。

Result: 提供了一个实用的替代方案，使研究人员和开发者能够以较低成本获取高质量区块链数据。

Conclusion: LinkXplore是首个用于收集和管理链上数据的开源框架，有效解决了区块链数据收集成本高和缺乏系统化分析框架的问题。

Abstract: Blockchain technologies are rapidly transforming both academia and industry. However, large-scale blockchain data collection remains prohibitively expensive, as many RPC providers only offer enhanced APIs with high pricing tiers that are unsuitable for budget-constrained research or industrial-scale applications, which has significantly slowed down academic studies and product development. Moreover, there is a clear lack of a systematic framework that allows flexible integration of new modules for analyzing on-chain data.
  To address these challenges, we introduce LinkXplore, the first open framework for collecting and managing on-chain data. LinkXplore enables users to bypass costly blockchain data providers by directly analyzing raw data from RPC queries or streams, thereby offering high-quality blockchain data at a fraction of the cost. Through a simple API and backend processing logic, any type of chain data can be integrated into the framework. This makes it a practical alternative for both researchers and developers with limited budgets. Code and dataset used in this project are publicly available at https://github.com/Linkis-Project/LinkXplore

</details>


### [20] [An LLM-based Quantitative Framework for Evaluating High-Stealthy Backdoor Risks in OSS Supply Chains](https://arxiv.org/abs/2511.13341)
*Zihe Yan,Kai Luo,Haoyu Yang,Yang Yu,Zhuosheng Zhang,Guancheng Li*

Main category: cs.SE

TL;DR: 提出了一个细粒度的开源软件后门风险评估框架，通过建模隐蔽后门攻击过程并定义针对性指标，结合LLM进行语义分析来评估代码仓库安全性。


<details>
  <summary>Details</summary>
Motivation: 开源软件供应链在现代软件开发中广泛使用，但依赖维护不足和社区审计不充分导致安全风险，特别是在XZ-Util事件等高度隐蔽后门攻击下。

Method: 从攻击者视角建模隐蔽后门攻击过程，为每个攻击阶段定义针对性指标，使用大语言模型对代码仓库进行语义评估，克服静态分析在评估仓库维护活动可靠性方面的局限性。

Result: 在Debian生态系统的66个高优先级软件包上评估，实验结果表明当前开源软件供应链面临多种安全风险。

Conclusion: 该框架能有效评估开源软件的后门风险，揭示了开源软件供应链存在的安全隐患，为提升软件供应链安全提供了解决方案。

Abstract: In modern software development workflows, the open-source software supply chain contributes significantly to efficient and convenient engineering practices. With increasing system complexity, using open-source software as third-party dependencies has become a common practice. However, the lack of maintenance for underlying dependencies and insufficient community auditing create challenges in ensuring source code security and the legitimacy of repository maintainers, especially under high-stealthy backdoor attacks exemplified by the XZ-Util incident. To address these problems, we propose a fine-grained project evaluation framework for backdoor risk assessment in open-source software. The framework models stealthy backdoor attacks from the viewpoint of the attacker and defines targeted metrics for each attack stage. In addition, to overcome the limitations of static analysis in assessing the reliability of repository maintenance activities such as irregular committer privilege escalation and limited participation in reviews, the framework uses large language models (LLMs) to conduct semantic evaluation of code repositories without relying on manually crafted patterns. The framework is evaluated on sixty six high-priority packages in the Debian ecosystem. The experimental results indicate that the current open-source software supply chain is exposed to various security risks.

</details>


### [21] [FLOWER: Flow-Oriented Entity-Relationship Tool](https://arxiv.org/abs/2511.13357)
*Dmitry Moskalev*

Main category: cs.SE

TL;DR: FLOWER是一个面向流程的实体关系工具，首个端到端解决方案，能够自动处理、创建和可视化SQL数据库中的显性和隐性依赖关系，通过动态采样和鲁棒数据分析技术改进实体关系模型和数据叙事。


<details>
  <summary>Details</summary>
Motivation: 由于数据库存储大量合成和有机数据，正确服务所有对象数量是重要任务，但构建实体关系模型的决策与人为因素相关，需要消除处理、创建和可视化依赖关系的常规和资源密集型问题。

Method: FLOWER自动检测内置约束，使用动态采样和鲁棒数据分析技术创建正确必要的约束，支持SQL或自然语言查询，适用于改进实体关系模型和数据叙事。

Result: 在STATS基准测试中，FLOWER在分布表示上比水库采样快2.4倍，约束学习快2.6倍，加速2.15倍；在数据叙事方面，准确率提高1.19倍，上下文减少1.86倍，支持23种语言，兼容CPU和GPU。

Conclusion: FLOWER能够更好地处理真实世界数据，确保质量、可扩展性和不同用例的适用性。

Abstract: Exploring relationships across data sources is a crucial optimization for entities recognition. Since databases can store big amount of information with synthetic and organic data, serving all quantity of objects correctly is an important task to deal with. However, the decision of how to construct entity relationship model is associated with human factor. In this paper, we present flow-oriented entity-relationship tool. This is first and unique end-to-end solution that eliminates routine and resource-intensive problems of processing, creating and visualizing both of explicit and implicit dependencies for prominent SQL dialects on-the-fly. Once launched, FLOWER automatically detects built-in constraints and starting to create own correct and necessary one using dynamic sampling and robust data analysis techniques. This approach applies to improve entity-relationship model and data storytelling to better understand the foundation of data and get unseen insights from DB sources using SQL or natural language. Evaluated on state-of-the-art STATS benchmark, experiments show that FLOWER is superior to reservoir sampling by 2.4x for distribution representation and 2.6x for constraint learning with 2.15x acceleration. For data storytelling, our tool archives 1.19x for accuracy enhance with 1.86x context decrease compare to LLM. Presented tool is also support 23 languages and compatible with both of CPU and GPU. Those results show that FLOWER can manage with real-world data a way better to ensure with quality, scalability and applicability for different use-cases.

</details>


### [22] [BIOMERO 2.0: end-to-end FAIR infrastructure for bioimaging data import, analysis, and provenance](https://arxiv.org/abs/2511.13611)
*Torec T. Luik,Joost de Folter,Rodrigo Rosas-Bertolini,Eric A. J. Reits,Ron A. Hoebe,Przemek M. Krawczyk*

Main category: cs.SE

TL;DR: BIOMERO 2.0将OMERO升级为FAIR合规、支持溯源的可视化生物成像平台，通过容器化组件实现数据导入、预处理、分析和流程监控的集成。


<details>
  <summary>Details</summary>
Motivation: 解决生物成像数据分析中数据导入、预处理、分析和共享之间的脱节问题，确保数据从采集到分析的完整溯源，提升OMERO平台的FAIR合规性。

Method: 采用OMERO.web插件和容器化组件，包含导入子系统（支持原位导入和元数据丰富）和分析子系统（通过BIOMERO Python库协调高性能计算分析），所有操作记录参数、版本和结果。

Result: 实现了实时溯源追踪，通过集成仪表板可访问所有导入和分析操作的完整记录，确保工作流程的可追溯性和可重用性。

Conclusion: BIOMERO 2.0将OMERO置于生物成像分析流程的核心位置，通过双重方法（导入器确保从采集到导入的溯源，分析器记录下游处理溯源）弥合了数据导入、分析和共享之间的差距。

Abstract: We present BIOMERO 2.0, a major evolution of the BIOMERO framework that transforms OMERO into a FAIR-compliant (findable, accessible, interoperable, and reusable), provenance-aware bioimaging platform. BIOMERO 2.0 integrates data import, preprocessing, analysis, and workflow monitoring through an OMERO.web plugin and containerized components. The importer subsystem facilitates in-place import using containerized preprocessing and metadata enrichment via forms, while the analyzer subsystem coordinates and tracks containerized analyses on high-performance computing systems via the BIOMERO Python library. All imports and analyses are recorded with parameters, versions, and results, ensuring real-time provenance accessible through integrated dashboards. This dual approach places OMERO at the heart of the bioimaging analysis process: the importer ensures provenance from image acquisition through preprocessing and import into OMERO, while the analyzer records it for downstream processing. These integrated layers enhance OMEROs FAIRification, supporting traceable, reusable workflows for image analysis that bridge the gap between data import, analysis, and sharing.

</details>


### [23] [Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?](https://arxiv.org/abs/2511.13646)
*Chunqiu Steven Xia,Zhe Wang,Yan Yang,Yuxiang Wei,Lingming Zhang*

Main category: cs.SE

TL;DR: Live-SWE-agent是首个能够在运行时自主持续演化的软件代理，从基础bash工具开始，在解决真实软件问题时自主进化其脚手架实现，在SWE-bench Verified基准上达到75.4%的解决率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM软件代理需要专门设计且可能不是最优的，而自改进代理需要昂贵的离线训练且泛化能力有限。需要一种能够在运行时自主进化的软件代理。

Method: Live-SWE-agent从仅具有bash工具的基础代理脚手架开始，在解决真实软件问题时自主进化其脚手架实现，实现实时自改进。

Result: 在SWE-bench Verified基准上达到75.4%的解决率，优于所有现有开源软件代理，接近最佳专有解决方案性能；在SWE-Bench Pro基准上达到45.8%的最佳已知解决率。

Conclusion: Live-SWE-agent证明了软件代理能够在运行时自主持续进化，无需昂贵的离线训练即可实现卓越性能，为软件工程自动化提供了新范式。

Abstract: Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-Gödel Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that Live-SWE-agent can achieve an impressive solve rate of 75.4% without test-time scaling, outperforming all existing open-source software agents and approaching the performance of the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.

</details>


### [24] [What's in a Software Engineering Job Posting?](https://arxiv.org/abs/2511.13656)
*Marvin Wyrich,Lloyd Montgomery*

Main category: cs.SE

TL;DR: 分析100个软件工程职位招聘信息，揭示雇主对软件工程师的非技术能力期望，包括与公司目标一致、文化契合、个人成长和人际交往能力。


<details>
  <summary>Details</summary>
Motivation: 探索软件工程师理想候选人的演变，了解雇主在技术能力之外对软件工程师的社会技术和组织期望。

Method: 对100个软件工程职位招聘信息进行主题分析。

Result: 雇主期望候选人：与公司目标一致、适应公司文化、追求个人和职业成长、在人际互动中表现出色。

Conclusion: 软件工程师的角色正在超越纯技术技能，雇主越来越重视社会技术和组织能力，这对研究者、教育者、从业者和招聘者都有重要启示。

Abstract: A well-rounded software engineer is often defined by technical prowess and the ability to deliver on complex projects. However, the narrative around the ideal Software Engineering (SE) candidate is evolving, suggesting that there is more to the story. This article explores the non-technical aspects emphasized in SE job postings, revealing the sociotechnical and organizational expectations of employers. Our Thematic Analysis of 100 job postings shows that employers seek candidates who align with their sense of purpose, fit within company culture, pursue personal and career growth, and excel in interpersonal interactions. This study contributes to ongoing discussions in the SE community about the evolving role and workplace context of software engineers beyond technical skills. By highlighting these expectations, we provide relevant insights for researchers, educators, practitioners, and recruiters. Additionally, our analysis offers a valuable snapshot of SE job postings in 2023, providing a scientific record of prevailing trends and expectations.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [25] [A Logspace Constructive Proof of L=SL](https://arxiv.org/abs/2511.12011)
*Sam Buss,Anant Dhayal,Valentine Kabanets,Antonina Kolokolova,Sasank Mouli*

Main category: cs.LO

TL;DR: 在VL理论中形式化Reingold定理的证明，证明SL=L，从而得出VL=VSL，解决了Kolokolova提出的开放性问题。


<details>
  <summary>Details</summary>
Motivation: 解决Kolokolova提出的关于VL和VSL理论等价性的开放性问题，通过形式化Reingold定理来建立有界算术理论中的对应关系。

Method: 使用Rozenman-Vadhan对Reingold定理的替代证明，避免涉及特征值和特征向量的推理，转而利用Buss等人的组合图展开结果在VL中进行推理。

Result: 成功在VL理论中形式化证明了Reingold定理，得出VL=VSL的结论。

Conclusion: 该工作确认了VL和VSL理论的等价性，为有界算术理论提供了重要进展，并展示了组合方法在形式化复杂定理证明中的有效性。

Abstract: We formalize the proof of Reingold's Theorem that SL=L [Rei05] in the theory of bounded arithmetic VL, which corresponds to ``logspace reasoning''. As a consequence, we get that VL=VSL, where VSL is the theory of bounded arithmetic for ``symmetric-logspace reasoning''. This resolves in the affirmative an old open question from Kolokolova [Kol05] (see also Cook-Nguyen [NC10]).
  Our proof relies on the Rozenman-Vadhan alternative proof of Reingold's Theorem ([RV05]). To formalize this proof in VL, we need to avoid reasoning about eigenvalues and eigenvectors (common in both original proofs of SL=L). We achieve this by using some results from Buss-Kabanets-Kolokolova-Koucký [Bus+20] that allow VL to reason about graph expansion in combinatorial terms.

</details>


### [26] [Proceedings Seventh International Workshop on Formal Methods for Autonomous Systems](https://arxiv.org/abs/2511.13245)
*Matt Luckcuck,Maike Schwammberger,Mengwei Xu*

Main category: cs.LO

TL;DR: 这是第七届自主系统形式化方法国际研讨会(FMAS 2025)的论文集，收录了来自12个国家研究机构的16篇论文，展示了该领域的研究进展和国际合作。


<details>
  <summary>Details</summary>
Motivation: 汇集使用形式化方法解决自主系统独特挑战的研究人员，促进学术交流和社区建设。

Method: 通过国际研讨会形式，邀请全球研究人员提交论文并进行学术讨论，与iFM'25会议联合举办。

Result: 收到来自加拿大、中国、法国、德国等12个国家的16篇投稿，既有老作者也有新作者参与。

Conclusion: FMAS社区在过去7年建立了良好的学术网络，展现出持续发展的潜力，吸引了来自全球的新老研究者参与。

Abstract: This EPTCS volume contains the papers from the Seventh International Workshop on Formal Methods for Autonomous Systems (FMAS 2025), which was held between the 17th and 19th of November 2025. The goal of the FMAS workshop series is to bring together leading researchers who are using formal methods to tackle the unique challenges that autonomous systems present, so that they can publish and discuss their work with a growing community of researchers. FMAS 2025 was co-located with the 20th International Conference on integrated Formal Methods (iFM'25), hosted by Inria Paris, France at the Inria Paris Center. 
  In total, FMAS 2025 received 16 submissions from researchers at institutions in: Canada, China, France, Germany, Ireland, Italy, Japan, the Netherlands, Portugal, Sweden, the United States of America, and the United Kingdom. Though we received fewer submissions than last year, we are encouraged to see the submissions being sent from a wide range of countries. Submissions come from both past and new FMAS authors, which shows us that the existing community appreciates the network that FMAS has built over the past 7 years, while new authors also show the FMAS community's great potential of growth.

</details>


### [27] [Multi-Objective Statistical Model Checking using Lightweight Strategy Sampling (extended version)](https://arxiv.org/abs/2511.13460)
*Pedro R. D'Argenio,Arnd Hartmanns,Patrick Wienhöft,Mark van Wijk*

Main category: cs.LO

TL;DR: 本文提出了首个用于多目标Pareto查询的统计模型检查方法，通过轻量级策略采样优化模型中的非确定性选择，收敛到具有统计保证的置信带，并在有限时间内获得真实Pareto前沿的近似。


<details>
  <summary>Details</summary>
Motivation: 统计模型检查目前只能一次评估一个属性值，但许多实际问题需要在多个可能冲突的优化目标之间找到最优权衡的Pareto前沿。

Method: 使用轻量级策略采样优化非确定性选择，提出增量方案收敛到统计置信带，并开发三种启发式方法在有限采样预算下获得近似Pareto前沿。

Result: 在Modest Toolset的'modes'模拟器中实现新技术，并在定量验证基准上实验证明了其有效性。

Conclusion: 该方法能够处理传统技术无法应对的模型规模和类型，为多目标优化问题提供了统计保证的解决方案。

Abstract: Statistical model checking delivers quantitative verification results with statistical guarantees by applying Monte Carlo simulation to formal models. It scales to model sizes and model types that are out of reach for exhaustive, analytical techniques. So far, it has been used to evaluate one property value at a time only. Many practical problems, however, require finding the Pareto front of optimal tradeoffs between multiple possibly conflicting optimisation objectives. In this paper, we present the first statistical model checking approach for such multi-objective Pareto queries, using lightweight strategy sampling to optimise over the model's nondeterministic choices. We first introduce an incremental scheme that almost surely converges to a statistically sound confidence band bounding the true Pareto front from both sides in the long run. To obtain a close underapproximation of the true front in finite time, we then propose three heuristic approaches that try to make the best of an a-priori fixed sampling budget. We implement our new techniques in the Modest Toolset's 'modes' simulator, and experimentally show their effectiveness on quantitative verification benchmarks.

</details>


### [28] [Subgraph Isomorphism: Prolog vs. Conventional](https://arxiv.org/abs/2511.13600)
*Claire Y. Yin,Peter M. Kogge*

Main category: cs.LO

TL;DR: 本文比较了使用Prolog逻辑编程与传统编程方法解决子图同构问题的复杂度差异，展示了逻辑编程在处理复杂图模式问题上的效率优势。


<details>
  <summary>Details</summary>
Motivation: 研究逻辑编程与传统编程在解决复杂图模式问题时的复杂度差异，特别是针对子图同构问题，旨在理解两种编程范式的效率特性。

Method: 将特定的复杂图模式转换为Prolog逻辑语句，实现子图同构算法，并与多种传统实现（包括并行实现）进行对比分析。

Result: 随着图规模的增大，逻辑编程方法展现出良好的效率特性，分析表明逻辑范式是解决复杂图问题的有效方式。

Conclusion: 使用逻辑编程范式是解决复杂图问题的高效方法，特别是在处理子图同构等图模式匹配问题时具有显著优势。

Abstract: Subgraph Isomorphism uses a small graph as a pattern to identify within a larger graph a set of vertices that have matching edges. This paper addresses a logic program written in Prolog for a specific relatively complex graph pattern for which multiple conventional implementations (including parallel) exist. The goal is to understand the complexity differences between programming logically and programming conventionally. Discussion includes the process of converting the graph pattern into logic statements in Prolog, and the resulting characteristics as the size of the graph increased. The analysis shows that using a logic paradigm is an efficient way to attack complex graph problems.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [29] [Modular GPU Programming with Typed Perspectives](https://arxiv.org/abs/2511.11939)
*Manya Bansal,Daniel Sainati,Joseph W. Cutler,Saman Amarasinghe,Jonathan Ragan-Kelley*

Main category: cs.PL

TL;DR: Prism是一种新的GPU编程语言，通过类型化视角（typed perspectives）在类型层面体现线程控制粒度，解决了GPU编程中个体线程控制与集体操作之间的模块化矛盾。


<details>
  <summary>Details</summary>
Motivation: 现代GPU编程需要在控制单个线程行为的同时跟踪多个线程执行集体操作（如Tensor Core指令）的收敛性，这种思维模式的冲突使得模块化编程容易出错。

Method: 引入类型化视角概念，在类型层面体现程序员控制线程行为的粒度；设计Prism语言并实现编译器；建立核心演算Bundl的理论基础。

Result: 在Prism中实现了最先进的GPU内核，证明该语言能为程序员提供编写模块化代码所需的安全保证，同时不牺牲性能。

Conclusion: Prism语言通过类型化视角恢复了GPU编程的模块性，同时保持了程序员对集体操作的低级控制能力，实现了安全性与高性能的平衡。

Abstract: To achieve peak performance on modern GPUs, one must balance two frames of mind: issuing instructions to individual threads to control their behavior, while simultaneously tracking the convergence of many threads acting in concert to perform collective operations like Tensor Core instructions. The tension between these two mindsets makes modular programming error prone. Functions that encapsulate collective operations, despite being called per-thread, must be executed cooperatively by groups of threads.
  In this work, we introduce Prism, a new GPU language that restores modularity while still giving programmers the low-level control over collective operations necessary for high performance. Our core idea is typed perspectives, which materialize, at the type level, the granularity at which the programmer is controlling the behavior of threads. We describe the design of Prism, implement a compiler for it, and lay its theoretical foundations in a core calculus called Bundl. We implement state-of-the-art GPU kernels in Prism and find that it offers programmers the safety guarantees needed to confidently write modular code without sacrificing performance.

</details>


### [30] [The Search for Constrained Random Generators](https://arxiv.org/abs/2511.12253)
*Harrison Goldstein,Hila Peleg,Cassia Torczon,Daniel Sainati,Leonidas Lampropoulos,Benjamin C. Pierce*

Main category: cs.PL

TL;DR: 提出基于演绎程序综合的新方法来解决基于属性测试中的约束随机生成问题，通过生成器语义的合成规则自动合成正确生成器。


<details>
  <summary>Details</summary>
Motivation: 基于属性测试面临的核心挑战是约束随机生成问题：在满足给定谓词的程序值集合中进行随机采样，而满足条件的值通常稀疏分布，需要高效解决方案。

Method: 基于生成器的指称语义提出合成规则，将递归谓词重写为catamorphisms并与适当的anamorphisms匹配，使用标准证明搜索策略实现合成算法。

Result: 开发了Palamedes实现，作为Lean定理证明器的可扩展库，利用现有证明自动化技术降低实现负担。

Conclusion: 该方法为递归函数合成提供了理论上更简单但表达能力强的解决方案，能够有效处理基于属性测试中的约束随机生成问题。

Abstract: Among the biggest challenges in property-based testing (PBT) is the constrained random generation problem: given a predicate on program values, randomly sample from the set of all values satisfying that predicate, and only those values. Efficient solutions to this problem are critical, since the executable specifications used by PBT often have preconditions that input values must satisfy in order to be valid test cases, and satisfying values are often sparsely distributed.
  We propose a novel approach to this problem using ideas from deductive program synthesis. We present a set of synthesis rules, based on a denotational semantics of generators, that give rise to an automatic procedure for synthesizing correct generators. Our system handles recursive predicates by rewriting them as catamorphisms and then matching with appropriate anamorphisms; this is theoretically simpler than other approaches to synthesis for recursive functions, yet still extremely expressive.
  Our implementation, Palamedes, is an extensible library for the Lean theorem prover. The synthesis algorithm itself is built on standard proof-search tactics, reducing implementation burden and allowing the algorithm to benefit from further advances in Lean proof automation.

</details>


### [31] [Equivalence Checking of ML GPU Kernels](https://arxiv.org/abs/2511.12638)
*Kshitij Dubey,Benjamin Driscoll,Anjiang Wei,Neeraj Kayal,Rahul Sharma,Alex Aiken*

Main category: cs.PL

TL;DR: 提出了第一个GPU内核等价检查器VOLTA，用于形式化验证手工优化、LLM生成和编译器优化的机器学习内核的正确性，证明该检查器对于特定类别的GPU内核是完备的。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习和大型语言模型的快速发展，公司花费巨资执行GPU内核，这些内核成为优化的主要目标。现有基于LLM生成GPU内核的方法缺乏形式化保证。

Method: 开发了VOLTA等价检查器，能够验证卷积、矩阵乘法、注意力机制等ML计算，证明该检查器对于特定类别的GPU内核是完备的。

Result: 实现了首个GPU内核等价检查器，能够形式化验证各种优化后的机器学习内核的正确性。

Conclusion: VOLTA为GPU内核优化提供了可靠的形式化验证工具，确保手工优化、LLM生成和编译器优化的内核的正确性。

Abstract: With the rapid progress of deep learning and large language models (LLMs), companies now spend enormous sums executing GPU kernels. These kernels have, therefore, become prime targets for aggressive optimization. Recent efforts increasingly leverage LLMs to generate GPU kernels, but make no formal guarantees about the generated kernels. We present the first equivalence checker for GPU kernels and use it to formally verify the correctness of machine learning (ML) kernels optimized by hand, by LLMs, and by compilers. We show that our equivalence checker is sound and, for a well-defined class of GPU kernels which includes the programs of interest, complete. Our implementation, VOLTA, can verify ML computations such as convolutions, matrix multiplications, and various attention mechanisms.

</details>


### [32] [Cost-Driven Synthesis of Sound Abstract Interpreters](https://arxiv.org/abs/2511.13663)
*Qiuhan Gu,Avaljot Singh,Gagandeep Singh*

Main category: cs.PL

TL;DR: 利用现代LLM合成神经网络验证中跨多个抽象域的全局可靠抽象解释器，通过约束优化和数学基础的成本函数来确保可靠性。


<details>
  <summary>Details</summary>
Motivation: 构建提供全局可靠性保证的抽象解释器仍然是抽象解释中的主要障碍，研究是否可以利用现代LLM来减轻这一负担。

Method: 将合成建模为约束优化问题，引入基于数学的成本函数来衡量可靠性，开发统一框架结合LLM生成、语法语义验证和成本引导反馈机制。

Result: 实证结果表明，该框架不仅匹配手工构建的转换器质量，更重要的是发现了复杂非线性算子的可靠、高精度转换器，这在现有文献中尚不存在。

Conclusion: LLM可以成功合成可靠的抽象解释器，特别是在处理复杂非线性算子方面展现出潜力，为自动化抽象解释器开发提供了新途径。

Abstract: Constructing abstract interpreters that provide global soundness guarantees remains a major obstacle in abstract interpretation. We investigate whether modern LLMs can reduce this burden by leveraging them to synthesize sound, non-trivial abstract interpreters across multiple abstract domains in the setting of neural network verification. We formulate synthesis as a constrained optimization problem and introduce a novel mathematically grounded cost function for measuring unsoundness under strict syntactic and semantic constraints. Based on this formulation, we develop a unified framework that unifies LLM-based generation with syntactic and semantic validation and a quantitative cost-guided feedback mechanism. Empirical results demonstrate that our framework not only matches the quality of handcrafted transformers, but more importantly, discovers sound, high-precision transformers for complex nonlinear operators that are absent from existing literature.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [33] [Positive Characteristic Sets for Relational Pattern Languages](https://arxiv.org/abs/2511.12039)
*S. Mahmoud Mousawi,Sandra Zilles*

Main category: cs.FL

TL;DR: 本文引入了正特征集的概念，用于仅从正例中学习关系模式语言类。


<details>
  <summary>Details</summary>
Motivation: 在形式语言学习中，传统特征集需要正负例，但在某些应用中只能获得正例。研究仅从正例学习的特征集对字符串处理应用很重要。

Method: 提出了正特征集的定义，即仅包含正例的特征集，并研究了其在关系模式语言类中的应用。

Result: 建立了正特征集的理论框架，为仅从正例学习关系模式语言提供了理论基础。

Conclusion: 正特征集是仅从正例学习的重要工具，对关系模式语言类的学习具有实际应用价值。

Abstract: In the context of learning formal languages, data about an unknown target language L is given in terms of a set of (word,label) pairs, where a binary label indicates whether or not the given word belongs to L. A (polynomial-size) characteristic set for L, with respect to a reference class L of languages, is a set of such pairs that satisfies certain conditions allowing a learning algorithm to (efficiently) identify L within L. In this paper, we introduce the notion of positive characteristic set, referring to characteristic sets of only positive examples. These are of importance in the context of learning from positive examples only. We study this notion for classes of relational pattern languages, which are of relevance to various applications in string processing.

</details>


### [34] [Formal Foundations for Controlled Stochastic Activity Networks](https://arxiv.org/abs/2511.12974)
*Ali Movaghar*

Main category: cs.FL

TL;DR: 提出了受控随机活动网络(Controlled SANs)，这是经典随机活动网络的扩展，集成了显式控制动作，为分布式实时系统建模提供了统一的语义框架。


<details>
  <summary>Details</summary>
Motivation: 需要系统性地捕捉涉及非确定性、概率分支和随机时序的动态行为，同时在严格的数学框架内支持基于策略的决策制定。

Method: 开发了层次化的自动机理论语义，涵盖了非确定性、概率性和随机模型；形式化了控制策略的分类体系；引入了行为等价关系如互模拟和随机同构。

Result: Controlled SANs推广了经典框架如连续时间马尔可夫决策过程(CTMDPs)，为在不确定性下运行的可信赖系统的规范、验证和综合提供了严格基础。

Conclusion: 该框架支持定量和定性分析，推进了控制、时序和随机性紧密耦合的安全关键系统的设计。

Abstract: We introduce Controlled Stochastic Activity Networks (Controlled SANs), a formal extension of classical Stochastic Activity Networks that integrates explicit control actions into a unified semantic framework for modeling distributed real-time systems. Controlled SANs systematically capture dynamic behavior involving nondeterminism, probabilistic branching, and stochastic timing, while enabling policy-driven decision-making within a rigorous mathematical framework.
  We develop a hierarchical, automata-theoretic semantics for Controlled SANs that encompasses nondeterministic, probabilistic, and stochastic models in a uniform manner. A structured taxonomy of control policies, ranging from memoryless and finite-memory strategies to computationally augmented policies, is formalized, and their expressive power is characterized through associated language classes. To support model abstraction and compositional reasoning, we introduce behavioral equivalences, including bisimulation and stochastic isomorphism.
  Controlled SANs generalize classical frameworks such as continuous-time Markov decision processes (CTMDPs), providing a rigorous foundation for the specification, verification, and synthesis of dependable systems operating under uncertainty. This framework enables both quantitative and qualitative analysis, advancing the design of safety-critical systems where control, timing, and stochasticity are tightly coupled.

</details>


### [35] [Atomic Gliders and CA as Language Generators (Extended Version)](https://arxiv.org/abs/2511.12656)
*Dana Fisman,Noa Izsak*

Main category: cs.FL

TL;DR: 该论文将一维元胞自动机形式化为语言生成器，通过定义基于滑翔体的生成语义，证明了即使从正则初始配置出发，元胞自动机也能生成非正则甚至非上下文无关的语言。


<details>
  <summary>Details</summary>
Motivation: 虽然元胞自动机的动力学行为已被广泛研究，但将其作为真正的语言生成器进行形式化处理仍然不足。本文旨在填补这一空白，探索元胞自动机在语言生成方面的计算能力。

Method: 定义元胞自动机可表达语言为从双无限网格上的一维确定性同步元胞自动机可达配置中投影非静止段得到的有限词集合。提出基于滑翔体的生成语义，将滑翔体定义为在特定速度下携带符号的单细胞实体。

Result: 研究表明，尽管初始配置是正则的且转移规则是局部的，但产生的语言可以表现出非正则甚至非上下文无关的结构。

Conclusion: 正则初始化的元胞自动机语言是一个计算能力丰富的模型，在线性有序多智能体系统的形式分析中具有潜在应用价值。

Abstract: Cellular automata (CA) are well-studied models of decentralized parallel computation, known for their ability to exhibit complex global behavior from simple local rules. While their dynamics have been widely explored through simulations, a formal treatment of CA as genuine language generators remains underdeveloped. We formalize CA-expressible languages as sets of finite words obtained by projecting the non-quiescent segments of configurations reachable by one-dimensional, deterministic, synchronous CA over bi-infinite grids. These languages are defined with respect to sets of initial configurations specified by a regular language as in regular model checking. To capture structured dynamics, we propose a glider-based generative semantics for CA. Inspired by the classical notion of gliders, we define a glider as a one-cell entity carrying a symbol in a certain velocity under well defined interaction semantics. We show that despite the regularity of the initial configurations and the locality of the transition rules, the resulting languages can exhibit non-regular and even non-context-free structure. This positions regular-initialized CA languages as a surprisingly rich computational model, with potential applications in the formal analysis of linearly ordered MAS.

</details>
