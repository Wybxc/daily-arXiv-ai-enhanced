{"id": "2509.08090", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.08090", "abs": "https://arxiv.org/abs/2509.08090", "authors": ["Eman Abdullah AlOmar", "Luo Xu", "Sofia Martinez", "Anthony Peruma", "Mohamed Wiem Mkaouer", "Christian D. Newman", "Ali Ouni"], "title": "ChatGPT for Code Refactoring: Analyzing Topics, Interaction, and Effective Prompts", "comment": null, "summary": "Large Language Models (LLMs), such as ChatGPT, have become widely popular and\nwidely used in various software engineering tasks such as refactoring, testing,\ncode review, and program comprehension. Although recent studies have examined\nthe effectiveness of LLMs in recommending and suggesting refactoring, there is\na limited understanding of how developers express their refactoring needs when\ninteracting with ChatGPT. In this paper, our goal is to explore interactions\nrelated to refactoring between developers and ChatGPT to better understand how\ndevelopers identify areas for improvement in code, and how ChatGPT addresses\ndevelopers' needs. Our approach involves text mining 715 refactoring-related\ninteractions from 29,778 ChatGPT prompts and responses, as well as the analysis\nof developers' explicit refactoring intentions."}
{"id": "2509.08285", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.08285", "abs": "https://arxiv.org/abs/2509.08285", "authors": ["Carmen Cârlan", "Daniel Ratiu", "Michael Wagner"], "title": "Safety Factories -- a Manifesto", "comment": "Presented at The 44th International Conference on Computer Safety,\n  Reliability and Security (SafeComp 2025)", "summary": "Modern cyber-physical systems are operated by complex software that\nincreasingly takes over safety-critical functions. Software enables rapid\niterations and continuous delivery of new functionality that meets the\never-changing expectations of users. As high-speed development requires\ndiscipline, rigor, and automation, software factories are used. These entail\nmethods and tools used for software development, such as build systems and\npipelines. To keep up with the rapid evolution of software, we need to bridge\nthe disconnect in methods and tools between software development and safety\nengineering today. We need to invest more in formality upfront - capturing\nsafety work products in semantically rich models that are machine-processable,\ndefining automatic consistency checks, and automating the generation of\ndocumentation - to benefit later. Transferring best practices from software to\nsafety engineering is worth exploring. We advocate for safety factories, which\nintegrate safety tooling and methods into software development pipelines."}
{"id": "2509.08389", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.08389", "abs": "https://arxiv.org/abs/2509.08389", "authors": ["Marco Torchiano", "Riccardo Coppola", "Antonio Vetro'", "Xhoi Musaj"], "title": "The Impact of Team Diversity in Agile Development Education", "comment": "Post-print of paper published at FSE Companion '25: Proceedings of\n  the 33rd ACM International Conference on the Foundations of Software\n  Engineering", "summary": "Software Engineering is mostly a male-dominated sector, where gender\ndiversity is a key feature for improving equality of opportunities,\nproductivity, and innovation. Other diversity aspects, including but not\nlimited to nationality and ethnicity, are often understudied.In this work we\naim to assess the impact of team diversity, focusing mainly on gender and\nnationality, in the context of an agile software development project-based\ncourse. We analyzed 51 teams over three academic years, measuring three\ndifferent Diversity indexes - regarding Gender, Nationality and their\nco-presence - to examine how different aspects of diversity impact the quality\nof team project outcomes.Statistical analysis revealed a moderate,\nstatistically significant correlation between gender diversity and project\nsuccess, aligning with existing literature. Diversity in nationality showed a\nnegative but negligible effect on project results, indicating that promoting\nthese aspects does not harm students' performance. Analyzing their co-presence\nwithin a team, gender and nationality combined had a negative impact, likely\ndue to increased communication barriers and differing cultural norms.This study\nunderscores the importance of considering multiple diversity dimensions and\ntheir interactions in educational settings. Our findings, overall, show that\npromoting diversity in teams does not negatively impact their performance and\nachievement of educational goals."}
{"id": "2509.08524", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.08524", "abs": "https://arxiv.org/abs/2509.08524", "authors": ["Felix Mächtle", "Nils Loose", "Jan-Niclas Serr", "Jonas Sander", "Thomas Eisenbarth"], "title": "AutoStub: Genetic Programming-Based Stub Creation for Symbolic Execution", "comment": "2025 HUMIES finalist", "summary": "Symbolic execution is a powerful technique for software testing, but suffers\nfrom limitations when encountering external functions, such as native methods\nor third-party libraries. Existing solutions often require additional context,\nexpensive SMT solvers, or manual intervention to approximate these functions\nthrough symbolic stubs. In this work, we propose a novel approach to\nautomatically generate symbolic stubs for external functions during symbolic\nexecution that leverages Genetic Programming. When the symbolic executor\nencounters an external function, AutoStub generates training data by executing\nthe function on randomly generated inputs and collecting the outputs. Genetic\nProgramming then derives expressions that approximate the behavior of the\nfunction, serving as symbolic stubs. These automatically generated stubs allow\nthe symbolic executor to continue the analysis without manual intervention,\nenabling the exploration of program paths that were previously intractable. We\ndemonstrate that AutoStub can automatically approximate external functions with\nover 90% accuracy for 55% of the functions evaluated, and can infer\nlanguage-specific behaviors that reveal edge cases crucial for software\ntesting."}
{"id": "2509.08182", "categories": ["cs.PL", "cs.AI", "cs.CL", "03B70, 06B23, 47H10, 68T27, 68T50", "I.2.7; I.2.8; F.4.1; F.4.3; H.5.2"], "pdf": "https://arxiv.org/pdf/2509.08182", "abs": "https://arxiv.org/abs/2509.08182", "authors": ["Faruk Alpay", "Taylan Alpay"], "title": "XML Prompting as Grammar-Constrained Interaction: Fixed-Point Semantics, Convergence Guarantees, and Human-AI Protocols", "comment": "7 pages, multiple XML prompts", "summary": "Structured prompting with XML tags has emerged as an effective way to steer\nlarge language models (LLMs) toward parseable, schema-adherent outputs in\nreal-world systems. We develop a logic-first treatment of XML prompting that\nunifies (i) grammar-constrained decoding, (ii) fixed-point semantics over\nlattices of hierarchical prompts, and (iii) convergent human-AI interaction\nloops. We formalize a complete lattice of XML trees under a refinement order\nand prove that monotone prompt-to-prompt operators admit least fixed points\n(Knaster-Tarski) that characterize steady-state protocols; under a task-aware\ncontraction metric on trees, we further prove Banach-style convergence of\niterative guidance. We instantiate these results with context-free grammars\n(CFGs) for XML schemas and show how constrained decoding guarantees\nwell-formedness while preserving task performance. A set of multi-layer\nhuman-AI interaction recipes demonstrates practical deployment patterns,\nincluding multi-pass \"plan $\\to$ verify $\\to$ revise\" routines and agentic tool\nuse. We provide mathematically complete proofs and tie our framework to recent\nadvances in grammar-aligned decoding, chain-of-verification, and programmatic\nprompting."}
{"id": "2509.08165", "categories": ["cs.LO", "math.LO"], "pdf": "https://arxiv.org/pdf/2509.08165", "abs": "https://arxiv.org/abs/2509.08165", "authors": ["Alessandro Artale", "Christopher Hampson", "Roman Kontchakov", "Andrea Mazzullo", "Frank Wolter"], "title": "Decidability in First-Order Modal Logic with Non-Rigid Constants and Definite Descriptions", "comment": null, "summary": "While modal extensions of decidable fragments of first-order logic are\nusually undecidable, their monodic counterparts, in which formulas in the scope\nof modal operators have at most one free variable, are typically decidable.\nThis only holds, however, under the provision that non-rigid constants,\ndefinite descriptions and non-trivial counting are not admitted. Indeed,\nseveral monodic fragments having at least one of these features are known to be\nundecidable. We investigate these features systematically and show that\nfundamental monodic fragments such as the two-variable fragment with counting\nand the guarded fragment of standard first-order modal logics $\\mathbf{K}_{n}$\nand $\\mathbf{S5}_{n}$ are decidable. Tight complexity bounds are established as\nwell. Under the expanding-domain semantics, we show decidability of the basic\nmodal logic extended with the transitive closure operator on finite acyclic\nframes; this logic, however, is Ackermann-hard."}
{"id": "2509.08546", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.08546", "abs": "https://arxiv.org/abs/2509.08546", "authors": ["Yu Zhu", "Jiyuan Ye"], "title": "Beyond the Binary: The System of All-round Evaluation of Research and Its Practices in China", "comment": "STI-ENID 2025 Conference Paer", "summary": "The lack of a macro-level, systematic evaluation theory to guide the\nimplementation of evaluation practices has become a key bottleneck in the\nreform of global research evaluation systems. By reviewing the historical\ndevelopment of research evaluation, this paper highlights the current binary\nopposition between qualitative and quantitative methods in evaluation\npractices. This paper introduces the System of All-round Evaluation of Research\n(SAER), a framework that integrates form, content, and utility evaluations with\nsix key elements. SAER offers a theoretical breakthrough by transcending the\nbinary, providing a comprehensive foundation for global evaluation reforms. The\ncomprehensive system proposes a trinity of three evaluation dimensions,\ncombined with six evaluation elements, which would help academic evaluators and\nresearchers reconcile binary oppositions in evaluation methods. The system\nhighlights the dialectical wisdom and experience embedded in Chinese research\nevaluation theory, offering valuable insights and references for the reform and\nadvancement of global research evaluation systems."}
{"id": "2509.08264", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2509.08264", "abs": "https://arxiv.org/abs/2509.08264", "authors": ["Chad E. Brown", "Cezary Kaliszyk", "Martin Suda", "Josef Urban"], "title": "Hammering Higher Order Set Theory", "comment": "Accepted for publication at CICM 2025", "summary": "We use automated theorem provers to significantly shorten a formal\ndevelopment in higher order set theory. The development includes many standard\ntheorems such as the fundamental theorem of arithmetic and irrationality of\nsquare root of two. Higher order automated theorem provers are particularly\nuseful here, since the underlying framework of higher order set theory\ncoincides with the classical extensional higher order logic of (most) higher\norder automated theorem provers, so no significant translation or encoding is\nrequired. Additionally, many subgoals are first order and so first order\nautomated provers often suffice. We compare the performance of different\nprovers on the subgoals generated from the development. We also discuss\npossibilities for proof reconstruction, i.e., obtaining formal proof terms when\nan automated theorem prover claims to have proven the subgoal."}
{"id": "2509.08667", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.08667", "abs": "https://arxiv.org/abs/2509.08667", "authors": ["Amirali Rayegan", "Tim Menzies"], "title": "Minimal Data, Maximum Clarity: A Heuristic for Explaining Optimization", "comment": null, "summary": "Efficient, interpretable optimization is a critical but underexplored\nchallenge in software engineering, where practitioners routinely face vast\nconfiguration spaces and costly, error-prone labeling processes. This paper\nintroduces EZR, a novel and modular framework for multi-objective optimization\nthat unifies active sampling, learning, and explanation within a single,\nlightweight pipeline. Departing from conventional wisdom, our Maximum Clarity\nHeuristic demonstrates that using less (but more informative) data can yield\noptimization models that are both effective and deeply understandable. EZR\nemploys an active learning strategy based on Naive Bayes sampling to\nefficiently identify high-quality configurations with a fraction of the labels\nrequired by fully supervised approaches. It then distills optimization logic\ninto concise decision trees, offering transparent, actionable explanations for\nboth global and local decision-making. Extensive experiments across 60\nreal-world datasets establish that EZR reliably achieves over 90% of the\nbest-known optimization performance in most cases, while providing clear,\ncohort-based rationales that surpass standard attribution-based explainable AI\n(XAI) methods (LIME, SHAP, BreakDown) in clarity and utility. These results\nendorse \"less but better\"; it is both possible and often preferable to use\nfewer (but more informative) examples to generate label-efficient optimization\nand explanations in software systems. To support transparency and\nreproducibility, all code and experimental materials are publicly available at\nhttps://github.com/amiiralii/Minimal-Data-Maximum-Clarity."}
{"id": "2509.08267", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2509.08267", "abs": "https://arxiv.org/abs/2509.08267", "authors": ["Chad E. Brown", "Cezary Kaliszyk", "Josef Urban"], "title": "Exploring Formal Math on the Blockchain: An Explorer for Proofgold", "comment": "Accepted for publication at CICM 2025", "summary": "Proofgold is a blockchain that supports formalized mathematics alongside\nstandard cryptocurrency functionality. It incorporates logical constructs into\nthe blockchain, including declarations of formal theories, definitions,\npropositions and proofs. It also supports placing and collecting bounties on\nproving these propositions, incentivizing the development of the formal\nlibraries contained in Proofgold. In this paper, we present a web-based\nblockchain explorer for Proofgold. The system exposes not only the usual\ntransactional data but also the formal mathematical components embedded in the\nchain and allows some interaction with them. The explorer allows users to\ninspect blocks, transactions, and addresses, as well as formal objects:\ntheories, definitions, theorems and their proofs. We also support the\nsubmission of transactions to the blockchain using our interface. We describe\nthe system architecture and its integration with the Proofgold Lava software,\nhighlighting how the explorer supports navigation of formal content and\nfacilitates mathematical knowledge management in a decentralized setting, as\nwell as a number of formalizations in category theory done in the system."}
{"id": "2509.08724", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.08724", "abs": "https://arxiv.org/abs/2509.08724", "authors": ["Junhao Wang", "Daoguang Zan", "Shulin Xin", "Siyao Liu", "Yurong Wu", "Kai Shen"], "title": "SWE-Mirror: Scaling Issue-Resolving Datasets by Mirroring Issues Across Repositories", "comment": null, "summary": "Creating large-scale verifiable training datasets for issue-resolving tasks\nis a critical yet notoriously difficult challenge. Existing methods on\nautomating the Gym environment setup process for real-world issues suffer from\nlow success rates and high overhead. Meanwhile, synthesizing new tasks within\nexisting Gym environments leaves the vast pool of authentic, human-reported\nproblems untapped. To maximize the utilization of existing Gym environments and\nalso the rich data of issue-resolving history on GitHub, we introduce\nSWE-Mirror, a pipeline that distills a real-world issue's semantic essence,\nmirrors it into another repository with a configured Gym environment, and\nre-animates it as a verifiable issue-resolving task. SWE-Mirror reuses existing\nGym environments along with the vast pool of issue-resolving history hosted on\nGitHub to construct a large-scale dataset of mirrored authentic and verifiable\ntasks. Applying SWE-Mirror to 40 repositories across 4 languages, we have\ncurated a dataset with 60,671 issue-resolving tasks and demonstrated the value\nof our dataset by training and evaluating coding agents at various scale.\nPost-training experiments show that models trained with the dataset exhibit\nimprovements in issue-resolving capabilities. Furthermore, by extending the\ndataset size to over 12,000 high-quality trajectories, we established a new\nstate-of-the-art (SOTA) among Qwen2.5-Coder-Instruct based LLMs on the\nOpenHands agent framework, which increases the resolve rate on\nSWE-Bench-Verified by +21.8% for the 7B model and +46.0% for the 32B model and\nvalidates the effectiveness of our approach."}
{"id": "2509.08268", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2509.08268", "abs": "https://arxiv.org/abs/2509.08268", "authors": ["Chad E. Brown", "Cezary Kaliszyk", "Josef Urban"], "title": "Payment Channels with Proofs", "comment": "Accepted for publication at BCCA 2025", "summary": "The fundamental building blocks of the Bitcoin lightning network are\nbidirectional payment channels. We describe an extension of payment channels in\nthe Proofgold network which allow the two parties to bet on whether a\nproposition will be proven by a certain time. These provide the foundation for\na Proofgold lightning network that would allow parties to request proofs (by\nbetting there will be no proof by a certain time) and other parties to provide\nproofs (and be rewarded by betting there will be a proof). The bets may also\nprovide a way to approximate the probability that a certain proposition is\nprovable (in the given amount of time). We describe the implementation of\npayment channels supporting proofs in Proofgold and discuss a potential\nlightning network that could be built as a result. One application of such\nlightning network would be a large decentralized infrastructure for fast\ncollaborative formalization projects."}
{"id": "2509.08808", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.08808", "abs": "https://arxiv.org/abs/2509.08808", "authors": ["Mohammad Saqib Hasan", "Sayontan Ghosh", "Dhruv Verma", "Geoff Kuenning", "Erez Zadok", "Scott A. Smolka", "Niranjan Balasubramanian"], "title": "Handling Open-Vocabulary Constructs in Formalizing Specifications: Retrieval-Augmented Parsing with Expert Knowledge", "comment": "Accepted to COLM 2024", "summary": "We study the problem of Open-Vocabulary Constructs(OVCs) -- ones not known\nbeforehand -- in the context of converting natural language (NL) specifications\ninto formal languages (e.g., temporal logic or code). Models fare poorly on\nOVCs due to a lack of necessary knowledge a priori. In such situations, a\ndomain expert can provide correct constructs at inference time based on their\npreferences or domain knowledge. Our goal is to effectively reuse this\ninference-time, expert-provided knowledge for future parses without retraining\nthe model. We present dynamic knowledge-augmented parsing(DKAP), where in\naddition to the input sentence, the model receives (dynamically growing) expert\nknowledge as a key-value lexicon that associates NL phrases with correct OVC\nconstructs. We propose ROLex, a retrieval-augmented parsing approach that uses\nthis lexicon. A retriever and a generator are trained to find and use the\nkey-value store to produce the correct parse. A key challenge lies in curating\ndata for this retrieval-augmented parser. We utilize synthetic data generation\nand the data augmentation techniques on annotated (NL sentence, FL statement)\npairs to train the augmented parser. To improve training effectiveness, we\npropose multiple strategies to teach models to focus on the relevant subset of\nretrieved knowledge. Finally, we introduce a new evaluation paradigm modeled\nafter the DKAP problem and simulate the scenario across three formalization\ntasks (NL2LTL, NL2Code, and NL2CMD). Our evaluations show that DKAP is a\ndifficult challenge, and ROLex helps improve the performance of baseline models\nby using dynamic expert knowledge effectively."}
{"id": "2509.08610", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2509.08610", "abs": "https://arxiv.org/abs/2509.08610", "authors": ["Sebastian Schirmer", "Philipp Schitz", "Johann C. Dauer", "Bernd Finkbeiner", "Sriram Sankaranarayanan"], "title": "Trace Repair for Temporal Behavior Trees", "comment": null, "summary": "We present methods for repairing traces against specifications given as\ntemporal behavior trees (TBT). TBT are a specification formalism for action\nsequences in robotics and cyber-physical systems, where specifications of\nsub-behaviors, given in signal temporal logic, are composed using operators for\nsequential and parallel composition, fallbacks, and repetition. Trace repairs\nare useful to explain failures and as training examples that avoid the observed\nproblems. In principle, repairs can be obtained via mixed-integer linear\nprogramming (MILP), but this is far too expensive for practical applications.\nWe present two practical repair strategies: (1) incremental repair, which\nreduces the MILP by splitting the trace into segments, and (2) landmark-based\nrepair, which solves the repair problem iteratively using TBT's robust\nsemantics as a heuristic that approximates MILP with more efficient linear\nprogramming. In our experiments, we were able to repair traces with more than\n25,000 entries in under ten minutes, while MILP runs out of memory."}
