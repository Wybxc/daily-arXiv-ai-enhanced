<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 22]
- [cs.LO](#cs.LO) [Total: 7]
- [cs.FL](#cs.FL) [Total: 3]
- [cs.PL](#cs.PL) [Total: 6]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [SIADAFIX: issue description response for adaptive program repair](https://arxiv.org/abs/2510.16059)
*Xin Cao,Nan Yu*

Main category: cs.SE

TL;DR: 提出SIADAFIX方法，利用快慢思维增强大语言模型在程序修复任务中的能力，通过问题描述响应自适应选择三种修复模式，在SWE-bench Lite上达到60.67%的pass@1性能。


<details>
  <summary>Details</summary>
Motivation: 利用快慢思维来增强基于大语言模型的智能体在复杂任务（如程序修复）上的能力，平衡修复效率和准确性。

Method: 设计基于问题描述响应的自适应程序修复方法SIADAFIX，使用慢思维错误修复代理完成复杂修复任务，快思维工作流决策组件优化和分类问题描述，根据问题复杂度自适应选择简单、中等、困难三种修复模式。

Result: 在SWE-bench Lite上的实验结果显示，使用Claude-4 Sonnet模型达到60.67%的pass@1性能，在所有开源方法中达到最先进水平。

Conclusion: SIADAFIX有效平衡了修复效率和准确性，为自动化程序修复提供了新的思路。

Abstract: We propose utilizing fast and slow thinking to enhance the capabilities of
large language model-based agents on complex tasks such as program repair. In
particular, we design an adaptive program repair method based on issue
description response, called SIADAFIX. The proposed method utilizes slow
thinking bug fix agent to complete complex program repair tasks, and employs
fast thinking workflow decision components to optimize and classify issue
descriptions, using issue description response results to guide the
orchestration of bug fix agent workflows. SIADAFIX adaptively selects three
repair modes, i.e., easy, middle and hard mode, based on problem complexity. It
employs fast generalization for simple problems and test-time scaling
techniques for complex problems. Experimental results on the SWE-bench Lite
show that the proposed method achieves 60.67% pass@1 performance using the
Claude-4 Sonnet model, reaching state-of-the-art levels among all open-source
methods. SIADAFIX effectively balances repair efficiency and accuracy,
providing new insights for automated program repair. Our code is available at
https://github.com/liauto-siada/siada-cli.

</details>


### [2] [Code Contribution and Credit in Science](https://arxiv.org/abs/2510.16242)
*Eva Maxfield Brown,Isaac Slaughter,Nicholas Weber*

Main category: cs.SE

TL;DR: 研究发现科学软件开发与学术认可之间存在脱节：近30%论文有代码贡献者未获署名，频繁编码的作者h指数更低，软件贡献未在传统学术评价体系中得到充分认可。


<details>
  <summary>Details</summary>
Motivation: 探究软件开发活动在科学合作中如何影响学术认可分配，理解软件贡献与传统学术评价指标之间的关系。

Method: 构建包含14万篇研究论文与代码仓库配对的数据集，开发预测模型匹配论文作者与代码仓库开发者账户，分析软件贡献与学术认可的关系。

Result: 30%论文有非作者代码贡献者；代码贡献作者引用率仅增加4.2%；第一作者更可能是代码贡献者；编码频率与h指数呈负相关。

Conclusion: 软件贡献与学术认可之间存在脱节，这对机构奖励机制和科学政策具有重要启示意义。

Abstract: Software development has become essential to scientific research, but its
relationship to traditional metrics of scholarly credit remains poorly
understood. We develop a dataset of approximately 140,000 paired research
articles and code repositories, as well as a predictive model that matches
research article authors with software repository developer accounts. We use
this data to investigate how software development activities influence credit
allocation in collaborative scientific settings. Our findings reveal
significant patterns distinguishing software contributions from traditional
authorship credit. We find that nearly 30% of articles include non-author code
contributors- individuals who participated in software development but received
no formal authorship recognition. While code-contributing authors show a modest
$\sim$4.2% increase in article citations, this effect becomes non-significant
when controlling for domain, article type, and open access status. First
authors are significantly more likely to be code contributors than other author
positions. Notably, we identify a negative relationship between coding
frequency and scholarly impact metrics. Authors who contribute code more
frequently exhibit progressively lower h-indices than non-coding colleagues,
even when controlling for publication count, author position, domain, and
article type. These results suggest a disconnect between software contributions
and credit, highlighting important implications for institutional reward
structures and science policy.

</details>


### [3] [MLCPD: A Unified Multi-Language Code Parsing Dataset with Universal AST Schema](https://arxiv.org/abs/2510.16357)
*Jugal Gajjar,Kamalasankari Subramaniakuppusamy*

Main category: cs.SE

TL;DR: MLCPD是一个大规模、语言无关的代码解析数据集，统一了10种主要编程语言的语法和结构表示，包含超过700万个解析后的源文件，采用统一的抽象语法树模式。


<details>
  <summary>Details</summary>
Motivation: 现有语料库主要关注词法级代码或孤立解析器，缺乏统一的跨语言结构表示，需要建立能够支持跨语言推理、结构学习和多语言软件分析的数据集。

Method: 提出通用抽象语法树模式，对10种编程语言的源代码进行解析和规范化，提供层次化树表示和丰富元数据，确保无损语法覆盖和结构一致性。

Result: 经验分析显示存在强烈的跨语言结构规律性，即使是Python、Java和Go等不同语言的语法图也能在共享模式下对齐。数据集以Parquet格式存储，便于扩展检索。

Conclusion: MLCPD为跨语言表示学习和程序分析的未来研究建立了开放、可复现的基础，相关资源已在Hugging Face和GitHub上公开。

Abstract: We introduce the MultiLang Code Parser Dataset (MLCPD), a large-scale,
language-agnostic dataset unifying syntactic and structural representations of
code across ten major programming languages. MLCPD contains over seven million
parsed source files normalized under our proposed universal Abstract Syntax
Tree (AST) schema, enabling consistent cross-language reasoning, structural
learning, and multilingual software analysis. Unlike existing corpora that
focus purely on token-level code or isolated parsers, MLCPD provides both
hierarchical tree representations and rich metadata for every file, ensuring
lossless syntactic coverage and structural uniformity. Each entry includes a
normalized schema, language-level metadata, and abstracted node semantics
stored in Parquet format for scalable retrieval. Empirical analyses reveal
strong cross-language structural regularities-demonstrating that syntactic
graphs from languages as diverse as Python, Java, and Go can be aligned under a
shared schema. We release the dataset publicly on Hugging Face and the
accompanying codebase on GitHub, which includes complete pipelines for dataset
reproduction, grammar compilation, and a visualization tool for exploring the
unified AST across languages. Together, these resources establish MLCPD as an
open, reproducible foundation for future research in cross-language
representation learning and program analysis.

</details>


### [4] [SemOpt: LLM-Driven Code Optimization via Rule-Based Analysis](https://arxiv.org/abs/2510.16384)
*Yuwei Zhao,Yuan-An Xiao,Qianyu Xiao,Zhao Zhang,Yingfei Xiong*

Main category: cs.SE

TL;DR: SemOpt是一个利用静态程序分析和LLM的代码优化框架，通过策略库构建、规则生成和优化器三个组件，显著提升了代码优化效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的代码优化方法依赖信息检索技术从开源代码库中获取优化示例，但由于语义等效的优化可能表现为语法不同的代码片段，现有检索方法往往无法识别相关示例，导致优化性能不佳。

Method: SemOpt框架包含三个关键组件：(1) 策略库构建器：从真实代码修改中提取和聚类优化策略；(2) 规则生成器：生成Semgrep静态分析规则来捕获应用优化策略的条件；(3) 优化器：利用策略库生成优化代码结果。所有组件都由LLM驱动。

Result: 在包含151个优化任务的基准测试中，SemOpt在不同LLM下将成功优化数量提高了1.38到28倍。在大型C/C++项目中，可将单个性能指标提升5.04%到218.07%。

Conclusion: SemOpt通过结合静态程序分析和LLM，有效解决了现有代码优化方法中检索相关示例的局限性，显著提升了优化效果，具有实际应用价值。

Abstract: Automated code optimization aims to improve performance in programs by
refactoring code, and recent studies focus on utilizing LLMs for the
optimization. Typical existing approaches mine optimization commits from
open-source codebases to construct a large-scale knowledge base, then employ
information retrieval techniques such as BM25 to retrieve relevant optimization
examples for hotspot code locations, thereby guiding LLMs to optimize these
hotspots. However, since semantically equivalent optimizations can manifest in
syntactically dissimilar code snippets, current retrieval methods often fail to
identify pertinent examples, leading to suboptimal optimization performance.
This limitation significantly reduces the effectiveness of existing
optimization approaches.
  To address these limitations, we propose SemOpt, a novel framework that
leverages static program analysis to precisely identify optimizable code
segments, retrieve the corresponding optimization strategies, and generate the
optimized results. SemOpt consists of three key components: (1) A strategy
library builder that extracts and clusters optimization strategies from
real-world code modifications. (2) A rule generator that generates Semgrep
static analysis rules to capture the condition of applying the optimization
strategy. (3) An optimizer that utilizes the strategy library to generate
optimized code results. All the three components are powered by LLMs.
  On our benchmark containing 151 optimization tasks, SemOpt demonstrates its
effectiveness under different LLMs by increasing the number of successful
optimizations by 1.38 to 28 times compared to the baseline. Moreover, on
popular large-scale C/C++ projects, it can improve individual performance
metrics by 5.04% to 218.07%, demonstrating its practical utility.

</details>


### [5] [Code Digital Twin: Empowering LLMs with Tacit Knowledge for Complex Software Development](https://arxiv.org/abs/2510.16395)
*Xin Peng,Chong Wang*

Main category: cs.SE

TL;DR: 提出Code Digital Twin框架，通过混合知识表示和多阶段提取管道，将企业软件开发中的隐性知识转化为显性可操作表示，弥合AI能力与企业软件现实之间的差距。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在软件工程任务中表现出色，但企业软件开发主要依赖增量演进，其挑战远超常规编码，关键在于隐性知识（设计决策和历史权衡）。需要将新兴AI能力与企业开发实践对齐。

Method: 提出Code Digital Twin框架，包含：混合知识表示、多阶段提取管道、增量更新、LLM赋能应用和人机协同反馈，对软件的物理层和概念层进行建模。

Result: 该框架将碎片化知识转化为显性可操作表示，为问题定位和影响分析等任务提供AI支持，实现可持续、智能和弹性的超复杂系统开发和演进。

Conclusion: Code Digital Twin作为AI进展与企业软件现实之间的桥梁，为实现智能软件开发提供了具体路线图。

Abstract: Recent advances in large language models (LLMs) have demonstrated strong
capabilities in software engineering tasks, raising expectations of
revolutionary productivity gains. However, enterprise software development is
largely driven by incremental evolution, where challenges extend far beyond
routine coding and depend critically on tacit knowledge, including design
decisions at different levels and historical trade-offs. To achieve effective
AI-powered support for complex software development, we should align emerging
AI capabilities with the practical realities of enterprise development. To this
end, we systematically identify challenges from both software and LLM
perspectives. Alongside these challenges, we outline opportunities where AI and
structured knowledge frameworks can enhance decision-making in tasks such as
issue localization and impact analysis. To address these needs, we propose the
Code Digital Twin, a living framework that models both the physical and
conceptual layers of software, preserves tacit knowledge, and co-evolves with
the codebase. By integrating hybrid knowledge representations, multi-stage
extraction pipelines, incremental updates, LLM-empowered applications, and
human-in-the-loop feedback, the Code Digital Twin transforms fragmented
knowledge into explicit and actionable representations. Our vision positions it
as a bridge between AI advancements and enterprise software realities,
providing a concrete roadmap toward sustainable, intelligent, and resilient
development and evolution of ultra-complex systems.

</details>


### [6] [Large-Scale Empirical Analysis of Continuous Fuzzing: Insights from 1 Million Fuzzing Sessions](https://arxiv.org/abs/2510.16433)
*Tatsuya Shirai,Olivier Nourry,Yutaro Kashiwa,Kenji Fujiwara,Yasutaka Kamei,Hajimu Iida*

Main category: cs.SE

TL;DR: 该研究通过分析OSS-Fuzz中878个项目的约112万次模糊测试会话，揭示了持续模糊测试在漏洞检测中的作用：早期阶段检测率高、代码覆盖率持续增长、覆盖率变化有助于漏洞发现。


<details>
  <summary>Details</summary>
Motivation: 尽管持续模糊测试已被数千个项目采用，但其在漏洞检测中的具体贡献尚不明确。本研究旨在阐明持续模糊测试在漏洞检测中的作用。

Method: 收集OSS-Fuzz的问题报告、覆盖率报告和模糊测试日志，对878个项目的约112万次模糊测试会话进行实证研究。

Result: 发现大量模糊测试漏洞在持续模糊测试集成前就已存在，导致早期检测率高；代码覆盖率随持续模糊测试进展而持续增加；覆盖率变化有助于模糊测试漏洞的检测。

Conclusion: 本研究为持续模糊测试如何促进模糊测试漏洞检测提供了实证见解，对未来的持续模糊测试策略和工具开发具有实际意义。

Abstract: Software vulnerabilities are constantly being reported and exploited in
software products, causing significant impacts on society. In recent years, the
main approach to vulnerability detection, fuzzing, has been integrated into the
continuous integration process to run in short and frequent cycles. This
continuous fuzzing allows for fast identification and remediation of
vulnerabilities during the development process. Despite adoption by thousands
of projects, however, it is unclear how continuous fuzzing contributes to
vulnerability detection. This study aims to elucidate the role of continuous
fuzzing in vulnerability detection. Specifically, we investigate the coverage
and the total number of fuzzing sessions when fuzzing bugs are discovered. We
collect issue reports, coverage reports, and fuzzing logs from OSS-Fuzz, an
online service provided by Google that performs fuzzing during continuous
integration. Through an empirical study of a total of approximately 1.12
million fuzzing sessions from 878 projects participating in OSS-Fuzz, we reveal
that (i) a substantial number of fuzzing bugs exist prior to the integration of
continuous fuzzing, leading to a high detection rate in the early stages; (ii)
code coverage continues to increase as continuous fuzzing progresses; and (iii)
changes in coverage contribute to the detection of fuzzing bugs. This study
provides empirical insights into how continuous fuzzing contributes to fuzzing
bug detection, offering practical implications for future strategies and tool
development in continuous fuzzing.

</details>


### [7] [On the Use of Large Language Models for Qualitative Synthesis](https://arxiv.org/abs/2510.16502)
*Sebastián Pizard,Ramiro Moreira,Federico Galiano,Ignacio Sastre,Lorena Etcheverry*

Main category: cs.SE

TL;DR: 本文探讨了在系统性综述的定性综合阶段使用大型语言模型的挑战，通过协作自民族志方法评估其方法严谨性和实际有用性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在系统性综述中显示出潜力，但将其应用于报告不一致、执行多变的定性综合阶段存在风险，可能放大现有弱点并削弱综述结果的可信度。

Method: 采用协作自民族志方法，包含两个试验，从方法严谨性和实际有用性角度评估，并结合LLMs构建方式及当前局限的技术视角进行解读。

Result: 研究发现使用LLMs进行定性综合面临重要挑战，需要谨慎考虑其应用方式以避免放大系统性综述中的现有问题。

Conclusion: 在系统性综述的定性综合阶段使用LLMs需要谨慎，必须考虑其技术局限性和可能带来的风险，以确保研究结果的可靠性。

Abstract: Large language models (LLMs) show promise for supporting systematic reviews
(SR), even complex tasks such as qualitative synthesis (QS). However, applying
them to a stage that is unevenly reported and variably conducted carries
important risks: misuse can amplify existing weaknesses and erode confidence in
the SR findings. To examine the challenges of using LLMs for QS, we conducted a
collaborative autoethnography involving two trials. We evaluated each trial for
methodological rigor and practical usefulness, and interpreted the results
through a technical lens informed by how LLMs are built and their current
limitations.

</details>


### [8] [Human-Aligned Code Readability Assessment with Large Language Models](https://arxiv.org/abs/2510.16579)
*Wendkûuni C. Ouédraogo,Yinghua Li,Xueqi Dang,Pawel Borsukiewicz,Xin Zhou,Anil Koyuncu,Jacques Klein,David Lo,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: CoReEval是首个大规模评估LLM代码可读性评估能力的基准，包含140万次模型-代码片段-提示评估，涵盖10个先进LLM、3种编程语言、2种代码类型、4种提示策略和9种解码设置。研究发现基于开发者指导的提示能改善对齐度和解释质量，但会增加评分变异性。


<details>
  <summary>Details</summary>
Motivation: 代码可读性对软件理解和维护至关重要，但难以大规模评估。传统静态指标无法捕捉人类判断的主观性和上下文敏感性，而LLM作为可扩展替代方案的行为尚未充分探索。

Method: 构建CoReEval基准，比较LLM输出与人工标注和验证静态模型，分析数值对齐（MAE、Pearson、Spearman）和理由质量（情感、方面覆盖、语义聚类）。使用开发者指导的提示策略，针对初级和高级开发者角色进行个性化。

Result: 基于人类定义可读性维度的开发者指导提示在结构化上下文中改善了对齐度，增强了解释质量，并通过角色框架实现轻量级个性化。但评分变异性增加，揭示了对齐度、稳定性和可解释性之间的权衡。

Conclusion: CoReEval为提示工程、模型对齐研究和人机协同评估提供了坚实基础，在教育培训、入职流程和CI/CD管道中，LLM可作为可解释、适应性强的评审者。

Abstract: Code readability is crucial for software comprehension and maintenance, yet
difficult to assess at scale. Traditional static metrics often fail to capture
the subjective, context-sensitive nature of human judgments. Large Language
Models (LLMs) offer a scalable alternative, but their behavior as readability
evaluators remains underexplored. We introduce CoReEval, the first large-scale
benchmark for evaluating LLM-based code readability assessment, comprising over
1.4 million model-snippet-prompt evaluations across 10 state of the art LLMs.
The benchmark spans 3 programming languages (Java, Python, CUDA), 2 code types
(functional code and unit tests), 4 prompting strategies (ZSL, FSL, CoT, ToT),
9 decoding settings, and developer-guided prompts tailored to junior and senior
personas. We compare LLM outputs against human annotations and a validated
static model, analyzing numerical alignment (MAE, Pearson's, Spearman's) and
justification quality (sentiment, aspect coverage, semantic clustering). Our
findings show that developer-guided prompting grounded in human-defined
readability dimensions improves alignment in structured contexts, enhances
explanation quality, and enables lightweight personalization through persona
framing. However, increased score variability highlights trade-offs between
alignment, stability, and interpretability. CoReEval provides a robust
foundation for prompt engineering, model alignment studies, and human in the
loop evaluation, with applications in education, onboarding, and CI/CD
pipelines where LLMs can serve as explainable, adaptable reviewers.

</details>


### [9] [When Many-Shot Prompting Fails: An Empirical Study of LLM Code Translation](https://arxiv.org/abs/2510.16809)
*Amirkia Rafiei Oskooei,Kaan Baturalp Cosdan,Husamettin Isiktas,Mehmet S. Aktas*

Main category: cs.SE

TL;DR: 论文发现代码翻译任务中存在"多样本悖论"：虽然静态相似度指标随样本数量增加略有改善，但功能正确性在少样本提示（5-25个示例）时达到峰值，提供更多样本反而会降低性能。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在代码翻译任务中，随着上下文窗口中提供更多示例（从零样本到多样本配置）对性能的影响，挑战"越多越好"的普遍假设。

Method: 通过大规模实证研究，评估了超过90,000次翻译，系统性地测试了从零样本到625个示例的多样本配置，提示长度从约100,000到800,000个token。

Result: 功能正确性在少样本提示（5-25个示例）时达到最佳，提供更多示例会降低性能，而静态相似度指标仅略有改善。

Conclusion: 对于代码翻译任务，少量精心选择的示例质量比数量更重要，挑战了ICL中"越多越好"的普遍有效性，强调了最优提示策略的任务依赖性。

Abstract: Large Language Models (LLMs) with vast context windows offer new avenues for
in-context learning (ICL), where providing many examples ("many-shot"
prompting) is often assumed to enhance performance. We investigate this
assumption for the complex task of code translation. Through a large-scale
empirical study of over 90,000 translations, we systematically evaluate the
impact of scaling in-context examples from zero-shot to many-shot
configurations of up to 625 examples, with prompts spanning from approximately
100,000 to 800,000 tokens. Our findings reveal a "many-shot paradox": while
static similarity metrics may modestly improve with more examples, functional
correctness consistently peaks with few-shot prompting (5-25 examples).
Providing substantially more examples often degrades this crucial functional
performance. This study highlights that for code translation, the quality of a
few well-chosen examples outweighs sheer quantity, challenging the universal
efficacy of "more is better" for ICL and underscoring the task-dependent nature
of optimal prompting strategies. Our results have significant implications for
effectively leveraging LLMs in software engineering.

</details>


### [10] [Contrasting the Hyperparameter Tuning Impact Across Software Defect Prediction Scenarios](https://arxiv.org/abs/2510.16665)
*Mohamed Sami Rakha,Andriy Miranskyy,Daniel Alencar da Costa*

Main category: cs.SE

TL;DR: 该研究对比了超参数调优在两种软件缺陷预测场景（IVDP和CVDP）中的影响差异，发现IVDP场景中的性能提升显著大于CVDP场景，且小数据集更容易受到性能影响差异的影响。


<details>
  <summary>Details</summary>
Motivation: 虽然超参数调优可以提升软件缺陷预测性能，但其影响程度可能因不同的SDP场景而异。需要比较超参数调优在不同SDP场景中的影响，以提供全面见解并增强SDP建模的鲁棒性和实用性。

Method: 使用28种机器学习算法、53个发布后软件数据集、两种调优算法和五个优化指标，在IVDP和CVDP两种关键SDP场景下进行实验，并应用统计分析比较性能影响差异。

Result: IVDP场景中的SDP性能增益显著大于CVDP场景；28种ML算法中有多达24种算法的性能增益断言可能无法跨多个SDP场景成立；小软件数据集更容易出现较大的性能影响差异。

Conclusion: 软件工程研究者和实践者在期望从超参数调优中获得性能增益时，应考虑所选SDP场景的影响。

Abstract: Software defect prediction (SDP) is crucial for delivering high-quality
software products. Recent research has indicated that prediction performance
improvements in SDP are achievable by applying hyperparameter tuning to a
particular SDP scenario. However, the positive impact resulting from the
hyperparameter tuning step may differ based on the targeted SDP scenario.
Comparing the impact of hyperparameter tuning across SDP scenarios is necessary
to provide comprehensive insights and enhance the robustness, generalizability,
and, eventually, the practicality of SDP modeling for quality assurance.
  Therefore, in this study, we contrast the impact of hyperparameter tuning
across two pivotal and consecutive SDP scenarios: (1) Inner Version Defect
Prediction (IVDP) and (2) Cross Version Defect Prediction (CVDP). The main
distinctions between the two scenarios lie in the scope of defect prediction
and the selected evaluation setups. This study's experiments use common
evaluation setups, 28 machine learning (ML) algorithms, 53 post-release
software datasets, two tuning algorithms, and five optimization metrics. We
apply statistical analytics to compare the SDP performance impact differences
by investigating the overall impact, the single ML algorithm impact, and
variations across different software dataset sizes.
  The results indicate that the SDP gains within the IVDP scenario are
significantly larger than those within the CVDP scenario. The results reveal
that asserting performance gains for up to 24 out of 28 ML algorithms may not
hold across multiple SDP scenarios. Furthermore, we found that small software
datasets are more susceptible to larger differences in performance impacts.
Overall, the study findings recommend software engineering researchers and
practitioners to consider the effect of the selected SDP scenario when
expecting performance gains from hyperparameter tuning.

</details>


### [11] [QuanBench: Benchmarking Quantum Code Generation with Large Language Models](https://arxiv.org/abs/2510.16779)
*Xiaoyu Guo,Minggu Wang,Jianjun Zhao*

Main category: cs.SE

TL;DR: 提出了QuanBench基准测试，用于评估大语言模型在量子代码生成方面的性能，包含44个编程任务，涵盖量子算法、状态准备、门分解和量子机器学习等领域。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在通用代码生成方面表现良好，但在量子代码生成方面的能力尚未得到充分研究，需要专门的基准测试来评估其性能。

Method: 开发了QuanBench基准测试，包含44个量子编程任务，每个任务都有可执行的规范解决方案，并通过功能正确性（Pass@K）和量子语义等价性（过程保真度）进行评估。

Result: 评估结果显示当前大语言模型在生成正确量子代码方面的能力有限，总体准确率低于40%，且经常出现语义错误，常见失败案例包括过时的API使用、电路构建错误和算法逻辑错误。

Conclusion: QuanBench为未来改进大语言模型的量子代码生成能力提供了基础，当前模型在此领域仍有显著提升空间。

Abstract: Large language models (LLMs) have demonstrated good performance in general
code generation; however, their capabilities in quantum code generation remain
insufficiently studied. This paper presents QuanBench, a benchmark for
evaluating LLMs on quantum code generation. QuanBench includes 44 programming
tasks that cover quantum algorithms, state preparation, gate decomposition, and
quantum machine learning. Each task has an executable canonical solution and is
evaluated by functional correctness (Pass@K) and quantum semantic equivalence
(Process Fidelity). We evaluate several recent LLMs, including general-purpose
and code-specialized models. The results show that current LLMs have limited
capability in generating the correct quantum code, with overall accuracy below
40% and frequent semantic errors. We also analyze common failure cases, such as
outdated API usage, circuit construction errors, and incorrect algorithm logic.
QuanBench provides a basis for future work on improving quantum code generation
with LLMs.

</details>


### [12] [More with Less: An Empirical Study of Turn-Control Strategies for Efficient Coding Agents](https://arxiv.org/abs/2510.16786)
*Pengfei Gao,Chao Peng*

Main category: cs.SE

TL;DR: 本文系统研究了LLM编码代理的轮次控制策略，发现动态轮次策略在保持性能的同时能显著降低成本。


<details>
  <summary>Details</summary>
Motivation: LLM编码代理在实际部署中面临显著且不可预测的成本问题，主要源于轮次增加导致的令牌数量二次增长、模型价格高昂、任务所需轮次多以及代理效率低下。现有研究主要关注单轮优化，而轮次总数控制策略尚未充分探索。

Method: 在SWE-bench上使用三种最先进模型进行实证研究，评估三种轮次控制策略：无限制基线、带提醒的固定轮次限制、以及按需扩展的新型动态轮次策略。

Result: 发现无限制设置中存在基本权衡，没有单一模型在性能、成本和轮次效率方面表现优异。固定轮次限制在基线75%分位数时是"最佳点"，可大幅降低成本(24%-68%)且对解决率影响最小。动态轮次策略持续优于固定限制方法，在进一步降低成本12%-24%的同时达到相当或更好的解决率。

Conclusion: 动态资源分配是部署强大且经济可行的编码代理的优越且易于实施的方法，为开发者提供了平衡成本与效能的简单有效指南。

Abstract: LLM-powered coding agents, which operate in iterative loops (turns) to solve
software engineering tasks, are becoming increasingly powerful. However, their
practical deployment is hindered by significant and unpredictable costs. This
challenge arises from a combination of factors: quadratically growing token
counts with each turn, the high price of models, the large number of turns
required for real-world tasks, and the tendency of agents to take inefficient
or unnecessary actions. While existing research focuses on optimizing
individual turns, the strategic control of the total number of turns remains an
underexplored area for managing agent performance and cost. To address this
gap, we conduct a comprehensive empirical study on SWE-bench using three
state-of-the-art models and evaluate the impact of three distinct turn-control
strategies: an unrestricted baseline, a fixed-turn limit with reminders, and a
novel dynamic-turn strategy that grants extensions on-demand. Our findings
first reveal a fundamental trade-off in the unrestricted setting, where no
single model excels across performance, cost, and turn efficiency. We then show
that a fixed-turn limit, specifically at the 75th percentile of the baseline,
serves as a "sweet spot", substantially reducing costs (by 24%-68%) with
minimal impact on solve rates. Most significantly, the dynamic-turn strategy
consistently outperforms fixed-limit approaches, achieving comparable or better
solve rates while further reducing costs by an additional 12%-24% by
intelligently allocating resources only to tasks that need them. This work
provides the first systematic analysis of turn-control strategies, offering
simple yet effective guidelines for developers to balance cost and efficacy. We
demonstrate that dynamic resource allocation is a superior, easy-to-implement
approach for deploying powerful yet economically viable coding agents.

</details>


### [13] [When AI Takes the Wheel: Security Analysis of Framework-Constrained Program Generation](https://arxiv.org/abs/2510.16823)
*Yue Liu,Zhenchang Xing,Shidong Pan,Chakkrit Tantithamthavorn*

Main category: cs.SE

TL;DR: 研究发现，LLM生成的Chrome扩展程序存在严重安全漏洞，漏洞率高达18%-50%，特别是在身份验证和Cookie管理场景中漏洞率分别达到83%和78%。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在软件开发中的广泛应用，开发者可能只关注程序功能而忽略实现中的安全问题，特别是在框架约束程序中的安全隐患。

Method: 构建了包含140个基于已知漏洞扩展提示的ChromeSecBench数据集，使用9个最先进的LLM生成完整Chrome扩展，并从场景类型、模型差异和漏洞类别三个维度分析安全性。

Result: LLM生成的程序存在高漏洞率，大多数漏洞会将敏感浏览器数据暴露给不可信代码。令人意外的是，高级推理模型表现更差，比简单模型生成更多漏洞。

Conclusion: LLM的编码能力与编写安全框架约束程序的能力之间存在关键差距，需要加强LLM在安全编程方面的能力。

Abstract: In recent years, the AI wave has grown rapidly in software development. Even
novice developers can now design and generate complex framework-constrained
software systems based on their high-level requirements with the help of Large
Language Models (LLMs). However, when LLMs gradually "take the wheel" of
software development, developers may only check whether the program works. They
often miss security problems hidden in how the generated programs are
implemented.
  In this work, we investigate the security properties of framework-constrained
programs generated by state-of-the-art LLMs. We focus specifically on Chrome
extensions due to their complex security model involving multiple privilege
boundaries and isolated components. To achieve this, we built ChromeSecBench, a
dataset with 140 prompts based on known vulnerable extensions. We used these
prompts to instruct nine state-of-the-art LLMs to generate complete Chrome
extensions, and then analyzed them for vulnerabilities across three dimensions:
scenario types, model differences, and vulnerability categories. Our results
show that LLMs produced vulnerable programs at alarmingly high rates (18%-50%),
particularly in Authentication & Identity and Cookie Management scenarios (up
to 83% and 78% respectively). Most vulnerabilities exposed sensitive browser
data like cookies, history, or bookmarks to untrusted code. Interestingly, we
found that advanced reasoning models performed worse, generating more
vulnerabilities than simpler models. These findings highlight a critical gap
between LLMs' coding skills and their ability to write secure
framework-constrained programs.

</details>


### [14] [Will AI also replace inspectors? Investigating the potential of generative AIs in usability inspection](https://arxiv.org/abs/2510.17056)
*Luis F. G. Campos,Leonardo C. Marques,Walter T. Nakamura*

Main category: cs.SE

TL;DR: 本研究比较了生成式AI与人类专家在可用性检查中的表现，发现AI能发现许多新缺陷但假阳性率较高，AI与人类检查员结合可获得最佳结果。


<details>
  <summary>Details</summary>
Motivation: 可用性检查成本高且需要专业知识，AI技术为支持这一任务提供了新机会，本研究旨在评估AI在识别可用性问题方面的性能。

Method: 使用软件原型，由4名专家和2个AI模型（GPT-4o和Gemini 2.5 Flash）进行评估，采用精确率、召回率和F1分数等指标。

Result: 人类检查员在精确率和整体覆盖率方面表现最佳，AI表现出高个体性能并发现许多新缺陷，但假阳性率和冗余报告较高。AI与人类结合产生最佳结果。

Conclusion: 当前阶段的AI无法替代人类检查员，但可作为有价值的增强工具来提高效率并扩大缺陷覆盖范围，在软件质量评估中具有互补使用价值。

Abstract: Usability inspection is a well-established technique for identifying
interaction issues in software interfaces, thereby contributing to improved
product quality. However, it is a costly process that requires time and
specialized knowledge from inspectors. With advances in Artificial Intelligence
(AI), new opportunities have emerged to support this task, particularly through
generative models capable of interpreting interfaces and performing inspections
more efficiently. This study examines the performance of generative AIs in
identifying usability problems, comparing them to those of experienced human
inspectors. A software prototype was evaluated by four specialists and two AI
models (GPT-4o and Gemini 2.5 Flash), using metrics such as precision, recall,
and F1-score. While inspectors achieved the highest levels of precision and
overall coverage, the AIs demonstrated high individual performance and
discovered many novel defects, but with a higher rate of false positives and
redundant reports. The combination of AIs and human inspectors produced the
best results, revealing their complementarity. These findings suggest that AI,
in its current stage, cannot replace human inspectors but can serve as a
valuable augmentation tool to improve efficiency and expand defect coverage.
The results provide evidence based on quantitative analysis to inform the
discussion on the role of AI in usability inspections, pointing to viable paths
for its complementary use in software quality assessment contexts.

</details>


### [15] [M2QCode: A Model-Driven Framework for Generating Multi-Platform Quantum Programs](https://arxiv.org/abs/2510.17110)
*Xiaoyu Guo,Shinobu Saito,Jianjun Zhao*

Main category: cs.SE

TL;DR: 本文提出了一种基于模型驱动开发（MDD）的方法，用于支持量子系统的结构化设计和实现，能够自动为多种量子编程语言生成代码。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算的发展，量子编程语言不断涌现，但模型驱动开发在量子系统工程中的应用仍未被充分探索。

Method: 开发了一个MDD框架，支持从模型自动生成多种量子编程语言的代码。

Result: 通过多个案例研究证明了该方法的有效性和实用性。

Conclusion: 该MDD方法能够提高量子系统开发效率，并确保在不同量子平台上的一致性。

Abstract: With the growing interest in quantum computing, the emergence of quantum
supremacy has marked a pivotal milestone in the field. As a result, numerous
quantum programming languages (QPLs) have been introduced to support the
development of quantum algorithms. However, the application of Model-Driven
Development (MDD) in quantum system engineering remains largely underexplored.
This paper presents an MDD-based approach to support the structured design and
implementation of quantum systems. Our framework enables the automatic
generation of quantum code for multiple QPLs, thereby enhancing development
efficiency and consistency across heterogeneous quantum platforms. The
effectiveness and practicality of our approach have been demonstrated through
multiple case studies.

</details>


### [16] [SEER: Enhancing Chain-of-Thought Code Generation through Self-Exploring Deep Reasoning](https://arxiv.org/abs/2510.17130)
*Shuzheng Gao,Chaozheng Wang,Cuiyun Gao,Michael R. Lyu*

Main category: cs.SE

TL;DR: SEER是一个自探索深度推理框架，将代码生成的思维链推理构建为决策问题，通过多样化推理路径探索、推理质量感知模型训练和自适应思维链推理来解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有思维链推理方法存在三个关键限制：推理路径多样性有限、缺乏中间推理步骤质量评估、以及'过度思考'可能导致的负面影响。

Method: SEER包含三个核心组件：多样化推理路径探索（无需专家或闭源模型）、推理质量感知模型训练（策略模型生成推理步骤，价值模型评估质量）、自适应思维链推理（根据问题动态切换直接生成与逐步推理）。

Result: 该框架能够实现准确且自适应的代码生成推理，解决了现有方法的局限性。

Conclusion: SEER通过将思维链代码生成构建为决策问题，提供了一种更可靠和灵活的推理框架，能够适应不同的编程场景。

Abstract: Code generation, the task of creating executable programs from natural
language requirements, has recently seen tremendous advances through
Chain-of-Thought (CoT) reasoning, which enables Large Language Models (LLMs) to
develop high-level reasoning plans before writing code. Recent research has
proposed various methods to enhance models' CoT reasoning for code generation
such as prompt engineering and supervised fine-tuning. However, existing
approaches still face three critical limitations: (1) limited exploration of
diverse reasoning paths, which constrains generalization across various
programming scenarios, (2) lack of quality assessment for intermediate
reasoning steps, which hampers the reliability of the generated plans and code,
and (3) the potential negative impact of "overthinking", potentially leading to
unnecessarily complex and incorrect solutions. To address these limitations, we
frame CoT code generation as a decision making problem and present SEER, a
SElf-Exploring deep Reasoning framework that enables accurate and adaptive
reasoning for code generation. SEER introduces three key components: (1)
Diverse reasoning path exploration, which aims at exploring diverse reasoning
paths and annotating intermediate steps without relying on manual experts or
closed-source proprietary models; (2) Reasoning quality-aware model training,
which trains a policy model for generating candidate reasoning steps and a
value model for assessing their quality; and (3) Adaptive CoT reasoning, which
dynamically switches between direct generation and step-by-step reasoning for
different problems.

</details>


### [17] [PEACE: Towards Efficient Project-Level Efficiency Optimization via Hybrid Code Editing](https://arxiv.org/abs/2510.17142)
*Xiaoxue Ren,Jun Wan,Yun Peng,Zhongxin Liu,Ming Liang,Dajun Chen,Wei Jiang,Yong Li*

Main category: cs.SE

TL;DR: Peace是一个用于项目级代码效率优化的混合框架，通过自动代码编辑实现函数间交互优化，在真实项目中显著提升执行效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代码优化方法仅关注函数级优化，忽视了函数间交互，无法适应真实开发场景；代码编辑技术虽具潜力但面临无效编辑和次优内部函数的挑战。

Method: Peace框架包含三个关键阶段：依赖感知的优化函数序列构建、有效关联编辑识别、效率优化编辑迭代，确保项目整体正确性和完整性。

Result: 在PeacExec基准测试中，Peace达到69.2%正确率、+46.9%优化率和0.840执行效率加速，在复杂多函数优化任务中显著优于现有方法。

Conclusion: Peace证明了项目级代码效率优化的可行性，其混合框架设计有效解决了现有方法的局限性，为LLM在代码优化领域的应用开辟了新方向。

Abstract: Large Language Models (LLMs) have demonstrated significant capability in code
generation, but their potential in code efficiency optimization remains
underexplored. Previous LLM-based code efficiency optimization approaches
exclusively focus on function-level optimization and overlook interaction
between functions, failing to generalize to real-world development scenarios.
Code editing techniques show great potential for conducting project-level
optimization, yet they face challenges associated with invalid edits and
suboptimal internal functions. To address these gaps, we propose Peace, a novel
hybrid framework for Project-level code Efficiency optimization through
Automatic Code Editing, which also ensures the overall correctness and
integrity of the project. Peace integrates three key phases: dependency-aware
optimizing function sequence construction, valid associated edits
identification, and efficiency optimization editing iteration. To rigorously
evaluate the effectiveness of Peace, we construct PeacExec, the first benchmark
comprising 146 real-world optimization tasks from 47 high-impact GitHub Python
projects, along with highly qualified test cases and executable environments.
Extensive experiments demonstrate Peace's superiority over the state-of-the-art
baselines, achieving a 69.2% correctness rate (pass@1), +46.9% opt rate, and
0.840 speedup in execution efficiency. Notably, our Peace outperforms all
baselines by significant margins, particularly in complex optimization tasks
with multiple functions. Moreover, extensive experiments are also conducted to
validate the contributions of each component in Peace, as well as the rationale
and effectiveness of our hybrid framework design.

</details>


### [18] [TREAT: A Code LLMs Trustworthiness / Reliability Evaluation and Testing Framework](https://arxiv.org/abs/2510.17163)
*Shuzheng Gao,Eric John Li,Man Ho Lam,Jingyu Xiao,Yuxuan Wan,Chaozheng Wang,Ng Man Tik,Michael R. Lyu*

Main category: cs.SE

TL;DR: TREAT是一个评估代码大模型可信度的框架，通过多任务、多语言、多模态和鲁棒性评估来全面测试模型在软件工程任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估代码大模型可信度方面存在局限，任务范围有限且缺乏对模型鲁棒性和可靠性的评估。

Method: 提出TREAT评估框架，包含四个主要改进：多任务综合评估、多语言多模态评估、鲁棒性评估和严谨评估方法。

Result: 评估了26个最先进模型，发现模型在不同编程任务中性能差异显著，多模态模型在UI代码生成和编辑方面存在特定局限性。

Conclusion: TREAT框架为代码大模型的可信度评估提供了更全面的方法，揭示了当前模型的优势和不足。

Abstract: Large foundation models are fundamentally transforming the software
engineering landscape, demonstrating exceptional capabilities across diverse
tasks such as code generation, debugging, and testing. Despite this rapid
progress, a significant gap remains in how to comprehensively evaluate these
models' trustworthiness in real-world software engineering scenarios. Existing
benchmarks suffer from limited task scope and fail to incorporate critical
evaluation aspects such as the robustness and reliability of models. To bridge
this gap, we present an evaluation framework called TREAT (Code LLMs
Trustworthiness / Reliability Evaluation And Testing) that provides a holistic
assessment of model performance in code intelligence tasks. Our evaluation
framework addresses key limitations in existing approaches with four main
improvements: (1) Multi-Task Holistic Evaluation that spans diverse software
engineering activities rather than limited coding tasks; (2) Multi-Language and
Multi-Modality Assessment that extends beyond traditional single-language,
text-only benchmarks to include multi-modality coding tasks; (3) Robustness
Assessment that evaluates model reliability under semantically-preserving code
transformations; and (4) Rigorous Evaluation Methodology that enhances the
trustworthiness of evaluation results through diverse evaluation prompts and
adaptive solution extraction. Based on this evaluation framework, we assess 26
state-of-the-art models and uncover both their strengths and limitations,
yielding several key insights:(1) Current models show substantial performance
variation across programming tasks; (2) Multi-modal language models demonstrate
specific performance limitations in UI code generation and edit;

</details>


### [19] [Software Testing with Large Language Models: An Interview Study with Practitioners](https://arxiv.org/abs/2510.17164)
*Maria Deolinda Santana,Cleyton Magalhaes,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 本研究通过定性访谈提出了软件测试中LLM使用的初步指南，强调迭代式提示工程、人工监督和验证的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在软件测试中的快速应用，目前缺乏结构化指导，主要依赖非正式实验。本研究旨在了解测试专业人员如何实际使用LLM，为集成到测试工作流提供实践指导。

Method: 对15名来自不同角色和领域的软件测试人员进行半结构化访谈，使用基于扎根理论的主题分析方法。

Result: 测试人员描述了包括定义测试目标、应用提示工程策略、优化提示、评估输出和持续学习的迭代反思过程，强调需要人工监督和仔细验证。

Conclusion: LLM在软件测试中的应用正在增长，但仍受到实践演变和对风险谨慎态度的影响。本研究为结构化使用LLM提供了起点，并邀请未来研究进一步完善实践。

Abstract: \textit{Background:} The use of large language models in software testing is
growing fast as they support numerous tasks, from test case generation to
automation, and documentation. However, their adoption often relies on informal
experimentation rather than structured guidance. \textit{Aims:} This study
investigates how software testing professionals use LLMs in practice to propose
a preliminary, practitioner-informed guideline to support their integration
into testing workflows. \textit{Method:} We conducted a qualitative study with
15 software testers from diverse roles and domains. Data were collected through
semi-structured interviews and analyzed using grounded theory-based processes
focused on thematic analysis. \textit{Results:} Testers described an iterative
and reflective process that included defining testing objectives, applying
prompt engineering strategies, refining prompts, evaluating outputs, and
learning over time. They emphasized the need for human oversight and careful
validation, especially due to known limitations of LLMs such as hallucinations
and inconsistent reasoning. \textit{Conclusions:} LLM adoption in software
testing is growing, but remains shaped by evolving practices and caution around
risks. This study offers a starting point for structuring LLM use in testing
contexts and invites future research to refine these practices across teams,
tools, and tasks.

</details>


### [20] [OLIVAW: ACIMOV's GitHub robot assisting agile collaborative ontology development](https://arxiv.org/abs/2510.17184)
*Nicolas Robert,Fabien Gandon,Maxime Lefrançois*

Main category: cs.SE

TL;DR: OLIVAW是一个支持ACIMOV方法论的GitHub工具，用于敏捷协作式本体开发，通过W3C标准帮助开发模块化本体。


<details>
  <summary>Details</summary>
Motivation: 敏捷协作的本体设计方法对于确保本体用户驱动、保持更新并随系统演进至关重要，因此需要持续验证工具来保证本体始终符合开发者需求。

Method: OLIVAW基于W3C标准，通过GitHub Composite Actions、预提交钩子或命令行界面来支持模块化本体的开发。

Result: OLIVAW在多个本体项目中进行了测试，证明了其有用性、通用性和可重用性，并提供了模板仓库以便快速上手。

Conclusion: OLIVAW是一个有效的工具，支持在GitHub上进行敏捷协作的本体开发，确保本体质量并促进其持续演进。

Abstract: Agile and collaborative approaches to ontologies design are crucial because
they contribute to making them userdriven, up-to-date, and able to evolve
alongside the systems they support, hence proper continuous validation tooling
is required to ensure ontologies match developers' requirements all along their
development. We propose OLIVAW (Ontology Long-lived Integration Via ACIMOV
Workflow), a tool supporting the ACIMOV methodology on GitHub. It relies on W3C
Standards to assist the development of modular ontologies through GitHub
Composite Actions, pre-commit hooks, or a command line interface. OLIVAW was
tested on several ontology projects to ensure its usefulness, genericity and
reusability. A template repository is available for a quick start. OLIVAW is

</details>


### [21] [AdapTrack: Constrained Decoding without Distorting LLM's Output Intent](https://arxiv.org/abs/2510.17376)
*Yongmin Li,Jia Li,Ge Li,Zhi Jin*

Main category: cs.SE

TL;DR: AdapTrack是一种改进的约束解码方法，通过引入回溯机制避免扭曲语言模型的输出意图，在保持约束合规的同时更好地与模型语义对齐。


<details>
  <summary>Details</summary>
Motivation: 传统的约束解码技术虽然能确保生成的代码满足约束条件，但会扭曲模型的输出意图，导致生成的代码虽然符合约束但不正确。

Method: AdapTrack在生成过程中引入回溯机制，避免强制模型选择违反其意图的选项，确保生成的代码既满足约束又符合模型语义。

Result: 在API补全数据集上，AdapTrack相比约束解码提升360.87%；在真实API补全数据集上提升38.93%；在HumanEval和MBPP基准测试中分别提升7.84%和6.42%。

Conclusion: AdapTrack通过更好地遵循模型输出意图，在保持约束合规的同时显著提升代码生成质量，理论证明其分布与模型分布一致。

Abstract: Language model-based code generation and completion tools have been widely
adopted, but they may sometimes produce code that does not meet necessary
constraints, such as syntactic correctness or API existence. Constrained
decoding techniques are developed to help the model generate code adhering to
the constraints by greedily eliminating generation options that violate
constraints at each step of the generation process. However, there is a severe
limitation of constrained decoding, that it distorts the model's output intent,
forcing it to produce code that may satisfy the constraint but does not match
the development intent and is therefore incorrect. In response to this
challenge, we propose AdapTrack. By incorporating backtracking into the
generation process, AdapTrack avoids distorting the output intent of the model,
thereby producing results that are not only constraint-compliant but also more
semantically aligned with model's output intent. On our synthetic API
completion dataset, AdapTrack can achieve up to 360.87% improvement compared to
constrained decoding; on the real-world API completion dataset we collect that
exhibits similar issues, AdapTrack can achieve up to 38.93% improvement over
constrained decoding; in general code genration benchmarks, compared to
constrained decoding, AdapTrack can achieve up to 7.84% improvement on
HumanEval, and up to 6.42% improvement on MBPP. This indicates that, simply by
better adhering to the model's output intent, AdapTrack can achieve significant
improvements. We provide a theoretical proof that the distribution produced by
AdapTrack aligns with the model's distribution given the generated tokens,
thereby ensuring that the model's output intent is not distorted. Experiments
on DSL problems show that, compared to existing methods, our approach can
provide generation results that are more consistent with the language model's
distribution.

</details>


### [22] [Scalable CI/CD for Legacy Modernization: An Industrial Experience Addressing Internal Challenges Related to the 2025 Japan Cliff](https://arxiv.org/abs/2510.17430)
*Kuniaki Kudo,Sherine Devi*

Main category: cs.SE

TL;DR: 开发了可扩展的CI/CD流水线来解决日本2025年悬崖问题，通过动态创建隔离开发环境来降低遗留IT系统的维护成本并推动数字化转型。


<details>
  <summary>Details</summary>
Motivation: 日本面临2025年悬崖问题，大量遗留核心IT系统即将达到服务寿命终点，导致维护成本激增和数字化转型受阻。朝日公司也面临类似挑战，手动维护流程和有限的QA环境使关键系统难以更新。

Method: 设计并实施了可扩展的CI/CD流水线，整合GitHub进行源代码控制、Jenkins实现流水线自动化、AWS提供可扩展环境、Docker实现环境容器化，支持动态创建和删除隔离开发环境。

Result: 通过可扩展CI/CD流水线，开发人员可以在自己的环境中自由安全地测试维护程序和实验新技术，显著降低了维护成本。

Conclusion: 可扩展CI/CD流水线有效解决了遗留IT系统现代化问题，为应对日本2025年悬崖问题提供了可行方案，推动了数字化转型进程。

Abstract: We have developed a Scalable CI/CD Pipeline to address internal challenges
related to Japan 2025 cliff problem, a critical issue where the mass end of
service life of legacy core IT systems threatens to significantly increase the
maintenance cost and black box nature of these system also leads to difficult
update moreover replace, which leads to lack of progress in Digital
Transformation (DX). If not addressed, Japan could potentially lose up to 12
trillion yen per year after 2025, which is 3 times more than the cost in
previous years. Asahi also faced the same internal challenges regarding legacy
system, where manual maintenance workflows and limited QA environment have left
critical systems outdated and difficult to update. Middleware and OS version
have remained unchanged for years, leading to now its nearing end of service
life which require huge maintenance cost and effort to continue its operation.
To address this problem, we have developed and implemented a Scalable CI/CD
Pipeline where isolated development environments can be created and deleted
dynamically and is scalable as needed. This Scalable CI/CD Pipeline incorporate
GitHub for source code control and branching, Jenkins for pipeline automation,
Amazon Web Services for scalable environment, and Docker for environment
containerization. This paper presents the design and architecture of the
Scalable CI/CD Pipeline, with the implementation along with some use cases.
Through Scalable CI/CD, developers can freely and safely test maintenance
procedures and do experiments with new technology in their own environment,
reducing maintenance cost and drive Digital Transformation (DX).
  key words: 2025 Japan Cliff, Scalable CI/CD, DevOps, Legacy IT Modernization.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [23] [Six Proofs of Interpolation for the Modal Logic K](https://arxiv.org/abs/2510.16398)
*Nick Bezhanishvili,Balder ten Cate,Rosalie Iemhoff*

Main category: cs.LO

TL;DR: 本文提供了模态逻辑K的Craig插值定理的六种不同证明方法，分别基于模型论、证明论、句法、自动机理论、准模型和代数方法，并比较了各种证明技术的优缺点。


<details>
  <summary>Details</summary>
Motivation: 研究模态逻辑K的Craig插值定理的多种证明方法，旨在展示不同数学工具在证明同一重要逻辑定理时的应用和比较。

Method: 使用六种不同的证明技术：模型论方法、证明论方法、句法方法、自动机理论方法、准模型方法和代数方法。

Result: 成功地为模态逻辑K的Craig插值定理提供了六种不同的完整证明。

Conclusion: 不同证明技术各有优缺点，展示了数学工具多样性在逻辑定理证明中的价值，为理解Craig插值定理提供了多角度视角。

Abstract: In this chapter, we present six different proofs of Craig interpolation for
the modal logic K, each using a different set of techniques (model-theoretic,
proof-theoretic, syntactic, automata-theoretic, using quasi-models, and
algebraic). We compare the pros and cons of each proof technique.

</details>


### [24] [Explainability Requirements as Hyperproperties](https://arxiv.org/abs/2510.16402)
*Bernd Finkbeiner,Julian Siber*

Main category: cs.LO

TL;DR: 本文提出了一种基于超属性的形式化方法来验证多智能体系统中的反事实可解释性，结合了三种模态逻辑并证明了其模型检测问题的可判定性。


<details>
  <summary>Details</summary>
Motivation: 可解释性正成为自主系统的关键需求，但目前很少有工作将可解释性形式化为系统属性，本文旨在填补这一空白。

Method: 结合Lewis反事实逻辑、线性时序逻辑和知识模态逻辑，构建了一种用于指定和验证多智能体系统反事实可解释性的逻辑框架，并将其嵌入到超逻辑中。

Result: 开发了用于形式化系统层面可解释性概念的新逻辑，并证明了该逻辑的模型检测问题是可判定的。

Conclusion: 该方法为自动化验证可解释性需求铺平了道路，为自主系统的可解释性提供了形式化基础。

Abstract: Explainability is emerging as a key requirement for autonomous systems. While
many works have focused on what constitutes a valid explanation, few have
considered formalizing explainability as a system property. In this work, we
approach this problem from the perspective of hyperproperties. We start with a
combination of three prominent flavors of modal logic and show how they can be
used for specifying and verifying counterfactual explainability in multi-agent
systems: With Lewis' counterfactuals, linear-time temporal logic, and a
knowledge modality, we can reason about whether agents know why a specific
observation occurs, i.e., whether that observation is explainable to them. We
use this logic to formalize multiple notions of explainability on the system
level. We then show how this logic can be embedded into a hyperlogic. Notably,
from this analysis we conclude that the model-checking problem of our logic is
decidable, which paves the way for the automated verification of explainability
requirements.

</details>


### [25] [Bilateralist base-extension semantics with incompatible proofs and refutations](https://arxiv.org/abs/2510.16763)
*Victor Barroso-Nascimento,Maria Osório Costa,Elaine Pimentel*

Main category: cs.LO

TL;DR: 本文提出了一个双边逻辑系统，其中公式不能同时可证明和可反驳，为建模排除不一致性的认知实体（如数学证明和反驳）提供了框架。


<details>
  <summary>Details</summary>
Motivation: 挑战传统逻辑概念，将断言和否认视为独立但对立的行为，旨在为经典逻辑提供正当性，同时展示两种行为都允许直觉主义解释。

Method: 通过具有良好证明论性质（包括正规化）的双边自然演绎系统形式化逻辑，并引入基扩展语义，要求明确构造证明和反驳，同时防止它们对同一公式成立。

Result: 证明了语义相对于演算的健全性和完备性，并表明反驳概念对应于David Nelson的构造性虚假，扩展而非修正直觉主义逻辑。

Conclusion: 该系统强化了表示构造性认知推理的适用性，为建模数学证明和反驳等认知实体提供了合适的框架。

Abstract: Logical bilateralism challenges traditional concepts of logic by treating
assertion and denial as independent yet opposed acts. While initially devised
to justify classical logic, its constructive variants show that both acts admit
intuitionistic interpretations. This paper presents a bilateral system where a
formula cannot be both provable and refutable without contradiction, offering a
framework for modelling epistemic entities, such as mathematical proofs and
refutations, that exclude inconsistency.
  The logic is formalised through a bilateral natural deduction system with
desirable proof-theoretic properties, including normalisation. We also
introduce a base-extension semantics requiring explicit constructions of proofs
and refutations while preventing them from being established for the same
formula. The semantics is proven sound and complete with respect to the
calculus. Finally, we show that our notion of refutation corresponds to David
Nelson's constructive falsity, extending rather than revising intuitionistic
logic and reinforcing the system's suitability for representing constructive
epistemic reasoning.

</details>


### [26] [ATL*AS: An Automata-Theoretic Approach and Tool for the Verification of Strategic Abilities in Multi-Agent Systems](https://arxiv.org/abs/2510.17306)
*Sofia Garcia de Blas Garcia-Alcalde,Francesco Belardinelli*

Main category: cs.LO

TL;DR: 提出了两种新的符号算法用于模型检查ATL*逻辑，包括无限迹和有限迹语义。通过符号化方法显著优于显式状态表示，其中基于奇偶博弈的算法在无限迹验证中表现更佳。


<details>
  <summary>Details</summary>
Motivation: 为多智能体系统提供全面的ATL*规范验证工具集，解决现有工具在可扩展性和效率方面的不足。

Method: 设计了符号化方法，包括基于奇偶博弈的无限迹验证算法和有限迹验证方法，并在ATL*AS模型检查器中实现。

Result: 符号化方法显著优于显式状态表示，基于奇偶博弈的算法在无限迹验证中更高效可扩展，有限迹模型检查比无限迹验证性能更好。

Conclusion: 提供了全面的ATL*验证工具集，符号化方法特别是基于奇偶博弈的算法为多智能体系统验证提供了高效解决方案。

Abstract: We present two novel symbolic algorithms for model checking the
Alternating-time Temporal Logic ATL*, over both the infinite-trace and the
finite-trace semantics. In particular, for infinite traces we design a novel
symbolic reduction to parity games. We implement both methods in the ATL*AS
model checker and evaluate it using synthetic benchmarks as well as a
cybersecurity scenario. Our results demonstrate that the symbolic approach
significantly outperforms the explicit-state representation and we find that
our parity-game-based algorithm offers a more scalable and efficient solution
for infinite-trace verification, outperforming previously available tools. Our
results also confirm that finite-trace model checking yields substantial
performance benefits over infinite-trace verification. As such, we provide a
comprehensive toolset for verifying multiagent systems against specifications
in ATL*.

</details>


### [27] [A Judgmental Construction of Directed Type Theory](https://arxiv.org/abs/2510.17494)
*Jacob Neumann*

Main category: cs.LO

TL;DR: 将定向类型论重新表述为具有多个上下文区域的逻辑演算，引入中性和极性变量，并应用于重写理论，同时发展了对偶上下文系统的范畴语义。


<details>
  <summary>Details</summary>
Motivation: 将定向类型论（类型具有合成范畴结构的类型论）重新表述为更清晰的逻辑演算形式，以便更好地表达重写理论中的概念，并建立对偶上下文系统的语义基础。

Method: 使用多上下文区域逻辑演算，引入中性和极性两种变量类型，构建对偶CwF作为模型理论基础，并应用于合成预序的低维版本理论。

Result: 成功将定向类型论重新表述为多上下文逻辑演算，建立了对偶上下文系统的范畴语义框架，并展示了在重写理论中的应用。

Conclusion: 多上下文区域方法为定向类型论提供了更清晰的逻辑表述，对偶CwF为这类逻辑的模型理论提供了统一的语义基础，在重写理论中具有应用价值。

Abstract: We reformulate recent advances in directed type theory--a type theory where
the types have the structure of synthetic (higher) categories--as a logical
calculus with multiple context 'zones', following the example of Pfenning and
Davies. This allows us to have two kinds of variables--'neutral' and
'polar'--with different functoriality requirements. We focus on the
lowest-dimension version of this theory (where types are synthetic preorders)
and apply the logical language to articulate concepts from the theory of
rewriting. We also take the occasion to develop the categorical semantics of
dual-context systems, proposing a notion of dual CwF to serve as a common
structural base for the model theories of such logics.

</details>


### [28] [Just-In-Time Piecewise-Linear Semantics for ReLU-type Networks](https://arxiv.org/abs/2510.17622)
*Hongyi Duan,Haoyang Liu,Jian'an Zhang,Fengrui Liu,Yiyi Wang*

Main category: cs.LO

TL;DR: 提出了一种用于ReLU型网络的即时编译PL语义，将模型编译为带有共享守卫的分段线性转换器，实现任意时间正确性、精确性和单调进展。


<details>
  <summary>Details</summary>
Motivation: 为ReLU型神经网络提供精确的语义分析和验证方法，支持区域提取、决策复杂性分析、雅可比矩阵计算、精确Lipschitz常数认证以及鲁棒性验证等功能。

Method: 使用带共享守卫的分段线性转换器，仅在操作数在当前单元上为仿射时添加超平面，维护全局上下包络，采用预算分支定界法。

Result: 实现了任意时间正确性、完全细化单元上的精确性、单调进展、守卫线性复杂度、优势剪枝以及在有限细化下的可判定性。

Conclusion: 该系统为神经网络分析提供了强大的工具，支持多种验证和分析任务，原型实现能够以与访问子域成本成比例的方式返回证书或反例。

Abstract: We present a JIT PL semantics for ReLU-type networks that compiles models
into a guarded CPWL transducer with shared guards. The system adds hyperplanes
only when operands are affine on the current cell, maintains global lower/upper
envelopes, and uses a budgeted branch-and-bound. We obtain anytime soundness,
exactness on fully refined cells, monotone progress, guard-linear complexity
(avoiding global $\binom{k}{2}$), dominance pruning, and decidability under
finite refinement. The shared carrier supports region extraction, decision
complexes, Jacobians, exact/certified Lipschitz, LP/SOCP robustness, and
maximal causal influence. A minimal prototype returns certificates or
counterexamples with cost proportional to visited subdomains.

</details>


### [29] [A Mimamsa Inspired Framework For Instruction Sequencing In AI Agents](https://arxiv.org/abs/2510.17691)
*Bama Srinivasan*

Main category: cs.LO

TL;DR: 提出基于印度哲学Mimamsa系统的AI指令排序框架，通过动作对象对实现三种排序机制：直接断言、目的驱动排序和迭代程序，并建立了形式化验证系统。


<details>
  <summary>Details</summary>
Motivation: 受印度哲学Mimamsa系统启发，解决AI代理中指令排序的可靠性和形式化验证问题，特别是在任务规划和机器人应用中处理时间推理和依赖建模。

Method: 扩展MIRA形式化系统，引入动作对象命令逻辑的语法和语义，定义三种排序机制：Srutikrama（直接断言）、Arthakrama（目的驱动）和Pravrittikrama（迭代程序），并建立明确的演绎规则。

Result: 基于对象依赖关系建立了指令排序正确性的验证定理，证明了系统的可靠性和完备性，为AI应用提供了可靠的指令排序能力。

Conclusion: 该框架通过形式化验证实现了可靠的指令排序，能够显著影响AI在任务规划和机器人等领域的应用，有效解决了时间推理和依赖建模问题。

Abstract: This paper presents a formal framework for sequencing instructions in AI
agents, inspired by the Indian philosophical system of Mimamsa. The framework
formalizes sequencing mechanisms through action object pairs in three distinct
ways: direct assertion (Srutikrama) for temporal precedence, purpose driven
sequencing (Arthakrama) for functional dependencies, and iterative procedures
(Pravrittikrama) for distinguishing between parallel and sequential execution
in repetitive tasks. It introduces the syntax and semantics of an action object
imperative logic, extending the MIRA formalism (Srinivasan and Parthasarathi,
2021) with explicit deduction rules for sequencing. The correctness of
instruction sequencing is established through a validated theorem, which is
based on object dependencies across successive instructions. This is further
supported by proofs of soundness and completeness. This formal verification
enables reliable instruction sequencing, impacting AI applications across areas
like task planning and robotics by addressing temporal reasoning and dependency
modeling.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [30] [Inference of Deterministic Finite Automata via Q-Learning](https://arxiv.org/abs/2510.17386)
*Elaheh Hosseinkhani,Martin Leucker*

Main category: cs.FL

TL;DR: 使用Q学习算法进行确定性有限自动机(DFA)的被动推断，将Q函数重新解释为DFA的转移函数，在子符号学习和符号表示之间建立桥梁。


<details>
  <summary>Details</summary>
Motivation: 传统DFA推断方法基于符号AI，而子符号AI（特别是机器学习）提供了从数据中学习的替代范式。本文探索使用强化学习中的Q学习算法进行DFA被动推断的可能性。

Method: 将Q学习算法应用于自动机推断，核心洞察是将学习到的Q函数（映射状态-动作对到奖励）重新解释为有限域上DFA的转移函数。

Result: 在多个示例上进行了评估，展示了Q学习如何适应自动机推断任务。

Conclusion: Q学习为DFA被动推断提供了一种新颖的方法，在子符号学习和符号表示之间建立了桥梁。

Abstract: Traditional approaches to inference of deterministic finite-state automata
(DFA) stem from symbolic AI, including both active learning methods (e.g.,
Angluin's L* algorithm and its variants) and passive techniques (e.g., Biermann
and Feldman's method, RPNI). Meanwhile, sub-symbolic AI, particularly machine
learning, offers alternative paradigms for learning from data, such as
supervised, unsupervised, and reinforcement learning (RL). This paper
investigates the use of Q-learning, a well-known reinforcement learning
algorithm, for the passive inference of deterministic finite automata. It
builds on the core insight that the learned Q-function, which maps state-action
pairs to rewards, can be reinterpreted as the transition function of a DFA over
a finite domain. This provides a novel bridge between sub-symbolic learning and
symbolic representations. The paper demonstrates how Q-learning can be adapted
for automaton inference and provides an evaluation on several examples.

</details>


### [31] [Castor Ministerialis](https://arxiv.org/abs/2510.17438)
*Christian Hercher*

Main category: cs.FL

TL;DR: 该论文研究了一个Busy Beaver问题的变体：只考虑从空白磁带开始并在空白磁带上停止的图灵机。作者给出了最多5个状态图灵机的明确答案，分析了6状态候选机的行为，并探讨了多符号字母表的推广。


<details>
  <summary>Details</summary>
Motivation: 研究Busy Beaver问题的变体，限制图灵机必须从空白磁带开始并在空白磁带上停止，这为理解图灵机行为提供了新的视角。

Method: 通过分析图灵机的状态转换和运行行为，系统地研究从空白磁带开始并在空白磁带上停止的图灵机的最长运行时间。

Result: 给出了最多5个状态图灵机的明确最长运行时间，分析了6状态候选机的行为，并提供了多符号字母表推广的初步发现。

Conclusion: 这种Busy Beaver问题的变体为理解图灵机行为提供了有价值的见解，特别是在限制起始和停止条件为空白磁带的情况下。

Abstract: The famous problem of Busy Beavers can be stated as the question on how long
a $n$-state Turing machine (using a 2-symbol alphabet or -- in a generalization
-- a $m$-symbol alphabet) can run if it is started on the blank tape before it
holds. Thus, not halting Turing machines are excluded. For up to four states
the answer to this question is well-known. Recently, it could be verified that
the widely assumed candidate for five states is in fact the champion. And there
is progress in searching for good candidates with six or more states.
  We investigate a variant of this problem: Additionally to the requirement
that the Turing machines have to start from the blank tape we only consider
such Turing machines that hold on the blank tape, too. For this variant we give
definitive answers on how long such a Turing machine with up to five states can
run, analyze the behavior of a six-states candidate and give some findings on
the generalization of Turing-machines with $m$-symbol alphabet.

</details>


### [32] [Non-interference analysis of bounded labeled Petri nets](https://arxiv.org/abs/2510.17582)
*Ning Ran,Zhengguang Wu,Shaokang Zhang,Zhou He,Carla Seatzu*

Main category: cs.FL

TL;DR: 本文研究了有界标记Petri网的非干涉分析问题，提出了使用SNNI验证器的必要充分条件来验证强非确定性非干涉属性。


<details>
  <summary>Details</summary>
Motivation: 在分层控制系统安全中，不同级别用户对系统输出的观察能力不同，需要确保低级别用户无法通过观察低级输出来推断高级别事件的发生。

Method: 使用一种特殊的自动机——SNNI验证器，来分析系统的强非确定性非干涉属性。

Result: 提出了验证SNNI属性的必要充分条件。

Conclusion: 通过SNNI验证器可以有效分析有界标记Petri网的非干涉安全性。

Abstract: This paper focuses on a fundamental problem on information security of
bounded labeled Petri nets: non-interference analysis. As in hierarchical
control, we assume that a system is observed by users at different levels,
namely high-level users and low-level users. The output events produced by the
firing of transitions are also partitioned into high-level output events and
low-level output events. In general, high-level users can observe the
occurrence of all the output events, while low-level users can only observe the
occurrence of low-level output events. A system is said to be non-interferent
if low-level users cannot infer the firing of transitions labeled with
high-level output events by looking at low-level outputs. In this paper, we
study a particular non-interference property, namely strong non-deterministic
non-interference (SNNI), using a special automaton called SNNI Verifier, and
propose a necessary and sufficient condition for SNNI.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [33] [Latency Based Tiling](https://arxiv.org/abs/2510.15912)
*Jack Cashman*

Main category: cs.PL

TL;DR: Latency Based Tiling是一种系统方法，通过三角循环分析机器缓存缺失率缩放特性，快速推导近似分块方案，在保持快速编译时间的同时最大化数据局部性。


<details>
  <summary>Details</summary>
Motivation: 传统自动调优方法虽然有效但编译时间过长，需要一种能够快速确定缓存层次结构并实现高效分块的轻量级方法。

Method: 使用三角循环分析缓存缺失率缩放，通过延迟突增识别L1、L2、L3缓存大小，结合多面体模型对循环嵌套进行基于内存层次结构和数据足迹的分块。

Result: 该方法实现了可忽略的编译时间开销，提供了硬件无关的便携式解决方案，在Rust支持的任何平台上都能运行。

Conclusion: Latency Based Tiling提供了一种快速、便携的分块方法，能够在多进程共享缓存环境中有效工作，是自动调优的轻量级替代方案。

Abstract: Latency Based Tiling provides a systems based approach to deriving
approximate tiling solution that maximizes locality while maintaining a fast
compile time. The method uses triangular loops to characterize miss ratio
scaling of a machine avoiding prefetcher distortion. Miss ratio scaling
captures the relationship between data access latency and working set size with
sharp increases in latency indicating the data footprint exceeds capacity from
a cache level. Through these noticeable increases in latency we can determine
an approximate location for L1, L2, and L3 memory sizes. These sizes are
expected to be under approximations of a systems true memory sizes which is in
line with our expectations given the shared nature of cache in a multi process
system as described in defensive loop tiling. Unlike auto tuning, which can be
effective but prohibitively slow, Latency Based Tiling achieves negligible
compile time overhead. The implementation in Rust enables a hardware agnostic
approach which combined with a cache timing based techniques, yields a
portable, memory safe system running wherever Rust is supported. The tiling
strategy is applied to a subset of the polyhedral model, where loop nestings
are tiled based on both the derived memory hierarchy and the observed data
footprint per iteration.

</details>


### [34] [Typing Strictness (Extended Version)](https://arxiv.org/abs/2510.16133)
*Daniel Sainati,Joseph W. Cutler,Benjamin C. Pierce,Stephanie Weirich*

Main category: cs.PL

TL;DR: 提出了一种新的严格性定义，通过更精确描述变量使用来改进传统定义，在按名调用和按值推送调用设置中建立了类型理论基础，并通过逻辑关系证明了严格性属性能准确描述运行时变量使用。


<details>
  <summary>Details</summary>
Motivation: 严格性分析对于非严格求值语言的高效实现至关重要，但源级别的严格性推理具有挑战性和反直觉性。

Method: 基于跟踪效果和共效果的类型系统文献，在按名调用和按值推送调用设置中建立了类型理论基础，并通过逻辑关系证明严格性属性的准确性。

Result: 开发了新的严格性定义和类型系统，能够更精确地描述变量使用，并提供了严格性注释保持的翻译机制。

Conclusion: 所提出的严格性定义和类型系统为严格性分析提供了更精确的理论基础，所有结果已在Rocq中机械化验证。

Abstract: Strictness analysis is critical to efficient implementation of languages with
non-strict evaluation, mitigating much of the performance overhead of laziness.
However, reasoning about strictness at the source level can be challenging and
unintuitive. We propose a new definition of strictness that refines the
traditional one by describing variable usage more precisely. We lay
type-theoretic foundations for this definition in both call-by-name and
call-by-push-value settings, drawing inspiration from the literature on type
systems tracking effects and coeffects. We prove via a logical relation that
the strictness attributes computed by our type systems accurately describe the
use of variables at runtime, and we offer a strictness-annotation-preserving
translation from the call-by-name system to the call-by-push-value one. All our
results are mechanized in Rocq.

</details>


### [35] [SimpliPy: A Source-Tracking Notional Machine for Simplified Python](https://arxiv.org/abs/2510.16594)
*Moida Praneeth Jain,Venkatesh Choppella*

Main category: cs.PL

TL;DR: SimpliPy是一个为Python子集设计的教学性概念机器，通过精确的操作语义和静态分析来澄清程序执行中的控制流和作用域概念，并提供了基于web的交互式调试器。


<details>
  <summary>Details</summary>
Motivation: 解决初学者对程序执行的误解问题，特别是控制流和作用域等核心概念的理解困难。

Method: 开发SimpliPy概念机器，包含精确的操作语义（明确跟踪源代码行号）、静态分析生成控制流图和词法作用域识别，并构建交互式web调试器进行可视化。

Result: 实现了将形式语义、程序分析和可视化集成在一起的教学工具，能够清晰地展示代码与执行行为之间的联系。

Conclusion: SimpliPy成功地将形式方法应用于程序理解教学，提供了一个结合形式语义、程序分析和可视化的综合教学方法和实践演示。

Abstract: Misconceptions about program execution hinder many novice programmers. We
introduce SimpliPy, a notional machine designed around a carefully chosen
Python subset to clarify core control flow and scoping concepts. Its foundation
is a precise operational semantics that explicitly tracks source code line
numbers for each execution step, making the link between code and behavior
unambiguous. Complementing the dynamic semantics, SimpliPy uses static analysis
to generate Control Flow Graphs (CFGs) and identify lexical scopes, helping
students build a structural understanding before tracing. We also present an
interactive web-based debugger built on these principles. This tool embodies
the formal techniques, visualizing the operational state (environments, stack)
and using the static CFG to animate control flow directly on the graph during
step-by-step execution. SimpliPy thus integrates formal semantics, program
analysis, and visualization to offer both a pedagogical approach and a
practical demonstration of applying formal methods to program understanding.

</details>


### [36] [JAX Autodiff from a Linear Logic Perspective (Extended Version)](https://arxiv.org/abs/2510.16883)
*Giulia Giusti,Michele Pagani*

Main category: cs.PL

TL;DR: 该论文将Autodiff编码到线性λ演算中，建立了与Girard线性逻辑的Curry-Howard对应关系，证明了编码的定性和定量正确性，并发现反向传播中的unzipping变换是可选的。


<details>
  <summary>Details</summary>
Motivation: Radul等人的Autodiff形式化虽然能表达主要程序变换，但其类型系统非常特定于该任务，不清楚是否构成有独立意义的子结构逻辑。

Method: 提出将Autodiff编码到线性λ演算中，该演算与Girard线性逻辑具有Curry-Howard对应关系。

Result: 证明了编码的定性正确性（编码项与原始项外延等价）和定量正确性（保留原始工作成本），并发现unzipping变换是可选的。

Conclusion: Autodiff可以成功编码到线性逻辑中，这为理解自动微分提供了新的逻辑基础，并简化了反向传播的实现。

Abstract: Autodiff refers to the core of the automatic differentiation systems
developed in projects like JAX and Dex. Autodiff has recently been formalised
in a linear typed calculus by Radul et al in arXiv:2204.10923. Although this
formalisation suffices to express the main program transformations of Autodiff,
the calculus is very specific to this task, and it is not clear whether the
type system yields a substructural logic that has interest on its own.
  We propose an encoding of Autodiff into a linear $\lambda$-calculus that
enjoys a Curry-Howard correspondence with Girard's linear logic. We prove that
the encoding is sound both qualitatively (the encoded terms are extensionally
equivalent to the original ones) and quantitatively (the encoding preserves the
original work cost as described in arXiv:2204.10923). As a byproduct, we show
that unzipping, one of the transformations used to implement backpropagation in
Autodiff, is, in fact, optional.

</details>


### [37] [Introducing Linear Implication Types to $λ_{GT}$ for Computing With Incomplete Graphs](https://arxiv.org/abs/2510.17429)
*Jin Sano,Naoki Yamamoto,Kazunori Ueda*

Main category: cs.PL

TL;DR: 该论文通过在线性类型系统中引入线性蕴涵，解决了λ_GT语言类型系统的两个关键问题：支持不完整图结构和消除模式匹配中的动态类型检查。


<details>
  <summary>Details</summary>
Motivation: 现有λ_GT语言的类型系统存在两个主要缺陷：不支持不完整图结构（即用户定义类型中缺失某些元素的图），以及在模式匹配时依赖动态类型检查。这些问题限制了语言的表达能力和安全性。

Method: 在λ_GT的类型系统中引入线性蕴涵，并添加新的约束条件来确保类型系统的可靠性。这种方法使得类型系统能够处理不完整图结构，同时消除动态类型检查的需求。

Result: 提出的改进类型系统能够支持不完整图结构，并通过静态类型检查保证模式匹配的安全性，从而提高了语言的表达能力和可靠性。

Conclusion: 通过在线性类型系统中集成线性蕴涵，成功解决了λ_GT语言的两个关键类型系统问题，为实现更安全、更表达性的图操作编程语言提供了理论基础。

Abstract: Designing programming languages that enable intuitive and safe manipulation
of data structures is a critical research challenge. Conventional destructive
memory operations using pointers are complex and prone to errors. Existing type
systems, such as affine types and shape types, address this problem towards
safe manipulation of heaps and pointers, but design of high-level declarative
languages that allow us to manipulate complex pointer data structures at a
higher level of abstraction is largely an open problem. The $\lambda_{GT}$
language, a purely functional programming language that treats hypergraphs
(hereafter referred to as graphs) as primary data structures, addresses some of
these challenges. By abstracting data with shared references and cycles as
graphs, it enables declarative operations through pattern matching and
leverages its type system to guarantee safety of these operations.
Nevertheless, the previously proposed type system of $\lambda_{GT}$ leaves two
significant open challenges. First, the type system does not support
\emph{incomplete graphs}, that is, graphs in which some elements are missing
from the graphs of user-defined types. Second, the type system relies on
dynamic type checking during pattern matching. This study addresses these two
challenges by incorporating linear implication into the $\lambda_{GT}$ type
system, while introducing new constraints to ensure its soundness.

</details>


### [38] [Insum: Sparse GPU Kernels Simplified and Optimized with Indirect Einsums](https://arxiv.org/abs/2510.17505)
*Jaeyeon Won,Willow Ahrens,Joel S. Emer,Saman Amarasinghe*

Main category: cs.PL

TL;DR: 提出了一种新的稀疏计算表达方法，通过将格式无关的稀疏张量Einsums重写为格式感知的间接Einsums，并开发Insum编译器生成高效的GPU代码，在稀疏GPU应用中实现了1.14x到3.81x的加速。


<details>
  <summary>Details</summary>
Motivation: 现有的稀疏编译器主要针对CPU设计，很少能生成高性能GPU代码，且在稀疏和密集区域混合计算时无法有效优化密集部分。

Method: 从格式无关的稀疏张量Einsums出发，重写为格式感知的间接Einsums，通过间接索引将稀疏数据和元数据映射到密集张量操作，并使用Insum编译器生成GPU代码。

Result: 在多种稀疏GPU应用中实现了1.14x到3.81x的加速，相比手工实现减少了202x到4491x的代码行数。

Conclusion: 该方法通过间接Einsums和专门的编译器，成功解决了稀疏GPU内核编程的困难，实现了显著的性能提升和代码简化。

Abstract: Programming high-performance sparse GPU kernels is notoriously difficult,
requiring both substantial effort and deep expertise. Sparse compilers aim to
simplify this process, but existing systems fall short in two key ways. First,
they are primarily designed for CPUs and rarely produce high-performance GPU
code. Second, when computations involve both sparse and dense regions, these
compilers often fail to optimize the dense portions effectively. In this paper,
we propose a new approach for expressing sparse computations. We start from
format-agnostic Einsums over sparse tensors and rewrite them into
format-conscious indirect Einsums, which explicitly encode format information
by mapping sparse data and metadata onto dense tensor operations through
indirect indexing. To execute indirect Einsums, we introduce the Insum
compiler, which generates efficient GPU code for these Einsums by lowering to
the PyTorch compiler, extended to better support Tensor Core-enabled indirect
Einsums. We also present two fixed-length sparse formats, GroupCOO and
BlockGroupCOO, designed to fit naturally with indirect Einsums. Our approach
achieves 1.14x to 3.81x speedups across a range of sparse GPU applications
while reducing lines of code by 202x to 4491x compared to hand-written
implementations.

</details>
