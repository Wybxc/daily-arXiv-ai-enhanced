<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.SE](#cs.SE) [Total: 18]
- [cs.FL](#cs.FL) [Total: 2]
- [cs.LO](#cs.LO) [Total: 6]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [StackPilot: Autonomous Function Agents for Scalable and Environment-Free Code Execution](https://arxiv.org/abs/2508.11665)
*Xinkui Zhao,Yifan Zhang,Zhengyi Zhou,Yueshen Xu*

Main category: cs.PL

TL;DR: StackPilot是一个基于LLM的多智能体框架，用于语言无关的代码验证和执行，无需传统工具链，通过函数即智能体、LLM作为执行器和快照机制实现高效验证。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖特定语言编译器和环境相关的运行时来验证LLM生成的代码，存在局限性，需要一种语言无关的验证解决方案。

Method: 采用多智能体框架：1)函数即智能体范式，每个函数作为自主智能体进行细粒度推理和协作验证；2)LLM作为执行器策略，通过基于堆栈的调度实现可扩展验证；3)快照机制保存完整执行上下文，支持确定性上下文切换。

Result: 实证评估显示StackPilot框架可靠性达到89%-97%，显著优于基线方法，能够可靠验证和执行更多LLM生成的代码。

Conclusion: StackPilot提供了一种有效的语言无关代码验证方法，大幅提升了LLM生成代码的验证可靠性和执行能力。

Abstract: Recent advances in large language models (LLMs) have substantially enhanced
automated code generation across a wide range of programming languages.
Nonetheless, verifying the correctness and executability of LLM-generated code
remains a significant challenge, as traditional methods rely on
language-specific compilers and environment-dependent runtimes. To overcome
these limitations, we introduce StackPilot, an LLM-native, multi-agent
framework designed for language-agnostic code verification and execution, which
operates independently of conventional toolchains. StackPilot offers three
principal innovations: (1) a Function-as-Agents paradigm, in which each
function is modeled as an autonomous agent capable of fine-grained reasoning
and collaborative verification; (2) an LLM-as-Executor strategy, which enables
scalable verification via stack-based scheduling; and (3) a novel snapshot
mechanism that preserves complete execution contexts, facilitating
deterministic and lossless context switching during verification. Empirical
evaluations demonstrate that StackPilot achieves framework reliability rates
between 89% and 97%, substantially outperforming baseline approaches. These
results indicate that StackPilot can reliably verify and execute a
significantly larger proportion of LLM-generated code across diverse
programming tasks compared to existing methods.

</details>


### [2] [Certified Compilation based on Gödel Numbers](https://arxiv.org/abs/2508.12054)
*Guilherme de Oliveira Silva,Fernando Magno Quintão Pereira*

Main category: cs.PL

TL;DR: 这篇论文提出了一种新的证书生成方法，通过整数证书验证编译后的二进制文件是否实际上代表源代码，解决Ken Thompson编译器后门问题。


<details>
  <summary>Details</summary>
Motivation: 应对Ken Thompson在1984年图灵奖演讲中提出的编译器后门攻击，虽然有多重编译等防御技术，但根本问题在于如何信任编译器。

Method: 设计了一种证书生成机制，通过从源代码和二进制文件派生整数证书，确保二进制文件包含所有且仅包含源代码语句，保持语句顺序和等价的def-use依赖关系。每个派生规则都在常数时间内完成。

Result: 实现了Charon编译器，能够处理足够编译FaCT（灵活且常数时间的加密编程语言）的C语言子集，证明了方法的实用性。

Conclusion: 该方法提供了一种可行的解决方案，通过数学证书来验证编译过程的完整性，最终解决了"如何信任编译器"这一根本问题。

Abstract: In his 1984 Turing Award lecture, Ken Thompson showed that a compiler could
be maliciously altered to insert backdoors into programs it compiles and
perpetuate this behavior by modifying any compiler it subsequently builds.
Thompson's hack has been reproduced in real-world systems for demonstration
purposes. Several countermeasures have been proposed to defend against
Thompson-style backdoors, including the well-known {\it Diverse
Double-Compiling} (DDC) technique, as well as methods like translation
validation and CompCert-style compilation. However, these approaches ultimately
circle back to the fundamental question: "How can we trust the compiler used to
compile the tools we rely on?" In this paper, we introduce a novel approach to
generating certificates to guarantee that a binary image faithfully represents
the source code. These certificates ensure that the binary contains all and
only the statements from the source code, preserves their order, and maintains
equivalent def-use dependencies. The certificate is represented as an integer
derivable from both the source code and the binary using a concise set of
derivation rules, each applied in constant time. To demonstrate the
practicality of our method, we present Charon, a compiler designed to handle a
subset of C expressive enough to compile FaCT, the Flexible and Constant Time
cryptographic programming language.

</details>


### [3] [Type-Driven Prompt Programming: From Typed Interfaces to a Calculus of Constraints](https://arxiv.org/abs/2508.12475)
*Abhijit Paul*

Main category: cs.PL

TL;DR: Prompt Programming将大语言模型提示视为具有类型接口的软件组件。本文基于对2023-2025年15篇文献的调查，发现类型系统是新兴提示编程框架的核心，但在约束表达性和算法支持方面存在不足。作者提出Lambda Prompt概念，这是一个带有概率精化的依赖类型演算，用于处理语法和语义约束。


<details>
  <summary>Details</summary>
Motivation: 当前提示编程框架虽然普遍采用类型系统，但在约束表达能力和算法支持方面存在明显不足，需要建立更坚实的理论基础来支持提示编程的发展。

Method: 通过文献调查分析现有框架趋势，提出Lambda Prompt概念——一个带有概率精化的依赖类型演算，包含13种约束分类，并提出约束保持优化规则。

Result: 识别出约束表达性方面的未探索领域（约束9-13），提出了类型理论基础，并设计了约束保持的优化算法。

Conclusion: Lambda Prompt为提示编程提供了类型理论基础，指出了未来研究方向，包括开发提示程序编译器等工具支持。

Abstract: Prompt programming treats large language model prompts as software components
with typed interfaces. Based on a literature survey of 15 recent works from
2023 to 2025, we observe a consistent trend: type systems are central to
emerging prompt programming frameworks. However, there are gaps in constraint
expressiveness and in supporting algorithms. To address these issues, we
introduce the notion of Lambda Prompt, a dependently typed calculus with
probabilistic refinements for syntactic and semantic constraints. While this is
not yet a full calculus, the formulation motivates a type-theoretic foundation
for prompt programming. Our catalog of 13 constraints highlights underexplored
areas in constraint expressiveness (constraints 9 through 13). To address the
algorithmic gap, we propose a constraint-preserving optimization rule. Finally,
we outline research directions on developing a compiler for prompt programs.

</details>


### [4] [Controlling Copatterns: There and Back Again (Extended Version)](https://arxiv.org/abs/2508.12427)
*Paul Downen*

Main category: cs.PL

TL;DR: 通过Danvy的力学对应关系，从单一性copatterns的小步操作语义推导到抽象机器和续体传递风格，然后在CPS中重构得到组合性copatterns的更通用微积分，最后逆向推导其他语义工具。


<details>
  <summary>Details</summary>
Motivation: Copatterns为函数式程序提供了灵活的上下文响应机制，组合性大大提高了表达力，但这种表达力也使得精确规范程序行为更加困难。

Method: 利用Danvy的力学和语法对应关系，从单一性copatterns的小步操作语义出发，逐步推导到抽象机器和续体传递风格(CPS)，然后在CPS内部重构语义得到组合性copatterns的更通用微积分。

Result: 得到了一套完整的copatterns语义套装，包括两套不同的推导路径：从单一性到组合性的正向推导，以及从组合性到其他语义工具的逆向推导。

Conclusion: 通过Danvy的力学对应方法，成功为copatterns开发了一套完整的语义套装，为理解和规范其行为提供了坚实的理论基础，同时展示了这种方法在复杂语言构造中的强大通用性。

Abstract: Copatterns give functional programs a flexible mechanism for responding to
their context, and composition can greatly enhance their expressiveness.
However, that same expressive power makes it harder to precisely specify the
behavior of programs. Using Danvy's functional and syntactic correspondence
between different semantic artifacts, we derive a full suite of semantics for
copatterns, twice. First, a calculus of monolithic copatterns is taken on a
journey from small-step operational semantics to abstract machine to
continuation-passing style. Then within continuation-passing style, we refactor
the semantics to derive a more general calculus of compositional copatterns,
and take the return journey back to derive the other semantic artifacts in
reverse order.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [5] [Benchmark Dataset Generation and Evaluation for Excel Formula Repair with LLMs](https://arxiv.org/abs/2508.11715)
*Ananya Singha,Harshita Sahijwani,Walt Williams,Emmanuel Aboah Boateng,Nick Hausman,Miguel Di Luca,Keegan Choudhury,Chaya Binet,Vu Le,Tianwei Chen,Oryan Rokeah Chen,Sulaiman Vesal,Sadid Hasan*

Main category: cs.SE

TL;DR: 这篇论文提出了一种用于Excel公式修复的标准数据集构建方法，通过继承小量线上示例并利用LLM扩展生成高质量数据，最终构建了618个样本的评测数据集，并评估了多种LLM在Excel公式修复任务上的性能。


<details>
  <summary>Details</summary>
Motivation: Excel作为普遍但复杂的工具，新手用户常遇到语义运行错误。虽然LLM能够解释公式错误，但自动修复这些错误仍是个未解决问题，主要挑战是缺乏高质量的评测数据集。

Method: 提出一种数据生成流水线，利用少量精选的线上论坛示例进行合成扩展。流程整合了少览提示技术和LLM-as-a-Judge验证框架，结合执行基于检查确保数据正确性和语义保真度。

Result: 生成了618个高质量样本的标准数据集，覆盖常见运行错误。对多种LLM（GPT-4o、GPT-4.1、Phi-3、Mistral）进行了执行基于评估，通过手动注释和错误分布分析验证了数据集质量。

Conclusion: 该生成方法具有高度可扩展性，可以轻松适配到其他低资源编程语言的代码修复任务中，为Excel公式修复领域提供了重要的评测基准。

Abstract: Excel is a pervasive yet often complex tool, particularly for novice users,
where runtime errors arising from logical mistakes or misinterpretations of
functions pose a significant challenge. While large language models (LLMs)
offer promising assistance by explaining formula errors, the automated
correction of these semantic runtime errors remains an open problem. A primary
challenge to advancing models for such scenarios is the severe lack of
high-quality, comprehensive datasets for training and rigorous evaluation. This
paper addresses this gap by introducing a novel approach for constructing a
benchmark dataset specifically designed for Excel formula repair. We propose a
data generation pipeline, which leverages a small set of curated seed samples
from online forums to synthetically expand the dataset. Our pipeline integrates
few-shot prompting with LLMs and employs a robust \textit{LLM-as-a-Judge}
validation framework, combined with execution-based checks to ensure the
correctness and semantic fidelity of the generated data. This process produced
a benchmark dataset of 618 high-quality samples, covering common runtime
errors. Furthermore, we propose a context-aware baseline technique for Excel
formula repair that utilizes LLMs to leverage both the faulty formula, and
relevant spreadsheet context. We evaluate the performance of various LLMs
(GPT-4o, GPT-4.1, Phi-3, Mistral) on our newly generated benchmark using
execution-based metrics. Our analysis demonstrates the dataset's quality
through manual annotation and provides insights into error and function
distributions. The proposed generation methodology is highly scalable and can
be readily adapted to create evaluation benchmarks for similar code repair
tasks in other low-resource programming languages.

</details>


### [6] [WIP: Leveraging LLMs for Enforcing Design Principles in Student Code: Analysis of Prompting Strategies and RAG](https://arxiv.org/abs/2508.11717)
*Dhruv Kolhatkar,Soubhagya Akkena,Edward F. Gehringer*

Main category: cs.SE

TL;DR: 使用大语言模型和RAG技术开发自动化代码审查工具，用于评估学生代码遵循面向对象设计原则的情况


<details>
  <summary>Details</summary>
Motivation: 解决在计算机科学和软件工程课程中教授软件设计最佳实践时的效果性和可扩展性问题

Method: 结合大语言模型(LLMs)和检索增强生成(RAG)技术，开发自动化反馈系统，分析各种提示策略的效果

Result: 预期结果显示代码质量有明显提升，展示了该方法的潜力

Conclusion: 该研究为教学软件设计最佳实践提供了有前景的自动化方案，未来工作将重点改进模型准确性和扩展支持的设计原则范围

Abstract: This work-in-progress research-to-practice paper explores the integration of
Large Language Models (LLMs) into the code-review process for open-source
software projects developed in computer science and software engineering
courses. The focus is on developing an automated feedback tool that evaluates
student code for adherence to key object-oriented design principles, addressing
the need for more effective and scalable methods to teach software design best
practices. The innovative practice involves leveraging LLMs and
Retrieval-Augmented Generation (RAG) to create an automated feedback system
that assesses student code for principles like SOLID, DRY, and design patterns.
It analyzes the effectiveness of various prompting strategies and the RAG
integration. Preliminary findings show promising improvements in code quality.
Future work will aim to improve model accuracy and expand support for
additional design principles.

</details>


### [7] [Rethinking Autonomy: Preventing Failures in AI-Driven Software Engineering](https://arxiv.org/abs/2508.11824)
*Satyam Kumar Navneet,Joydeep Chandra*

Main category: cs.SE

TL;DR: 本文提出SAFE-AI框架，通过安全、可审计、反馈和可解释性控制措施，解决LLM辅助代码生成的安全风险和管理挑战


<details>
  <summary>Details</summary>
Motivation: 大语言模型在软件工程中的应用带来了代码生成效率的大幅提升，但同时也引发了严重的安全风险，如不安全代码生成、幻觉输出、不可逆操作等，需要建立健壮的安全管理机制

Method: 提出SAFE-AI整体框架，包括拦截网、沙箱环境、运行时验证、风险认知日志、人在循环系统和可解释AI技术；创建了人工智能行为分类法，将AI行为分为建议性、生成性、自主性和破坏性四类

Result: 建立了一套完整的风险管理框架，能够有效减少LLM辅助代码生成的安全风险，提高透明度和负责任性，为相关法规规制提供技术支撑

Conclusion: 该研究为质量辅助代码生成提供了重要的安全管理解决方案，SAFE-AI框架能够在保持生产力的同时确保AI驱动开发的安全性、透明度和可负责性，为质量AI集成到软件工程领域提供了路线图

Abstract: The integration of Large Language Models (LLMs) into software engineering has
revolutionized code generation, enabling unprecedented productivity through
promptware and autonomous AI agents. However, this transformation introduces
significant risks, including insecure code generation, hallucinated outputs,
irreversible actions, and a lack of transparency and accountability. Incidents
like the Replit database deletion underscore the urgent need for robust safety
and governance mechanisms. This paper comprehensively analyzes the inherent
challenges of LLM-assisted code generation, such as vulnerability inheritance,
overtrust, misinterpretation, and the absence of standardized validation and
rollback protocols. To address these, we propose the SAFE-AI Framework, a
holistic approach emphasizing Safety, Auditability, Feedback, and
Explainability. The framework integrates guardrails, sandboxing, runtime
verification, risk-aware logging, human-in-the-loop systems, and explainable AI
techniques to mitigate risks while fostering trust and compliance. We introduce
a novel taxonomy of AI behaviors categorizing suggestive, generative,
autonomous, and destructive actions to guide risk assessment and oversight.
Additionally, we identify open problems, including the lack of standardized
benchmarks for code specific hallucinations and autonomy levels, and propose
future research directions for hybrid verification, semantic guardrails, and
proactive governance tools. Through detailed comparisons of autonomy control,
prompt engineering, explainability, and governance frameworks, this paper
provides a roadmap for responsible AI integration in software engineering,
aligning with emerging regulations like the EU AI Act and Canada's AIDA to
ensure safe, transparent, and accountable AI-driven development.

</details>


### [8] [AI-Augmented CI/CD Pipelines: From Code Commit to Production with Autonomous Decisions](https://arxiv.org/abs/2508.11867)
*Mohammad Baqar,Saba Naqvi,Rajat Khanda*

Main category: cs.SE

TL;DR: 论文提出AI增强的CI/CD流水线，使用LLM和自主代理作为策略约束的协同决策者，以减少人工决策延迟和操作负担


<details>
  <summary>Details</summary>
Motivation: 现代软件交付已加速到每日多次部署，但人工决策点（如解释不稳定测试、选择回滚策略等）仍然是延迟和操作负担的主要来源

Method: 提出参考架构将智能决策点嵌入CI/CD，包括决策分类法、策略即代码护栏模式、信任层级框架、使用DORA指标和AI特定指标的评估方法，以及工业级案例研究

Result: 通过React 19微服务迁移到AI增强流水线的案例研究，展示了该方法的有效性

Conclusion: 讨论了伦理、验证、可审计性和有效性威胁，并为生产交付系统中的可验证自主性制定了路线图

Abstract: Modern software delivery has accelerated from quarterly releases to multiple
deployments per day. While CI/CD tooling has matured, human decision points
interpreting flaky tests, choosing rollback strategies, tuning feature flags,
and deciding when to promote a canary remain major sources of latency and
operational toil. We propose AI-Augmented CI/CD Pipelines, where large language
models (LLMs) and autonomous agents act as policy-bounded co-pilots and
progressively as decision makers. We contribute: (1) a reference architecture
for embedding agentic decision points into CI/CD, (2) a decision taxonomy and
policy-as-code guardrail pattern, (3) a trust-tier framework for staged
autonomy, (4) an evaluation methodology using DevOps Research and Assessment (
DORA) metrics and AI-specific indicators, and (5) a detailed industrial-style
case study migrating a React 19 microservice to an AI-augmented pipeline. We
discuss ethics, verification, auditability, and threats to validity, and chart
a roadmap for verifiable autonomy in production delivery systems.

</details>


### [9] [Clean Code, Better Models: Enhancing LLM Performance with Smell-Cleaned Dataset](https://arxiv.org/abs/2508.11958)
*Zhipeng Xue,Xiaoting Zhang,Zhipeng Gao,Xing Hu,Shan Gao,Xin Xia,Shanping Li*

Main category: cs.SE

TL;DR: 这篇论文首次系统研究了大语言模型在代码相关任务中的代码味道问题，提出了自动化代码重构工具SmellCC，并证明清除代码味道能够提升LLM生成代码的质量。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在代码任务中表现出良好潜力，但现有研究多关注输出质量，而忽视了训练数据集的代码味道问题。代码味道广泛存在于实践中，会负面影响软件维护性和可读性。

Method: 1) 首先对CodeSearchNet-Python数据集进行代码味道预研究
2) 提出LLM基于的代码味道清理工具SmellCC
3) 构建50个仓库的测试集进行功能测试
4) 使用清洁后的数据集微调LLM
5) 研究代码味道对代码补全和代码搜索两个下游任务的影响

Result: 研究发现代码味道问题在LLM的输入（标准数据集）和输出（生成代码）中广泛存在。通过SmellCC工具清除代码味道后，微调的LLM能够生成更高质量的代码。

Conclusion: 这项研究为软件工程领域提供了可操作的建议：需要关注LLM训练数据集的质量问题，代码味道清理对提升代码生成质量具有重要价值，应将代码质量考量纳入LLM评估体系。

Abstract: The Large Language Models (LLMs) have demonstrated great potential in
code-related tasks. However, most research focuses on improving the output
quality of LLMs (e.g., correctness), and less attention has been paid to the
LLM input (e.g., the training code quality). Given that code smells are widely
existed in practice and can negatively impact software maintainability and
readability, this study takes the first systematic research to assess and
improve dataset quality in terms of code smells. In this work, we first conduct
a preliminary study to explore the presence of code smells in a popular
benchmark dataset (i.e., CodeSearchNet-Python}) and evaluate the output of
several popular LLMs (i.e., DeepSeek-Coder, CodeLlama, and MagiCoder),
revealing that code smell issues extensively exist in LLM's input (e.g.,
benchmark dataset) and output (e.g., generated code). We then conduct our
systematic research by taking three main steps: Firstly, we propose an
LLM-based code smell cleaning tool, named SmellCC, which automatically
refactors and removes code smells. To evaluate the correctness of the code
refactoring, we construct a test set of 50 repositories sourced from the
CodeSearchNet-Python benchmark for functional testing. Then we apply our
curated smell-cleaned dataset to fine-tune two LLMs (i.e., DeepSeek-V2 and
Qwen-Coder) to explore their potential for generating high-quality code.
Thirdly, we investigate the impact of code smells on two downstream tasks: code
completion and code search. Lastly, we derive several actionable implications
for software engineering researchers and industry practitioners from our
findings.

</details>


### [10] [How Much Can a Behavior-Preserving Changeset Be Decomposed into Refactoring Operations?](https://arxiv.org/abs/2508.11993)
*Kota Someya,Lei Chen,Michael J. Decker,Shinpei Hayashi*

Main category: cs.SE

TL;DR: 研究量化了行为保持修改中可被分解为重构操作的比例，发现现有检测器只能识别33.9%，通过新定义的67个操作可将覆盖率提高128%


<details>
  <summary>Details</summary>
Motivation: 开发者经常在修改中混合行为保持和行为改变的更改，需要工具来分离识别这两部分修改，特别是将行为保持部分分解为基础的重构操作

Method: 使用功能等效方法对比的数据集，量化行为保持修改中可被现有重构检测器识别的比例，并新定义67个功能等效操作来提高覆盖率

Result: 现有重构检测器只能识别33.9%的行为保持修改，添加67个新操作后覆盖率提高了128%以上，但仍有部分差异无法解释

Conclusion: 现有重构检测技术对行为保持修改的覆盖率有限，需要扩展重构操作集来提高分离识别能力，对于剩余差异的研究为进一步改进提供了方向

Abstract: Developers sometimes mix behavior-preserving modifications, such as
refactorings, with behavior-altering modifications, such as feature additions.
Several approaches have been proposed to support understanding such
modifications by separating them into those two parts. Such refactoring-aware
approaches are expected to be particularly effective when the
behavior-preserving parts can be decomposed into a sequence of more primitive
behavior-preserving operations, such as refactorings, but this has not been
explored. In this paper, as an initial validation, we quantify how much of the
behavior-preserving modifications can be decomposed into refactoring operations
using a dataset of functionally-equivalent method pairs. As a result, when
using an existing refactoring detector, only 33.9% of the changes could be
identified as refactoring operations. In contrast, when including 67 newly
defined functionally-equivalent operations, the coverage increased by over
128%. Further investigation into the remaining unexplained differences was
conducted, suggesting improvement opportunities.

</details>


### [11] [LinkAnchor: An Autonomous LLM-Based Agent for Issue-to-Commit Link Recovery](https://arxiv.org/abs/2508.12232)
*Arshia Akhavan,Alireza Hosseinpour,Abbas Heydarnoori,Mehdi Keshani*

Main category: cs.SE

TL;DR: LinkAnchor是一个基于LLM的自主代理，用于恢复issue到commit的链接，通过惰性访问架构动态检索相关上下文，避免token限制，并能自动定位目标commit而非穷举所有候选。


<details>
  <summary>Details</summary>
Motivation: 现有issue-commit链接恢复方法存在两个主要问题：LLM受限于上下文窗口无法处理大量数据源，以及大多数方法需要逐个处理issue-commit对，这在真实项目中不实用。

Method: 提出了LinkAnchor系统，采用惰性访问架构，让底层LLM能够动态检索最相关的上下文数据（包括commit历史、issue评论和代码文件），而不会超出token限制，并能自动定位目标commit。

Result: 评估显示LinkAnchor在所有案例研究项目中比最先进的issue-commit链接恢复方法在Hit@1分数上提升了60-262%。

Conclusion: LinkAnchor是第一个用于issue-commit链接恢复的自主LLM代理，解决了现有方法的局限性，性能显著优于现有技术，并已作为即用工具公开发布。

Abstract: Issue-to-commit link recovery plays an important role in software
traceability and improves project management. However, it remains a challenging
task. A study on GitHub shows that only 42.2% of the issues are correctly
linked to their commits. This highlights the potential for further development
and research in this area. Existing studies have employed various AI/ML-based
approaches, and with the recent development of large language models,
researchers have leveraged LLMs to tackle this problem. These approaches suffer
from two main issues. First, LLMs are constrained by limited context windows
and cannot ingest all of the available data sources, such as long commit
histories, extensive issue comments, and large code repositories. Second, most
methods operate on individual issue-commit pairs; that is, given a single
issue-commit pair, they determine whether the commit resolves the issue. This
quickly becomes impractical in real-world repositories containing tens of
thousands of commits. To address these limitations, we present LinkAnchor, the
first autonomous LLM-based agent designed for issue-to-commit link recovery.
The lazy-access architecture of LinkAnchor enables the underlying LLM to access
the rich context of software, spanning commits, issue comments, and code files,
without exceeding the token limit by dynamically retrieving only the most
relevant contextual data. Additionally, LinkAnchor is able to automatically
pinpoint the target commit rather than exhaustively scoring every possible
candidate. Our evaluations show that LinkAnchor outperforms state-of-the-art
issue-to-commit link recovery approaches by 60-262% in Hit@1 score across all
our case study projects. We also publicly release LinkAnchor as a ready-to-use
tool, along with our replication package. LinkAnchor is designed and tested for
GitHub and Jira, and is easily extendable to other platforms.

</details>


### [12] ["My productivity is boosted, but ..." Demystifying Users' Perception on AI Coding Assistants](https://arxiv.org/abs/2508.12285)
*Yunbo Lyu,Zhou Yang,Jieke Shi,Jianming Chang,Yue Liu,David Lo*

Main category: cs.SE

TL;DR: 本文通过分析VS Code市场中1,085款AI编码助手的用户评论，揭示了开发者对AI编码助手的真实需求和批评，并提出五项改进建议。


<details>
  <summary>Details</summary>
Motivation: 探索AI编码助手广泛采用时期开发者的真实需求和期望，充分了解实际工作环境中的用户体验。

Method: 从VS Code市场获取1,085款AI编码助手，手动分析32款高安装量工具的用户评论，构建用户关注点分类系统，并注解每条评论的态度。

Result: 发现用户不仅需要智能建议，更强调上下文感知、可自定义性和资源效率。评论分析揭示了用户对具体功能、问题和工具性能的满意度和不满。

Conclusion: 研究为AI编码助手的改进提供了五项实践建议，帮助开发者更好地满足用户需求，推动AI编码助手在实际软件开发中的效果提升。

Abstract: This paper aims to explore fundamental questions in the era when AI coding
assistants like GitHub Copilot are widely adopted: what do developers truly
value and criticize in AI coding assistants, and what does this reveal about
their needs and expectations in real-world software development? Unlike
previous studies that conduct observational research in controlled and
simulated environments, we analyze extensive, first-hand user reviews of AI
coding assistants, which capture developers' authentic perspectives and
experiences drawn directly from their actual day-to-day work contexts. We
identify 1,085 AI coding assistants from the Visual Studio Code Marketplace.
Although they only account for 1.64% of all extensions, we observe a surge in
these assistants: over 90% of them are released within the past two years. We
then manually analyze the user reviews sampled from 32 AI coding assistants
that have sufficient installations and reviews to construct a comprehensive
taxonomy of user concerns and feedback about these assistants. We manually
annotate each review's attitude when mentioning certain aspects of coding
assistants, yielding nuanced insights into user satisfaction and
dissatisfaction regarding specific features, concerns, and overall tool
performance. Built on top of the findings-including how users demand not just
intelligent suggestions but also context-aware, customizable, and
resource-efficient interactions-we propose five practical implications and
suggestions to guide the enhancement of AI coding assistants that satisfy user
needs.

</details>


### [13] [From Fomo3D to Lottery DAPP: Analysis of Ethereum-Based Gambling Applications](https://arxiv.org/abs/2508.12303)
*Xu Long,Yishun Wang,Xiaoqi Li*

Main category: cs.SE

TL;DR: 这篇论文分析了基于以太坊的赌博DApps，详细介绍了其概念、运作原理、实现技术和发展前景。重点论述了智能合约在自动化彩票过程、保障公平性和透明度方面的优势。


<details>
  <summary>Details</summary>
Motivation: 研究基于区块链技术的赌博DApps的概念、原理和实现，以探讨其如何通过去中心化特性改变传统在线彩票行业的运作模式。

Method: 首先概述赌博DApps的概念和运作原理，然后分析现有的以太坊赌博DApp的技术原理、实现方式、运行状况、漏洞和解决方案，最后详细说明彩票DApps的实现技术。

Result: 论文证明了赌博DApps通过智能合约实现了彩票发行、下注、开奖和奖金分配的全自动化处理，减少了管理成本并提高了盈利能力，同时保证了游戏的公平性和透明度。

Conclusion: 随着区块链技术和智能合约的不断发展，赌博DApps有望对在线彩票行业产生重大影响。其去中心化、自动化和透明性优势将促进更广泛的未来应用。

Abstract: As blockchain technology advances, Ethereum based gambling decentralized
applications (DApps) represent a new paradigm in online gambling. This paper
examines the concepts, principles, implementation, and prospects of Ethereum
based gambling DApps. First, we outline the concept and operational principles
of gambling DApps. These DApps are blockchain based online lottery platforms.
They utilize smart contracts to manage the entire lottery process, including
issuance, betting, drawing, and prize distribution. Being decentralized,
lottery DApps operate without central oversight, unlike traditional lotteries.
This ensures fairness and eliminates control by any single entity. Automated
smart contract execution further reduces management costs, increases
profitability, and enhances game transparency and credibility. Next, we analyze
an existing Ethereum based gambling DApp, detailing its technical principles,
implementation, operational status, vulnerabilities, and potential solutions.
We then elaborate on the implementation of lottery DApps. Smart contracts
automate the entire lottery process including betting, drawing, and prize
distribution. Although developing lottery DApps requires technical expertise,
the expanding Ethereum ecosystem provides growing tools and frameworks,
lowering development barriers. Finally, we discuss current limitations and
prospects of lottery DApps. As blockchain technology and smart contracts
evolve, lottery DApps are positioned to significantly transform the online
lottery industry. Advantages like decentralization, automation, and
transparency will likely drive broader future adoption.

</details>


### [14] [Towards the Coordination and Verification of Heterogeneous Systems with Data and Time](https://arxiv.org/abs/2508.12325)
*Tim Kräuter,Adrian Rutle,Yngve Lamo,Harald König,Francisco Durán*

Main category: cs.SE

TL;DR: 一种非侵入式协调框架，通过语言扩展和重写逻辑实现，用于异构系统的正式分析验证


<details>
  <summary>Details</summary>
Motivation: 现代软件系统通常由多个异构部分协调构成，需要确保各部分无缝协同工作以满足整体要求，而传统验证方法难以处理这种复杂性

Method: 开发了一种协调框架，采用语言扩展技术，通过中央线紋和域特定语言集成异构语言。使用摘要规则模板作为语言适配器实现非侵入式通信，基于重写逻辑（Maude）实现

Result: 框架能够对包含实时能力的异构组件进行正式分析，并通过异构路轨交叉系统的正确性属性验证展示了其可用性

Conclusion: 该非侵入式协调框架为复杂异构系统的正式验证提供了有效解决方案，能够处理数据交换和实时要求，具有良好的应用前景

Abstract: Modern software systems are often realized by coordinating multiple
heterogeneous parts, each responsible for specific tasks. These parts must work
together seamlessly to satisfy the overall system requirements. To verify such
complex systems, we have developed a non-intrusive coordination framework
capable of performing formal analysis of heterogeneous parts that exchange data
and include real-time capabilities. The framework utilizes a linguistic
extension, which is implemented as a central broker and a domain-specific
language for the integration of heterogeneous languages and coordination of
parts. Moreover, abstract rule templates are reified as language adapters for
non-intrusive communications with the broker. The framework is implemented
using rewriting logic (Maude), and its applicability is demonstrated by
verifying certain correctness properties of a heterogeneous road-rail crossing
system.

</details>


### [15] [Uncovering Systematic Failures of LLMs in Verifying Code Against Natural Language Specifications](https://arxiv.org/abs/2508.12358)
*Haolin Jin,Huaming Chen*

Main category: cs.SE

TL;DR: LLMs在代码审查中存在系统性缺陷，经常错误地将正确代码判定为不满足需求或有缺陷，且更复杂的提示工程反而会增加误判率


<details>
  <summary>Details</summary>
Motivation: 探究LLMs是否能可靠地判断代码实现是否完全符合自然语言需求描述，这在软件开发中至关重要

Method: 使用统一提示词在广泛使用的基准测试上评估代码正确性，分析误判根源并提出两种改进的提示策略

Result: 发现LLMs频繁误判正确代码，复杂提示技术导致更高误判率，揭示了LLMs在代码需求匹配方面的未识别局限性

Conclusion: 揭示了LLMs作为代码审查助手的可靠性问题，为自动化代码审查和任务导向代理场景提供了新的见解和实用指导

Abstract: Large language models (LLMs) have become essential tools in software
development, widely used for requirements engineering, code generation and
review tasks. Software engineers often rely on LLMs to assess whether system
code implementation satisfy task requirements, thereby enhancing code
robustness and accuracy. However, it remains unclear whether LLMs can reliably
determine whether the code complies fully with the given task descriptions,
which is usually natural language specifications. In this paper, we uncover a
systematic failure of LLMs in evaluating whether code aligns with natural
language requirements. Specifically, with widely used benchmarks, we employ
unified prompts to judge code correctness. Our results reveal that LLMs
frequently misclassify correct code implementations as either ``not satisfying
requirements'' or containing potential defects. Surprisingly, more complex
prompting, especially when leveraging prompt engineering techniques involving
explanations and proposed corrections, leads to higher misjudgment rate, which
highlights the critical reliability issues in using LLMs as code review
assistants. We further analyze the root causes of these misjudgments, and
propose two improved prompting strategies for mitigation. For the first time,
our findings reveals unrecognized limitations in LLMs to match code with
requirements. We also offer novel insights and practical guidance for effective
use of LLMs in automated code review and task-oriented agent scenarios.

</details>


### [16] [Feature Request Analysis and Processing: Tasks, Techniques, and Trends](https://arxiv.org/abs/2508.12436)
*Feifei Niu,Chuanyi Li,Haosheng Zuo,Jionghan Wu,Xin Xia*

Main category: cs.SE

TL;DR: 这是一份关于软件功能请求研究的系统性综述报告，通过对131份相关研究的分析识别了该领域的挑战和机遇。


<details>
  <summary>Details</summary>
Motivation: 软件功能请求代表用户需求，满足这些需求对产品竞争力和用户满意度都有积极作用。由于该领域研究主题多样化，需要系统性的总结分析来识别挑战和机遇。

Method: 采用系统性综述方法，通过定义的搜索协议选取和分析131份主要研究。使用描述性统计和定性分析方法，将研究按需求工程活动进行分类。

Result: 研究识别了该领域的关键挑战和机遇，包括：(1)确保功能请求质量，(2)改进功能请求的规范和验证，(3)为大语言模型任务开发高质量的基准测试集。同时调查了可用于未来研究的开源工具和数据集。

Conclusion: 该系统性综述为软件功能请求研究领域提供了全面的概览，明确了未来研究的重点方向，包括质量保证、规范改进和大模型应用等关键问题，有助于推动该领域的发展和进步。

Abstract: Feature requests are proposed by users to request new features or
enhancements of existing features of software products, which represent users'
wishes and demands. Satisfying users' demands can benefit the product from both
competitiveness and user satisfaction. Feature requests have seen a rise in
interest in the past few years and the amount of research has been growing.
However, the diversity in the research topics suggests the need for their
collective analysis to identify the challenges and opportunities so as to
promote new advances in the future. In this work, following a defined process
and a search protocol, we provide a systematic overview of the research area by
searching and categorizing relevant studies. We select and analyze 131 primary
studies using descriptive statistics and qualitative analysis methods. We
classify the studies into different topics and group them from the perspective
of requirements engineering activities. We investigate open tools as well as
datasets for future research. In addition, we identify several key challenges
and opportunities, such as: (1) ensuring the quality of feature requests, (2)
improving their specification and validation, and (3) developing high-quality
benchmarks for large language model-driven tasks.

</details>


### [17] [XAMT: Cross-Framework API Matching for Testing Deep Learning Libraries](https://arxiv.org/abs/2508.12546)
*Bin Duan,Ruican Dong,Naipeng Dong,Dan Dongseong Kim,Guowei Yang*

Main category: cs.SE

TL;DR: XAMT是一种跨框架模糊测试方法，通过匹配不同深度学习框架中功能等效的API进行差异测试，发现传统框架内测试无法检测的bug


<details>
  <summary>Details</summary>
Motivation: 深度学习库的bug会传播到下游系统造成严重后果，现有模糊测试技术通过跨硬件后端测试可能错过在后端间表现一致的bug

Method: 基于名称、描述和参数结构的相似性规则匹配API，对齐输入并应用方差引导的差异测试来检测bug

Result: 在5个流行框架中匹配839个API和238个API组，检测到17个bug（12个已确认），发现传统方法无法检测的跨后端一致bug

Conclusion: XAMT为现有测试方法提供了补充，为深度学习库测试提供了新视角，特别擅长检测跨后端表现一致的bug

Abstract: Deep learning powers critical applications such as autonomous driving,
healthcare, and finance, where the correctness of underlying libraries is
essential. Bugs in widely used deep learning APIs can propagate to downstream
systems, causing serious consequences. While existing fuzzing techniques detect
bugs through intra-framework testing across hardware backends (CPU vs. GPU),
they may miss bugs that manifest identically across backends and thus escape
detection under these strategies. To address this problem, we propose XAMT, a
cross-framework fuzzing method that tests deep learning libraries by matching
and comparing functionally equivalent APIs across different frameworks. XAMT
matches APIs using similarity-based rules based on names, descriptions, and
parameter structures. It then aligns inputs and applies variance-guided
differential testing to detect bugs. We evaluated XAMT on five popular
frameworks, including PyTorch, TensorFlow, Keras, Chainer, and JAX. XAMT
matched 839 APIs and identified 238 matched API groups, and detected 17 bugs,
12 of which have been confirmed. Our results show that XAMT uncovers bugs
undetectable by intra-framework testing, especially those that manifest
consistently across backends. XAMT offers a complementary approach to existing
methods and offers a new perspective on the testing of deep learning libraries.

</details>


### [18] [Strengthening Programming Comprehension in Large Language Models through Code Generation](https://arxiv.org/abs/2508.12620)
*Xiaoning Ren,Qiang Hu,Wei Ma,Yan Li,Yao Zhang,Lingxiao Jiang,Yinxing Xue*

Main category: cs.SE

TL;DR: 提出了一种反事实代码增强框架结合概念感知调优的方法，用于提升大语言模型对编程基础概念的理解能力


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在代码相关任务上表现优异，但对数据流、控制流等基础编程概念的理解仍然肤浅，导致在需要深度推理的代码任务中表现脆弱，限制了在实际软件开发中的应用

Method: 采用反事实代码增强框架结合概念感知调优的方法，通过增强模型对编程概念的理解来提升性能

Result: 在多个模型和基准测试上的综合评估证明了所提出方法的有效性

Conclusion: 该方法能够有效引导大语言模型建立更强的概念理解，解决了现有模型在深度代码推理方面的局限性

Abstract: Large language models (LLMs) have recently shown impressive results on
diverse code-related tasks, benefiting from large-scale training and
instruction tuning. However, studies reveal that their grasp of fundamental
programming concepts, such as data flow and control flow, remains shallow,
leading to fragile performance when code requires deeper reasoning. This
limitation restricts the practical adoption of LLMs in real-world software
development. To address this issue, this work introduces a counterfactual code
augmentation framework combined with concept-aware tuning, designed to guide
LLMs toward stronger conceptual understanding. Comprehensive evaluation across
multiple models and benchmarks demonstrates the effectiveness of the proposed
approach.

</details>


### [19] [ChangePrism: Visualizing the Essence of Code Changes](https://arxiv.org/abs/2508.12649)
*Lei Chen,Michele Lanza,Shinpei Hayashi*

Main category: cs.SE

TL;DR: 提出了一种新的可视化工具ChangePrism，用于更好地理解代码变更，解决传统代码差异查看方式的不足。


<details>
  <summary>Details</summary>
Motivation: 传统的代码diff查看方式对开发者来说很繁琐，难以获得代码变更的全局概览，且某些重要变更类型需要特别标识以提高代码理解效果。

Method: 开发了ChangePrism工具，包含两个核心组件：提取组件从git历史中获取代码变更和相关信息，可视化组件提供代码变更的概览和详细视图。

Result: 工具能够提供代码变更类型的概览视图，以及每个提交中具体代码变更的详细显示，帮助开发者更好地理解代码修改。

Conclusion: ChangePrism通过可视化方式改善了代码变更的理解效果，为软件维护和迭代提供了更有效的工具支持。

Abstract: Understanding the changes made by developers when they submit a pull request
and/or perform a commit on a repository is a crucial activity in software
maintenance and evolution. The common way to review changes relies on examining
code diffs, where textual differences between two file versions are highlighted
in red and green to indicate additions and deletions of lines. This can be
cumbersome for developers, making it difficult to obtain a comprehensive
overview of all changes in a commit. Moreover, certain types of code changes
can be particularly significant and may warrant differentiation from standard
modifications to enhance code comprehension. We present a novel visualization
approach supported by a tool named ChangePrism, which provides a way to better
understand code changes. The tool comprises two components: extraction, which
retrieves code changes and relevant information from the git history, and
visualization, which offers both general and detailed views of code changes in
commits. The general view provides an overview of different types of code
changes across commits, while the detailed view displays the exact changes in
the source code for each commit.

</details>


### [20] [RUM: Rule+LLM-Based Comprehensive Assessment on Testing Skills](https://arxiv.org/abs/2508.12922)
*Yue Wang,Zhenyu Chen,Yuan Zhao,Chunrong Fang,Ziyuan Wang,Song Huang*

Main category: cs.SE

TL;DR: RUM方法结合规则和大型语言模型，实现了软件测试技能的全面自动化评估，在保持高准确性的同时显著提升了评估效率和降低了成本。


<details>
  <summary>Details</summary>
Motivation: 现有的META方法只能评估测试脚本等客观内容，无法自动评估测试用例和测试报告等主观内容，需要一种更全面的自动化评估方法。

Method: 提出RUM方法，通过规则快速处理客观指标，利用大型语言模型(LLMs)对测试用例文档、测试脚本和测试报告进行深度主观分析。

Result: 相比传统人工评估，RUM将评估效率提高了80.77%，成本降低了97.38%，同时保持了高准确性和一致性。

Conclusion: RUM不仅提升了软件测试教育中技能评估的效率和可扩展性，还为教师提供了更全面客观的学生能力评估依据，促进了个性化教学。

Abstract: Over the past eight years, the META method has served as a multidimensional
testing skill assessment system in the National College Student Contest on
Software Testing, successfully assessing over 100,000 students' testing skills.
However, META is primarily limited to the objective assessment of test scripts,
lacking the ability to automatically assess subjective aspects such as test
case and test report. To address this limitation, this paper proposes RUM, a
comprehensive assessment approach that combines rules and large language models
(LLMs). RUM achieves a comprehensive assessment by rapidly processing objective
indicators through rules while utilizing LLMs for in-depth subjective analysis
of test case documents, test scripts, and test reports. The experimental
results show that compared to traditional manual testing skill assessment, RUM
improves assessment efficiency by 80.77\% and reduces costs by 97.38\%, while
maintaining high accuracy and consistency of assessment. By applying RUM on the
contest on software testing, we find that it not only enhances the efficiency
and scalability of skill assessment in software testing education, but also
provides teachers with more comprehensive and objective evidence for student
ability assessment, facilitating personalized teaching and learning. This study
offers new insights into the assessment of testing skills, which are expected
to promote further development in test process optimization and software
quality assurance.

</details>


### [21] [Investigating VR Accessibility Reviews for Users with Disabilities: A Qualitative Analysis](https://arxiv.org/abs/2508.13051)
*Yi Wang,Chetan Arora,Xiao Liu,Thuong Hoang,ZHengxin Zhang,Henry Been Lirn Duh,John Grundy*

Main category: cs.SE

TL;DR: 这篇论文分析了Meta和Steam平台上VR应用的用户评论，研究障碍用户遇到的可访问性问题，发现VR可访问性评论数量极少且大多数问题未得到支持。


<details>
  <summary>Details</summary>
Motivation: 虚拟现实(VR)应用的可访问性评估对障碍用户很重要，但目前缺乏全面的研究。本研究旨在填补这一空白，通过分析用户评论了解障碍用户遇到的可访问性问题。

Method: 研究分析了Meta和Steam平台上最热门40款、20款最受欢迎和40款最低分评分的VR应用的1,367,419条评论。通过选择标准找到了1,076条(0.078%)与可访问性相关的评论，涵盖100款VR应用。

Result: 识别出16种不同类型的障碍，分为6个大类。动作类应用收到最多的可访问性相关评论。研究还分析了用户报告的可访问性问题原因。总体而言，VR可访问性评论数量极少且大多数问题未得到支持。

Conclusion: 这项研究弥补了VR可访问性领域的研究空白，揭示了VR应用在支持障碍用户方面的不足，为改善VR可访问性提供了重要的用户反馈和建议。

Abstract: Accessibility reviews provide valuable insights into both the limitations and
benefits experienced by users with disabilities when using virtual reality (VR)
applications. However, a comprehensive investigation into VR accessibility for
users with disabilities is still lacking. To fill this gap, this study analyzes
user reviews from the Meta and Steam stores of VR apps, focusing on the
reported issues affecting users with disabilities. We applied selection
criteria to 1,367,419 reviews from the top 40, the 20 most popular, and the 40
lowest-rated VR applications on both platforms. In total, 1,076 (0.078%) VR
accessibility reviews referenced various disabilities across 100 VR
applications. These applications were categorized into Action, Sports, Social,
Puzzle, Horror, and Simulation, with Action receiving the highest number of
accessibility related-reviews. We identified 16 different types of disabilities
across six categories. Furthermore, we examined the causes of accessibility
issues as reported by users with disabilities. Overall, VR accessibility
reviews were predominantly under-supported.

</details>


### [22] [Influencia de fatores organizacionais e sociais na etapa de levantamento de requisitos](https://arxiv.org/abs/2508.13134)
*Glauber da Rocha Balthazar,Marcia Ito*

Main category: cs.SE

TL;DR: 需求式工程中对非技术因素的研究综述，包括情感、组织环境和社会背景等人文因素的考量


<details>
  <summary>Details</summary>
Motivation: 软件开发中需求收集阶段最关键也最脆弱，尽管需求工程技术不断发展，但很少研究考虑到参与者的人文关系和行为特征

Method: 通过调查研究的方式，综述分析在需求收集阶段考虑非技术因素的相关研究

Result: 本文展示了一些考虑情感、组织环境和社会背景等人文因素的需求工程研究

Conclusion: 需求工程领域需要更多地关注人文因素，这些非技术方面的考量对需求收集成功至关重要

Abstract: The most critical and fragile stage of a software development project is
requirements gathering. Because of this, Requirements Engineering has been
evolving its techniques to minimize the challenges faced by Requirements
Analysts. However, few studies consider the humanistic relationships and
behaviors of those involved in this stage. This article presents a survey of
some studies conducted at this stage that consider non-technical factors such
as emotions, organizational environment, and social context.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [23] [Structural Abstraction and Refinement for Probabilistic Programs](https://arxiv.org/abs/2508.12344)
*Guanyan Li,Juanen Li,Zhilei Han,Peixin Wang,Hongfei Fu,Fei He*

Main category: cs.FL

TL;DR: 提出结构抽象精化框架，通过将概率控制流自动机抽象为马尔可夫决策过程来验证概率程序的阈值问题，实现了概率计算和语义关注的分离。


<details>
  <summary>Details</summary>
Motivation: 传统方法在概率程序验证中需要同时处理概率和语义，限制了非随机程序验证技术的直接应用。需要一种能够分离关注点的新方法。

Method: 将概率控制流自动机的结构抽象为马尔可夫决策过程，忽略语句语义。通过反例引导的抽象精化框架，利用成熟的非概率技术进行概率验证。

Result: 实验结果表明该方法在多样化示例上表现优异，相比现有工具具有更好的通用性和处理灵活结构的能力。

Conclusion: 结构抽象精化框架提供了一种新颖的概率程序验证方法，成功实现了概率计算和语义处理的分离，为非概率验证技术的重用提供了有效途径。

Abstract: In this paper, we present structural abstraction refinement, a novel
framework for verifying the threshold problem of probabilistic programs. Our
approach represents the structure of a Probabilistic Control-Flow Automaton
(PCFA) as a Markov Decision Process (MDP) by abstracting away statement
semantics. The maximum reachability of the MDP naturally provides a proper
upper bound of the violation probability, termed the structural upper bound.
This introduces a fresh ``structural'' characterization of the relationship
between PCFA and MDP, contrasting with the traditional ``semantical'' view,
where the MDP reflects semantics. The method uniquely features a clean
separation of concerns between probability and computational semantics that the
abstraction focuses solely on probabilistic computation and the refinement
handles only the semantics aspect, where the latter allows non-random program
verification techniques to be employed without modification.
  Building upon this feature, we propose a general counterexample-guided
abstraction refinement (CEGAR) framework, capable of leveraging established
non-probabilistic techniques for probabilistic verification. We explore its
instantiations using trace abstraction. Our method was evaluated on a diverse
set of examples against state-of-the-art tools, and the experimental results
highlight its versatility and ability to handle more flexible structures
swiftly.

</details>


### [24] [Box-Reachability in Vector Addition Systems](https://arxiv.org/abs/2508.12853)
*Shaull Almagor,Itay Hasson,Michał Pilipczuk,Michael Zaslavski*

Main category: cs.FL

TL;DR: 本文研究向量加法系统(VAS)中的box可达性变体，主要证明在二维VAS中，当向量坐标超过某个阈值W时，box可达集与标准可达集几乎完全一致。


<details>
  <summary>Details</summary>
Motivation: 探索VAS中一种新的可达性概念——box可达性，即在从0到v的路径中不仅要求保持在正象限内，还要求路径始终位于由0和v定义的"盒子"内，研究这种可达性与标准可达性的关系。

Method: 使用凸几何的强大数学工具进行理论证明，特别针对二维VAS系统进行分析，比较box可达集和标准可达集的性质。

Result: 对于二维VAS，当向量v的两个坐标都大于某个阈值W时，box可达集与标准可达集完全一致，揭示了这两种可达性概念在大多数情况下的等价性。

Conclusion: box可达性在二维VAS中与标准可达性具有紧密联系，通过凸几何方法证明了在足够大的向量范围内两者的等价性，为VAS可达性理论提供了新的见解。

Abstract: We consider a variant of reachability in Vector Addition Systems (VAS) dubbed
\emph{box reachability}, whereby a vector $v\in \mathbb{N}^d$ is box-reachable
from $0$ in a VAS $V$ if $V$ admits a path from $0$ to $v$ that not only stays
in the positive orthant (as in the standard VAS semantics), but also stays
below $v$, i.e., within the ``box'' whose opposite corners are $0$ and $v$.
  Our main result is that for two-dimensional VAS, the set of box-reachable
vertices almost coincides with the standard reachability set: the two sets
coincide for all vectors whose coordinates are both above some threshold $W$.
We also study properties of box-reachability, exploring the differences and
similarities with standard reachability.
  Technically, our main result is proved using powerful machinery from convex
geometry.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [25] [Categorical Construction of Logically Verifiable Neural Architectures](https://arxiv.org/abs/2508.11647)
*Logan Nye*

Main category: cs.LO

TL;DR: 提出基于范畴论的神经网络架构构造方法，将逻辑理论转化为具有可证明逻辑保证的神经网络，确保逻辑原则直接嵌入网络结构而无法违反。


<details>
  <summary>Details</summary>
Motivation: 神经网络擅长模式识别但在逻辑推理方面不可靠，经常违反基本逻辑原则，需要开发具有可证明逻辑保证的神经网络架构。

Method: 使用Lawvere理论作为代数结构，通过范畴代数在参数映射的2-范畴中将逻辑理论转化为神经网络架构，直接在架构中嵌入逻辑原则。

Result: 构建了可微分的命题逻辑神经网络架构，保持布尔推理能力同时可通过梯度下降训练，建立了有限逻辑理论与神经网络架构之间的双射对应关系。

Conclusion: 该框架将范畴深度学习从几何对称性扩展到语义约束，为可信AI系统提供数学基础，适用于定理证明、形式验证和安全关键推理任务。

Abstract: Neural networks excel at pattern recognition but struggle with reliable
logical reasoning, often violating basic logical principles during inference.
We address this limitation by developing a categorical framework that
systematically constructs neural architectures with provable logical
guarantees. Our approach treats logical theories as algebraic structures called
Lawvere theories, which we transform into neural networks using categorical
algebra in the 2-category of parametric maps. Unlike existing methods that
impose logical constraints during training, our categorical construction embeds
logical principles directly into the network's architectural structure, making
logical violations mathematically impossible. We demonstrate this framework by
constructing differentiable neural architectures for propositional logic that
preserve boolean reasoning while remaining trainable via gradient descent. Our
main theoretical result establishes a bijective correspondence between finitary
logical theories and neural architectures, proving that every logically
constrained network arises uniquely from our construction. This extends
Categorical Deep Learning beyond geometric symmetries to semantic constraints,
enabling automatic derivation of verified architectures from logical
specifications. The framework provides mathematical foundations for trustworthy
AI systems, with applications to theorem proving, formal verification, and
safety-critical reasoning tasks requiring verifiable logical behavior.

</details>


### [26] [Queen Domination by SAT Solving](https://arxiv.org/abs/2508.11945)
*Taha Rostami,Curtis Bright*

Main category: cs.LO

TL;DR: 通过SAT编码和证书生成技术，解决了国际象棋板的皇后封装问题，突破了n=19的未解案例，并纠正了n=16的之前结果错误


<details>
  <summary>Details</summary>
Motivation: 以前的优化搜索方法虽然高效，但无法生成可被第三方验证的证明，需要结合效率与可验证性的新方法

Method: 使用直接的SAT编码将问题转换为命题满足问题，通过新的文字顺序策略改善编码，采用静态对称破坏和Cube-and-Conquer范弋，利用能生成证书的SAT求解器

Result: 发现并纠正了n=16时之前结果的差异，解决了之前未解的n=19案例

Conclusion: 该方法成功结合了高效性与可验证性，为组合优化问题提供了可靠的解决方案，并在实践中验证了其效果

Abstract: The queen domination problem asks for the minimum number of queens needed to
attack all squares on an $n\times n$ chessboard. Once this optimal number is
known, determining the number of distinct solutions up to isomorphism has also
attracted considerable attention. Previous work has introduced specialized and
highly optimized search procedures to address open instances of the problem.
While efficient in terms of runtime, these approaches have not provided proofs
that can be independently verified by third-party checkers. In contrast, this
paper aims to combine efficiency with verifiability. We reduce the problem to a
propositional satisfiability problem (SAT) using a straightforward encoding,
and solve the resulting formulas with modern SAT solvers capable of generating
proof certificates. By improving the SAT encoding with a novel literal ordering
strategy, and leveraging established techniques such as static symmetry
breaking and the Cube-and-Conquer paradigm, this paper achieves both
performance and trustworthiness. Our approach discovers and corrects a
discrepancy in previous results for $n=16$ and resolves the previously open
case $n=19$.

</details>


### [27] [Finite Axiomatizability by Disjunctive Existential Rules](https://arxiv.org/abs/2508.11946)
*Marco Calautti,Marco Console,Andreas Pieris*

Main category: cs.LO

TL;DR: 本文研究了析取存在规则的表达能力，通过模型论性质给出了有限析取存在规则集可公理化的充要条件，并针对线性和守卫析取存在规则建立了类似特征，最后利用图式兼容性将守卫规则重写为线性规则。


<details>
  <summary>Details</summary>
Motivation: 析取存在规则在数据库和人工智能领域具有核心重要性，但对其表达能力的系统理解仍不充分。本文旨在通过模型论性质精确刻画析取存在规则的表达能力。

Method: 采用临界性、直接积闭包的精细化版本以及新提出的图式兼容性方法，结合图式方法建立理论特征。针对线性和守卫规则，采用相应语法限制下的精细化图式兼容性版本。

Result: 证明了关系结构集合可通过有限析取存在规则集公理化当且仅当该集合具有特定模型论性质。对线性和守卫规则也建立了类似特征，并展示了图式兼容性的鲁棒性。

Conclusion: 图式兼容性是刻画析取存在规则表达能力的关键性质，能够有效支持规则重写，将守卫规则转换为等价的线性规则，为规则语言的表达能力分析提供了系统框架。

Abstract: Rule-based languages lie at the core of several areas of central importance
to databases and artificial intelligence such as deductive databases and
knowledge representation and reasoning. Disjunctive existential rules (a.k.a.
disjunctive tuple-generating dependencies in the database literature) form such
a prominent rule-based language. The goal of this work is to pinpoint the
expressive power of disjunctive existential rules in terms of insightful
model-theoretic properties. More precisely, given a collection $\mathcal{C}$ of
relational structures, we show that $\mathcal{C}$ is axiomatizable via a finite
set $\Sigma$ of disjunctive existential rules (i.e., $\mathcal{C}$ is precisely
the set of models of $\Sigma$) iff $\mathcal{C}$ enjoys certain model-theoretic
properties. This is achieved by using the well-known property of criticality, a
refined version of closure under direct products, and a novel property called
diagrammatic compatibility that relies on the method of diagrams. We further
establish analogous characterizations for the well-behaved classes of linear
and guarded disjunctive existential rules by adopting refined versions of
diagrammatic compatibility that consider the syntactic restrictions imposed by
linearity and guardedness; this illustrates the robustness of diagrammatic
compatibility. We finally exploit diagrammatic compatibility to rewrite a set
of guarded disjunctive existential rules into an equivalent set that falls in
the weaker class of linear disjunctive existential rules, if one exists.

</details>


### [28] [Reachability is Decidable for ATM-Typable Finitary PCF with Effect Handlers](https://arxiv.org/abs/2508.12572)
*Ryunosuke Endo,Tachio Terauchi*

Main category: cs.LO

TL;DR: 这篇论文证明了在简单类型化λ求值语言中，尾类型修改(ATM)使得包含效果处理器的达性问题变得可判定，这与直觉相反但通过新的CPS转换得以证明。


<details>
  <summary>Details</summary>
Motivation: 直觉上认为尾类型修改(ATM)会增加程序的类型能力，但论文发现它实际上可以将包含效果处理器的达性问题变为可判定，这与之前的研究结果相反。

Method: 设计了一种新的续体传递风格(CPS)转换，将包含ATM和效果处理器的finitary PCF程序转换为不包含效果处理器的finitary PCF程序。

Result: 证明了包含ATM和效果处理器的finitary PCF的达性问题是可判定的，并发现某些无递归的程序在有ATM时必然终止，而没有ATM时可能分散。

Conclusion: ATM虽然增加了类型能力，但通过CPS转换反而使达性问题变得可判定，为验证包含效果处理器的程序奠定了基础。

Abstract: It is well known that the reachability problem for simply-typed lambda
calculus with recursive definitions and finite base-type values (finitary PCF)
is decidable. A recent paper by Dal Lago and Ghyselen has shown that the same
problem becomes undecidable when the language is extended with algebraic effect
and handlers (effect handlers). We show that, perhaps surprisingly, the problem
becomes decidable even with effect handlers when the type system is extended
with answer type modification (ATM). A natural intuition may find the result
contradictory, because one would expect allowing ATM makes more programs
typable. Indeed, this intuition is correct in that there are programs that are
typable with ATM but not without it, as we shall show in the paper. However, a
corollary of our decidability result is that the converse is true as well:
there are programs that are typable without ATM but becomes untypable with ATM,
and we will show concrete examples of such programs in the paper. Our
decidability result is proven by a novel continuation passing style (CPS)
transformation that transforms an ATM-typable finitary PCF program with effect
handlers to a finitary PCF program without effect handlers. Additionally, as
another application of our CPS transformation, we show that every
recursive-function-free ATM-typable finitary PCF program with effect handlers
terminates, while there are (necessarily ATM-untypable) recursive-function-free
finitary PCF programs with effect handlers that may diverge. Finally, we
disprove a claim made in a recent work that proved a similar but strictly
weaker decidability result. We foresee our decidability result to lay a
foundation for developing verification methods for programs with effect
handlers, just as the decidability result for reachability of finitary PCF has
done such for programs without effect handlers.

</details>


### [29] [From Interpolating Formulas to Separating Languages and Back Again](https://arxiv.org/abs/2508.12805)
*Agi Kurucz,Frank Wolter,Michael Zakharyaschev*

Main category: cs.LO

TL;DR: 本文调查克雷格插值的变体与普遍化，包括在缺乏CIP的逻辑中插值存在性判定、语言限制下的插值问题、正规语言分离问题，并将分离问题与LTL插值存在性联系起来。


<details>
  <summary>Details</summary>
Motivation: 传统克雷格插值研究仅关注具有CIP的逻辑，而忽略了缺乏CIP的逻辑中插值存在性问题。本文动机在于探索克雷格插值的更广泛应用和理论扩展。

Method: 采用综述性方法，从四个方面展开研究：1)在缺乏CIP的逻辑中研究插值存在性的可判定性与复杂度；2)考虑语言L'比原语言L更弱时的插值问题；3)将插值问题拓展到正规语言分离问题；4)利用正规语言分离的可判定性证明LTL插值存在性的可判定性。

Result: 本文系统地展示了克雷格插值理论在多个方向的扩展，特别是通过正规语言分离问题与逻辑插值问题的联系，实现了从形式语言到逻辑语言的跨领域应用，并完整地解决了LTL插值存在性的可判定性问题。

Conclusion: 克雷格插值理论可以较传统理解更为普遍，在缺乏CIP的逻辑中仍有重要价值。通过语言限制、正规分离等多角度的扩展，插值理论与语言理论、自动机理论等领域完美融合，为逻辑学和计算科学提供了新的研究视角和工具。

Abstract: Traditionally, research on Craig interpolation is concerned with (a)
establishing the Craig interpolation property (CIP) of a logic saying that
every valid implication in the logic has a Craig interpolant and (b) designing
algorithms that extract Craig interpolants from proofs. Logics that lack the
CIP are regarded as `pathological' and excluded from consideration. In this
chapter, we survey variations and generalisations of traditional Craig
interpolation. First, we consider Craig interpolants for implications in logics
without the CIP, focusing on the decidability and complexity of deciding their
existence. We then generalise interpolation by looking for Craig interpolants
in languages L' that can be weaker than the language L of the given
implication. Thus, do not only we restrict the non-logical symbols of Craig
interpolants but also the logical ones. The resulting L/L'-interpolation
problem generalises L/L'-definability, the question whether an L-formula is
equivalent to some L'-formula. After that, we move from logical languages to
formal languages where interpolation disguises itself as separation: given two
disjoint languages in a class C, does there exist a separating language in a
smaller class C'? This question is particularly well-studied in the case when
the input languages are regular and the separating language is first-order
definable. Finally, we connect the different research strands by showing how
the decidability of the separation problem for regular languages can be used to
prove the decidability of Craig interpolant existence for linear temporal logic
LTL.

</details>


### [30] [Compositional Verification of Almost-Sure Büchi Objectives in MDPs](https://arxiv.org/abs/2508.13087)
*Marck van der Vegt,Kazuki Watanabe,Ichiro Hasuo,Sebastian Junges*

Main category: cs.LO

TL;DR: 该论文研究在具有基于字符串图的可组合结构的MDP中验证几乎必然Büchi目标，提出了两种算法来分析策略是否能确保目标几乎必然满足。


<details>
  <summary>Details</summary>
Motivation: 研究如何在具有已知组合结构的马尔可夫决策过程(MDP)中验证几乎必然Büchi目标，利用字符串图的组合性质来简化验证过程。

Method: 提出了proper exit sets的概念作为充分必要条件统计量，开发了两种算法：自底向上的递归算法和多项式时间的迭代策略精化算法。

Result: 证明了proper exit sets与Büchi状态可达性一起构成了验证几乎必然Büchi目标的充分必要条件统计量。

Conclusion: 通过组合方法可以有效验证MDP中的几乎必然Büchi目标，迭代算法避免了指数级复杂度，提供了多项式时间的解决方案。

Abstract: This paper studies the verification of almost-sure B\"uchi objectives in MDPs
with a known, compositional structure based on string diagrams. In particular,
we ask whether there is a strategy that ensures that a B\"uchi objective is
almost-surely satisfied. We first show that proper exit sets -- the sets of
exits that can be reached within a component without losing locally -- together
with the reachability of a B\"uchi state are a sufficient and necessary
statistic for the compositional verification of almost-sure B\"uchi objectives.
The number of proper exit sets may grow exponentially in the number of exits.
We define two algorithms: (1) A straightforward bottom-up algorithm that
computes this statistic in a recursive manner to obtain the verification result
of the entire string diagram and (2) a polynomial-time iterative algorithm
which avoids computing all proper exit sets by performing iterative strategy
refinement.

</details>
