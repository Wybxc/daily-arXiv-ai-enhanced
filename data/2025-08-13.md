<div id=toc></div>

# Table of Contents

- [cs.LO](#cs.LO) [Total: 2]
- [cs.SE](#cs.SE) [Total: 8]


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [1] [Solving Set Constraints with Comprehensions and Bounded Quantifiers](https://arxiv.org/abs/2508.08496)
*Mudathir Mohamed,Nick Feng,Andrew Reynolds,Cesare Tinelli,Clark Barrett,Marsha Chechik*

Main category: cs.LO

TL;DR: 论文提出了一种使用集合有界量词的方法，通过有限关系理论和过滤器操作符实现，优于现有技术，并在特定问题中表现优异。


<details>
  <summary>Details</summary>
Motivation: SMT求解器在处理量化公式时存在困难，尤其是某些应用生成的公式。本文旨在探索更高效的量化技术。

Method: 采用集合有界量词，利用有限关系理论和过滤器操作符实现量化公式的求解。

Result: 该方法在SLEEC工具生成的可满足问题上表现优异，在不可满足问题上与专用求解器LEGOS竞争。同时发现了一类可判定的约束类。

Conclusion: 集合有界量词方法在特定场景下高效，且部分约束类可判定，但无限制应用会导致不可判定性。

Abstract: Many real applications problems can be encoded easily as quantified formulas
in SMT. However, this simplicity comes at the cost of difficulty during solving
by SMT solvers. Different strategies and quantifier instantiation techniques
have been developed to tackle this. However, SMT solvers still struggle with
quantified formulas generated by some applications. In this paper, we discuss
the use of set-bounded quantifiers, quantifiers whose variable ranges over a
finite set. These quantifiers can be implemented using quantifier-free fragment
of the theory of finite relations with a filter operator, a form of restricted
comprehension, that constructs a subset from a finite set using a predicate. We
show that this approach outperforms other quantification techniques in
satisfiable problems generated by the SLEEC tool, and is very competitive on
unsatisfiable benchmarks compared to LEGOS, a specialized solver for SLEEC. We
also identify a decidable class of constraints with restricted applications of
the filter operator, while showing that unrestricted applications lead to
undecidability.

</details>


### [2] [Behavioural Theory of Reflective Algorithms II: Reflective Parallel Algorithms](https://arxiv.org/abs/2508.09053)
*Klaus-Dieter Schewe,Flavio Ferrarotti*

Main category: cs.LO

TL;DR: 提出了反射并行算法（RAs）的行为理论，包括定义RAs的公理、抽象机器模型及证明该模型能捕获所有RAs。RAs是顺序时间并行算法，支持语言反射，通过多集理解项保持有界探索。


<details>
  <summary>Details</summary>
Motivation: 研究能够自我修改行为的同步并行算法（RAs），为其建立理论基础和计算模型。

Method: 提出反射抽象状态机（rASMs）作为RAs的抽象机器模型，扩展了ASMs以支持状态中包含可更新的主规则表示。

Result: 证明所有RAs均可由rASMs捕获，且通过多集理解项保持了有界探索性质。

Conclusion: rASMs为RAs提供了有效的计算模型，支持语言反射和行为修改。

Abstract: We develop a behavioural theory of reflective parallel algorithms (RAs), i.e.
synchronous parallel algorithms that can modify their own behaviour. The theory
comprises a set of postulates defining the class of RAs, an abstract machine
model, and the proof that all RAs are captured by this machine model. RAs are
sequential-time, parallel algorithms, where every state includes a
representation of the algorithm in that state, thus enabling linguistic
reflection. Bounded exploration is preserved using multiset comprehension terms
as values. The abstract machine model is defined by reflective Abstract State
Machines (rASMs), which extend ASMs using extended states that include an
updatable representation of the main ASM rule to be executed by the machine in
that state.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [Context Engineering for Multi-Agent LLM Code Assistants Using Elicit, NotebookLM, ChatGPT, and Claude Code](https://arxiv.org/abs/2508.08322)
*Muhammad Haseeb*

Main category: cs.SE

TL;DR: 论文提出了一种结合多AI组件的上下文工程工作流，显著提升了代码生成助手在复杂项目中的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成和软件工程任务中表现良好，但在处理复杂多文件项目时存在上下文限制和知识缺口。

Method: 结合意图翻译、语义文献检索、文档合成和多代理代码生成与验证，通过Claude代理框架协调。

Result: 方法在真实代码库中显著提高了单次成功率，并在Next.js代码库中展示了高效规划和测试能力。

Conclusion: 多代理系统和上下文注入提升了性能，为LLM代码助手在生产环境中的应用提供了新方向。

Abstract: Large Language Models (LLMs) have shown promise in automating code generation
and software engineering tasks, yet they often struggle with complex,
multi-file projects due to context limitations and knowledge gaps. We propose a
novel context engineering workflow that combines multiple AI components: an
Intent Translator (GPT-5) for clarifying user requirements, an Elicit-powered
semantic literature retrieval for injecting domain knowledge, NotebookLM-based
document synthesis for contextual understanding, and a Claude Code multi-agent
system for code generation and validation. Our integrated approach leverages
intent clarification, retrieval-augmented generation, and specialized
sub-agents orchestrated via Claude's agent framework. We demonstrate that this
method significantly improves the accuracy and reliability of code assistants
in real-world repositories, yielding higher single-shot success rates and
better adherence to project context than baseline single-agent approaches.
Qualitative results on a large Next.js codebase show the multi-agent system
effectively plans, edits, and tests complex features with minimal human
intervention. We compare our system with recent frameworks like CodePlan,
MASAI, and HyperAgent, highlighting how targeted context injection and agent
role decomposition lead to state-of-the-art performance. Finally, we discuss
the implications for deploying LLM-based coding assistants in production, along
with lessons learned on context management and future research directions.

</details>


### [4] [Energy-Aware Code Generation with LLMs: Benchmarking Small vs. Large Language Models for Sustainable AI Programming](https://arxiv.org/abs/2508.08332)
*Humza Ashraf,Syed Muhammad Danish,Aris Leivadeas,Yazan Otoum,Zeeshan Sattar*

Main category: cs.SE

TL;DR: 研究比较开源小型语言模型（SLMs）与大型商业模型（LLMs）在代码生成中的性能和能源效率，发现SLMs在正确时更节能。


<details>
  <summary>Details</summary>
Motivation: 商业LLMs的高能耗和碳排放引发环境担忧，研究旨在探索SLMs是否能替代LLMs并更节能。

Method: 评估150个LeetCode编程问题，比较3个开源SLMs和2个商业LLMs的运行时、内存、能耗和正确性。

Result: LLMs正确率最高，但SLMs在52%的问题中能耗相同或更低。

Conclusion: SLMs在正确时更节能，可作为LLMs的环保替代方案。

Abstract: Large Language Models (LLMs) are widely used for code generation. However,
commercial models like ChatGPT require significant computing power, which leads
to high energy use and carbon emissions. This has raised concerns about their
environmental impact. In this study, we evaluate open-source Small Language
Models (SLMs) trained explicitly for code generation and compare their
performance and energy efficiency against large LLMs and efficient
human-written Python code. The goal is to investigate whether SLMs can match
the performance of LLMs on certain types of programming problems while
producing more energy-efficient code. We evaluate 150 coding problems from
LeetCode, evenly distributed across three difficulty levels: easy, medium, and
hard. Our comparison includes three small open-source models, StableCode-3B,
StarCoderBase-3B, and Qwen2.5-Coder-3B-Instruct, and two large commercial
models, GPT-4.0 and DeepSeek-Reasoner. The generated code is evaluated using
four key metrics: run-time, memory usage, energy consumption, and correctness.
We use human-written solutions as a baseline to assess the quality and
efficiency of the model-generated code. Results indicate that LLMs achieve the
highest correctness across all difficulty levels, but SLMs are often more
energy-efficient when their outputs are correct. In over 52% of the evaluated
problems, SLMs consumed the same or less energy than LLMs.

</details>


### [5] [Improving Merge Pipeline Throughput in Continuous Integration via Pull Request Prioritization](https://arxiv.org/abs/2508.08342)
*Maximilian Jungwirth,Martin Gruber,Gordon Fraser*

Main category: cs.SE

TL;DR: 论文提出了一种基于历史构建数据和PR元数据的预测方法，优化合并管道中PR的顺序，以提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 大型单体软件仓库的合并管道常因高负载成为瓶颈，现有优化方法依赖特定构建系统，通用性不足。

Method: 利用历史构建数据、PR元数据和上下文信息预测构建成功概率，动态优先处理可能通过的PR。

Result: 实验表明，该方法在真实大规模项目中显著优于FIFO和非学习型排序策略。

Conclusion: 该方法不依赖特定构建系统，易于集成到现有合并管道中，有效提升效率。

Abstract: Integrating changes into large monolithic software repositories is a critical
step in modern software development that substantially impacts the speed of
feature delivery, the stability of the codebase, and the overall productivity
of development teams. To ensure the stability of the main branch, many
organizations use merge pipelines that test software versions before the
changes are permanently integrated. However, the load on merge pipelines is
often so high that they become bottlenecks, despite the use of parallelization.
Existing optimizations frequently rely on specific build systems, limiting
their generalizability and applicability. In this paper we propose to optimize
the order of PRs in merge pipelines using practical build predictions utilizing
only historical build data, PR metadata, and contextual information to estimate
the likelihood of successful builds in the merge pipeline. By dynamically
prioritizing likely passing PRs during peak hours, this approach maximizes
throughput when it matters most. Experiments conducted on a real-world,
large-scale project demonstrate that predictive ordering significantly
outperforms traditional first-in-first-out (FIFO), as well as
non-learning-based ordering strategies. Unlike alternative optimizations, this
approach is agnostic to the underlying build system and thus easily integrable
into existing automated merge pipelines.

</details>


### [6] [OmniLLP: Enhancing LLM-based Log Level Prediction with Context-Aware Retrieval](https://arxiv.org/abs/2508.08545)
*Youssef Esseddiq Ouatiti,Mohammed Sayagh,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: OmniLLP通过基于语义相似性和开发者所有权凝聚力的聚类方法，改进了基于LLM的日志级别预测（LLP），显著提升了预测准确性（AUC达0.88-0.96）。


<details>
  <summary>Details</summary>
Motivation: 日志级别选择对系统可观测性和性能至关重要，但现有基于LLM的LLP依赖随机上下文示例，忽略了代码结构和开发实践多样性。

Method: 提出OmniLLP框架，通过语义相似性和开发者所有权凝聚力聚类源文件，从中选择上下文示例，优化LLP预测。

Result: 语义和所有权聚类显著提升LLP准确性（AUC提高8%），结合两种信号的方法在多个项目中表现优异（AUC 0.88-0.96）。

Conclusion: 将代码语义和开发者所有权信号融入LLM-LLP，可提供更准确的日志级别预测，提升系统可维护性和可观测性。

Abstract: Developers insert logging statements in source code to capture relevant
runtime information essential for maintenance and debugging activities. Log
level choice is an integral, yet tricky part of the logging activity as it
controls log verbosity and therefore influences systems' observability and
performance. Recent advances in ML-based log level prediction have leveraged
large language models (LLMs) to propose log level predictors (LLPs) that
demonstrated promising performance improvements (AUC between 0.64 and 0.8).
Nevertheless, current LLM-based LLPs rely on randomly selected in-context
examples, overlooking the structure and the diverse logging practices within
modern software projects. In this paper, we propose OmniLLP, a novel LLP
enhancement framework that clusters source files based on (1) semantic
similarity reflecting the code's functional purpose, and (2) developer
ownership cohesion. By retrieving in-context learning examples exclusively from
these semantic and ownership aware clusters, we aim to provide more coherent
prompts to LLPs leveraging LLMs, thereby improving their predictive accuracy.
Our results show that both semantic and ownership-aware clusterings
statistically significantly improve the accuracy (by up to 8\% AUC) of the
evaluated LLM-based LLPs compared to random predictors (i.e., leveraging
randomly selected in-context examples from the whole project). Additionally,
our approach that combines the semantic and ownership signal for in-context
prediction achieves an impressive 0.88 to 0.96 AUC across our evaluated
projects. Our findings highlight the value of integrating software
engineering-specific context, such as code semantic and developer ownership
signals into LLM-LLPs, offering developers a more accurate, contextually-aware
approach to logging and therefore, enhancing system maintainability and
observability.

</details>


### [7] [Hallucinations in Code Change to Natural Language Generation: Prevalence and Evaluation of Detection Metrics](https://arxiv.org/abs/2508.08661)
*Chunhua Liu,Hong Yi Lin,Patanamon Thongtanunam*

Main category: cs.SE

TL;DR: 该论文首次全面分析了代码变更任务中的幻觉问题，发现50%的代码审查和20%的提交消息存在幻觉，并提出多指标结合的方法显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 研究代码变更任务（如提交消息生成和代码审查评论生成）中语言模型的幻觉问题，填补了现有研究的空白。

Method: 量化了近期语言模型中幻觉的普遍性，并探索了多种基于指标的自动检测方法。

Result: 约50%的代码审查和20%的提交消息包含幻觉；多指标结合显著提升了检测性能，模型置信度和特征归因指标表现突出。

Conclusion: 多指标结合的方法在幻觉检测中表现优异，为推理时检测提供了可行方案。

Abstract: Language models have shown strong capabilities across a wide range of tasks
in software engineering, such as code generation, yet they suffer from
hallucinations. While hallucinations have been studied independently in natural
language and code generation, their occurrence in tasks involving code changes
which have a structurally complex and context-dependent format of code remains
largely unexplored. This paper presents the first comprehensive analysis of
hallucinations in two critical tasks involving code change to natural language
generation: commit message generation and code review comment generation. We
quantify the prevalence of hallucinations in recent language models and explore
a range of metric-based approaches to automatically detect them. Our findings
reveal that approximately 50\% of generated code reviews and 20\% of generated
commit messages contain hallucinations. Whilst commonly used metrics are weak
detectors on their own, combining multiple metrics substantially improves
performance. Notably, model confidence and feature attribution metrics
effectively contribute to hallucination detection, showing promise for
inference-time detection.\footnote{All code and data will be released upon
acceptance.

</details>


### [8] [Description and Comparative Analysis of QuRE: A New Industrial Requirements Quality Dataset](https://arxiv.org/abs/2508.08868)
*Henning Femmer,Frank Houdek,Max Unterbusch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: 论文介绍了QuRE数据集，包含2111条工业需求，标注详细且来自真实工业环境，旨在提升需求质量研究的透明度和可比性。


<details>
  <summary>Details</summary>
Motivation: 需求质量对软件和系统工程至关重要，但现有数据集往往不完整或缺乏细节，阻碍了实证研究。

Method: 引入QuRE数据集，提供描述性统计（如词汇多样性和可读性），并与现有数据集及合成数据对比。

Result: QuRE在语言上与现有数据集相似，但标注更系统化且来自长期工业实践，提供详细上下文。

Conclusion: QuRE支持建立需求质量研究的共同标准，促进更严谨和协作的研究。

Abstract: Requirements quality is central to successful software and systems
engineering. Empirical research on quality defects in natural language
requirements relies heavily on datasets, ideally as realistic and
representative as possible. However, such datasets are often inaccessible,
small, or lack sufficient detail. This paper introduces QuRE (Quality in
Requirements), a new dataset comprising 2,111 industrial requirements that have
been annotated through a real-world review process. Previously used for over
five years as part of an industrial contract, this dataset is now being
released to the research community. In this work, we furthermore provide
descriptive statistics on the dataset, including measures such as lexical
diversity and readability, and compare it to existing requirements datasets and
synthetically generated requirements. In contrast to synthetic datasets, QuRE
is linguistically similar to existing ones. However, this dataset comes with a
detailed context description, and its labels have been created and used
systematically and extensively in an industrial context over a period of close
to a decade. Our goal is to foster transparency, comparability, and empirical
rigor by supporting the development of a common gold standard for requirements
quality datasets. This, in turn, will enable more sound and collaborative
research efforts in the field.

</details>


### [9] [Empirical Analysis of Temporal and Spatial Fault Characteristics in Multi-Fault Bug Repositories](https://arxiv.org/abs/2508.08872)
*Dylan Callaghan,Alexandra van der Spuy,Bernd Fischer*

Main category: cs.SE

TL;DR: 论文分析了开源Java和Python项目中软件故障的时间和空间特性，发现许多故障长期存在且分布均匀，挑战了现有数据集的假设。


<details>
  <summary>Details</summary>
Motivation: 降低软件维护成本需要了解故障特性，但现有数据集假设单一故障版本，与实际不符。

Method: 对Defects4J和BugsInPy数据集中的16个开源项目进行实证分析。

Result: 发现故障长期存在且分布均匀，多数版本共存多个故障，而非单一故障。

Conclusion: 研究结果挑战了现有数据集的假设，为优化测试和评估提供了新视角。

Abstract: Fixing software faults contributes significantly to the cost of software
maintenance and evolution. Techniques for reducing these costs require datasets
of software faults, as well as an understanding of the faults, for optimal
testing and evaluation. In this paper, we present an empirical analysis of the
temporal and spatial characteristics of faults existing in 16 open-source Java
and Python projects, which form part of the Defects4J and BugsInPy datasets,
respectively. Our findings show that many faults in these software systems are
long-lived, leading to the majority of software versions having multiple
coexisting faults. This is in contrast to the assumptions of the original
datasets, where the majority of versions only identify a single fault. In
addition, we show that although the faults are found in only a small subset of
the systems, these faults are often evenly distributed amongst this subset,
leading to relatively few bug hotspots.

</details>


### [10] [Toward Automated Hypervisor Scenario Generation Based on VM Workload Profiling for Resource-Constrained Environments](https://arxiv.org/abs/2508.08952)
*Hyunwoo Kim,Jaeseong Lee,Sunpyo Hong,Changmin Han*

Main category: cs.SE

TL;DR: 本文提出了一种自动化场景生成框架，帮助汽车行业高效分配硬件资源，优化虚拟机配置。


<details>
  <summary>Details</summary>
Motivation: 随着软件定义车辆（SDV）的兴起，汽车行业需要适应动态系统需求和工作负载的虚拟化架构，但现有静态配置难以满足这一需求。

Method: 通过分析运行时行为并结合理论模型和供应商启发式方法，开发了一种工具，生成优化的虚拟机配置。比较了两种建模方法：领域引导的参数化建模和基于深度学习的建模。

Result: 实际部署表明，该框架显著提高了集成效率并缩短了开发时间。

Conclusion: 该自动化框架为资源受限环境中的虚拟机配置提供了高效解决方案。

Abstract: In the automotive industry, the rise of software-defined vehicles (SDVs) has
  driven a shift toward virtualization-based architectures that consolidate
  diverse automotive workloads on a shared hardware platform. To support this
  evolution, chipset vendors provide board support packages (BSPs), hypervisor
  setups, and resource allocation guidelines. However, adapting these static
  configurations to varying system requirements and workloads remain a
  significant challenge for Tier 1 integrators.
  This paper presents an automated scenario generation framework, which helps
  automotive vendors to allocate hardware resources efficiently across multiple
  VMs. By profiling runtime behavior and integrating both theoretical models
and
  vendor heuristics, the proposed tool generates optimized hypervisor
  configurations tailored to system constraints.
  We compare two main approaches for modeling target QoS based on profiled data
  and resource allocation: domain-guided parametric modeling and deep
  learning-based modeling. We further describe our optimization strategy using
  the selected QoS model to derive efficient resource allocations. Finally, we
  report on real-world deployments to demonstrate the effectiveness of our
  framework in improving integration efficiency and reducing development time
in
  resource-constrained environments.

</details>
