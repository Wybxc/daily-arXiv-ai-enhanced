<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.LO](#cs.LO) [Total: 4]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 11]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Optimism in Equality Saturation](https://arxiv.org/abs/2511.20782)
*Russel Arbore,Alvin Cheung,Max Willsey*

Main category: cs.PL

TL;DR: 提出了一种基于抽象解释的乐观分析算法，用于在等式饱和过程中精确分析循环程序，特别是在SSA形式的程序中。


<details>
  <summary>Details</summary>
Motivation: 现有的e-class分析是悲观的，无法有效分析循环程序（如SSA形式程序），这限制了等式饱和在程序优化中的应用效果。

Method: 开发了一种抽象解释算法，将乐观分析与非破坏性重写统一起来，并在SSA程序的原型抽象解释器中实例化该方法，使用新的SSA语义。

Result: 原型系统能够比clang和gcc更精确地分析简单的示例程序。

Conclusion: 该方法为等式饱和提供了更精确的循环程序分析能力，在程序优化方面具有潜力。

Abstract: Equality saturation is a technique for program optimization based on non-destructive rewriting and a form of program analysis called e-class analysis. The current form of e-class analysis is pessimistic and therefore ineffective at analyzing cyclic programs, such as those in SSA form. We propose an abstract interpretation algorithm that can precisely analyze cycles during equality saturation. This results in a unified algorithm for optimistic analysis and non-destructive rewriting. We instantiate this approach on a prototype abstract interpreter for SSA programs using a new semantics of SSA. Our prototype can analyze simple example programs more precisely than clang and gcc.

</details>


### [2] [Towards Computational UIP in Cubical Agda](https://arxiv.org/abs/2511.21209)
*Yee-Jian Tan,Andreas Nuyts,Dominique Devriese*

Main category: cs.PL

TL;DR: 本文探讨了在Cubical Agda中实现h-集合立方类型论的方法，分析了UIP（身份证明唯一性）的不同表述形式及其计算规则，并实现了一个无Glue类型的Cubical Agda变体。


<details>
  <summary>Details</summary>
Motivation: 立方类型论具有商归纳类型和函数外延性等优势，但高阶等式层次可能使形式化变得复杂。通过截断等式层次到h-集合级别，可以保留这些优势同时简化理论。

Method: 分析UIP的不同表述形式及其计算规则，评估在Cubical Agda中实现的适用性。实现了一个无Glue类型的Cubical Agda变体，该变体已与假设的UIP兼容。

Result: 提出了在Cubical Agda中实现h-集合立方类型论的具体方法，包括UIP的表述分析和无Glue类型变体的实现。

Conclusion: 通过移除Glue类型和截断等式层次，可以在保留立方类型论优势的同时简化理论，为Cubical Agda中自动实现UIP提供了理论基础。

Abstract: Some advantages of Cubical Type Theory, as implemented by Cubical Agda, over intensional Martin-Löf Type Theory include Quotient Inductive Types (QITs), which exist as instances of Higher Inductive Types, and functional extensionality, which is provable in Cubical Type Theory. However, HoTT features an infinite hierarchy of equalities that may become unwieldy in formalisations. Fortunately, QITs and functional extensionality are both preserved even if the equality levels of Cubical Type Theory are truncated to only homotopical Sets (h-Sets). In other words, removing the univalence axiom from Cubical Type Theory and instead postulating a conflicting axiom: the Uniqueness of Identity Proofs (UIP) postulate. Since univalence is proved in Cubical Type Theory from the so-called Glue Types, therefore, it is known that one can first remove the Glue Types (thus removing univalence) and then set-truncate all equalities (essentially assuming UIP), à la XTT. The result is a "h-Set Cubical Type Theory" that retains features such as functional extensionality and QITs.
  However, in Cubical Agda, there are currently only two unsatisfying ways to achieve h-Set Cubical Type Theory. The first is to give up on the canonicity of the theory and simply postulate the UIP axiom, while the second way is to use a standard result stating "type formers preserve h-levels" to manually prove UIP for every defined type. The latter is, however, laborious work best suited for an automatic implementation by the proof assistant. In this project, we analyse formulations of UIP and detail their computation rules for Cubical Agda, and evaluate their suitability for implementation. We also implement a variant of Cubical Agda without Glue, which is already compatible with postulated UIP, in anticipation of a future implementation of UIP in Cubical Agda.

</details>


### [3] [SV-LIB 1.0: A Standard Exchange Format for Software-Verification Tasks](https://arxiv.org/abs/2511.21509)
*Dirk Beyer,Gidon Ernst,Martin Jonáš,Marian Lingsch-Rosenfeld*

Main category: cs.PL

TL;DR: SV-LIB是一个用于软件验证任务的交换格式和中间语言，基于命令式编程语言概念，使用SMT-LIB表示表达式和类型，支持验证见证格式。


<details>
  <summary>Details</summary>
Motivation: 解决不同编程语言验证工具之间的互操作性问题，促进验证技术的转移和重用。

Method: 设计基于SMT-LIB的中间语言格式，定义程序、规范和验证见证的统一表示，支持正确和错误程序的见证格式。

Result: 提出了SV-LIB 1.0版本，包括设计目标、语法和非形式化语义，为验证工具提供通用交换格式。

Conclusion: SV-LIB为软件验证工具提供了语言无关的中间表示，支持验证见证的独立验证，未来计划扩展形式化语义和并发支持。

Abstract: In the past two decades, significant research and development effort went into the development of verification tools for individual languages, such asC, C++, and Java. Many of the used verification approaches are in fact language-agnostic and it would be beneficial for the technology transfer to allow for using the implementations also for other programming and modeling languages. To address the problem, we propose SV-LIB, an exchange format and intermediate language for software-verification tasks, including programs, specifications, and verification witnesses. SV-LIBis based on well-known concepts from imperative programming languages and uses SMT-LIB to represent expressions and sorts used in the program. This makes it easy to parse and to build into existing infrastructure, since many verification tools are based on SMT solvers already. Furthermore, SV-LIBdefines a witness format for both correct and incorrect SV-LIB programs, together with means for specifying witness-validation tasks. This makes it possible both to implement independent witness validators and to reuse some verifiers also as validators for witnesses. This paper presents version 1.0 of the SV-LIBformat, including its design goals, the syntax, and informal semantics. Formal semantics and further extensions to concurrency are planned for future versions.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [4] [Coco: Corecursion with Compositional Heterogeneous Productivity](https://arxiv.org/abs/2511.21093)
*Jaewoo Kim,Yeonwoo Nam,Chung-Kil Hur*

Main category: cs.LO

TL;DR: 提出了CHP理论框架和Coco库，解决了证明助手中核心递归定义的限制问题，实现了高自动化与广泛覆盖的统一。


<details>
  <summary>Details</summary>
Motivation: 现有的证明助手对核心递归定义施加了严格的语法保护条件，拒绝了许多有效的核心递归定义。现有方法在覆盖范围和自动化之间存在根本性权衡。

Method: 提出了组合式异构生产力（CHP）理论框架，引入适用于不同域和共域类型的异构生产力概念，关键创新是组合性：复合函数的生产力从其组件系统计算得出。基于CHP开发了Coco核心递归库。

Result: CHP框架统一了高自动化与广泛覆盖，Coco库为Rocq提供了广泛的生产力计算和不动点生成自动化。

Conclusion: CHP和Coco成功解决了核心递归定义的限制问题，实现了覆盖范围和自动化的统一，为证明助手提供了更强大的核心递归支持。

Abstract: Contemporary proof assistants impose restrictive syntactic guardedness conditions that reject many valid corecursive definitions. Existing approaches to overcome these restrictions present a fundamental trade-off between coverage and automation.
  We present Compositional Heterogeneous Productivity (CHP), a theoretical framework that unifies high automation with extensive coverage for corecursive definitions. CHP introduces heterogeneous productivity applicable to functions with diverse domain and codomain types, including non-coinductive types. Its key innovation is compositionality: the productivity of composite functions is systematically computed from their components, enabling modular reasoning about complex corecursive patterns.
  Building on CHP, we develop Coco, a corecursion library for Rocq that provides extensive automation for productivity computation and fixed-point generation.

</details>


### [5] [Common Knowledge, Sailboats, and Publicity](https://arxiv.org/abs/2511.21261)
*Sena Bozdag,Olivier Roy*

Main category: cs.LO

TL;DR: 本文重新审视了关于共同知识的"帆船"案例，论证Lewis式共同知识能够调和直觉上某些事实是"公共"的，而这些事实在经典迭代意义上不能成为共同知识的矛盾。


<details>
  <summary>Details</summary>
Motivation: 解决Lederman(2018)提出的"帆船"案例中的共同知识难题，调和直觉上某些事实是公共的与这些事实在经典迭代意义上不能成为共同知识之间的矛盾。

Method: 首先非正式地阐述论证以澄清哲学承诺，然后在认知-可能性模型中提出一种形式化方法来捕捉这一论证。

Result: 论证了Lewis式共同知识能够解释"帆船"案例中直觉上的公共性，而经典迭代共同知识无法做到这一点。

Conclusion: 哲学论证和形式化论证共同表明，Lewis式共同知识是解释事件公共性含义的合理理论。

Abstract: We revisit a recent puzzle about common knowledge, the ``sailboat" case (Lederman, 2018), and argue that Lewisian common knowledge allows us to reconcile the pre-theoretical intuition that certain facts are ``public" in such situations, while these facts cannot be common knowledge in the classical, iterative sense. The crux of the argument is to understand Lewisian common knowledge as an account of what it means for an event to be public. We first formulate this argument informally to clarify its philosophical commitment and then propose one way to capture it formally in epistemic-plausibility models. Taken together, we take the philosophical and the formal arguments as providing evidence that Lewisian common knowledge is a plausible account of what it means for an event to be public.

</details>


### [6] [Bifurcation Logic: Separation Through Ordering](https://arxiv.org/abs/2511.21263)
*Didier Galmiche,Timo Lang,Daniel Méry,David Pym*

Main category: cs.LO

TL;DR: 本文介绍了分叉逻辑(BL)，它结合了经典模态逻辑和分离合取*及其相关的乘法蕴含，使用模态序定义。提供了BL的标记表列演算，证明了相对于关系语义的可靠性和完备性。BL不具有标准有限模型性质，但在没有乘法蕴含的情况下，每个模型都有等价的有限表示，从而获得可判定性。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够建模多智能体访问控制等场景的逻辑框架，其中分离合取*能够表达资源在分支结构上的分离性，乘法蕴含则自然地与模态序相关联。

Method: 定义了分叉逻辑BL的语法和语义，其中A*B在世界上成立当且仅当A和B分别在该世界上方不同分支上的世界中成立，且这些分支没有共同上界。提供了标记表列演算系统，并建立了相对于关系语义的可靠性和完备性。

Result: 证明了BL不具有标准有限模型性质，但在没有乘法蕴含但保留*的情况下，每个模型都有等价的有限表示，从而获得可判定性。通过多智能体访问控制的例子展示了BL的应用潜力。

Conclusion: 分叉逻辑BL提供了一种结合经典模态逻辑和分离合取的有用框架，能够建模分支结构上的资源分离，在访问控制等领域具有应用价值，同时在某些限制条件下具有可判定性。

Abstract: We introduce Bifurcation Logic, BL, which combines a basic classical modality with separating conjunction * together with its naturally associated multiplicative implication, that is defined using the modal ordering. Specifically, a formula A*B is true at a world w if and only if each of A,B holds at worlds that are each above w, on separate branches of the order, and have no common upper bound. We provide a labelled tableaux calculus for BL and establish soundness and completeness relative to its relational semantics. The standard finite model property fails for BL. However, we show that, in the absence of multiplicative implication, but in the presence of *, every model has an equivalent finite representation and that this is sufficient to obtain decidability. We illustrate the use of BL through an example of modelling multi-agent access control that is quite generic in its form, suggesting many applications.

</details>


### [7] [Two behavioural pseudometrics for continuous-time Markov processes](https://arxiv.org/abs/2511.21621)
*Linan Chen,Florence Clerc,Prakash Panangaden*

Main category: cs.LO

TL;DR: 本文提出了基于轨迹的第二种行为伪度量，用于度量连续时间马尔可夫过程（如扩散过程）中状态的行为等价性，并与之前基于时间索引马尔可夫核的伪度量进行比较。


<details>
  <summary>Details</summary>
Motivation: 在连续时间马尔可夫过程中，轨迹在行为等价性考虑中具有重要性，而之前的工作[11]依赖于时间索引马尔可夫核。本文旨在基于轨迹构建新的行为伪度量。

Method: 采用两种迭代方法构建伪度量：通过函数方法和实值逻辑方法，并证明这两种方法得到的结果是一致的。

Result: 提出了基于轨迹的第二种行为伪度量，该伪度量是从逻辑得到的特定不动点，与函数方法得到的伪度量一致。

Conclusion: 成功构建了基于轨迹的行为伪度量，为连续时间马尔可夫过程的行为等价性提供了新的度量工具，并与之前的方法进行了比较。

Abstract: Bisimulation is a concept that captures behavioural equivalence of states in a variety of types of transition systems. It has been widely studied in discrete-time settings where a key notion is the bisimulation metric which quantifies "how similar two states are". In [ 11], we generalized the concept of bisimulation metric in order to metrize the behaviour of continuous-time Markov processes. Similarly to the discrete-time case, we constructed a pseudometric following two iterative approaches - through a functional and through a real-valued logic, and showed that the outcomes coincide: the pseudometric obtained from the logic is a specific fixpoint of the functional which yields our first pseudometric. However, different from the discrete-time setting, in which the process has a step-by-step dynamics, the behavioural pseudometric we constructed applies to Markov processes that evolve continuously through time, such as diffusions and jump diffusions. While our treatment of the pseudometric in [11] relied on the time-indexed Markov kernels, in [ 8 , 9, 10 ], we showed the importance of trajectories in the consideration of behavioural equivalences for true continuous-time Markov processes. In this paper, we take the work from [11 ] further and propose a second behavioural pseudometric for diffusions based on trajectories. We conduct a similar study of this pseudometric from both the perspective of a functional and the viewpoint of a real-valued logic. We also compare this pseudometric with the first pseudometric obtained in [11].

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [8] [General Decidability Results for Systems with Continuous Counters](https://arxiv.org/abs/2511.21559)
*A. R. Balasubramanian,Matthew Hague,Rupak Majumdar,Ramanathan S. Thinniyam,Georg Zetzsche*

Main category: cs.FL

TL;DR: 连续计数器是离散计数器的有效过近似，通过将计数器值放松为非负有理数。本文证明连续计数器在可判定性方面表现极好，主要结果是：尽管连续计数器是无限状态的，但能够到达给定目标配置的计数器指令序列的语言是正则的，且可以有效地计算其有限自动机。


<details>
  <summary>Details</summary>
Motivation: 离散计数器在软件系统建模和验证中无处不在，但往往导致极高的复杂度。连续计数器作为离散计数器的过近似，旨在降低复杂度同时保持可判定性。

Method: 将离散计数器放松为连续计数器（非负有理数值），分析连续计数器指令序列的语言特性，证明其正则性并构造相应的有限自动机。

Result: 证明了连续计数器指令序列的语言是正则的，可以有效地计算其有限自动机，这意味着多种过渡系统可以配备连续计数器而保持可达性属性的可判定性。同时证明了所得有限自动机的大小存在非初等下界。

Conclusion: 连续计数器在保持可判定性的同时显著降低了复杂度，为高阶递归方案、良结构过渡系统等提供了有效的扩展方法。

Abstract: Counters that hold natural numbers are ubiquitous in modeling and verifying software systems; for example, they model dynamic creation and use of resources in concurrent programs. Unfortunately, such discrete counters often lead to extremely high complexity. Continuous counters are an efficient over-approximation of discrete counters. They are obtained by relaxing the original counters to hold values over the non-negative rational numbers.
  This work shows that continuous counters are extraordinarily well-behaved in terms of decidability. Our main result is that, despite continuous counters being infinite-state, the language of sequences of counter instructions that can arrive in a given target configuration, is regular. Moreover, a finite automaton for this language can be computed effectively. This implies that a wide variety of transition systems can be equipped with continuous counters, while maintaining decidability of reachability properties. Examples include higher-order recursion schemes, well-structured transition systems, and decidable extensions of discrete counter systems.
  We also prove a non-elementary lower bound for the size of the resulting finite automaton.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [9] [DUALGUAGE: Automated Joint Security-Functionality Benchmarking for Secure Code Generation](https://arxiv.org/abs/2511.20709)
*Abhijeet Pathak,Suvadra Barua,Dinesh Gudimetla,Rupam Patir,Jiawei Guo,Hongxin Hu,Haipeng Cai*

Main category: cs.SE

TL;DR: DUALGAUGE是首个完全自动化的基准测试框架，用于同时严格评估LLM生成代码的安全性和正确性，解决了现有基准测试仅关注漏洞减少或忽视正确性保持的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估安全代码生成方面存在不足——许多只衡量漏洞减少，忽视正确性保持，或在单独数据集上评估安全性和功能性，违反了同时联合评估的基本需求。

Method: 提出了DUALGAUGE框架，包括：1) DUALGAUGE-BENCH基准套件，包含多样化编码任务和手动验证的安全与功能测试套件；2) 基于代理的程序执行器，在沙盒环境中运行程序测试；3) LLM评估器，根据预期结果评估正确性和漏洞行为。

Result: 对十个领先LLM在数千个测试场景中的评估结果显示，这些LLM在生成正确和安全代码方面存在关键差距。

Conclusion: DUALGAUGE的开源系统和数据集有助于通过可重复、可扩展和严格的评估加速安全代码生成领域的进展。

Abstract: Large language models (LLMs) and autonomous coding agents are increasingly used to generate software across a wide range of domains. Yet a core requirement remains unmet: ensuring that generated code is secure without compromising its functional correctness. Existing benchmarks and evaluations for secure code generation fall short-many measure only vulnerability reduction, disregard correctness preservation, or evaluate security and functionality on separate datasets, violating the fundamental need for simultaneous joint evaluation. We present DUALGAUGE, the first fully automated benchmarking framework designed to rigorously evaluate the security and correctness of LLM-generated code in unison. Given the lack of datasets enabling joint evaluation of secure code generation, we also present DUALGAUGE-BENCH, a curated benchmark suite of diverse coding tasks, each paired with manually validated test suites for both security and functionality, designed for full coverage of specification requirements. At the core of DUALGAUGE is an agentic program executor, which runs a program against given tests in sandboxed environments, and an LLM-based evaluator, which assesses both correctness and vulnerability behavior against expected outcomes. We rigorously evaluated and ensured the quality of DUALGAUGE-BENCH and the accuracy of DUALGAUGE, and applied DUALGAUGE to benchmarking ten leading LLMs on DUALGAUGE-BENCH across thousands of test scenarios. Our results reveal critical gaps in correct and secure code generation by these LLMs, for which our open-source system and datasets help accelerate progress via reproducible, scalable, and rigorous evaluation.

</details>


### [10] [Data-Driven Methods and AI in Engineering Design: A Systematic Literature Review Focusing on Challenges and Opportunities](https://arxiv.org/abs/2511.20730)
*Nehal Afifi,Christoph Wittig,Lukas Paehler,Andreas Lindenmann,Kai Wolter,Felix Leitenberger,Melih Dogru,Patric Grauberger,Tobias Düser,Albert Albers,Sven Matthiesen*

Main category: cs.SE

TL;DR: 本文通过系统文献综述分析了数据驱动方法在工程设计中应用的现状、趋势和挑战，发现机器学习和统计方法占主导，深度学习呈上升趋势，但在验证阶段应用有限，存在模型可解释性差等问题。


<details>
  <summary>Details</summary>
Motivation: 数据驱动方法在产品开发中的应用日益增多，但集成仍然零散，主要由于不确定在何时使用何种方法。需要调查DDM在工程设计中的使用情况，包括方法类型、开发阶段和应用领域。

Method: 采用PRISMA系统文献综述方法，使用V模型作为产品开发框架简化为四个阶段，在Scopus、Web of Science和IEEE Xplore数据库(2014-2024)进行结构化检索，筛选后对114篇文献进行全文分析。

Result: 研究发现机器学习和统计方法占主导地位，深度学习虽然较少但呈上升趋势；监督学习、聚类、回归分析和代理建模在设计、实施和集成阶段普遍，但在验证阶段贡献有限；主要挑战包括模型可解释性差、跨阶段可追溯性不足、真实条件下验证不充分。

Conclusion: 这是制定设计阶段指南的第一步，后续需要将计算机科学算法映射到工程设计问题和活动中，并开发可解释的混合模型。

Abstract: The increasing availability of data and advancements in computational intelligence have accelerated the adoption of data-driven methods (DDMs) in product development. However, their integration into product development remains fragmented. This fragmentation stems from uncertainty, particularly the lack of clarity on what types of DDMs to use and when to employ them across the product development lifecycle. To address this, a necessary first step is to investigate the usage of DDM in engineering design by identifying which methods are being used, at which development stages, and for what application. This paper presents a PRISMA systematic literature review. The V-model as a product development framework was adopted and simplified into four stages: system design, system implementation, system integration, and validation. A structured search across Scopus, Web of Science, and IEEE Xplore (2014--2024) retrieved 1{,}689 records. After screening, 114 publications underwent full-text analysis. Findings show that machine learning (ML) and statistical methods dominate current practice, whereas deep learning (DL), though still less common, exhibits a clear upward trend in adoption. Additionally, supervised learning, clustering, regression analysis, and surrogate modeling are prevalent in design, implementation, and integration system stages but contributions to validation remain limited. Key challenges in existing applications include limited model interpretability, poor cross-stage traceability, and insufficient validation under real-world conditions. Additionally, it highlights key limitations and opportunities such as the need for interpretable hybrid models. This review is a first step toward design-stage guidelines; a follow-up synthesis should map computer science algorithms to engineering design problems and activities.

</details>


### [11] [Train While You Fight -- Technical Requirements for Advanced Distributed Learning Platforms](https://arxiv.org/abs/2511.20813)
*Simon Hacks*

Main category: cs.SE

TL;DR: 本文探讨了支持"边战边训"(TWYF)的先进分布式学习平台所需技术需求，以及现有软件工程模式如何满足这些需求。


<details>
  <summary>Details</summary>
Motivation: 推动在作战行动期间进行持续学习，而不仅仅是在行动前后进行训练，以提升军事训练效率和适应性。

Method: 采用设计科学研究方法：从PfPC/北约文档和近期实践中推导挑战、定义解决方案目标、进行从挑战到已验证模式的系统映射。

Result: 识别了七个技术挑战：互操作性、弹性、多语言支持、数据安全与隐私、可扩展性、平台独立性和模块化，并通过德国武装部队的国家用例说明模式应用。

Conclusion: 现有软件工程模式可以有效支持"边战边训"的技术需求，为先进分布式学习平台的设计提供了实用指导。

Abstract: "Train While You Fight" (TWYF) advocates for continuous learning that occurs during operations, not just before or after. This paper examines the technical requirements that advanced distributed learning (ADL) platforms must meet to support TWYF, and how existing software engineering patterns can fulfill these requirements. Using a Design Science Research approach, we (i) derive challenges from PfPC/NATO documentation and recent practice, (ii) define solution objectives, and (iii) conduct a systematic mapping from challenges to proven patterns. We identify seven technical challenges: interoperability, resilience, multilingual support, data security and privacy, scalability, platform independence, and modularity. We illustrate the patterns with a national use case from the German armed forces.

</details>


### [12] [Application of machine learning for infrastructure reconstruction programs management](https://arxiv.org/abs/2511.20916)
*Illia Khudiakov,Vladyslav Pliuhin,Sergiy Plankovskyy,Yevgen Tsegelnyk*

Main category: cs.SE

TL;DR: 开发了一个自适应决策支持模型，用于提高工程基础设施重建项目的管理效率，通过系统建模和机器学习预测目标函数值来优化项目架构和工作分解结构。


<details>
  <summary>Details</summary>
Motivation: 现有工程基础设施重建项目管理工具效率不足，需要自适应模型来优化项目架构和工作分解结构的创建，以适应不同的项目目标和条件。

Method: 使用系统建模工具和机器学习方法（包括人工神经网络），基于历史数据集预测目标函数值，在Microsoft Azure Machine Learning Studio中实现功能组成。

Result: 定义了模型的主要组件，包括决策者偏好、决策任务、输入数据集和应用软件组件，并给出了神经网络参数和评估结果。

Conclusion: 该自适应模型可应用于热力、燃气、电力供应、供水和排水等工程系统重建项目的管理，能够根据项目目标调整决策过程。

Abstract: The purpose of this article is to describe an adaptive decision-making support model aimed at improving the efficiency of engineering infrastructure reconstruction program management in the context of developing the architecture and work breakdown structure of programs. As part of the study, the existing adaptive program management tools are analyzed, the use of infrastructure systems modelling tools is justified for program architecture and WBS creation. Existing models and modelling methods are viewed, and machine learning and artificial neural networks are selected for the model. The main components of the model are defined, which include a set of decision-maker preferences, decision-making tasks, sets of input data, and applied software components of the model. To support decision-making, the adaptive model applies the method of system modeling and predicting the value of the objective function at a given system configuration. Prediction is done using machine learning methods based on a dataset consisting of historical data related to existing engineering systems. The work describes the components of the redistribution of varied model parameters, which modify the model dataset based on the selected object type, which allows adapting the decision-making process to the existing program implementation goals. The functional composition done in Microsoft Azure Machine Learning Studio is described. The neural network parameters and evaluation results are given. The application of the developed adaptive model is possible in the management of programs for the reconstruction of such engineering systems as systems of heat, gas, electricity supply, water supply, and drainage, etc.

</details>


### [13] [Hierarchical Evaluation of Software Design Capabilities of Large Language Models of Code](https://arxiv.org/abs/2511.20933)
*Mootez Saad,Boqi Chen,José Antonio Hernández López,Dániel Varró,Tushar Sharma*

Main category: cs.SE

TL;DR: 评估DeepSeek-R1模型家族对软件设计概念（内聚性和耦合性）的理解，发现在理想条件下表现良好，但在实际噪声环境中性能显著下降，特别是对耦合性的推理能力很脆弱。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在软件工程领域的应用增加，需要系统评估其对核心软件设计概念的理解鲁棒性，特别是在实际开发环境中。

Method: 通过程序化生成设计不良的代码片段，在不同指导级别（验证、引导、开放式生成）下测试DeepSeek-R1模型家族（14B、32B、70B），并注入干扰元素来模拟上下文噪声。

Result: 模型在理想条件下对两个概念都有良好理解，但实际知识脆弱且高度不对称。耦合性推理在噪声开放式场景中性能崩溃（F1分数下降超50%），而内聚性分析在引导任务中对内部噪声具有鲁棒性。

Conclusion: LLM在识别设计缺陷方面能提供可靠帮助，但在噪声现实环境中自主推理能力有限，需要更可扩展和鲁棒的程序理解能力。

Abstract: Large language models (LLMs) are being increasingly adopted in the software engineering domain, yet the robustness of their grasp on core software design concepts remains unclear. We conduct an empirical study to systematically evaluate their understanding of cohesion (intra-module) and coupling (inter-module). We programmatically generate poorly designed code fragments and test the DeepSeek-R1 model family ($14$B, $32$B, $70$B) under varying levels of guidance, from simple \textit{Verification} to \textit{Guided} and \textit{Open-ended Generation}, while varying contextual noise by injecting distractor elements. While models exhibit a solid baseline understanding of both concepts in ideal conditions, their practical knowledge is fragile and highly asymmetrical. Reasoning about coupling proves brittle; performance collapses in noisy, open-ended scenarios, with F1 scores dropping by over $50\%$. In contrast, the models' analysis of cohesion is remarkably robust to internal noise in guided tasks, showing little performance degradation. However, this resilience also fails when all guidance is removed. Reasoning-trace analysis confirms these failure modes, revealing \textit{cognitive shortcutting} for coupling versus a more exhaustive (yet still failing) analysis for cohesion. To summarize, while LLMs can provide reliable assistance for recognizing design flaws, their ability to reason autonomously in noisy, realistic contexts is limited, highlighting the critical need for more scalable and robust program understanding capabilities.

</details>


### [14] [SpaceX: Exploring metrics with the SPACE model for developer productivity](https://arxiv.org/abs/2511.20955)
*Sanchit Kaul,Kevin Nhu,Jason Eissayou,Ivan Eser,Victor Borup*

Main category: cs.SE

TL;DR: 本研究通过SPACE框架分析开源仓库数据，发现负面情绪与提交频率正相关，提出复合生产力评分(CPS)来更全面衡量开发者效率


<details>
  <summary>Details</summary>
Motivation: 传统单一维度的生产力启发式方法存在局限性，需要更全面、多方面的生产力评估框架

Method: 使用广义线性混合模型(GLM)和RoBERTa情感分类，通过大规模仓库挖掘分析开发者行为和情感状态

Result: 负面情感状态与提交频率呈显著正相关，表明存在由挫败感驱动的迭代修复循环；贡献者互动拓扑分析比传统基于数量的指标能更好地映射协作动态

Conclusion: 提出复合生产力评分(CPS)来应对开发者效率的异质性，为生产力评估提供更全面的框架

Abstract: This empirical investigation elucidates the limitations of deterministic, unidimensional productivity heuristics by operationalizing the SPACE framework through extensive repository mining. Utilizing a dataset derived from open-source repositories, the study employs rigorous statistical methodologies including Generalized Linear Mixed Models (GLMM) and RoBERTa-based sentiment classification to synthesize a holistic, multi-faceted productivity metric. Analytical results reveal a statistically significant positive correlation between negative affective states and commit frequency, implying a cycle of iterative remediation driven by frustration. Furthermore, the investigation has demonstrated that analyzing the topology of contributor interactions yields superior fidelity in mapping collaborative dynamics compared to traditional volume-based metrics. Ultimately, this research posits a Composite Productivity Score (CPS) to address the heterogeneity of developer efficacy.

</details>


### [15] [Lightweight Model Editing for LLMs to Correct Deprecated API Recommendations](https://arxiv.org/abs/2511.21022)
*Guancheng Lin,Xiao Yu,Jacky Keung,Xing Hu,Xin Xia,Alex X. Liu*

Main category: cs.SE

TL;DR: 本文研究了如何通过模型编辑技术更新LLM中的过时API知识，提出了AdaLoRA-L方法，通过区分通用API层和特定API层来提升编辑特异性。


<details>
  <summary>Details</summary>
Motivation: LLM在代码补全任务中经常生成已废弃的API，因为其训练数据存在时效性问题。重新训练模型成本高昂，因此需要轻量级的模型编辑方法来更新API知识。

Method: 系统评估了10种模型编辑技术在3个LLM上的效果，提出了AdaLoRA-L方法，该方法定义'通用API层'和'特定API层'，仅在特定API层进行编辑以避免影响其他知识。

Result: AdaLoRA在生成正确API方面表现最佳，但特异性不足。AdaLoRA-L显著提升了特异性，同时在其他评估指标上保持可比性能。

Conclusion: 模型编辑技术可以有效更新LLM中的过时API知识，AdaLoRA-L方法通过分层编辑策略在保持性能的同时提升了特异性。

Abstract: Pre-trained or fine-tuned on large code corpora, Large Language Models (LLMs) have demonstrated strong performance in code completion tasks. However, their embedded knowledge is constrained by the timeliness of training data, which often includes code using deprecated APIs. Consequently, LLMs frequently generate deprecated APIs that will no longer be supported in future versions of third-party libraries. While retraining LLMs on updated codebases could refresh their API knowledge, this approach is computationally expensive. Recently, lightweight model editing methods have emerged to efficiently correct specific knowledge in LLMs. However, it remains unclear whether these methods can effectively update deprecated API knowledge and enable edited models to generate up-to-date APIs. To address this gap, we conduct the first systematic study applying 10 state-of-the-art model editing techniques to update deprecated API knowledge in three LLMs: Qwen2.5-Coder, StarCoder2, and DeepSeek-Coder. We introduce EDAPIBench, a dedicated benchmark featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances. Our results show that the parameter-efficient fine-tuning method AdaLoRA achieves the best performance in enabling edited models to generate correct, up-to-date APIs, but falls short in Specificity (i.e., the editing influences untargeted knowledge). To resolve this, we propose AdaLoRA-L, which defines "Common API Layers" (layers within the LLMs with high importance across all APIs, storing general knowledge and excluded from editing) and restricts edits exclusively to "Specific API Layers" (layers with high importance only for the target API, storing the API-specific knowledge). Experimental results demonstrate that AdaLoRA-L significantly improves Specificity while maintaining comparable performance across other evaluation metrics.

</details>


### [16] [Exploring Hidden Geographic Disparities in Android Apps](https://arxiv.org/abs/2511.21151)
*M. Alecci,P. Jiménez,J. Samhi,T. Bissyandé,J. Klein*

Main category: cs.SE

TL;DR: 该论文研究了Android应用的地理差异现象，发现了GeoTwins（功能相似但在不同国家发布的应用变体）和App Bundle生态系统中base.apk文件的区域差异，这些差异对安全评估、隐私保护和功能一致性产生重要影响。


<details>
  <summary>Details</summary>
Motivation: 移动应用的地理差异现象尚未得到充分研究，这种差异可能导致安全评估的不一致、隐私披露的差异以及功能定制的不透明，影响研究的可重复性和用户权益。

Method: 构建了分布式应用收集管道，跨越多个地区收集和分析数千个应用，识别GeoTwins和App Bundle文件的区域差异。

Result: 发现了81,963个GeoTwins，揭示了应用在不同地区存在权限请求、第三方库和隐私披露的差异；同时发现base.apk文件也存在区域定制，挑战了传统认为其一致的假设。

Conclusion: 移动软件存在系统性的区域差异，这对研究人员、开发者、平台架构师和政策制定者都具有重要影响，需要关注地理差异带来的安全、隐私和公平性问题。

Abstract: While mobile app evolution has been widely studied, geographical variation in app behavior remains largely unexplored. This paper presents a large-scale study of location-based Android app differentiation, uncovering two important and underexamined phenomena with security and fairness implications. First, we introduce GeoTwins: apps that are functionally similar and share branding but are released under different package names across countries. Despite their similarity, GeoTwins often diverge in requested permissions, third-party libraries, and privacy disclosures. Second, we examine the Android App Bundle ecosystem and reveal unexpected regional differences in supposedly consistent base.apk files. Contrary to common assumptions, even base.apk files vary by region, exposing hidden customizations that may affect app behavior or security.
  These discrepancies have concrete consequences. Geographically distinct variants can lead the same app to be labeled benign in one malware study but suspicious in another, depending on the region of download. Such hidden variation undermines reproducibility and introduces geographic bias into assessments of security, privacy, and functionality. It also raises ethical concerns about transparency and consent: visually identical Google Play listings may mask subtle but important differences.
  To study these issues, we built a distributed app collection pipeline spanning multiple regions and analyzed thousands of apps. We also release a dataset of 81,963 GeoTwins to support future work. Our findings reveal systemic regional disparities in mobile software, with implications for researchers, developers, platform architects, and policymakers.

</details>


### [17] [Bug Detective and Quality Coach: Developers' Mental Models of AI-Assisted IDE Tools](https://arxiv.org/abs/2511.21197)
*Paolo Buono,Mary Cerullo,Stefano Cirillo,Giuseppe Desolda,Francesco Greco,Emanuela Guglielmi,Grazia Margarella,Giuseppe Polese,Simone Scalabrino,Cesare Tucci*

Main category: cs.SE

TL;DR: 开发者将AI辅助bug检测工具视为"bug侦探"，将可读性评估工具视为"质量教练"。信任取决于解释清晰度、时机和用户控制。研究提出了以人为本的IDE设计原则。


<details>
  <summary>Details</summary>
Motivation: 尽管AI辅助工具在技术特性上有所进步，但开发者如何心理建模这些工具以及不匹配如何影响信任、控制和采用尚不清楚。

Method: 通过6次协同设计工作坊，与58名开发者合作，了解他们对AI辅助bug检测和可读性功能的心理模型。

Result: 开发者将bug检测工具视为仅对关键问题发出警告的"bug侦探"，强调透明度、可操作反馈和信心提示；可读性评估工具则被视为提供情境化、个性化、渐进指导的"质量教练"。

Conclusion: 提出了以人为本的IDE设计原则，旨在平衡干扰与支持、简洁与深度、自动化与人类自主权。

Abstract: AI-assisted tools support developers in performing cognitively demanding tasks such as bug detection and code readability assessment. Despite the advancements in the technical characteristics of these tools, little is known about how developers mentally model them and how mismatches affect trust, control, and adoption. We conducted six co-design workshops with 58 developers to elicit their mental models about AI-assisted bug detection and readability features. It emerged that developers conceive bug detection tools as \textit{bug detectives}, which warn users only in case of critical issues, guaranteeing transparency, actionable feedback, and confidence cues. Readability assessment tools, on the other hand, are envisioned as \textit{quality coaches}, which provide contextual, personalized, and progressive guidance. Trust, in both tasks, depends on the clarity of explanations, timing, and user control. A set of design principles for Human-Centered AI in IDEs has been distilled, aiming to balance disruption with support, conciseness with depth, and automation with human agency.

</details>


### [18] [Multi-Agent Systems for Dataset Adaptation in Software Engineering: Capabilities, Limitations, and Future Directions](https://arxiv.org/abs/2511.21380)
*Jingyi Chen,Xiaoyan Guo,Songqiang Chen,Shing-Chi Cheung,Jiasi Shen*

Main category: cs.SE

TL;DR: 本文首次实证研究了最先进的多智能体系统在数据集适应任务中的表现，评估了基于GPT-4.1和Claude Sonnet 4的Copilot系统，发现当前系统能识别关键文件并生成部分适应，但很少能产生功能正确的实现。


<details>
  <summary>Details</summary>
Motivation: 自动化软件工程研究工件的跨数据集适应对于可扩展性和可复现性至关重要，但目前研究不足。多智能体系统有望通过协调推理、代码生成和工具交互来自动化复杂开发工作流。

Method: 通过五阶段评估管道（文件理解、代码编辑、命令生成、验证和最终执行）评估Copilot系统在ROCODE和LogHub2.0基准库上的表现，测量成功率、分析失败模式，并评估提示干预策略。

Result: 当前系统能够识别关键文件并生成部分适应，但很少产生功能正确的实现。提示级干预（特别是提供执行错误消息和参考代码）显著提高了与真实情况的结构相似性（从7.25%提升到67.14%）。

Conclusion: 研究揭示了当前多智能体LLM系统在数据集适应方面的潜力和局限性，为未来构建更可靠、自校正的SE研究智能体提供了具体方向。

Abstract: Automating the adaptation of software engineering (SE) research artifacts across datasets is essential for scalability and reproducibility, yet it remains largely unstudied. Recent advances in large language model (LLM)-based multi-agent systems, such as GitHub Copilot's agent mode, promise to automate complex development workflows through coordinated reasoning, code generation, and tool interaction. This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks. We evaluate Copilot, backed by GPT-4.1 and Claude Sonnet 4, on adapting SE research artifacts from benchmark repositories including ROCODE and LogHub2.0. Through a five-stage evaluation pipeline (file comprehension, code editing, command generation, validation, and final execution), we measure success rates, analyze failure patterns, and assess prompt-based interventions designed to enhance agent performance. Results show that current systems can identify key files and generate partial adaptations but rarely produce functionally correct implementations. Prompt-level interventions, especially providing execution error messages and reference code, substantially improve structural similarity to ground truth (from 7.25% to 67.14%), highlighting the importance of contextual and feedback-driven guidance. Our findings reveal both the promise and limitations of today's multi-agent LLM systems for dataset adaptation, and suggest concrete directions for building more reliable, self-correcting agents in future SE research.

</details>


### [19] [Large Language Models for Unit Test Generation: Achievements, Challenges, and the Road Ahead](https://arxiv.org/abs/2511.21382)
*Bei Chu,Yang Feng,Kui Liu,Zifan Nan,Zhaoqiang Guo,Baowen Xu*

Main category: cs.SE

TL;DR: 系统文献综述分析了115篇关于使用大语言模型(LLM)进行单元测试生成的论文，提出了基于测试生成生命周期的统一分类法，发现提示工程是主要策略，迭代验证修复循环成为标准机制，但存在故障检测能力弱和缺乏标准化评估基准等挑战。


<details>
  <summary>Details</summary>
Motivation: 传统自动化测试方法缺乏语义信息生成真实输入和断言，而LLM能够利用其数据驱动的代码语义和编程模式知识来弥补这一局限性。

Method: 通过对115篇文献的系统综述，提出基于单元测试生成生命周期的统一分类法，将LLM视为需要系统工程约束的随机生成器，分析核心生成策略和从生成前上下文丰富到生成后质量保证的增强技术。

Result: 提示工程成为主导利用策略(占研究的89%)，迭代验证修复循环显著提高了编译和执行通过率，但生成的测试在故障检测能力方面仍然薄弱。

Conclusion: 未来研究应朝着自主测试代理和结合LLM与传统软件工程工具的混合系统方向发展，将LLM潜力转化为工业级测试解决方案。

Abstract: Unit testing is an essential yet laborious technique for verifying software and mitigating regression risks. Although classic automated methods effectively explore program structures, they often lack the semantic information required to produce realistic inputs and assertions. Large Language Models (LLMs) address this limitation by utilizing by leveraging their data-driven knowledge of code semantics and programming patterns. To analyze the state of the art in this domain, we conducted a systematic literature review of 115 publications published between May 2021 and August 2025. We propose a unified taxonomy based on the unit test generation lifecycle that treats LLMs as stochastic generators requiring systematic engineering constraints. This framework analyzes the literature regarding core generative strategies and a set of enhancement techniques ranging from pre-generation context enrichment to post-generation quality assurance. Our analysis reveals that prompt engineering has emerged as the dominant utilization strategy and accounts for 89% of the studies due to its flexibility. We find that iterative validation and repair loops have become the standard mechanism to ensure robust usability and lead to significant improvements in compilation and execution pass rates. However, critical challenges remain regarding the weak fault detection capabilities of generated tests and the lack of standardized evaluation benchmarks. We conclude with a roadmap for future research that emphasizes the progression towards autonomous testing agents and hybrid systems combining LLMs with traditional software engineering tools. This survey provides researchers and practitioners with a comprehensive perspective on converting the potential of LLMs into industrial-grade testing solutions.

</details>
