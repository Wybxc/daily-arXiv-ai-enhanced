<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.PL](#cs.PL) [Total: 4]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Technique to Baseline QE Artefact Generation Aligned to Quality Metrics](https://arxiv.org/abs/2511.15733)
*Eitan Farchi,Kiran Nayak,Papia Ghosh Majumdar,Saritha Route*

Main category: cs.SE

TL;DR: 提出了一种使用量化指标来基准化和评估质量工程产物的系统技术，结合LLM驱动生成、反向生成和基于准则的迭代优化，确保产物的清晰度、完整性、一致性和可测试性。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型能够自动化生成质量工程产物，但确保这些产物的质量仍然是一个挑战，需要系统化的评估方法。

Method: 采用LLM驱动生成、反向生成和基于准则（清晰度、完整性、一致性、可测试性）的迭代优化技术。

Result: 在12个项目中的实验结果表明，反向生成的产物能够超越低质量输入，并在输入质量高时保持高标准。

Conclusion: 该框架实现了可扩展、可靠的质量工程产物验证，在自动化与责任性之间架起了桥梁。

Abstract: Large Language Models (LLMs) are transforming Quality Engineering (QE) by automating the generation of artefacts such as requirements, test cases, and Behavior Driven Development (BDD) scenarios. However, ensuring the quality of these outputs remains a challenge. This paper presents a systematic technique to baseline and evaluate QE artefacts using quantifiable metrics. The approach combines LLM-driven generation, reverse generation , and iterative refinement guided by rubrics technique for clarity, completeness, consistency, and testability. Experimental results across 12 projects show that reverse-generated artefacts can outperform low-quality inputs and maintain high standards when inputs are strong. The framework enables scalable, reliable QE artefact validation, bridging automation with accountability.

</details>


### [2] [Rethinking Kernel Program Repair: Benchmarking and Enhancing LLMs with RGym](https://arxiv.org/abs/2511.15757)
*Kareem Shehada,Yifan Wu,Wyatt D. Feng,Adithya Iyer,Gryphon Kumfert,Yangruibo Ding,Zhiyun Qian*

Main category: cs.SE

TL;DR: RGym是一个轻量级、平台无关的Linux内核自动程序修复评估框架，在本地硬件上运行，使用专门的定位技术，在143个bug上达到43.36%的修复成功率，成本低于每bug 0.20美元。


<details>
  <summary>Details</summary>
Motivation: 当前APR基准主要关注用户空间应用，忽略了内核空间调试和修复的复杂性。Linux内核由于其单体结构、并发性和低级硬件交互而带来独特挑战，现有方法成功率低或依赖昂贵复杂的基础设施。

Method: 基于RGym框架，提出简单有效的APR流水线，利用专门的定位技术（如调用栈和受指责提交）来克服KGym中不切实际的oracle使用。

Result: 在143个经过过滤和验证的bug上，使用GPT-5 Thinking达到43.36%的通过率，每bug成本低于0.20美元。消融研究表明反馈式重试可以显著提高成功率。

Conclusion: RGym提供了一个轻量级、成本效益高的Linux内核APR评估解决方案，展示了专门定位技术和反馈机制在提升内核bug修复成功率方面的重要性。

Abstract: Large Language Models (LLMs) have revolutionized automated program repair (APR) but current benchmarks like SWE-Bench predominantly focus on userspace applications and overlook the complexities of kernel-space debugging and repair. The Linux kernel poses unique challenges due to its monolithic structure, concurrency, and low-level hardware interactions. Prior efforts such as KGym and CrashFixer have highlighted the difficulty of APR in this domain, reporting low success rates or relying on costly and complex pipelines and pricey cloud infrastructure. In this work, we introduce RGym, a lightweight, platform-agnostic APR evaluation framework for the Linux kernel designed to operate on local commodity hardware. Built on RGym, we propose a simple yet effective APR pipeline leveraging specialized localization techniques (e.g., call stacks and blamed commits) to overcome the unrealistic usage of oracles in KGym. We test on a filtered and verified dataset of 143 bugs. Our method achieves up to a 43.36% pass rate with GPT-5 Thinking while maintaining a cost of under $0.20 per bug. We further conduct an ablation study to analyze contributions from our proposed localization strategy, prompt structure, and model choice, and demonstrate that feedback-based retries can significantly enhance success rates.

</details>


### [3] [A Causal Perspective on Measuring, Explaining and Mitigating Smells in \llm-Generated Code](https://arxiv.org/abs/2511.15817)
*Alejandro Velasco,Daniel Rodriguez-Cardenas,Dipin Khati,David N. Palacio,Luftar Rahman Alif,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 本文系统性地测量、解释和缓解LLM生成代码中的代码异味倾向，提出了PSC指标来评估代码结构质量，并通过因果分析识别影响异味倾向的关键因素。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在软件工程中应用广泛，但其生成的代码存在结构质量问题，经常复制不良编码实践引入代码异味。目前缺乏对这些问题的系统理解和缓解方法。

Method: 基于概率性度量PSC评估代码异味倾向，通过因果分析研究生成策略、模型大小、架构和提示设计对代码结构质量的影响，并提出实际缓解策略。

Result: 发现提示设计和架构选择对异味倾向起决定性作用，提出的缓解策略能有效减少异味出现。用户研究表明PSC能帮助开发者解释模型行为和评估代码质量。

Conclusion: 这项工作为在LLM代码评估和部署中集成质量感知评估奠定了基础，异味倾向信号可以支持人类判断。

Abstract: Recent advances in large language models (LLMs) have accelerated their adoption in software engineering contexts. However, concerns persist about the structural quality of the code they produce. In particular, LLMs often replicate poor coding practices, introducing code smells (i.e., patterns that hinder readability, maintainability, or design integrity). Although prior research has examined the detection or repair of smells, we still lack a clear understanding of how and when these issues emerge in generated code.
  This paper addresses this gap by systematically measuring, explaining and mitigating smell propensity in LLM-generated code. We build on the Propensity Smelly Score (PSC), a probabilistic metric that estimates the likelihood of generating particular smell types, and establish its robustness as a signal of structural quality. Using PSC as an instrument for causal analysis, we identify how generation strategy, model size, model architecture and prompt formulation shape the structural properties of generated code. Our findings show that prompt design and architectural choices play a decisive role in smell propensity and motivate practical mitigation strategies that reduce its occurrence. A user study further demonstrates that PSC helps developers interpret model behavior and assess code quality, providing evidence that smell propensity signals can support human judgement. Taken together, our work lays the groundwork for integrating quality-aware assessments into the evaluation and deployment of LLMs for code.

</details>


### [4] [AI-Enabled Orchestration of Event-Driven Business Processes in Workday ERP for Healthcare Enterprises](https://arxiv.org/abs/2511.15852)
*Monu Sharma*

Main category: cs.SE

TL;DR: 提出了一种在Workday ERP中基于AI的事件驱动编排框架，通过机器学习触发器和异常检测来同步医疗机构的财务和供应链工作流程，提高了运营效率和决策准确性。


<details>
  <summary>Details</summary>
Motivation: 传统ERP系统的工作流逻辑缺乏适应性，无法有效管理事件驱动和数据密集的医疗环境，需要更智能的自动化解决方案。

Method: 采用机器学习触发器、异常检测和流程挖掘分析技术，构建AI驱动的事件驱动编排框架，在Workday ERP中实现财务和供应链工作流程的智能同步。

Result: 多组织案例分析显示在流程效率、成本可视化和决策准确性方面取得了可衡量的改进，增强了运营弹性、治理和可扩展性。

Conclusion: 将AI能力嵌入Workday的事件驱动架构中，为医疗企业下一代自动化策略建立了参考，推动了智能ERP集成的更广泛理解。

Abstract: The adoption of cloud-based Enterprise Resource Planning (ERP) platforms such as Workday has transformed healthcare operations by integrating financial, supply-chain, and workforce processes into a unified ecosystem. However, traditional workflow logic in ERP systems often lacks the adaptability required to manage event-driven and data-intensive healthcare environments.
  This study proposes an AI-enabled event-driven orchestration framework within Workday ERP that intelligently synchronizes financial and supply-chain workflows across distributed healthcare entities. The framework employs machine-learning triggers, anomaly detection, and process mining analytics to anticipate and automate responses to operational events such as inventory depletion, payment delays, or patient demand fluctuations. A multi-organization case analysis demonstrates measurable gains in process efficiency, cost visibility, and decision accuracy.
  Results confirm that embedding AI capabilities into Workday's event-based architecture enhances operational resilience, governance, and scalability. The proposed model contributes to the broader understanding of intelligent ERP integration and establishes a reference for next-generation automation strategies in healthcare enterprises.

</details>


### [5] [RE for AI in Practice: Managing Data Annotation Requirements for AI Autonomous Driving Systems](https://arxiv.org/abs/2511.15859)
*Hina Saeeda,Mazen Mohamad,Eric Knauss,Jennifer Horkoff,Ali Nouri*

Main category: cs.SE

TL;DR: 该研究探讨了自动驾驶AI感知系统中数据标注要求的定义、挑战和改进方法，通过19个访谈揭示了标注要求与AI系统性能之间的关键关联。


<details>
  <summary>Details</summary>
Motivation: 高质量数据标注要求对开发安全可靠的自动驾驶AI感知系统至关重要，但目前其制定和管理研究不足，导致不一致性、安全风险和监管问题。

Method: 对来自6家国际公司和4个研究机构的参与者进行19次半结构化访谈，并进行主题分析。

Result: 识别出五个主要挑战：模糊性、边缘案例复杂性、需求演变、不一致性和资源限制；以及三类最佳实践：确保符合道德标准、改进数据标注要求指南、嵌入式质量保证。

Conclusion: 该研究首次提供基于实证的标注要求改进指导，揭示了标注要求缺陷如何在AI系统开发流程中传播，为软件工程和需求工程领域做出贡献。

Abstract: High-quality data annotation requirements are crucial for the development of safe and reliable AI-enabled perception systems (AIePS) in autonomous driving. Although these requirements play a vital role in reducing bias and enhancing performance, their formulation and management remain underexplored, leading to inconsistencies, safety risks, and regulatory concerns. Our study investigates how annotation requirements are defined and used in practice, the challenges in ensuring their quality, practitioner-recommended improvements, and their impact on AIePS development and performance. We conducted $19$ semi-structured interviews with participants from six international companies and four research organisations. Our thematic analysis reveals five main key challenges: ambiguity, edge case complexity, evolving requirements, inconsistencies, and resource constraints and three main categories of best practices, including ensuring compliance with ethical standards, improving data annotation requirements guidelines, and embedded quality assurance for data annotation requirements. We also uncover critical interrelationships between annotation requirements, annotation practices, annotated data quality, and AIePS performance and development, showing how requirement flaws propagate through the AIePS development pipeline. To the best of our knowledge, this study is the first to offer empirically grounded guidance on improving annotation requirements, offering actionable insights to enhance annotation quality, regulatory compliance, and system reliability. It also contributes to the emerging fields of Software Engineering (SE for AI) and Requirements Engineering (RE for AI) by bridging the gap between RE and AI in a timely and much-needed manner.

</details>


### [6] [InfCode: Adversarial Iterative Refinement of Tests and Patches for Reliable Software Issue Resolution](https://arxiv.org/abs/2511.16004)
*KeFan Li,Mengfei Wang,Hengzhi Zhang,Zhichao Li,Yuan Yuan,Mu Li,Xiang Gao,Hailong Sun,Chunming Hu,Weifeng Lv*

Main category: cs.SE

TL;DR: InfCode是一个对抗性多智能体框架，通过测试生成器和代码补丁生成器的对抗性交互，迭代优化测试和补丁，实现仓库级软件问题的自动化解决。


<details>
  <summary>Details</summary>
Motivation: 现有基于代理和流水线的方法依赖不充分的测试，可能导致补丁通过验证但未能修复底层缺陷，需要仓库级推理、准确诊断和强验证信号来解决真实世界软件问题。

Method: 采用对抗性多智能体框架，包含测试补丁生成器和代码补丁生成器进行迭代对抗交互，选择器智能体识别最可靠的修复，在容器化环境中运行支持真实仓库检查、修改和验证。

Result: 在SWE-bench Lite和SWE-bench Verified上的实验显示，InfCode持续优于强基线，在SWE-bench Verified上达到79.4%的性能，创造了新的最先进水平。

Conclusion: InfCode通过对抗性多智能体框架有效解决了仓库级软件问题，显著提升了自动化软件修复的性能，已作为开源项目发布。

Abstract: Large language models have advanced software engineering automation, yet resolving real-world software issues remains difficult because it requires repository-level reasoning, accurate diagnostics, and strong verification signals. Existing agent-based and pipeline-based methods often rely on insufficient tests, which can lead to patches that satisfy verification but fail to fix the underlying defect. We present InfCode, an adversarial multi-agent framework for automated repository-level issue resolution. InfCode iteratively refines both tests and patches through adversarial interaction between a Test Patch Generator and a Code Patch Generator, while a Selector agent identifies the most reliable fix. The framework runs inside a containerized environment that supports realistic repository inspection, modification, and validation. Experiments on SWE-bench Lite and SWE-bench Verified using models such as DeepSeek-V3 and Claude 4.5 Sonnet show that InfCode consistently outperforms strong baselines. It achieves 79.4% performance on SWE-bench Verified, establishing a new state-of-the-art. We have released InfCode as an open-source project at https://github.com/Tokfinity/InfCode.

</details>


### [7] [InfCode-C++: Intent-Guided Semantic Retrieval and AST-Structured Search for C++ Issue Resolution](https://arxiv.org/abs/2511.16005)
*Qingao Dong,Mengfei Wang,Hengzhi Zhang,Zhichao Li,Yuan Yuan,Mu Li,Xiang Gao,Hailong Sun,Chunming Hu,Weifeng Lv*

Main category: cs.SE

TL;DR: INFCODE-C++是首个专门针对C++的自主问题解决系统，通过结合语义代码意图检索和确定性AST结构化查询，在C++项目中实现了25.58%的问题解决率，比现有最佳代理提升了10.85个百分点。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理主要针对Python设计，在C++项目中表现不佳，因为C++的复杂特性（如重载标识符、嵌套命名空间、模板实例化、深度控制流结构）使得上下文检索和故障定位更加困难。

Method: 结合两种互补的检索机制：语义代码意图检索和确定性AST结构化查询，构建准确的语言感知上下文进行修复，实现精确的本地化和稳健的补丁合成。

Result: 在MultiSWE-bench-CPP基准测试中，INFCODE-C++实现了25.58%的问题解决率，比最强现有代理提升了10.85个百分点，是MSWE-agent性能的两倍以上。

Conclusion: INFCODE-C++强调了多语言软件代理中语言感知推理的必要性，为未来在复杂静态类型生态系统中进行可扩展的LLM驱动修复研究奠定了基础。

Abstract: Large language model (LLM) agents have recently shown strong performance on repository-level issue resolution, but existing systems are almost exclusively designed for Python and rely heavily on lexical retrieval and shallow code navigation. These approaches transfer poorly to C++ projects, where overloaded identifiers, nested namespaces, template instantiations, and deep control-flow structures make context retrieval and fault localization substantially more difficult. As a result, state-of-the-art Python-oriented agents show a drastic performance drop on the C++ subset of MultiSWE-bench. We introduce INFCODE-C++, the first C++-aware autonomous system for end-to-end issue resolution. The system combines two complementary retrieval mechanisms -- semantic code-intent retrieval and deterministic AST-structured querying -- to construct accurate, language-aware context for repair.These components enable precise localization and robust patch synthesis in large, statically typed C++ repositories. Evaluated on the \texttt{MultiSWE-bench-CPP} benchmark, INFCODE-C++ achieves a resolution rate of 25.58\%, outperforming the strongest prior agent by 10.85 percentage points and more than doubling the performance of MSWE-agent. Ablation and behavioral studies further demonstrate the critical role of semantic retrieval, structural analysis, and accurate reproduction in C++ issue resolution. INFCODE-C++ highlights the need for language-aware reasoning in multi-language software agents and establishes a foundation for future research on scalable, LLM-driven repair for complex, statically typed ecosystems.

</details>


### [8] [The Future of Development Environments with AI Foundation Models: NII Shonan Meeting 222 Report](https://arxiv.org/abs/2511.16092)
*Xing Hu,Raula Gaikovina Kula,Christoph Treude*

Main category: cs.SE

TL;DR: 33位专家讨论生成式AI对集成开发环境(IDE)的影响，探讨挑战与机遇


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI在代码生成、测试、代码审查和程序修复等任务中的出色表现如何改变IDE中的人机交互

Method: 召集33位来自软件工程、人工智能和人机交互领域的专家，在Shonan Meeting 222会议上进行讨论

Result: 生成了关于生成式AI对IDE影响的专家讨论报告

Conclusion: 生成式AI有潜力通过提高抽象级别来改变IDE中的人机交互方式

Abstract: Generative Artificial Intelligence (GenAI) models are achieving remarkable performance in various tasks, including code generation, testing, code review, and program repair. The ability to increase the level of abstraction away from writing code has the potential to change the Human-AI interaction within the integrated development environment (IDE). To explore the impact of GenAI on IDEs, 33 experts from the Software Engineering, Artificial Intelligence, and Human-Computer Interaction domains gathered to discuss challenges and opportunities at Shonan Meeting 222. This is the report

</details>


### [9] [Domain-constrained Synthesis of Inconsistent Key Aspects in Textual Vulnerability Descriptions](https://arxiv.org/abs/2511.16123)
*Linyi Han,Shidong Pan,Zhenchang Xing,Sofonias Yitagesu,Xiaowang Zhang,Zhiyong Feng,Jiamou Sun,Qing Huang*

Main category: cs.SE

TL;DR: 提出基于领域约束LLM的框架，通过提取、自评估和融合三阶段统一文本漏洞描述中的关键方面，提高合成性能并开发可视化工具。


<details>
  <summary>Details</summary>
Motivation: 不同存储库中文本漏洞描述的关键方面不一致，现有方法丢弃有价值信息且无法合成全面表示，需要解决这些挑战。

Method: 三阶段框架：1)基于规则模板的提取确保捕获关键细节；2)使用领域特定锚词评估语义变异性；3)利用信息熵调和不一致并优先相关细节。

Result: 关键方面增强的F1分数从0.82提高到0.87，理解和效率提升超过30%，开发的可视化工具显著提高可用性。

Conclusion: 提出的框架有效统一了文本漏洞描述的关键方面，提高了合成性能和可用性，为安全分析提供了更全面的漏洞理解。

Abstract: Textual Vulnerability Descriptions (TVDs) are crucial for security analysts to understand and address software vulnerabilities. However, the key aspect inconsistencies in TVDs from different repositories pose challenges for achieving a comprehensive understanding of vulnerabilities. Existing approaches aim to mitigate inconsistencies by aligning TVDs with external knowledge bases, but they often discard valuable information and fail to synthesize comprehensive representations. In this paper, we propose a domain-constrained LLM-based synthesis framework for unifying key aspects of TVDs. Our framework consists of three stages: 1) Extraction, guided by rule-based templates to ensure all critical details are captured; 2) Self-evaluation, using domain-specific anchor words to assess semantic variability across sources; and 3) Fusion, leveraging information entropy to reconcile inconsistencies and prioritize relevant details. This framework improves synthesis performance, increasing the F1 score for key aspect augmentation from 0.82 to 0.87, while enhancing comprehension and efficiency by over 30\%. We further develop Digest Labels, a practical tool for visualizing TVDs, which human evaluations show significantly boosts usability.

</details>


### [10] [Beyond Code Similarity: Benchmarking the Plausibility, Efficiency, and Complexity of LLM-Generated Smart Contracts](https://arxiv.org/abs/2511.16224)
*Francesco Salzano,Simone Scalabrino,Rocco Oliveto,Simone Scalabrino*

Main category: cs.SE

TL;DR: 评估LLM生成Solidity智能合约的可靠性，发现虽然代码语义相似度高，但功能正确性低（仅20-26%），生成的代码更简单但缺乏验证逻辑，RAG能显著提升性能


<details>
  <summary>Details</summary>
Motivation: 智能合约在区块链生态中至关重要，但LLM生成的Solidity代码在gas消耗、安全性和确定性等独特约束下的可靠性存在疑问，现有研究缺乏对这些关键功能和非功能属性的全面评估

Method: 对4个最先进模型进行零样本和检索增强生成基准测试，涵盖500个真实函数，使用代码相似性指标、语义嵌入、自动化测试执行、gas分析、认知和圈复杂度分析进行多维度评估

Result: LLM生成的代码语义相似度高但功能正确性低，零样本生成中只有20-26%与真实实现行为一致，生成的代码复杂度更低、gas消耗更少，但往往省略验证逻辑，RAG将功能正确性提升高达45%

Conclusion: LLM生成的智能合约在语义相似性和功能合理性之间存在显著差距，RAG是强大的增强工具，但实现稳健的生产就绪代码生成仍是重大挑战，需要专家仔细验证

Abstract: Smart Contracts are critical components of blockchain ecosystems, with Solidity as the dominant programming language. While LLMs excel at general-purpose code generation, the unique constraints of Smart Contracts, such as gas consumption, security, and determinism, raise open questions about the reliability of LLM-generated Solidity code. Existing studies lack a comprehensive evaluation of these critical functional and non-functional properties. We benchmark four state-of-the-art models under zero-shot and retrieval-augmented generation settings across 500 real-world functions. Our multi-faceted assessment employs code similarity metrics, semantic embeddings, automated test execution, gas profiling, and cognitive and cyclomatic complexity analysis. Results show that while LLMs produce code with high semantic similarity to real contracts, their functional correctness is low: only 20% to 26% of zero-shot generations behave identically to ground-truth implementations under testing. The generated code is consistently simpler, with significantly lower complexity and gas consumption, often due to omitted validation logic. Retrieval-Augmented Generation markedly improves performance, boosting functional correctness by up to 45% and yielding more concise and efficient code. Our findings reveal a significant gap between semantic similarity and functional plausibility in LLM-generated Smart Contracts. We conclude that while RAG is a powerful enhancer, achieving robust, production-ready code generation remains a substantial challenge, necessitating careful expert validation.

</details>


### [11] [Data Annotation Quality Problems in AI-Enabled Perception System Development](https://arxiv.org/abs/2511.16410)
*Hina Saeeda,Tommy Johansson,Mazen Mohamad,Eric Knauss*

Main category: cs.SE

TL;DR: 本研究通过多组织案例研究开发了一个包含18种标注错误类型的分类法，涵盖完整性、准确性和一致性三个数据质量维度，为构建可信的AI感知系统提供了共享词汇、诊断工具和行动指南。


<details>
  <summary>Details</summary>
Motivation: 数据标注在自动驾驶AI感知系统开发中至关重要但容易出错，而行业缺乏关于标注错误如何在多组织汽车供应链中产生和传播的实证见解。

Method: 采用多组织案例研究，涉及6家公司和4个研究机构，基于19次半结构化访谈（20位专家，50小时转录文本）和六阶段主题分析。

Result: 开发了包含18种重复出现的标注错误类型的分类法，涵盖完整性（如属性遗漏、边缘案例遗漏）、准确性（如错误标注、边界框不准确）和一致性（如标注者间分歧、跨模态不一致）三个维度，并得到行业从业者的验证。

Conclusion: 通过将标注质量概念化为生命周期和供应链问题，本研究为SE4AI提供了共享词汇、诊断工具集和可操作指导，有助于构建可信的AI感知系统。

Abstract: Data annotation is essential but highly error-prone in the development of AI-enabled perception systems (AIePS) for automated driving, and its quality directly influences model performance, safety, and reliability. However, the industry lacks empirical insights into how annotation errors emerge and spread across the multi-organisational automotive supply chain. This study addresses this gap through a multi-organisation case study involving six companies and four research institutes across Europe and the UK. Based on 19 semi-structured interviews with 20 experts (50 hours of transcripts) and a six-phase thematic analysis, we develop a taxonomy of 18 recurring annotation error types across three data-quality dimensions: completeness (e.g., attribute omission, missing feedback loops, edge-case omissions, selection bias), accuracy (e.g., mislabelling, bounding-box inaccuracies, granularity mismatches, bias-driven errors), and consistency (e.g., inter-annotator disagreement, ambiguous instructions, misaligned hand-offs, cross-modality inconsistencies). The taxonomy was validated with industry practitioners, who reported its usefulness for root-cause analysis, supplier quality reviews, onboarding, and improving annotation guidelines. They described it as a failure-mode catalogue similar to FMEA. By conceptualising annotation quality as a lifecycle and supply-chain issue, this study contributes to SE4AI by offering a shared vocabulary, diagnostic toolset, and actionable guidance for building trustworthy AI-enabled perception systems.

</details>


### [12] [Green Resilience of Cyber-Physical Systems: Doctoral Dissertation](https://arxiv.org/abs/2511.16593)
*Diaeddin Rimawi*

Main category: cs.SE

TL;DR: 该研究提出了GResilience框架，通过多目标优化、博弈论决策和强化学习来平衡在线协作AI系统的韧性与绿色性，实现系统在中断后的绿色恢复。


<details>
  <summary>Details</summary>
Motivation: 在线协作AI系统容易受到中断事件影响，需要在恢复性能的同时限制能耗，这产生了韧性与绿色性之间的权衡问题。

Method: 将系统建模为稳态、中断和最终三个状态，开发GResilience框架提供恢复策略，包括多目标优化、博弈论决策和强化学习三种方法，并设计了量化韧性与绿色性的测量框架。

Result: 实验表明韧性模型能有效捕捉中断期间的性能转换，GResilience策略通过缩短恢复时间、稳定性能和减少人类依赖来改善绿色恢复，强化学习策略效果最佳但CO2排放略有增加。

Conclusion: 该研究提供了确保在线协作AI系统绿色恢复的模型、指标和策略，容器化执行可将CO2排放减半。

Abstract: Cyber-physical systems (CPS) combine computational and physical components. Online Collaborative AI System (OL-CAIS) is a type of CPS that learn online in collaboration with humans to achieve a common goal, which makes it vulnerable to disruptive events that degrade performance. Decision-makers must therefore restore performance while limiting energy impact, creating a trade-off between resilience and greenness. This research addresses how to balance these two properties in OL-CAIS. It aims to model resilience for automatic state detection, develop agent-based policies that optimize the greenness-resilience trade-off, and understand catastrophic forgetting to maintain performance consistency. We model OL-CAIS behavior through three operational states: steady, disruptive, and final. To support recovery during disruptions, we introduce the GResilience framework, which provides recovery strategies through multi-objective optimization (one-agent), game-theoretic decision-making (two-agent), and reinforcement learning (RL-agent). We also design a measurement framework to quantify resilience and greenness. Empirical evaluation uses real and simulated experiments with a collaborative robot learning object classification from human demonstrations. Results show that the resilience model captures performance transitions during disruptions, and that GResilience policies improve green recovery by shortening recovery time, stabilizing performance, and reducing human dependency. RL-agent policies achieve the strongest results, although with a marginal increase in CO2 emissions. We also observe catastrophic forgetting after repeated disruptions, while our policies help maintain steadiness. A comparison with containerized execution shows that containerization cuts CO2 emissions by half. Overall, this research provides models, metrics, and policies that ensure the green recovery of OL-CAIS.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [13] [Synthesis of Safety Specifications for Probabilistic Systems](https://arxiv.org/abs/2511.16579)
*Gaspard Ohlmann,Edwin Hamel-De le Court,Francesco Belardinelli*

Main category: cs.LO

TL;DR: 提出了一种支持PCTL时序属性的控制器合成新方法，将全局规范转化为局部约束，并开发了基于值迭代的算法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要局限于概率规避约束，而形式化方法如PCTL能更丰富地表达概率系统中的安全性，需要支持更一般的时序属性。

Method: 开发了安全PCTL规范合成的理论框架，定义CPCTL片段，并提出基于值迭代的新算法来解决合成问题。

Result: 证明了方法的正确性和完备性，CPCTL片段在合成问题中具有相关表达能力。

Conclusion: 该方法能够处理更一般的PCTL时序属性，为安全关键环境中的控制器合成提供了更强大的工具。

Abstract: Ensuring that agents satisfy safety specifications can be crucial in safety-critical environments. While methods exist for controller synthesis with safe temporal specifications, most existing methods restrict safe temporal specifications to probabilistic-avoidance constraints. Formal methods typically offer more expressive ways to express safety in probabilistic systems, such as Probabilistic Computation Tree Logic (PCTL) formulas. Thus, in this paper, we develop a new approach that supports more general temporal properties expressed in PCTL. Our contribution is twofold. First, we develop a theoretical framework for the Synthesis of safe-PCTL specifications. We show how the reducing global specification satisfaction to local constraints, and define CPCTL, a fragment of safe-PCTL. We demonstrate how the expressiveness of CPCTL makes it a relevant fragment for the Synthesis Problem. Second, we leverage these results and propose a new Value Iteration-based algorithm to solve the synthesis problem for these more general temporal properties, and we prove the soundness and completeness of our method.

</details>


### [14] [Faster Certified Symmetry Breaking Using Orders With Auxiliary Variables](https://arxiv.org/abs/2511.16637)
*Markus Anders,Bart Bogaerts,Benjamin Bogø,Arthur Gontier,Wietze Koops,Ciaran McCreesh,Magnus O. Myreen,Jakob Nordström,Andy Oertel,Adrian Rebola-Pardo,Yong Kiam Tan*

Main category: cs.LO

TL;DR: 提出了一种使用辅助变量编码排序的新方法，用于在证明中验证对称性破坏，相比之前的大整数编码方法在理论和实践上都实现了数量级的加速。


<details>
  <summary>Details</summary>
Motivation: 对称性破坏是现代组合求解中的关键技术，但难以确保正确实现。现有基于大整数编码的方法在处理大规模对称性时不可行，需要更高效的验证方法。

Method: 开发了使用辅助变量编码排序的方法，替代之前的大整数编码，在satsuma对称性破坏器和VeriPB证明检查工具链上进行实验验证。

Result: 新方法在理论和实践上都实现了数量级的加速，显著提高了证明记录和检查的效率。

Conclusion: 辅助变量编码排序的方法有效解决了大规模对称性验证的可行性问题，为对称性破坏的正确性验证提供了实用解决方案。

Abstract: Symmetry breaking is a crucial technique in modern combinatorial solving, but it is difficult to be sure it is implemented correctly. The most successful approach to deal with bugs is to make solvers certifying, so that they output not just a solution, but also a mathematical proof of correctness in a standard format, which can then be checked by a formally verified checker. This requires justifying symmetry reasoning within the proof, but developing efficient methods for this has remained a long-standing open challenge. A fully general approach was recently proposed by Bogaerts et al. (2023), but it relies on encoding lexicographic orders with big integers, which quickly becomes infeasible for large symmetries. In this work, we develop a method for instead encoding orders with auxiliary variables. We show that this leads to orders-of-magnitude speed-ups in both theory and practice by running experiments on proof logging and checking for SAT symmetry breaking using the state-of-the-art satsuma symmetry breaker and the VeriPB proof checking toolchain.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [15] [Filling the Gaps of Polarity: Implementing Dependent Data and Codata Types with Implicit Arguments](https://arxiv.org/abs/2511.15819)
*Bohdan Liesnikov,David Binder,Tim Süberkrüb*

Main category: cs.PL

TL;DR: 本文提出了Polarity语言的算法类型系统和隐式参数推理算法，该语言对称处理归纳类型和余归纳类型，解决了表达式问题中的两种扩展性权衡。


<details>
  <summary>Details</summary>
Motivation: 大多数依赖类型语言通过归纳类型支持模式匹配操作扩展，但通过余归纳类型支持新构造器扩展的能力较差。Polarity语言虽然对称处理两种类型，但缺乏隐式参数等现代依赖类型语言特性。

Method: 提供了Polarity语言的完整算法类型系统描述，包括归约语义、转换检查和模式匹配的统一算法，特别设计了尊重语言核心对称性的隐式参数推理算法。

Result: 开发了涵盖任意归纳和余归纳类型的统一算法，给出了可用的实现规则，并提供了正在进行中的实现。

Conclusion: 本文的统一算法和设计决策可以为其他对称支持归纳和余归纳类型的依赖类型语言提供蓝图。

Abstract: The expression problem describes a fundamental tradeoff between two types of extensibility: extending a type with new operations, such as by pattern matching on an algebraic data type in functional programming, and extending a type with new constructors, such as by adding a new object implementing an interface in object-oriented programming. Most dependently typed languages have good support for the former style through inductive types, but support for the latter style through coinductive types is usually much poorer. Polarity is a language that treats both kinds of types symmetrically and allows the developer to switch between type representations.However, it currently lacks several features expected of a state-of-the-art dependently typed language, such as implicit arguments. The central aim of this paper is to provide an algorithmic type system and inference algorithm for implicit arguments that respect the core symmetry of the language. Our work provides two key contributions: a complete algorithmic description of the type system backing Polarity, and a comprehensive description of a unification algorithm that covers arbitrary inductive and coinductive types. We give rules for reduction semantics, conversion checking, and a unification algorithm for pattern-matching, which are essential for a usable implementation. A work-in-progress implementation of the algorithms in this paper is available at https://polarity-lang.github.io/. We expect that the comprehensive account of the unification algorithm and our design decisions can serve as a blueprint for other dependently typed languages that support inductive and coinductive types symmetrically.

</details>


### [16] [Chorex: Restartable, Language-Integrated Choreographies](https://arxiv.org/abs/2511.15820)
*Ashton Wiersdorf,Ben Greenman*

Main category: cs.PL

TL;DR: Chorex是一个将编排编程引入Elixir的语言，通过元编程实现完整的编排功能，支持容错机制，当参与者崩溃时能自动重启并恢复状态。


<details>
  <summary>Details</summary>
Motivation: 为构建健壮的分布式应用程序，需要一种能够容忍参与者故障的编排编程语言，同时实现与宿主语言的紧密集成。

Method: 通过元编程在Elixir中实现编排编程，采用检查点机制进行状态恢复，当参与者崩溃时自动生成新进程并更新网络配置。

Result: Chorex成功实现了容错编排编程，能够静态检测编排要求与参与者实现之间的不匹配，并在多个示例中验证了其有效性。

Conclusion: Chorex的投影策略（输出无状态函数集）是支持可重启参与者的可行方法，可为其他语言提供参考。

Abstract: We built Chorex, a language that brings choreographic programming to Elixir as a path toward robust distributed applications. Chorex is unique among choreographic languages because it tolerates failure among actors: when an actor crashes, Chorex spawns a new process, restores state using a checkpoint, and updates the network configuration for all actors. Chorex also proves that full-featured choreographies can be implemented via metaprogramming, and that doing so achieves tight integration with the host language. For example, mismatches between choreography requirements and an actor implementation are reported statically and in terms of source code rather than macro-expanded code. This paper illustrates Chorex on several examples, ranging from a higher-order bookseller to a secure remote password protocol, details its implementation, and measures the overhead of checkpointing. We conjecture that Chorex's projection strategy, which outputs sets of stateless functions, is a viable approach for other languages to support restartable actors.

</details>


### [17] [BlueScript: A Disaggregated Virtual Machine for Microcontrollers](https://arxiv.org/abs/2511.15821)
*Fumika Mochizuki,Tetsuro Yamazaki,Shigeru Chiba*

Main category: cs.PL

TL;DR: 提出了一种分解式虚拟机，将尽可能多的组件卸载到主机上，以在内存受限的微控制器上提供丰富的功能特性。


<details>
  <summary>Details</summary>
Motivation: 微控制器虚拟机由于内存限制，功能特性有限，缺乏交互响应性和高执行速度。现有研究虽然探索了将某些组件卸载到其他机器，但可卸载的组件类型仍然受限。

Method: 设计并实现了BlueScript VM，将大多数组件卸载到主机上。采用称为影子机器的数据结构来镜像微控制器的执行状态，以减少主机与微控制器之间的通信开销。

Result: 实验确认卸载组件不会严重损害预期收益。卸载的增量编译器比MicroPython和Espruino执行速度更快，同时保持与MicroPython相当的交互性。卸载的动态编译器提高了虚拟机性能。

Conclusion: 研究表明即使在内存受限的微控制器上，通过组件卸载也能提供丰富功能的可行性。

Abstract: Virtual machines (VMs) are highly beneficial for microcontroller development. 
In particular, interactive programming environments greatly facilitate iterative development processes, 
and higher execution speeds expand the range of applications that can be developed. 
However, due to their limited memory size, microcontroller VMs provide a limited set of features. 
Widely used VMs for microcontrollers often lack interactive responsiveness and/or high execution speed. 
While researchers have investigated offloading certain VM components to other machines,the types of components that can be offloaded are still restricted. 
In this paper, we propose a disaggregated VM that offloads as many components as possible to a host machine. 
This makes it possible to exploit the abundant memory of the host machine and its powerful processing capability to provide rich features through the VM. 
As an instance of a disaggregated VM, we design and implement a BlueScript VM. 
The BlueScript VM is a virtual machine for microcontrollers that provides an interactive development environment. 
We offload most of the components of the BlueScript VM to a host machine. 
To reduce communication overhead between the host machine and the microcontroller,  
we employed a data structure called a shadow machine on the host machine, 
which mirrors the execution state of the microcontroller. 
Through our experiments, we confirmed that offloading components does not seriously compromise their expected benefits.  
We assess that an offloaded incremental compiler results in faster execution speed than MicroPython and Espruino,  
while keeping interactivity comparable with MicroPython.  
In addition, our experiments observe that the offloaded dynamic compiler improves VM performance. 
Through this investigation, we demonstrate the feasibility of providing rich features even on VMs for memory-limited microcontrollers.

</details>


### [18] [Operon: Incremental Construction of Ragged Data via Named Dimensions](https://arxiv.org/abs/2511.16080)
*Sungbin Moon,Jiho Park,Suyoung Hwang,Donghyun Koh,Seunghyun Moon,Minhyeong Lee*

Main category: cs.PL

TL;DR: Operon是一个基于Rust的工作流引擎，专门处理不规则数据（ragged data），通过命名维度和显式依赖关系提供静态验证和动态调度，在机器学习数据生成管道中实现高效并行处理。


<details>
  <summary>Details</summary>
Motivation: 现代数据处理工作流经常遇到不规则数据（元素长度可变），现有工作流引擎缺乏对这类数据形状和依赖关系的原生支持，用户需要手动管理复杂的索引和依赖关系。

Method: 提出Operon工作流引擎，使用命名维度和显式依赖关系的形式化方法，提供领域特定语言进行静态验证，运行时系统根据数据形状的增量发现动态调度任务。

Result: Operon相比现有工作流引擎减少了14.94倍的基础开销，在负载扩展时保持接近线性的端到端输出率，特别适合机器学习应用中的大规模数据生成管道。

Conclusion: Operon通过显式建模部分已知状态实现了强大的持久化和恢复机制，其每任务多队列架构在异构任务类型间实现了高效并行，为不规则数据处理提供了有效的解决方案。

Abstract: Modern data processing workflows frequently encounter ragged data: collections with variable-length elements that arise naturally in domains like natural language processing, scientific measurements, and autonomous AI agents. Existing workflow engines lack native support for tracking the shapes and dependencies inherent to ragged data, forcing users to manage complex indexing and dependency bookkeeping manually. We present Operon, a Rust-based workflow engine that addresses these challenges through a novel formalism of named dimensions with explicit dependency relations. Operon provides a domain-specific language where users declare pipelines with dimension annotations that are statically verified for correctness, while the runtime system dynamically schedules tasks as data shapes are incrementally discovered during execution. We formalize the mathematical foundation for reasoning about partial shapes and prove that Operon's incremental construction algorithm guarantees deterministic and confluent execution in parallel settings. The system's explicit modeling of partially-known states enables robust persistence and recovery mechanisms, while its per-task multi-queue architecture achieves efficient parallelism across heterogeneous task types. Empirical evaluation demonstrates that Operon outperforms an existing workflow engine with 14.94x baseline overhead reduction while maintaining near-linear end-to-end output rates as workloads scale, making it particularly suitable for large-scale data generation pipelines in machine learning applications.

</details>
