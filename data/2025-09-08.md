<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 8]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.LO](#cs.LO) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Comparative Evaluation of Large Language Models for Test-Skeleton Generation](https://arxiv.org/abs/2509.04644)
*Subhang Boorlagadda,Nitya Naga Sai Atluri,Muhammet Mustafa Olmez,Edward F. Gehringer*

Main category: cs.SE

TL;DR: 评估四种大型语言模型（GPT-4、DeepSeek-Chat、Llama4-Maverick、Gemma2-9B）自动生成RSpec测试骨架的能力，DeepSeek生成最可维护的结构，GPT-4输出更完整但一致性较差


<details>
  <summary>Details</summary>
Motivation: 传统手动创建测试骨架耗时且易出错，特别是在教育和大型开发环境中，需要自动化解决方案来提高测试驱动开发的效率

Method: 使用静态分析和盲审专家评审评估四种LLM生成的RSpec测试骨架，从结构正确性、清晰度、可维护性和测试最佳实践符合度等方面进行衡量

Result: DeepSeek生成最可维护和结构良好的测试骨架，GPT-4产生更完整但在约定一致性方面较差的输出，提示设计和上下文输入是质量关键因素

Conclusion: LLM在自动测试骨架生成方面具有潜力，但不同模型在代码结构解释和测试约定理解上存在显著差异，提示工程和上下文信息对输出质量至关重要

Abstract: This paper explores the use of Large Language Models (LLMs) to automate the
generation of test skeletons -- structural templates that outline unit test
coverage without implementing full test logic. Test skeletons are especially
important in test-driven development (TDD), where they provide an early
framework for systematic verification. Traditionally authored manually, their
creation can be time-consuming and error-prone, particularly in educational or
large-scale development settings. We evaluate four LLMs -- GPT-4,
DeepSeek-Chat, Llama4-Maverick, and Gemma2-9B -- on their ability to generate
RSpec skeletons for a real-world Ruby class developed in a university software
engineering course. Each model's output is assessed using static analysis and a
blind expert review to measure structural correctness, clarity,
maintainability, and conformance to testing best practices. The study reveals
key differences in how models interpret code structure and testing conventions,
offering insights into the practical challenges of using LLMs for automated
test scaffolding. Our results show that DeepSeek generated the most
maintainable and well-structured skeletons, while GPT-4 produced more complete
but conventionally inconsistent output. The study reveals prompt design and
contextual input as key quality factors.

</details>


### [2] [Real-Time Performance Benchmarking of TinyML Models in Embedded Systems (PICO: Performance of Inference, CPU, and Operations)](https://arxiv.org/abs/2509.04721)
*Abhishek Dey,Saurabh Srivastava,Gaurav Singh,Robert G. Pettit*

Main category: cs.SE

TL;DR: PICO-TINYML-BENCHMARK是一个模块化、平台无关的框架，用于在资源受限的嵌入式系统上基准测试TinyML模型的实时性能，评估推理延迟、CPU利用率、内存效率和预测稳定性等关键指标。


<details>
  <summary>Details</summary>
Motivation: 为TinyML模型在嵌入式系统上的实际部署提供性能基准测试框架，帮助理解计算权衡和平台特定优化，弥合理论进展与实际应用之间的差距。

Method: 开发了模块化、平台无关的基准测试框架，在BeagleBone AI64和Raspberry Pi 4两种平台上对三种代表性TinyML模型（手势分类、关键词识别、MobileNet V2）进行基准测试，使用真实数据集评估关键性能指标。

Result: BeagleBone AI64在AI特定任务上表现出一致的推理延迟，而Raspberry Pi 4在资源效率和成本效益方面表现更优，揭示了关键的计算权衡。

Conclusion: 该框架为优化TinyML部署提供了可操作的指导，有助于在嵌入式系统中实现更好的性能平衡和成本效益优化。

Abstract: This paper presents PICO-TINYML-BENCHMARK, a modular and platform-agnostic
framework for benchmarking the real-time performance of TinyML models on
resource-constrained embedded systems. Evaluating key metrics such as inference
latency, CPU utilization, memory efficiency, and prediction stability, the
framework provides insights into computational trade-offs and platform-specific
optimizations. We benchmark three representative TinyML models -- Gesture
Classification, Keyword Spotting, and MobileNet V2 -- on two widely adopted
platforms, BeagleBone AI64 and Raspberry Pi 4, using real-world datasets.
Results reveal critical trade-offs: the BeagleBone AI64 demonstrates consistent
inference latency for AI-specific tasks, while the Raspberry Pi 4 excels in
resource efficiency and cost-effectiveness. These findings offer actionable
guidance for optimizing TinyML deployments, bridging the gap between
theoretical advancements and practical applications in embedded systems.

</details>


### [3] [NovaQ: Improving Quantum Program Testing through Diversity-Guided Test Case Generation](https://arxiv.org/abs/2509.04763)
*Tiancheng Jin,Shangzhou Xia,Jianjun Zhao*

Main category: cs.SE

TL;DR: NovaQ是一个多样性引导的量子程序测试框架，通过分布式测试用例生成和新颖性评估模块，有效探索量子程序的未测试行为，比现有方法检测更多bug。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算的发展，确保量子程序的可靠性变得越来越重要。量子程序利用量子电路解决经典机器难以处理的问题，需要有效的测试方法来保证其正确性。

Method: NovaQ结合了基于分布的测试用例生成器和新颖性驱动的评估模块。生成器通过变异电路参数产生多样化的量子态输入，评估器基于内部电路状态度量（包括幅度、相位和纠缠）量化行为新颖性，选择映射到度量空间中较少覆盖区域的输入。

Result: 在不同规模和复杂度的量子程序上评估NovaQ，实验结果表明NovaQ始终比现有基线方法实现更高的测试输入多样性，并检测到更多bug。

Conclusion: NovaQ框架通过多样性引导的测试方法，能够有效探索量子程序的未测试行为，提高测试覆盖率和bug检测能力，为量子程序可靠性保障提供了有效解决方案。

Abstract: Quantum programs are designed to run on quantum computers, leveraging quantum
circuits to solve problems that are intractable for classical machines. As
quantum computing advances, ensuring the reliability of quantum programs has
become increasingly important. This paper introduces NovaQ, a diversity-guided
testing framework for quantum programs. NovaQ combines a distribution-based
test case generator with a novelty-driven evaluation module. The generator
produces diverse quantum state inputs by mutating circuit parameters, while the
evaluator quantifies behavioral novelty based on internal circuit state
metrics, including magnitude, phase, and entanglement. By selecting inputs that
map to infrequently covered regions in the metric space, NovaQ effectively
explores under-tested program behaviors. We evaluate NovaQ on quantum programs
of varying sizes and complexities. Experimental results show that NovaQ
consistently achieves higher test input diversity and detects more bugs than
existing baseline approaches.

</details>


### [4] [Code Review Without Borders: Evaluating Synthetic vs. Real Data for Review Recommendation](https://arxiv.org/abs/2509.04810)
*Yogev Cohen,Dudi Ohayon,Romy Somkin,Yehudit Aperstein,Alexander Apartsin*

Main category: cs.SE

TL;DR: 利用LLM生成编程语言的合成代码变更数据，解决新语言标签数据穷乏问题，为代码审查推荐系统提供可扩展的训练数据源


<details>
  <summary>Details</summary>
Motivation: 新编程语言和框架的出现造成标签数据不足，无法训练有监督模型来判断代码变更是否需要手动审查，影响软件质量维护

Method: 利用大语言模型(LLM)将资源丰富语言的代码变更翻译为缺乏资源的新兴语言的等效变更，生成合成训练数据，然后训练有监督分类器

Result: 在多个GitHub仓库和语言对中证明，LLM生成的合成数据可以有效地启动审查推荐系统，缩小与真实标签数据训练模型的性能差距，甚至在低资源环境中也有效

Conclusion: 该方法为扩展自动化代码审查能力到快速发展的技术栈提供了可扩展的途径，无需注释数据即可实现

Abstract: Automating the decision of whether a code change requires manual review is
vital for maintaining software quality in modern development workflows.
However, the emergence of new programming languages and frameworks creates a
critical bottleneck: while large volumes of unlabelled code are readily
available, there is an insufficient amount of labelled data to train supervised
models for review classification. We address this challenge by leveraging Large
Language Models (LLMs) to translate code changes from well-resourced languages
into equivalent changes in underrepresented or emerging languages, generating
synthetic training data where labelled examples are scarce. We assume that
although LLMs have learned the syntax and semantics of new languages from
available unlabelled code, they have yet to fully grasp which code changes are
considered significant or review-worthy within these emerging ecosystems. To
overcome this, we use LLMs to generate synthetic change examples and train
supervised classifiers on them. We systematically compare the performance of
these classifiers against models trained on real labelled data. Our experiments
across multiple GitHub repositories and language pairs demonstrate that
LLM-generated synthetic data can effectively bootstrap review recommendation
systems, narrowing the performance gap even in low-resource settings. This
approach provides a scalable pathway to extend automated code review
capabilities to rapidly evolving technology stacks, even in the absence of
annotated data.

</details>


### [5] [Integrating Large Language Models in Software Engineering Education: A Pilot Study through GitHub Repositories Mining](https://arxiv.org/abs/2509.04877)
*Maryam Khan,Muhammad Azeem Akbar,Jussi Kasurinen*

Main category: cs.SE

TL;DR: 这项研究通过对400个GitHub项目的实证分析，验证了大语言模型在软件工程教育中的动机因素和阻碍因素，为负责任集成提供了基础支撑。


<details>
  <summary>Details</summary>
Motivation: 随着ChatGPT等大语言模型在软件工程教育中的日益普及，需要系统性研究来确保其负责任集成到课程中。

Method: 对400个GitHub项目进行了实验库挖掘研究，分析README文件和问题讨论，识别文献绵述中综合的动机因素和阻碍因素的存在情况。

Result: 动机因素如参与度和动机（227次）、软件工程过程理解（133次）、编程协助和调试支持（97次）表现突出。阻碍因素如抄袭和知识产权问题（385次）、安全隐私数据完整性（87次）、学习中过度依赖AI（39次）也显著存在。

Conclusion: 该研究提供了动机/阻碍因素分类的早期实证验证，强调了研究与实践的差距，为开发软件工程教育中大语言模型负责任集成的综合框架奠定了基础。

Abstract: Context: Large Language Models (LLMs) such as ChatGPT are increasingly
adopted in software engineering (SE) education, offering both opportunities and
challenges. Their adoption requires systematic investigation to ensure
responsible integration into curricula. Objective: This doctoral research aims
to develop a validated framework for integrating LLMs into SE education through
a multi-phase process, including taxonomies development, empirical
investigation, and case studies. This paper presents the first empirical step.
Method: We conducted a pilot repository mining study of 400 GitHub projects,
analyzing README files and issues discussions to identify the presence of
motivator and demotivator previously synthesized in our literature review [ 8]
study. Results: Motivators such as engagement and motivation (227 hits),
software engineering process understanding (133 hits), and programming
assistance and debugging support (97 hits) were strongly represented.
Demotivators, including plagiarism and IP concerns (385 hits), security,
privacy and data integrity (87 hits), and over-reliance on AI in learning (39
hits), also appeared prominently. In contrast, demotivators such as challenges
in evaluating learning outcomes and difficulty in curriculum redesign recorded
no hits across the repositories. Conclusion: The study provides early empirical
validation of motivators/demotivators taxonomies with respect to their themes,
highlights research practice gaps, and lays the foundation for developing a
comprehensive framework to guide the responsible adoption of LLMs in SE
education.

</details>


### [6] [FuzzRDUCC: Fuzzing with Reconstructed Def-Use Chain Coverage](https://arxiv.org/abs/2509.04967)
*Kai Feng,Jeremy Singer,Angelos K Marnerides*

Main category: cs.SE

TL;DR: FuzzRDUCC是一个新型模糊测试框架，通过符号执行从二进制文件中重建def-use链，结合数据流分析来发现传统控制流模糊测试可能遗漏的漏洞。


<details>
  <summary>Details</summary>
Motivation: 传统二进制模糊测试由于缺乏对程序内部数据流的洞察，难以实现全面的代码覆盖和发现隐藏漏洞。仅依赖控制流边覆盖的传统灰盒模糊测试器容易忽略那些无法通过控制流分析单独暴露的漏洞。

Method: 采用符号执行技术直接从二进制可执行文件重建定义-使用(def-use)链，使用新颖的启发式算法选择相关def-use链而不影响模糊测试的彻底性，从而识别关键数据流路径并暴露安全漏洞。

Result: 在binutils基准测试中评估表明，FuzzRDUCC能够发现最先进模糊测试器未发现的独特崩溃。

Conclusion: FuzzRDUCC被确立为下一代漏洞检测和发现机制的可行解决方案，通过数据流分析增强模糊测试效果，且不会产生过高的计算开销。

Abstract: Binary-only fuzzing often struggles with achieving thorough code coverage and
uncovering hidden vulnerabilities due to limited insight into a program's
internal dataflows. Traditional grey-box fuzzers guide test case generation
primarily using control flow edge coverage, which can overlook bugs not easily
exposed through control flow analysis alone. We argue that integrating dataflow
analysis into the fuzzing process can enhance its effectiveness by revealing
how data propagates through the program, thereby enabling the exploration of
execution paths that control flow-based methods might miss. In this context, we
introduce FuzzRDUCC, a novel fuzzing framework that employs symbolic execution
to reconstruct definition-use (def-use) chains directly from binary
executables. FuzzRDUCC identifies crucial dataflow paths and exposes security
vulnerabilities without incurring excessive computational overhead, due to a
novel heuristic algorithm that selects relevant def-use chains without
affecting the thoroughness of the fuzzing process. We evaluate FuzzRDUCC using
the binutils benchmark and demonstrate that it can identify unique crashes not
found by state-of-the-art fuzzers. Hence, establishing FuzzRDUCC as a feasible
solution for next generation vulnerability detection and discovery mechanisms.

</details>


### [7] [GenAI-based test case generation and execution in SDV platform](https://arxiv.org/abs/2509.05112)
*Denesa Zyberaj,Lukasz Mazur,Nenad Petrovic,Pankhuri Verma,Pascal Hirmer,Dirk Slama,Xiangwei Cheng,Alois Knoll*

Main category: cs.SE

TL;DR: 使用GenAI和视觉-语言模型自动生成测试案例，通过自然语言需求和系统图转换为Gherkin格式，重点关注汽车预警系统的子存在检测场景。


<details>
  <summary>Details</summary>
Motivation: 解决手动编写测试案例效率低下、测试工具兼容性差以及软件定义车辆功能验证过程复杂的问题。

Method: 结合大语言模型和视觉-语言模型，采用车辆信号规范模型标准化定义，在digital.auto平台执行生成的测试案例。

Result: 显著减少了手动测试规范化的工作量，并实现了生成测试的快速执行。

Conclusion: 虽然自动化程度高，但由于GenAI流程和平台的限制，测试案例和脚本生成仍需人工干预。

Abstract: This paper introduces a GenAI-driven approach for automated test case
generation, leveraging Large Language Models and Vision-Language Models to
translate natural language requirements and system diagrams into structured
Gherkin test cases. The methodology integrates Vehicle Signal Specification
modeling to standardize vehicle signal definitions, improve compatibility
across automotive subsystems, and streamline integration with third-party
testing tools. Generated test cases are executed within the digital.auto
playground, an open and vendor-neutral environment designed to facilitate rapid
validation of software-defined vehicle functionalities. We evaluate our
approach using the Child Presence Detection System use case, demonstrating
substantial reductions in manual test specification effort and rapid execution
of generated tests. Despite significant automation, the generation of test
cases and test scripts still requires manual intervention due to current
limitations in the GenAI pipeline and constraints of the digital.auto platform.

</details>


### [8] [AI Agents for Web Testing: A Case Study in the Wild](https://arxiv.org/abs/2509.05197)
*Naimeng Ye,Xiao Yu,Ruize Xu,Tianyi Peng,Zhou Yu*

Main category: cs.SE

TL;DR: WebProber是一个基于AI代理的网页测试框架，使用LLM模拟真实用户交互，自动发现传统工具遗漏的可用性问题


<details>
  <summary>Details</summary>
Motivation: 传统网页测试方法主要关注代码覆盖和负载测试，但无法捕捉复杂的用户行为，导致许多可用性问题未被发现。LLM和AI代理的出现为模拟人类交互和识别常见可用性问题提供了新可能

Method: 开发WebProber原型框架，给定URL后自主探索网站，模拟真实用户交互，识别bug和可用性问题，并生成可读报告

Result: 在120个学术个人网站案例研究中，发现了29个可用性问题，其中许多是传统工具遗漏的

Conclusion: 基于代理的测试是一个有前景的方向，为开发下一代以用户为中心的测试框架指明了方向

Abstract: Automated web testing plays a critical role in ensuring high-quality user
experiences and delivering business value. Traditional approaches primarily
focus on code coverage and load testing, but often fall short of capturing
complex user behaviors, leaving many usability issues undetected. The emergence
of large language models (LLM) and AI agents opens new possibilities for web
testing by enabling human-like interaction with websites and a general
awareness of common usability problems. In this work, we present WebProber, a
prototype AI agent-based web testing framework. Given a URL, WebProber
autonomously explores the website, simulating real user interactions,
identifying bugs and usability issues, and producing a human-readable report.
We evaluate WebProber through a case study of 120 academic personal websites,
where it uncovered 29 usability issues--many of which were missed by
traditional tools. Our findings highlight agent-based testing as a promising
direction while outlining directions for developing next-generation,
user-centered testing frameworks.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [9] [A Large-Scale Study of Floating-Point Usage in Statically Typed Languages](https://arxiv.org/abs/2509.04936)
*Andrea Gilot,Tobias Wrigstad,Eva Darulova*

Main category: cs.PL

TL;DR: 首个大规模浮点运算使用实证研究，分析GitHub公开库中类型语言的浮点代码特征


<details>
  <summary>Details</summary>
Motivation: 理解实际代码中浮点运算的真实使用情况，以提升静态分析、动态分析和程序修复技术的实际效果

Method: 采用随机采样和内在属性过滤的最佳实践，通过源代码关键词搜索和代码解析来识别浮点使用情况

Result: 证实了浮点运算被广泛使用的论断，发现文献中用于评估自动化推理技术的基准测试在某些方面代表实际代码，但非全部方面

Conclusion: 研究结果和数据集将有助于设计更符合实际用户需求的浮点运算处理技术和评估方法

Abstract: Reasoning about floating-point arithmetic is notoriously hard. While static
and dynamic analysis techniques or program repair have made significant
progress, more work is still needed to make them relevant to real-world code.
On the critical path to that goal is understanding what real-world
floating-point code looks like. To close that knowledge gap, this paper
presents the first large-scale empirical study of floating-point arithmetic
usage in statically typed languages across public GitHub repositories. We
follow state-of the art mining practices including random sampling and
filtering based on only intrinsic properties to avoid bias, and identify
floating-point usage by searching for keywords in the source code, and
programming language constructs (e.g., loops) by parsing the code. Our
evaluation supports the claim often made in papers that floating-point
arithmetic is widely used. Comparing statistics such as size and usage of
certain constructs and functions, we find that benchmarks used in literature to
evaluate automated reasoning techniques for floating-point arithmetic are in
certain aspects representative of 'real-world' code, but not in all. We aim for
our study and dataset to help future techniques for floating-point arithmetic
to be designed and evaluated to match actual users' expectations.

</details>


### [10] [AI-Assisted Modeling: DSL-Driven AI Interactions](https://arxiv.org/abs/2509.05160)
*Steven Smyth,Daniel Busch,Moez Ben Haj Hmida,Edward A. Lee,Bernhard Steffen*

Main category: cs.PL

TL;DR: 通过基于领域特定建模技术的可视化方法，提高AI辅助编程的透明度和可验证性，支持快速反馈和形式验证


<details>
  <summary>Details</summary>
Motivation: 解决AI生成代码的透明度不足问题，提高开发者对AI生成代码的理解、验证和信任度

Method: 集成领域特定建模技术，提供实时可视化展示，支持编程、自然语言提示、语音命令和阶段性精化多种建模方式

Result: 开发了Visual Studio Code扩展原型（基于Lingua Franca语言），展示了新颖的领域特定建模实践演示

Conclusion: 该方法能够显著提高AI辅助编程的透明性和可验证性，为模型创建、可视化和验证方式带来进步

Abstract: AI-assisted programming greatly increases software development performance.
We enhance this potential by integrating transparency through domain-specific
modeling techniques and providing instantaneous, graphical visualizations that
accurately represent the semantics of AI-generated code. This approach
facilitates visual inspection and formal verification, such as model checking.
  Formal models can be developed using programming, natural language prompts,
voice commands, and stage-wise refinement, with immediate feedback after each
transformation step. This support can be tailored to specific domains or
intended purposes, improving both code generation and subsequent validation
processes.
  To demonstrate the effectiveness of this approach, we have developed a
prototype as a Visual Studio Code extension for the Lingua Franca language.
This prototype showcases the potential for novel domain-specific modeling
practices, offering an advancement in how models are created, visualized, and
verified.

</details>


### [11] [Non-Termination Proving: 100 Million LoC and Beyond](https://arxiv.org/abs/2509.05293)
*Julien Vanegue,Jules Villard,Peter O'Hearn,Azalea Raad*

Main category: cs.PL

TL;DR: Pulse Infinite是一个使用证明技术检测大型程序中非终止性（发散）的工具，通过组合性和欠近似方法实现可扩展性和可靠性，在超过1亿行代码中发现了30多个未知问题


<details>
  <summary>Details</summary>
Motivation: 现有工作主要针对几十到几百行代码的小型基准测试，无法满足实际需求（单个公司可能有数千万甚至数亿行代码），需要开发能够处理大规模代码库的发散检测工具

Method: 采用组合性和欠近似证明技术：组合性支持大规模扩展，欠近似确保证明发散时的可靠性

Result: 在C、C++和Hack编写的超过1亿行开源和专有软件中应用，发现了30多个先前未知的问题，为现实代码库中的发散检测建立了新的技术水平

Conclusion: Pulse Infinite工具在检测大规模代码库中的发散问题方面取得了显著成功，证明了其在真实世界软件中的实用性和有效性

Abstract: We report on our tool, Pulse Infinite, that uses proof techniques to show
non-termination (divergence) in large programs. Pulse Infinite works
compositionally and under-approximately: the former supports scale, and the
latter ensures soundness for proving divergence. Prior work focused on small
benchmarks in the tens or hundreds of lines of code (LoC), and scale limits
their practicality: a single company may have tens of millions, or even
hundreds of millions of LoC or more. We report on applying Pulse Infinite to
over a hundred million lines of open-source and proprietary software written in
C, C++, and Hack, identifying over 30 previously unknown issues, establishing a
new state of the art for detecting divergence in real-world codebases.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [12] [Forall-Exists Relational Verification by Filtering to Forall-Forall](https://arxiv.org/abs/2509.04777)
*Ramana Nagasamudram,Anindya Banerjee,David A. Naumann*

Main category: cs.LO

TL;DR: 这篇论文提出了一种验证全秱存在(∀∃)性质的方法论，通过筛选-适当性变换将关系验证转换为产物程序的全秱全存在(∀∀)验证问题，从而利用现有工具进行自动化验证。


<details>
  <summary>Details</summary>
Motivation: 目前存在多种关系Hoare逻辑和工具用于验证全秱全存在(∀∀)性质，但缺少有效的方法和工具来处理非确定性下关键的全秱存在(∀∃)性质验证。

Method: 提出筛选-适当性变换技术，通过向产物程序添加断言来将∀∃性质转换为∀∀性质验证。发展了支持断言失败的基础∀∃判断逻辑，定义了bicoms产物程序表示形式，并证明了韧性定理。

Result: 实现了原型系统，能够进行自动化关系验证，并成功验证了论文中的所有示例。该方法允许使用标准断言语言和现有验证工具。

Conclusion: 该研究为验证全秱存在(∀∃)性质提供了一种实用的方法论，通过变换技术允许重用现有的全秱全存在验证工具，对于处理非确定性问题的关系验证具有重要意义。

Abstract: Relational verification encompasses research directions such as reasoning
about data abstraction, reasoning about security and privacy, secure
compilation, and functional specificaton of tensor programs, among others.
Several relational Hoare logics exist, with accompanying tool support for
compositional reasoning of $\forall\forall$ (2-safety) properties and,
generally, k-safety properties of product programs. In contrast, few logics and
tools exist for reasoning about $\forall\exists$ properties which are critical
in the context of nondeterminism.
  This paper's primary contribution is a methodology for verifying a
$\forall\exists$ judgment by way of a novel filter-adequacy transformation.
This transformation adds assertions to a product program in such a way that the
desired $\forall\exists$ property (of a pair of underlying unary programs) is
implied by a $\forall\forall$ property of the transformed product. The paper
develops a program logic for the basic $\forall\exists$ judgement extended with
assertion failures; develops bicoms, a form of product programs that represents
pairs of executions and that caters for direct translation of $\forall\forall$
properties to unary correctness; proves (using the logic) a soundness theorem
that says successful $\forall\forall$ verification of a transformed bicom
implies the $\forall\exists$ spec for its underlying unary commands; and
implements a proof of principle prototype for auto-active relational
verification which has been used to verify all examples in the paper. The
methodology thereby enables a user to work with ordinary assertions and
assumptions, and a standard assertion language, so that existing tools
including auto-active verifiers can be used.

</details>


### [13] [Higher order differential calculus in mathlib](https://arxiv.org/abs/2509.04922)
*Sébastien Gouëzel*

Main category: cs.LO

TL;DR: 在Lean数学库mathlib中开发的高阶微分计算库，采用更广泛的定义来支持多种应用场景


<details>
  <summary>Details</summary>
Motivation: 为了支持广泛的应用需求，需要突破标准教材定义的限制

Method: 采用三个重要的广义化：允许任意标量域、在域上定义函数而非全空间、将分析函数整合到更广泛的光滑函数范畴

Result: 这些广义化引入了重要的挑战，需要从数学和形式化两个角度来解决

Conclusion: 该工作为高阶微分计算提供了更灵活和广泛的形式化基础

Abstract: We report on the higher-order differential calculus library developed inside
the Lean mathematical library mathlib. To support a broad range of
applications, we depart in several ways from standard textbook definitions: we
allow arbitrary fields of scalars, we work with functions defined on domains
rather than full spaces, and we integrate analytic functions in the broader
scale of smooth functions. These generalizations introduce significant
challenges, which we address from both the mathematical and the formalization
perspectives.

</details>
