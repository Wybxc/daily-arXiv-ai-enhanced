<div id=toc></div>

# Table of Contents

- [cs.LO](#cs.LO) [Total: 2]
- [cs.SE](#cs.SE) [Total: 4]


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [1] [Verification of E-Voting Algorithms in Dafny](https://arxiv.org/abs/2512.21084)
*Robert Büttner,Fabian Franz Dießl,Patrick Janoschek,Ivana Kostadinovic,Henrik Oback,Kilian Voß,Franziska Alber,Roland Herrmann,Sibylle Möhle,Philipp Rümmer*

Main category: cs.LO

TL;DR: 开发了一个开源电子投票程序库，包含四种投票方法，使用Dafny进行形式化验证，并可通过代码提取建立投票网络服务。


<details>
  <summary>Details</summary>
Motivation: 电子投票程序是选举系统的实现，能够借助计算机进行投票或选举。需要开发可靠、经过形式化验证的开源投票程序库，以确保投票过程的正确性和一致性。

Method: 使用Dafny编程语言实现四种投票程序（计分投票、即时决选投票、博尔达计数、单一可转移投票），对其中两种进行详细讨论。通过形式化验证确保实现与功能规范的一致性及关键正确性属性。通过代码提取从Dafny实现建立投票网络服务。

Result: 成功开发了一个开源电子投票程序库，包含四种投票方法的实现。通过Dafny的形式化验证确保了实现与规范的一致性。建立了可用的投票网络服务。

Conclusion: 该研究展示了使用形式化方法开发可靠电子投票系统的可行性，开源库为电子投票提供了经过验证的实现基础，有助于提高投票系统的可信度和安全性。

Abstract: Electronic voting procedures are implementations of electoral systems, making it possible to conduct polls or elections with the help of computers. This paper reports on the development of an open-source library of electronic voting procedures, which currently covers Score Voting, Instant-Runoff Voting, Borda Count, and Single Transferable Vote. The four procedures, of which two are discussed in detail, have been implemented in Dafny, formally verifying the consistency with functional specifications and key correctness properties. Using code extraction from the Dafny implementation, the library has been used to set up a voting web service.

</details>


### [2] [Declarative distributed broadcast using three-valued modal logic and semitopologies](https://arxiv.org/abs/2512.21137)
*Murdoch J. Gabbay*

Main category: cs.LO

TL;DR: 使用模态逻辑将分布式算法形式化为声明式公理理论，提供精确、紧凑、人类可读的规范，抽象出实现细节，便于验证和改进。


<details>
  <summary>Details</summary>
Motivation: 传统分布式算法规范通常过于底层（如源代码）或不够精确（如自然语言描述），需要一种既能精确捕捉系统本质属性，又能抽象实现细节的规范方法。

Method: 使用模态逻辑将分布式算法形式化为声明式公理理论，通过模态逻辑捕捉系统本质属性的高层次表示，同时抽象掉实现算法的抽象机转换细节。

Result: 该方法在简单投票协议、广播协议和一致性协议上得到验证，具有良好的可扩展性，并已在工业协议中发现错误。

Conclusion: 模态逻辑公理化方法提供了精确、紧凑、人类可读的规范，为正确性、容错性推理以及人机验证、设计改进和替代实现奠定了基础。

Abstract: We demonstrate how to formally specify distributed algorithms as declarative axiomatic theories in a modal logic. We exhibit the method on a simple voting protocol, a simple broadcast protocol, and a simple agreement protocol. The methods scale well and have been used to find errors in a proposed industrial protocol. The key novelty is to use modal logic to capture a declarative, high-level representation of essential system properties -- the logical essence of the algorithm -- while abstracting away from transitions of an abstract machine that implements it. It is like the difference between specifying code in a functional or logic programming language, versus specifying code in an imperative one.
  A logical axiomatisation in the style we propose provides a precise, compact, human-readable specification that abstractly captures essential system properties, while eliding low-level implementation details; it is more precise than a natural language description, yet more abstract than source code or a logical specification thereof. This creates new opportunities for reasoning about correctness, resilience, and failure, and could serve as a foundation for human- and machine verification efforts, design improvements, and even alternative protocol implementations.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [Process Analytics -- Data-driven Business Process Management](https://arxiv.org/abs/2512.20703)
*Matthias Stierle,Karsten Kraume,Martin Matzner*

Main category: cs.SE

TL;DR: 论文提出"流程分析"新概念，强调数据驱动流程分析应结合技术、人类和组织维度，而非仅关注技术层面。


<details>
  <summary>Details</summary>
Motivation: 当前流程挖掘研究过于关注技术层面，忽视了人类和组织因素。随着数据驱动流程分析日益普及，需要重新审视其多维度特性，特别是社会技术视角下的综合考量。

Method: 采用归纳和演绎相结合的方法，概念化"流程分析"术语及其多个维度，并通过大型企业实施数据驱动流程分析和自动化的真实案例研究进行对比讨论。

Result: 提出了一个结合分析过程、组织及其利益相关者的新视角，明确了流程分析的多维度概念框架，并通过案例研究验证了该框架的实际应用价值。

Conclusion: 数据驱动的流程分析需要超越纯粹的技术视角，采用社会技术系统方法，将技术、人类和组织维度有机结合，才能实现更全面有效的业务流程分析。

Abstract: Data-driven analysis of business processes has a long tradition in research. However, recently the term of process mining is mostly used when referring to data-driven process analysis. As a consequence, awareness for the many facets of process analysis is decreasing. In particular, while an increasing focus is put onto technical aspects of the analysis, human and organisational concerns remain under the radar. Following the socio-technical perspective of information systems research, we propose a new perspective onto data-driven process analysis that combines the process of analysis with the organisation and its stakeholders. This paper conceptualises the term process analytics and its various dimensions by following both an inductive and deductive approach. The results are discussed by contrasting them to a real-life case study from a large company implementing data-driven process analysis and automation.

</details>


### [4] [One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents](https://arxiv.org/abs/2512.20957)
*Zhaoxi Zhang,Yitong Duan,Yanzhi Zhang,Yiming Xu,Jiyan He,Yunfang Wu*

Main category: cs.SE

TL;DR: RepoNavigator：一个使用强化学习训练的LLM智能体，通过单一的执行感知工具（跳转到被调用符号的定义）来定位大型开源软件仓库中需要修改的文件和函数，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 在大型开源软件仓库中定位需要修改的文件和函数具有挑战性，因为仓库规模大、结构复杂。现有的基于LLM的方法通常将其视为仓库级别的检索任务，依赖多个辅助工具，忽略了代码执行逻辑并使模型控制复杂化。

Method: 提出RepoNavigator，一个配备单一执行感知工具的LLM智能体，该工具能够跳转到被调用符号的定义。这种统一设计反映了代码执行的实际流程，同时简化了工具操作。通过强化学习从预训练模型端到端训练，无需闭源蒸馏。

Result: RL训练的RepoNavigator实现了最先进的性能：7B模型优于14B基线，14B模型超越32B竞争对手，32B模型甚至超过了Claude-3.7等闭源模型。

Conclusion: 将单一、结构基础的工具与强化学习训练相结合，为仓库级别的问题定位提供了高效且可扩展的解决方案。

Abstract: Locating the files and functions requiring modification in large open-source software (OSS) repositories is challenging due to their scale and structural complexity. Existing large language model (LLM)-based methods typically treat this as a repository-level retrieval task and rely on multiple auxiliary tools, which overlook code execution logic and complicate model control. We propose RepoNavigator, an LLM agent equipped with a single execution-aware tool-jumping to the definition of an invoked symbol. This unified design reflects the actual flow of code execution while simplifying tool manipulation. RepoNavigator is trained end-to-end via Reinforcement Learning (RL) directly from a pretrained model, without any closed-source distillation. Experiments demonstrate that RL-trained RepoNavigator achieves state-of-the-art performance, with the 7B model outperforming 14B baselines, the 14B model surpassing 32B competitors, and even the 32B model exceeding closed-source models such as Claude-3.7. These results confirm that integrating a single, structurally grounded tool with RL training provides an efficient and scalable solution for repository-level issue localization.

</details>


### [5] [Artificial or Just Artful? Do LLMs Bend the Rules in Programming?](https://arxiv.org/abs/2512.21028)
*Oussama Ben Sghaier,Kevin Delcourt,Houari Sahraoui*

Main category: cs.SE

TL;DR: LLMs在代码生成中会利用测试用例作为上下文信号，即使被明确禁止，测试可见性仍显著提升正确率，模型采用测试驱动精炼等策略来调和预训练目标与对齐约束的冲突。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在代码生成中如何应对预训练目标（利用所有可用信号）与对齐约束（限制使用某些信号）之间的冲突，特别是在智能体AI环境中，当模型可以访问本用于验证的单元测试时。

Method: 使用BigCodeBench（Hard）数据集，设计五种提示条件来操纵测试可见性并施加明确或隐式的使用限制，评估五个LLM（四个开源，一个闭源）在正确性、代码相似性、程序大小和代码变更方面的表现，并分析跨模型一致性以识别重复出现的适应策略。

Result: 测试可见性显著改变性能，某些模型的正确率几乎翻倍，而明确限制或部分暴露只能部分缓解这种影响。识别出四种重复出现的适应策略，其中测试驱动精炼最为频繁。

Conclusion: LLMs在暴露于与明确指令冲突的上下文信号时会调整其行为，这些结果揭示了模型如何调和预训练目标与对齐约束，为理解LLMs在冲突信号下的适应机制提供了有用见解。

Abstract: Large Language Models (LLMs) are widely used for automated code generation, yet their apparent successes often mask a tension between pretraining objectives and alignment choices. While pretraining encourages models to exploit all available signals to maximize success, alignment, whether through fine-tuning or prompting, may restrict their use. This conflict is especially salient in agentic AI settings, for instance when an agent has access to unit tests that, although intended for validation, act as strong contextual signals that can be leveraged regardless of explicit prohibitions. In this paper, we investigate how LLMs adapt their code generation strategies when exposed to test cases under different prompting conditions. Using the BigCodeBench (Hard) dataset, we design five prompting conditions that manipulate test visibility and impose explicit or implicit restrictions on their use. We evaluate five LLMs (four open-source and one closed-source) across correctness, code similarity, program size, and code churn, and analyze cross-model consistency to identify recurring adaptation strategies. Our results show that test visibility dramatically alters performance, correctness nearly doubles for some models, while explicit restrictions or partial exposure only partially mitigate this effect. Beyond raw performance, we identify four recurring adaptation strategies, with test-driven refinement emerging as the most frequent. These results highlight how LLMs adapt their behavior when exposed to contextual signals that conflict with explicit instructions, providing useful insight into how models reconcile pretraining objectives with alignment constraints.

</details>


### [6] [Assessing the Software Security Comprehension of Large Language Models](https://arxiv.org/abs/2512.21238)
*Mohammed Latif Siddiq,Natalie Sekerak,Antonio Karam,Maria Leal,Arvin Islam-Gomes,Joanna C. S. Santos*

Main category: cs.SE

TL;DR: 该研究系统评估了5个主流LLM在软件安全领域的认知能力，发现它们在低阶认知任务表现良好，但在需要推理、架构评估和安全系统创建等高阶任务上表现显著下降。


<details>
  <summary>Details</summary>
Motivation: LLM在软件开发中应用日益广泛，但其软件安全专业知识的真实水平尚不明确，需要系统评估以了解其安全理解能力。

Method: 使用布鲁姆分类法作为框架，评估6个认知维度：记忆、理解、应用、分析、评估和创造。整合多样化数据集，包括多选题、漏洞代码片段、课程评估、真实案例研究和项目创建任务。

Result: LLM在回忆事实和识别已知漏洞等低阶认知任务表现良好，但在需要推理、架构评估和安全系统创建等高阶任务上表现显著下降。研究还识别了51个LLM在布鲁姆各层级中反复出现的误解模式。

Conclusion: 虽然LLM具备一定的软件安全基础知识，但在复杂的安全推理和系统设计方面存在明显局限，需要引入软件安全知识边界概念来界定其可靠性能的最高认知水平。

Abstract: Large language models (LLMs) are increasingly used in software development, but their level of software security expertise remains unclear. This work systematically evaluates the security comprehension of five leading LLMs: GPT-4o-Mini, GPT-5-Mini, Gemini-2.5-Flash, Llama-3.1, and Qwen-2.5, using Blooms Taxonomy as a framework. We assess six cognitive dimensions: remembering, understanding, applying, analyzing, evaluating, and creating. Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments from an Introduction to Software Security course, real-world case studies (XBOW), and project-based creation tasks from a Secure Software Engineering course. Results show that while LLMs perform well on lower-level cognitive tasks such as recalling facts and identifying known vulnerabilities, their performance degrades significantly on higher-order tasks that require reasoning, architectural evaluation, and secure system creation. Beyond reporting aggregate accuracy, we introduce a software security knowledge boundary that identifies the highest cognitive level at which a model consistently maintains reliable performance. In addition, we identify 51 recurring misconception patterns exhibited by LLMs across Blooms levels.

</details>
