<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.FL](#cs.FL) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Evaluating Uncertainty and Quality of Visual Language Action-enabled Robots](https://arxiv.org/abs/2507.17049)
*Pablo Valle,Chengjie Lu,Shaukat Ali,Aitor Arrieta*

Main category: cs.SE

TL;DR: 本文针对视觉语言动作(VLA)模型提出了8个不确定性指标和5个质量指标，通过908次成功任务执行的大规模实证研究，发现这些指标与人类专家评估具有中等到强相关性，挑战了仅依赖二元成功率的传统评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型通常只通过任务成功率进行评估，无法捕捉任务执行质量和模型决策置信度。传统的二元成功率评估方法不足以全面评价VLA模型在机器人操作任务中的表现。

Method: 提出了针对VLA模型机器人操作任务的8个不确定性指标和5个质量指标，通过涉及3个最先进VLA模型在4个代表性机器人操作任务上的908次成功执行的大规模实证研究来评估这些指标的有效性，并由人类领域专家手动标注任务质量。

Result: 研究结果显示，多个提出的指标与人类专家评估之间呈现中等到强相关性，证明了这些指标在评估任务质量和模型置信度方面的实用性。部分指标能够区分高、中、低质量执行与失败任务。

Conclusion: 当前仅依赖二元成功率的评估实践存在不足，本研究提出的指标为VLA模型的实时监控和自适应增强铺平了道路，为机器人系统的评估提供了更全面的方法。

Abstract: Visual Language Action (VLA) models are a multi-modal class of Artificial
Intelligence (AI) systems that integrate visual perception, natural language
understanding, and action planning to enable agents to interpret their
environment, comprehend instructions, and perform embodied tasks autonomously.
Recently, significant progress has been made to advance this field. These kinds
of models are typically evaluated through task success rates, which fail to
capture the quality of task execution and the mode's confidence in its
decisions. In this paper, we propose eight uncertainty metrics and five quality
metrics specifically designed for VLA models for robotic manipulation tasks. We
assess their effectiveness through a large-scale empirical study involving 908
successful task executions from three state-of-the-art VLA models across four
representative robotic manipulation tasks. Human domain experts manually
labeled task quality, allowing us to analyze the correlation between our
proposed metrics and expert judgments. The results reveal that several metrics
show moderate to strong correlation with human assessments, highlighting their
utility for evaluating task quality and model confidence. Furthermore, we found
that some of the metrics can discriminate between high-, medium-, and
low-quality executions from unsuccessful tasks, which can be interesting when
test oracles are not available. Our findings challenge the adequacy of current
evaluation practices that rely solely on binary success rates and pave the way
for improved real-time monitoring and adaptive enhancement of VLA-enabled
robotic systems.

</details>


### [2] [CASCADE: LLM-Powered JavaScript Deobfuscator at Google](https://arxiv.org/abs/2507.17691)
*Shan Jiang,Pranoy Kovuri,David Tao,Zhixun Tan*

Main category: cs.SE

TL;DR: 本文提出CASCADE，一种结合Gemini大模型和JavaScript中间表示(JSIR)的混合方法，用于JavaScript代码去混淆，已在Google生产环境中部署并显著提升去混淆效率。


<details>
  <summary>Details</summary>
Motivation: JavaScript混淆代码严重阻碍代码理解和分析，对软件测试、静态分析和恶意软件检测造成重大挑战。现有的静态和动态去混淆技术存在局限性，需要大量硬编码规则。

Method: 提出CASCADE混合方法，结合Gemini大模型的高级编码能力和JavaScript中间表示(JSIR)的确定性转换能力。使用Gemini识别关键前导函数（主流混淆技术的基础组件），然后利用JSIR进行后续代码转换。

Result: CASCADE有效恢复了语义元素（如原始字符串和API名称），揭示了原始程序行为。消除了数百到数千条硬编码规则，同时实现了可靠性和灵活性。已在Google生产环境中部署，显著改善了JavaScript去混淆效率。

Conclusion: CASCADE克服了现有静态和动态去混淆技术的局限性，通过混合方法实现了高效的JavaScript去混淆，减少了逆向工程工作量，并已成功应用于实际生产环境。

Abstract: Software obfuscation, particularly prevalent in JavaScript, hinders code
comprehension and analysis, posing significant challenges to software testing,
static analysis, and malware detection. This paper introduces CASCADE, a novel
hybrid approach that integrates the advanced coding capabilities of Gemini with
the deterministic transformation capabilities of a compiler Intermediate
Representation (IR), specifically JavaScript IR (JSIR). By employing Gemini to
identify critical prelude functions, the foundational components underlying the
most prevalent obfuscation techniques, and leveraging JSIR for subsequent code
transformations, CASCADE effectively recovers semantic elements like original
strings and API names, and reveals original program behaviors. This method
overcomes limitations of existing static and dynamic deobfuscation techniques,
eliminating hundreds to thousands of hardcoded rules while achieving
reliability and flexibility. CASCADE is already deployed in Google's production
environment, demonstrating substantial improvements in JavaScript deobfuscation
efficiency and reducing reverse engineering efforts.

</details>


### [3] [Assessing Reliability of Statistical Maximum Coverage Estimators in Fuzzing](https://arxiv.org/abs/2507.17093)
*Danushka Liyanage,Nelum Attanayake,Zijian Luo,Rahul Gopinath*

Main category: cs.SE

TL;DR: 本文研究模糊测试中可达性估计器的可靠性问题，提出了合成程序生成框架和真实程序可靠性检验方法来评估现有估计器的准确性。


<details>
  <summary>Details</summary>
Motivation: 模糊测试中覆盖率估计至关重要，但现有的静态可达性分析不够准确，而基于生物统计学的统计估计方法缺乏可靠的标准测试集和真实标签，限制了对其准确性的严格评估。

Method: 提出双轴评估方法：(1)构建合成程序生成框架，生成具有复杂控制流的大型程序，确保明确定义的可达性并提供评估的真实标签；(2)针对无真实标签的真实程序，通过改变采样单元大小来检验可达性估计器的可靠性。

Result: 通过合成程序和真实程序的双重研究，能够回答当前可达性估计器是否可靠的问题，并为未来可达性估计改进的评估定义了协议。

Conclusion: 该研究建立了评估可达性估计器可靠性的完整框架，为模糊测试领域的可达性估计研究提供了标准化的评估方法和协议。

Abstract: Background: Fuzzers are often guided by coverage, making the estimation of
maximum achievable coverage a key concern in fuzzing. However, achieving 100%
coverage is infeasible for most real-world software systems, regardless of
effort. While static reachability analysis can provide an upper bound, it is
often highly inaccurate. Recently, statistical estimation methods based on
species richness estimators from biostatistics have been proposed as a
potential solution. Yet, the lack of reliable benchmarks with labeled ground
truth has limited rigorous evaluation of their accuracy.
  Objective: This work examines the reliability of reachability estimators from
two axes: addressing the lack of labeled ground truth and evaluating their
reliability on real-world programs.
  Methods: (1) To address the challenge of labeled ground truth, we propose an
evaluation framework that synthetically generates large programs with complex
control flows, ensuring well-defined reachability and providing ground truth
for evaluation. (2) To address the criticism from use of synthetic benchmarks,
we adapt a reliability check for reachability estimators on real-world
benchmarks without labeled ground truth -- by varying the size of sampling
units, which, in theory, should not affect the estimate.
  Results: These two studies together will help answer the question of whether
current reachability estimators are reliable, and defines a protocol to
evaluate future improvements in reachability estimation.

</details>


### [4] [Can LLMs Write CI? A Study on Automatic Generation of GitHub Actions Configurations](https://arxiv.org/abs/2507.17165)
*Taher A. Ghaleb,Dulina Rathnayake*

Main category: cs.SE

TL;DR: 本研究评估了6个大语言模型生成GitHub Actions配置文件的能力，发现零样本提示最高仅能达到69%的相似度和3%的完美匹配率，揭示了LLM在CI配置生成方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 持续集成服务需要开发者编写基于YAML的配置文件，这既繁琐又容易出错。尽管大语言模型在软件工程任务自动化方面应用增加，但其生成CI配置的能力仍未得到充分探索。

Method: 评估了6个LLM模型（3个通用基础模型：GPT-4o、Llama、Gemma；3个代码预训练模型：GPT-4.1、Code Llama、CodeGemma）从自然语言描述生成GitHub Actions配置的能力。构建了首个此类标注数据集，包含GitHub Actions文档中的描述与对应的最佳实践YAML配置配对。使用零样本提示进行评估。

Result: 零样本提示最高达到69%的与真实配置的相似度，但完美匹配率仅为3%。代码预训练模型在基于YAML的CI任务中表现略逊于通用模型。GPT-4o输出分析显示存在步骤缺失或重命名、描述误解、不必要添加等问题，影响结构和上下文正确性。

Conclusion: 研究揭示了LLM在CI配置生成方面的局限性，生成质量与可执行CI配置所需精度之间存在差距。研究为改进LLM与配置语言的对齐提供了见解，并为未来CI自动化和工具支持工作提供指导。

Abstract: Continuous Integration (CI) services, such as GitHub Actions, require
developers to write YAML-based configurations, which can be tedious and
error-prone. Despite the increasing use of Large Language Models (LLMs) to
automate software engineering tasks, their ability to generate CI
configurations remains underexplored. This paper presents a preliminary study
evaluating six LLMs for generating GitHub Actions configurations from natural
language descriptions. We assess three general-purpose foundation models
(GPT-4o, Llama, and Gemma) and three code-pretrained models (GPT-4.1, Code
Llama, and CodeGemma). We also introduce the first labeled dataset of its kind,
constructed from GitHub Actions documentation, pairing descriptions with
corresponding best-practice YAML configurations. Zero-shot prompting achieves
up to 69% similarity with the ground truth, with only 3% perfect matches.
Code-pretrained models slightly underperform compared to general-purpose ones
in YAML-based CI tasks, revealing LLM limitations for CI configuration
generation. Analyzing GPT-4o outputs reveals issues like missing or renamed
steps, misinterpreted descriptions, and unnecessary additions that may affect
structural and contextual correctness, indicating a gap between generation
quality and the precision required for executable CI configurations. Our
research offers insights for improving LLM alignment with configuration
languages and guiding future efforts on CI automation and tooling support.

</details>


### [5] [On the Feasibility of Quantum Unit Testing](https://arxiv.org/abs/2507.17235)
*Andriy Miranskyy,José Campos,Anila Mjeda,Lei Zhang,Ignacio García Rodríguez de Guzmán*

Main category: cs.SE

TL;DR: 本研究对量子软件单元测试进行了全面分析，比较了传统统计方法与量子专用测试方法，通过对179万个变异量子电路的实证研究，发现量子中心化测试（特别是状态向量测试和逆测试）在精度和效率方面具有明显优势。


<details>
  <summary>Details</summary>
Motivation: 随着量子软件复杂性的增加，软件验证和验证面临重大挑战，特别是在单元测试方面。现有的传统统计测试方法在检测量子电路中微妙差异方面存在局限性，需要开发更适合量子软件特性的测试策略。

Method: 研究采用了多种量子中心化单元测试方法，包括仅在经典计算机上运行的状态向量测试，以及可在量子硬件上执行的交换测试和新颖的逆测试。通过对1,796,880个变异量子电路进行大规模实证研究，分析每种测试方法检测量子电路期望状态与实际状态之间细微差异的能力，以及达到高可靠性所需的测量次数。

Result: 实验结果表明，量子中心化测试，特别是状态向量测试和逆测试，在精度和效率方面提供了明显优势。与统计测试相比，这些方法显著减少了假阳性和假阴性，提高了测试的可靠性和准确性。

Conclusion: 本研究为开发更强大和可扩展的量子软件测试策略做出了贡献，支持未来容错量子计算机的采用，并促进量子软件工程中更可靠的实践方法。量子中心化测试方法为量子软件验证提供了更有效的解决方案。

Abstract: The increasing complexity of quantum software presents significant challenges
for software verification and validation, particularly in the context of unit
testing. This work presents a comprehensive study on quantum-centric unit
tests, comparing traditional statistical approaches with tests specifically
designed for quantum circuits. These include tests that run only on a classical
computer, such as the Statevector test, as well as those executable on quantum
hardware, such as the Swap test and the novel Inverse test. Through an
empirical study and detailed analysis on 1,796,880 mutated quantum circuits, we
investigate (a) each test's ability to detect subtle discrepancies between the
expected and actual states of a quantum circuit, and (b) the number of
measurements required to achieve high reliability. The results demonstrate that
quantum-centric tests, particularly the Statevector test and the Inverse test,
provide clear advantages in terms of precision and efficiency, reducing both
false positives and false negatives compared to statistical tests. This work
contributes to the development of more robust and scalable strategies for
testing quantum software, supporting the future adoption of fault-tolerant
quantum computers and promoting more reliable practices in quantum software
engineering.

</details>


### [6] [Understanding Prompt Programming Tasks and Questions](https://arxiv.org/abs/2507.17264)
*Jenny T. Liang,Chenyang Yang,Agnia Sergeyuk,Travis D. Breaux,Brad A. Myers*

Main category: cs.SE

TL;DR: 研究者通过访谈、观察和调研开发了一个包含25个任务和51个问题的提示编程分类体系，发现当前的提示编程工具支持不足，大多数重要问题仍未得到解答，为未来工具开发指出了重要机会。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型等基础模型的发展，开发者开始在软件中嵌入提示进行编程，但提示编程过程中开发者需要频繁修改提示，而他们在更新提示时提出的问题类型尚不明确。同时，现有的研究和商业提示编程工具是否充分满足开发者需求也不清楚。

Method: 研究者采用了混合方法：(1)访谈16名提示程序员；(2)观察8名开发者进行提示修改；(3)调研50名开发者；基于这些数据开发了包含25个任务和51个问题的分类体系，并测量了每个任务和问题的重要性；最后将该分类体系与48个研究和商业工具进行对比分析。

Result: 研究发现提示编程支持不足：所有任务都需要手动完成，51个问题中有16个（包括大多数最重要的问题）仍然没有得到解答。这表明现有工具在满足提示程序员的核心需求方面存在显著差距。

Conclusion: 基于研究发现，作者指出了提示编程工具的重要发展机会，强调需要开发更好的工具来支持提示编程任务，特别是解决那些最重要但目前未被解决的问题，以更好地满足开发者在提示编程过程中的实际需求。

Abstract: Prompting foundation models (FMs) like large language models (LLMs) have
enabled new AI-powered software features (e.g., text summarization) that
previously were only possible by fine-tuning FMs. Now, developers are embedding
prompts in software, known as prompt programs. The process of prompt
programming requires the developer to make many changes to their prompt. Yet,
the questions developers ask to update their prompt is unknown, despite the
answers to these questions affecting how developers plan their changes. With
the growing number of research and commercial prompt programming tools, it is
unclear whether prompt programmers' needs are being adequately addressed. We
address these challenges by developing a taxonomy of 25 tasks prompt
programmers do and 51 questions they ask, measuring the importance of each task
and question. We interview 16 prompt programmers, observe 8 developers make
prompt changes, and survey 50 developers. We then compare the taxonomy with 48
research and commercial tools. We find that prompt programming is not
well-supported: all tasks are done manually, and 16 of the 51 questions --
including a majority of the most important ones -- remain unanswered. Based on
this, we outline important opportunities for prompt programming tools.

</details>


### [7] [Lessons from a Big-Bang Integration: Challenges in Edge Computing and Machine Learning](https://arxiv.org/abs/2507.17270)
*Alessandro Aneggi,Andrea Janes*

Main category: cs.SE

TL;DR: 这是一篇关于分布式实时分析系统开发失败的经验报告。项目采用大爆炸集成方法，最终系统只运行了6分钟而非预期的40分钟，研究分析了技术和组织障碍，并提出了早期模拟部署和结构化集成等改进建议。


<details>
  <summary>Details</summary>
Motivation: 分析一个为期一年的分布式实时分析系统项目的失败原因，该项目使用边缘计算和机器学习技术，但由于采用大爆炸集成方法导致严重挫折，需要识别技术和组织层面的问题并提出改进方案。

Method: 通过根因分析方法研究项目失败原因，识别技术和组织障碍（包括沟通不良、缺乏早期集成测试、抵制自上而下规划），同时考虑心理因素（如偏向完全开发的组件而非模拟组件）。

Result: 系统集成后仅实现6分钟的功能运行时间，远低于预期的40分钟。研究识别出了导致失败的主要障碍：技术障碍、组织沟通问题、缺乏早期集成测试，以及对自上而下规划的抵制等。

Conclusion: 传统敏捷方法在此类分布式项目中存在局限性，建议采用早期基于模拟的部署、建立健壮的沟通基础设施、采用自上而下的思维来管理复杂性和降低风险。提出仿真驱动工程和结构化集成周期作为未来成功的关键因素。

Abstract: This experience report analyses a one year project focused on building a
distributed real-time analytics system using edge computing and machine
learning. The project faced critical setbacks due to a big-bang integration
approach, where all components developed by multiple geographically dispersed
partners were merged at the final stage. The integration effort resulted in
only six minutes of system functionality, far below the expected 40 minutes.
Through root cause analysis, the study identifies technical and organisational
barriers, including poor communication, lack of early integration testing, and
resistance to topdown planning. It also considers psychological factors such as
a bias toward fully developed components over mockups. The paper advocates for
early mock based deployment, robust communication infrastructures, and the
adoption of topdown thinking to manage complexity and reduce risk in reactive,
distributed projects. These findings underscore the limitations of traditional
Agile methods in such contexts and propose simulation-driven engineering and
structured integration cycles as key enablers for future success.

</details>


### [8] [Seed&Steer: Guiding Large Language Models with Compilable Prefix and Branch Signals for Unit Test Generation](https://arxiv.org/abs/2507.17271)
*Shuaiyu Zhou,Zhengran Zeng,Xiaoling Zhou,Rui Xie,Shikun Zhang,Wei Ye*

Main category: cs.SE

TL;DR: 本文提出了Seed&Steer方法，通过解耦前缀生成和断言生成来改进基于大语言模型的单元测试自动生成，结合传统测试工具和LLM能力，显著提高了编译成功率和测试覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在单元测试生成中面临编译失败和测试覆盖率不足的问题。作者发现前缀生成的初始化复杂度主要影响编译成功，而断言生成的环复杂度影响测试覆盖率，因此需要针对性地解决这两个不同的挑战。

Method: 提出Seed&Steer两步法：1）使用传统单元测试工具（如EvoSuite）生成高编译成功率的方法调用作为种子，指导LLM构建有效的测试上下文；2）引入分支提示帮助LLM探索不同的执行路径（正常、边界、异常情况），生成高覆盖率的断言。

Result: 在五个真实Java项目上的评估显示：编译通过率提高约7%，在两个LLM上成功编译了792和887个之前失败的案例；在不同复杂度的目标方法上实现了高达73%的分支和行覆盖率，覆盖率改进范围为1.09倍到1.26倍。

Conclusion: Seed&Steer方法通过结合传统测试工具的可靠性和大语言模型的灵活性，有效解决了LLM在单元测试生成中的编译和覆盖率问题，为自动化测试生成提供了新的解决思路。

Abstract: Unit tests play a vital role in the software development lifecycle. Recent
advances in Large Language Model (LLM)-based approaches have significantly
improved automated test generation, garnering attention from both academia and
industry. We revisit LLM-based unit test generation from a novel perspective by
decoupling prefix generation and assertion generation. To characterize their
respective challenges, we define Initialization Complexity and adopt Cyclomatic
Complexity to measure the difficulty of prefix and assertion generation,
revealing that the former primarily affects compilation success, while the
latter influences test coverage. To address these challenges, we propose
Seed&Steer, a two-step approach that combines traditional unit testing
techniques with the capabilities of large language models. Seed&Steer leverages
conventional unit testing tools (e.g., EvoSuite) to generate method invocations
with high compilation success rates, which serve as seeds to guide LLMs in
constructing effective test contexts. It then introduces branching cues to help
LLMs explore diverse execution paths (e.g., normal, boundary, and exception
cases) and generate assertions with high coverage. We evaluate Seed&Steer on
five real-world Java projects against state-of-the-art baselines. Results show
that Seed&Steer improves the compilation pass rate by approximately 7%,
successfully compiling 792 and 887 previously failing cases on two LLMs. It
also achieves up to ~73% branch and line coverage across focal methods of
varying complexity, with coverage improvements ranging from 1.09* to 1.26*. Our
code, dataset, and experimental scripts will be publicly released to support
future research and reproducibility.

</details>


### [9] [Data Virtualization for Machine Learning](https://arxiv.org/abs/2507.17293)
*Saiful Khan,Joyraj Chakraborty,Philip Beaucamp,Niraj Bhujel,Min Chen*

Main category: cs.SE

TL;DR: 本文介绍了一个数据虚拟化服务的设计与实现，用于支持多个并发的机器学习工作流，解决了ML团队在数据存储、处理和维护方面的挑战


<details>
  <summary>Details</summary>
Motivation: 机器学习团队通常有多个并发的ML工作流用于不同应用，每个工作流涉及大量实验、迭代和协作活动，从数据处理到模型部署需要数月甚至数年时间。组织需要存储、处理和维护大量中间数据，因此需要数据虚拟化技术来服务ML工作流

Method: 设计并实现了一个数据虚拟化服务，重点关注服务架构和服务操作。该基础设施专门为支持机器学习工作流而设计

Result: 该基础设施目前支持六个ML应用，每个应用都有多个ML工作流。数据虚拟化服务具有良好的可扩展性，能够支持未来应用和工作流数量的增长

Conclusion: 数据虚拟化服务成功解决了ML团队在管理多个并发工作流时面临的数据管理挑战，为ML基础设施提供了一个可扩展的解决方案

Abstract: Nowadays, machine learning (ML) teams have multiple concurrent ML workflows
for different applications. Each workflow typically involves many experiments,
iterations, and collaborative activities and commonly takes months and
sometimes years from initial data wrangling to model deployment.
Organizationally, there is a large amount of intermediate data to be stored,
processed, and maintained. \emph{Data virtualization} becomes a critical
technology in an infrastructure to serve ML workflows. In this paper, we
present the design and implementation of a data virtualization service,
focusing on its service architecture and service operations. The infrastructure
currently supports six ML applications, each with more than one ML workflow.
The data virtualization service allows the number of applications and workflows
to grow in the coming years.

</details>


### [10] [How Do Code Smells Affect Skill Growth in Scratch Novice Programmers?](https://arxiv.org/abs/2507.17314)
*Ricardo Hidalgo Aragón,Jesús M. González-Barahona,Gregorio Robles*

Main category: cs.SE

TL;DR: 这项研究通过分析约200万个Scratch项目，探索了计算思维能力与代码异味之间的关系，旨在为编程教育提供循证依据并改进自动化反馈系统。


<details>
  <summary>Details</summary>
Motivation: 虽然代码异味在专业代码中被广泛研究，但在初学者创建的块编程项目中的重要性仍不明确。需要了解计算思维技能培养与设计问题之间的关系，为编程教育和工具开发提供科学依据。

Method: 从约200万个公开Scratch项目中随机抽样，使用开源检查工具提取9个计算思维评分和40个代码异味指标。经过严格预处理后，应用描述性分析、稳健相关性测试、分层交叉验证和探索性机器学习模型，并通过定性抽查验证定量模式。

Result: 研究将提供首个大规模、细粒度的特定计算思维能力与具体设计缺陷关联图谱，为未来教育干预提供效应量基准，并向研究社区提供开放的匿名数据集和可重现的分析流程。

Conclusion: 通过阐明编程习惯如何影响早期技能获得，这项工作推进了计算教育理论和可持续软件维护演进的实用工具发展，为循证课程设计和自动化反馈系统提供了重要支撑。

Abstract: Context. Code smells, which are recurring anomalies in design or style, have
been extensively researched in professional code. However, their significance
in block-based projects created by novices is still largely unknown.
Block-based environments such as Scratch offer a unique, data-rich setting to
examine how emergent design problems intersect with the cultivation of
computational-thinking (CT) skills. Objective. This research explores the
connection between CT proficiency and design-level code smells--issues that may
hinder software maintenance and evolution--in programs created by Scratch
developers. We seek to identify which CT dimensions align most strongly with
which code smells and whether task context moderates those associations.
Method. A random sample of aprox. 2 million public Scratch projects is mined.
Using open-source linters, we extract nine CT scores and 40 code smell
indicators from these projects. After rigorous pre-processing, we apply
descriptive analytics, robust correlation tests, stratified cross-validation,
and exploratory machine-learning models; qualitative spot-checks contextualize
quantitative patterns. Impact. The study will deliver the first large-scale,
fine-grained map linking specific CT competencies to concrete design flaws and
antipatterns. Results are poised to (i) inform evidence-based curricula and
automated feedback systems, (ii) provide effect-size benchmarks for future
educational interventions, and (iii) supply an open, pseudonymized dataset and
reproducible analysis pipeline for the research community. By clarifying how
programming habits influence early skill acquisition, the work advances both
computing-education theory and practical tooling for sustainable software
maintenance and evolution.

</details>


### [11] [Roseau: Fast, Accurate, Source-based API Breaking Change Analysis in Java](https://arxiv.org/abs/2507.17369)
*Corentin Latappy,Thomas Degueule,Jean-Rémy Falleri,Romain Robbes,Lina Ochoa*

Main category: cs.SE

TL;DR: 本文介绍了Roseau，一个用于检测Java库API演化和破坏性变更的静态分析工具，相比现有工具JApiCmp和Revapi，Roseau支持源码和字节码分析，在大规模纵向研究中表现更优，准确率达到99%


<details>
  <summary>Details</summary>
Motivation: 现有的Java API破坏性变更检测工具（如JApiCmp和Revapi）仅依赖二进制JAR文件，限制了其在大规模纵向API演化研究和细粒度分析（如提交级别的破坏性变更检测）中的应用

Method: 开发了Roseau静态分析工具，构建技术无关的API模型，支持从源码或字节码构建API模型，并针对大规模纵向分析进行优化。通过与JApiCmp和Revapi的对比评估工具的准确性、性能和适用性

Result: Roseau在破坏性变更检测准确率上达到F1=0.99，超过JApiCmp（F1=0.86）和Revapi（F1=0.91）。在60个Maven Central流行库的分析中，Roseau在2秒内完成版本间破坏性变更检测。在Google Guava 14年6839次提交的分析中，将分析时间从几天缩短到几分钟

Conclusion: Roseau为API演化的大规模纵向研究提供了更准确、高效的解决方案，克服了传统工具的局限性，能够支持更细粒度的API变更分析，为软件库维护者和研究者提供了强大的工具支持

Abstract: Understanding API evolution and the introduction of breaking changes (BCs) in
software libraries is essential for library maintainers to manage backward
compatibility and for researchers to conduct empirical studies on software
library evolution. In Java, tools such as JApiCmp and Revapi are commonly used
to detect BCs between library releases, but their reliance on binary JARs
limits their applicability. This restriction hinders large-scale longitudinal
studies of API evolution and fine-grained analyses such as commit-level BC
detection. In this paper, we introduce Roseau, a novel static analysis tool
that constructs technology-agnostic API models from library code equipped with
rich semantic analyses. API models can be analyzed to study API evolution and
compared to identify BCs between any two versions of a library (releases,
commits, branches, etc.). Unlike traditional approaches, Roseau can build API
models from source code or bytecode, and is optimized for large-scale
longitudinal analyses of library histories. We assess the accuracy,
performance, and suitability of Roseau for longitudinal studies of API
evolution, using JApiCmp and Revapi as baselines. We extend and refine an
established benchmark of BCs and show that Roseau achieves higher accuracy (F1
= 0.99) than JApiCmp (F1 = 0.86) and Revapi (F1 = 0.91). We analyze 60 popular
libraries from Maven Central and find that Roseau delivers excellent
performance, detecting BCs between versions in under two seconds, including in
libraries with hundreds of thousands of lines of code. We further illustrate
the limitations of JApiCmp and Revapi for longitudinal studies and the novel
analysis capabilities offered by Roseau by tracking the evolution of Google's
Guava API and the introduction of BCs over 14 years and 6,839 commits, reducing
analysis times from a few days to a few minutes.

</details>


### [12] [Investigating Training Data Detection in AI Coders](https://arxiv.org/abs/2507.17389)
*Tianlin Li,Yunxiang Wei,Zhiming Li,Aishan Liu,Qing Guo,Xianglong Liu,Dongning Sun,Yang Liu*

Main category: cs.SE

TL;DR: 该论文对代码大语言模型的训练数据检测方法进行了全面的实证研究，提出了CodeSnitch基准数据集，并评估了七种最先进的TDD方法在八个代码大语言模型上的性能表现。


<details>
  <summary>Details</summary>
Motivation: 代码大语言模型可能输出包含专有或敏感代码片段的内容，引发对训练数据不合规使用的担忧，并对隐私和知识产权构成风险。现有的训练数据检测方法在代码数据上的有效性尚未得到充分探索，特别是考虑到代码的结构化语法特点。

Method: 对七种最先进的训练数据检测(TDD)方法在八个代码大语言模型上进行综合实证研究；构建CodeSnitch函数级基准数据集，包含三种编程语言的9000个代码样本；设计基于Type-1到Type-4代码克隆检测分类法的目标突变策略来测试TDD方法的鲁棒性。

Result: 通过在原始CodeSnitch数据集和突变版本上的评估，系统性地评估了当前TDD技术在代码领域的性能表现，并测试了这些方法在三种不同设置下的鲁棒性。

Conclusion: 该研究为代码领域的训练数据检测技术提供了系统性评估，为未来开发更有效和鲁棒的检测方法提供了指导见解，有助于确保代码大语言模型的负责任和合规部署。

Abstract: Recent advances in code large language models (CodeLLMs) have made them
indispensable tools in modern software engineering. However, these models
occasionally produce outputs that contain proprietary or sensitive code
snippets, raising concerns about potential non-compliant use of training data,
and posing risks to privacy and intellectual property. To ensure responsible
and compliant deployment of CodeLLMs, training data detection (TDD) has become
a critical task. While recent TDD methods have shown promise in natural
language settings, their effectiveness on code data remains largely
underexplored. This gap is particularly important given code's structured
syntax and distinct similarity criteria compared to natural language. To
address this, we conduct a comprehensive empirical study of seven
state-of-the-art TDD methods on source code data, evaluating their performance
across eight CodeLLMs. To support this evaluation, we introduce CodeSnitch, a
function-level benchmark dataset comprising 9,000 code samples in three
programming languages, each explicitly labeled as either included or excluded
from CodeLLM training. Beyond evaluation on the original CodeSnitch, we design
targeted mutation strategies to test the robustness of TDD methods under three
distinct settings. These mutation strategies are grounded in the
well-established Type-1 to Type-4 code clone detection taxonomy. Our study
provides a systematic assessment of current TDD techniques for code and offers
insights to guide the development of more effective and robust detection
methods in the future.

</details>


### [13] [AssertFlip: Reproducing Bugs via Inversion of LLM-Generated Passing Tests](https://arxiv.org/abs/2507.17542)
*Lara Khatib,Noble Saji Mathews,Meiyappan Nagappan*

Main category: cs.SE

TL;DR: AssertFlip是一种利用大语言模型自动生成bug复现测试的新技术，通过先生成通过测试再反转为失败测试的方式，在SWT-Bench基准测试中达到43.6%的成功率，超越了所有已知技术。


<details>
  <summary>Details</summary>
Motivation: 大多数开源和工业环境中的bug在报告时缺乏可执行的复现测试，这使得bug诊断和修复变得更加困难和耗时。现有的直接生成失败测试的方法效果不佳，因此需要一种更有效的自动化bug复现测试生成技术。

Method: AssertFlip采用一种新颖的两步法：首先使用大语言模型在有bug的代码上生成能够通过的测试，然后将这些测试反转，使其在存在bug时失败。这种方法基于假设：大语言模型更擅长编写通过的测试，而不是故意崩溃或失败的测试。

Result: AssertFlip在SWT-Bench基准测试的排行榜中超越了所有已知技术。在SWT-Bench-Verified子集上，AssertFlip实现了43.6%的从失败到通过的成功率，显著优于现有方法。

Conclusion: AssertFlip证明了通过先生成通过测试再反转的策略能够有效提高bug复现测试的自动生成质量。这种方法利用了大语言模型在生成正常功能测试方面的优势，为软件调试和修复过程提供了更好的自动化解决方案。

Abstract: Bug reproduction is critical in the software debugging and repair process,
yet the majority of bugs in open-source and industrial settings lack executable
tests to reproduce them at the time they are reported, making diagnosis and
resolution more difficult and time-consuming. To address this challenge, we
introduce AssertFlip, a novel technique for automatically generating Bug
Reproducible Tests (BRTs) using large language models (LLMs). Unlike existing
methods that attempt direct generation of failing tests, AssertFlip first
generates passing tests on the buggy behaviour and then inverts these tests to
fail when the bug is present. We hypothesize that LLMs are better at writing
passing tests than ones that crash or fail on purpose. Our results show that
AssertFlip outperforms all known techniques in the leaderboard of SWT-Bench, a
benchmark curated for BRTs. Specifically, AssertFlip achieves a fail-to-pass
success rate of 43.6% on the SWT-Bench-Verified subset.

</details>


### [14] [CodeReasoner: Enhancing the Code Reasoning Ability with Reinforcement Learning](https://arxiv.org/abs/2507.17548)
*Lingxiao Tang,He Ye,Zhongxin Liu,Xiaoxue Ren,Lingfeng Bao*

Main category: cs.SE

TL;DR: CodeReasoner是一个通过数据集构建和两阶段训练来提升大语言模型代码推理能力的框架，在代码推理基准测试中相比先前方法提升27.1%-40.2%的性能


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖监督微调来提升代码推理性能，但效果有限且难以泛化。这是由于两个核心问题：训练数据质量低和监督微调的局限性，难以教授通用推理技能

Method: 提出CodeReasoner框架，包括：1）构建专注于Python程序核心执行逻辑的数据集；2）使用指令调优注入从强大教师模型中提取的执行特定知识；3）在微调模型基础上通过GRPO强化学习增强推理和泛化能力

Result: 在三个广泛使用的代码推理基准测试中，CodeReasoner相比先前方法提升27.1%-40.2%的性能。7B模型在输入/输出和覆盖率预测等关键任务上与GPT-4o性能相当，14B模型在所有基准测试中都超越了GPT-4o

Conclusion: CodeReasoner通过改进数据集构建和采用两阶段训练过程（指令调优+强化学习），有效提升了大语言模型的代码推理能力，消融研究证实了每个训练阶段的有效性并强调了推理链的重要性

Abstract: Code reasoning is a fundamental capability for large language models (LLMs)
in the code domain. It involves understanding and predicting a program's
execution behavior, such as determining the output for a given input or whether
a specific statement will be executed. This capability is essential for
downstream tasks like debugging, code generation, and program repair. Prior
approaches mainly rely on supervised fine-tuning to improve performance in code
reasoning tasks. However, they often show limited gains and fail to generalize
across diverse scenarios. We argue this is due to two core issues: the low
quality of training data and the limitations of supervised fine-tuning, which
struggles to teach general reasoning skills. To address these challenges, we
propose CodeReasoner, a framework that spans both dataset construction and a
two-stage training process. First, we introduce a method to construct datasets
that focus on the core execution logic of Python programs. Next, we apply
instruction tuning to inject execution-specific knowledge distilled from a
powerful teacher model. We then enhance reasoning and generalization through
GRPO reinforcement learning on top of the fine-tuned model. Experiments on
three widely-used code reasoning benchmarks show that CodeReasoner improves
performance by 27.1% to 40.2% over prior methods using a 7B model. Notably, the
7B model matches GPT-4o on key tasks like input/output and coverage prediction.
When scaled to 14B, CodeReasoner outperforms GPT-4o across all benchmarks.
Ablation studies confirm the effectiveness of each training stage and highlight
the importance of reasoning chains.

</details>


### [15] [Contextual Code Retrieval for Commit Message Generation: A Preliminary Study](https://arxiv.org/abs/2507.17690)
*Bo Xiong,Linghao Zhang,Chong Wang,Peng Liang*

Main category: cs.SE

TL;DR: 本文提出了C3Gen方法，通过检索仓库中相关代码片段来增强提交消息生成，解决了仅依赖代码差异信息不足的问题，实验表明该方法能生成更全面、更有实用价值的提交消息。


<details>
  <summary>Details</summary>
Motivation: 现有的提交消息生成方法仅依赖代码差异作为输入，但原始代码差异无法捕获生成高质量、信息丰富的提交消息所需的完整上下文信息，因此需要引入更丰富的上下文来提升提交消息的质量。

Method: 提出了基于上下文代码检索的C3Gen方法，通过从代码仓库中检索与提交相关的代码片段，并将这些片段融入模型输入中，为模型提供仓库级别的丰富上下文信息来增强提交消息生成。

Result: 通过四个客观指标和三个主观指标在多个模型上评估了C3Gen的有效性，并进行了人类评估研究。结果显示，通过融入上下文代码信息，C3Gen使模型能够有效利用额外信息生成更全面、更有信息量的提交消息，在实际开发场景中具有更大的实用价值。

Conclusion: C3Gen通过引入上下文代码检索显著提升了提交消息生成的质量和实用性，进一步分析强调了基于相似性指标可靠性的担忧，并为提交消息生成领域提供了实证见解。

Abstract: A commit message describes the main code changes in a commit and plays a
crucial role in software maintenance. Existing commit message generation (CMG)
approaches typically frame it as a direct mapping which inputs a code diff and
produces a brief descriptive sentence as output. However, we argue that relying
solely on the code diff is insufficient, as raw code diff fails to capture the
full context needed for generating high-quality and informative commit
messages. In this paper, we propose a contextual code retrieval-based method
called C3Gen to enhance CMG by retrieving commit-relevant code snippets from
the repository and incorporating them into the model input to provide richer
contextual information at the repository scope. In the experiments, we
evaluated the effectiveness of C3Gen across various models using four objective
and three subjective metrics. Meanwhile, we design and conduct a human
evaluation to investigate how C3Gen-generated commit messages are perceived by
human developers. The results show that by incorporating contextual code into
the input, C3Gen enables models to effectively leverage additional information
to generate more comprehensive and informative commit messages with greater
practical value in real-world development scenarios. Further analysis
underscores concerns about the reliability of similaritybased metrics and
provides empirical insights for CMG.

</details>


### [16] [Educational Insights from Code: Mapping Learning Challenges in Object-Oriented Programming through Code-Based Evidence](https://arxiv.org/abs/2507.17743)
*Andre Menolli,Bruno Strik*

Main category: cs.SE

TL;DR: 本研究探索了代码问题指标与面向对象编程学习困难之间的关系，通过定性分析和文献综述，开发了一个连接代码相关问题与面向对象编程学习挑战的概念模型。


<details>
  <summary>Details</summary>
Motivation: 面向对象编程对计算机科学本科生来说具有挑战性，特别是在理解封装、继承和多态等抽象概念方面。虽然文献中描述了通过源代码分析识别设计和编码问题的各种方法，但很少有研究探索这些代码级问题与面向对象编程学习困难的关系。

Method: 使用定性分析方法识别学习困难的主要类别，通过文献综述建立学习困难、代码异味和SOLID原则违反之间的联系，开发连接代码相关问题与面向对象编程学习挑战的概念图。

Result: 成功开发了一个概念模型，将代码相关问题与面向对象编程中的特定学习挑战联系起来。该模型经过专家评估，专家将其应用于学生代码分析中。

Conclusion: 研究建立了代码问题指标与面向对象编程学习困难之间的关系模型，该模型在教育环境中具有相关性和适用性，可用于分析学生代码并识别学习挑战。

Abstract: Object-Oriented programming is frequently challenging for undergraduate
Computer Science students, particularly in understanding abstract concepts such
as encapsulation, inheritance, and polymorphism. Although the literature
outlines various methods to identify potential design and coding issues in
object-oriented programming through source code analysis, such as code smells
and SOLID principles, few studies explore how these code-level issues relate to
learning difficulties in Object-Oriented Programming. In this study, we explore
the relationship of the code issue indicators with common challenges
encountered during the learning of object-oriented programming. Using
qualitative analysis, we identified the main categories of learning
difficulties and, through a literature review, established connections between
these difficulties, code smells, and violations of the SOLID principles. As a
result, we developed a conceptual map that links code-related issues to
specific learning challenges in Object-Oriented Programming. The model was then
evaluated by an expert who applied it in the analysis of the student code to
assess its relevance and applicability in educational contexts.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [17] [Hiord: An Approach to the Specification and Verification of Higher-Order (C)LP Programs](https://arxiv.org/abs/2507.17233)
*Marco Ciccalè,Daniel Jurjo-Rivas,Jose F. Morales,Pedro López-García,Manuel V. Hermenegildo*

Main category: cs.PL

TL;DR: 本文提出了一种静态验证高阶约束逻辑程序(CLP)的新方法，通过谓词属性来描述高阶参数，并将其简化为一阶属性来处理，在Ciao系统中实现了原型并进行了评估。


<details>
  <summary>Details</summary>
Motivation: 高阶程序中的断言可以描述高阶参数，虽然运行时验证已有研究，但编译时静态验证仍相对缺乏探索。需要一种方法来静态验证带有高阶断言的高阶约束逻辑程序。

Method: 提出使用谓词属性来描述高阶参数，这是一种利用Ciao断言语言的特殊属性。完善了这些属性的语法和语义，引入抽象准则来确定编译时对谓词属性的符合性，基于语义序关系比较谓词属性与谓词断言。通过将谓词属性简化为一阶属性，使用基于抽象解释的静态分析器来处理这些属性。

Result: 在Ciao系统中实现了原型，并通过各种示例进行了评估。展示了该方法能够有效地静态验证高阶约束逻辑程序中的高阶断言。

Conclusion: 成功提出了一种通用的静态验证高阶CLP程序的方法，该方法通过谓词属性描述高阶参数，并将问题简化为一阶属性处理，在Ciao系统的实现和评估验证了方法的可行性。

Abstract: Higher-order constructs enable more expressive and concise code by allowing
procedures to be parameterized by other procedures. Assertions allow expressing
partial program specifications, which can be verified either at compile time
(statically) or run time (dynamically). In higher-order programs, assertions
can also describe higher-order arguments. While in the context of (C)LP,
run-time verification of higher-order assertions has received some attention,
compile-time verification remains relatively unexplored. We propose a novel
approach for statically verifying higher-order (C)LP programs with higher-order
assertions. Although we use the Ciao assertion language for illustration, our
approach is quite general and we believe is applicable to similar contexts.
Higher-order arguments are described using predicate properties -- a special
kind of property which exploits the (Ciao) assertion language. We refine the
syntax and semantics of these properties and introduce an abstract criterion to
determine conformance to a predicate property at compile time, based on a
semantic order relation comparing the predicate property with the predicate
assertions. We then show how to handle these properties using an abstract
interpretation-based static analyzer for programs with first-order assertions
by reducing predicate properties to first-order properties. Finally, we report
on a prototype implementation and evaluate it through various examples within
the Ciao system.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [18] [Integrating Belief Domains into Probabilistic Logic Programs](https://arxiv.org/abs/2507.17291)
*Damiano Azzolini,Fabrizio Riguzzi,Theresa Swift*

Main category: cs.LO

TL;DR: 本文提出了基于区间的容量逻辑程序，通过将分布语义扩展到信念函数来处理认识不确定性，克服了现有概率逻辑编程只能使用点概率的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有分布语义下的概率逻辑编程使用点概率，难以表达认识不确定性（如计算机视觉模型的层次分类产生的不确定性）。需要一个能够处理区间概率和认识不确定性的框架。

Method: 将分布语义扩展到包含信念函数，引入基于区间的容量逻辑程序。信念函数作为非加性容量推广了概率测度，通过区间概率来处理认识不确定性。

Result: 成功建立了新的基于区间容量逻辑程序的框架，该框架具有适用于实际应用的特性，能够有效处理认识不确定性。

Conclusion: 提出的基于区间的容量逻辑程序框架成功扩展了分布语义，为处理认识不确定性提供了新的解决方案，具有良好的实用性。

Abstract: Probabilistic Logic Programming (PLP) under the Distribution Semantics is a
leading approach to practical reasoning under uncertainty. An advantage of the
Distribution Semantics is its suitability for implementation as a Prolog or
Python library, available through two well-maintained implementations, namely
ProbLog and cplint/PITA. However, current formulations of the Distribution
Semantics use point-probabilities, making it difficult to express epistemic
uncertainty, such as arises from, for example, hierarchical classifications
from computer vision models. Belief functions generalize probability measures
as non-additive capacities, and address epistemic uncertainty via interval
probabilities. This paper introduces interval-based Capacity Logic Programs
based on an extension of the Distribution Semantics to include belief
functions, and describes properties of the new framework that make it amenable
to practical applications.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [19] [Realisability and Complementability of Multiparty Session Types](https://arxiv.org/abs/2507.17354)
*Cinzia Di Giusto,Etienne Lozes,Pascal Urso*

Main category: cs.FL

TL;DR: 本文研究多方会话类型(MPST)中全局类型的可实现性与互补性之间的关系，证明了点对点通信可实现的全局类型在同步通信中也可实现，同步模型中可实现的全局类型是可互补的，并提供了判断互补全局类型在点对点通信中可实现性的PSPACE算法。


<details>
  <summary>Details</summary>
Motivation: 多方会话类型是基于类型的方法来规范消息传递分布式系统，需要研究全局类型的可实现性（即局部行为的组合是否符合全局类型指定的行为）与互补性（是否存在描述原始全局类型互补行为的全局类型）之间的关系。

Method: 通过理论分析建立可实现性与互补性之间的关系：1）证明点对点通信可实现性与同步通信可实现性的等价性；2）证明同步模型中的可实现性蕴含互补性；3）设计PSPACE复杂度的算法来判断给定互补的全局类型在点对点通信中的可实现性；4）提出发送者驱动选择的全局类型互补构造方法。

Result: 建立了三个主要理论结果：全局类型在点对点通信中可实现当且仅当在同步通信中可实现；同步模型中可实现的全局类型必然是可互补的；提供了判断互补全局类型在点对点通信中可实现性的PSPACE算法。此外，还提出了发送者驱动选择全局类型的线性大小互补构造方法。

Conclusion: 本文揭示了多方会话类型中可实现性与互补性的深层联系，为理解和验证分布式系统的行为一致性提供了新的理论基础和算法工具，推进了会话类型理论在实际应用中的发展。

Abstract: Multiparty session types (MPST) are a type-based approach for specifying
message-passing distributed systems. They rely on the notion of global type
specifying the global behaviour and local types, which are the projections of
the global behaviour onto each local participant. An essential property of
global types is realisability, i.e., whether the composition of the local
behaviours conforms to those specified by the global type. We explore how
realisability of MPST relates to their complementability, i.e., whether there
exists a global type that describes the complementary behaviour of the original
global type. First, we show that if a global type is realisable with p2p
communications, then it is realisable with synchronous communications. Second,
we show that if a global type is realisable in the synchronous model, then it
is complementable, in the sense that there exists a global type that describes
the complementary behaviour of the original global type. Third, we give an
algorithm to decide whether a complementable global type, given with an
explicit complement, is realisable in p2p. The algorithm is PSPACE in the size
of the global type and its complement. As a side contribution, we propose a
complementation construction for global types with sender-driven choice with a
linear blowup in the size of the global type.

</details>


### [20] [Reasoning about Rare-Event Reachability in Stochastic Vector Addition Systems via Affine Vector Spaces](https://arxiv.org/abs/2507.17711)
*Joshua Jeppson,Landon Taylor,Bingqing Hu,Zhen Zhang*

Main category: cs.FL

TL;DR: 本文提出了两种新的启发式方法ISR和SDP，用于分析随机向量加法系统（VAS）中罕见事件的概率，特别针对化学反应网络中的病理性罕见事件效应，通过部分状态空间扩展和轨迹生成来高效计算罕见事件的下界概率。


<details>
  <summary>Details</summary>
Motivation: 随机向量加法系统中的罕见事件虽然极不可能发生，但可能代表不良行为并产生负面影响。由于其低概率特性和潜在的极大状态空间，现有的概率模型检查和随机罕见事件仿真技术面临挑战。特别是在化学反应网络中，罕见事件效应可能是病理性的，需要新的分析方法。

Method: 提出了两种新颖的启发式方法：迭代子空间约简（ISR）和单距离优先级（SDP）。两种方法都构建包含所有解状态的封闭向量空间。SDP简单地优先考虑到"解空间"的较短距离，而ISR构建一组嵌套子空间，其中短且高概率的满足轨迹可能依次通过。通过优先级优先的部分状态空间扩展和轨迹生成进行瞬态分析。

Result: 两种方法都是确定性的、快速的，并在具有挑战性的化学反应网络模型上表现出显著的性能。生成的部分状态图包含到罕见事件状态的可能轨迹，允许高效的概率模型检查来计算感兴趣罕见事件的下界概率。在挑战性CRN模型上展现了卓越的性能表现。

Conclusion: ISR和SDP两种方法成功解决了VAS中罕见事件概率分析的计算挑战，通过构建包含关键轨迹的部分状态空间，实现了对罕见事件下界概率的高效计算，为化学反应网络等复杂系统的罕见事件分析提供了有效的解决方案。

Abstract: Rare events in Stochastic Vector Addition System (VAS) are of significant
interest because, while extremely unlikely, they may represent undesirable
behavior that can have adverse effects. Their low probabilities and potentially
extremely large state spaces challenge existing probabilistic model checking
and stochastic rare-event simulation techniques. In particular, in Chemical
Reaction Networks (CRNs), a chemical kinetic language often represented as VAS,
rare event effects may be pathological. We present two novel heuristics for
priority-first partial state space expansion and trace generation tuned to the
transient analysis of rare-event probability in VAS: Iterative Subspace
Reduction (ISR) and Single Distance Priority (SDP). Both methods construct a
closed vector space containing all solution states. SDP then simply prioritizes
shorter distances to this ``solution space'', while ISR constructs a set of
nested subspaces, where short and highly-probable satisfying traces are likely
to pass through in sequence. The resulting partial state graph from each method
contains likely traces to rare-event states, allowing efficient probabilistic
model checking to compute a lower-bound probability of a rare event of
interest. These methods are deterministic, fast, and demonstrate marked
performance on challenging CRN models.

</details>
