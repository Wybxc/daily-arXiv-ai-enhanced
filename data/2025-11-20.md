<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 18]
- [cs.PL](#cs.PL) [Total: 6]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Hybrid Quantum-Classical Machine Learning with PennyLane: A Comprehensive Guide for Computational Research](https://arxiv.org/abs/2511.14786)
*Sidney Shapiro*

Main category: cs.SE

TL;DR: PennyLane是一个连接量子电路和经典机器学习的Python框架，支持构建、优化和部署变分量子算法，应用于量子机器学习、优化和量子化学等领域。


<details>
  <summary>Details</summary>
Motivation: 结合量子计算的优势与经典优化技术，为研究人员提供构建混合量子-经典机器学习系统的工具，促进量子增强数据科学的发展。

Method: 提供Python框架，支持量子核方法、变分量子本征求解器、投资组合优化等用例，并与PyTorch、TensorFlow、JAX等经典ML框架集成。

Result: 通过具体Python示例展示PennyLane在高效量子电路构建、自动微分和混合优化工作流方面的能力。

Conclusion: PennyLane作为量子计算和机器学习之间的桥梁，成为基于Python研究的混合量子-经典工作流的标准引用工具。

Abstract: Hybrid quantum-classical machine learning represents a frontier in computational research, combining the potential advantages of quantum computing with established classical optimization techniques. PennyLane provides a Python framework that seamlessly bridges quantum circuits and classical machine learning, enabling researchers to build, optimize, and deploy variational quantum algorithms. This paper introduces PennyLane as a versatile tool for quantum machine learning, optimization, and quantum chemistry applications. We demonstrate use cases including quantum kernel methods, variational quantum eigensolvers, portfolio optimization, and integration with classical ML frameworks such as PyTorch, TensorFlow, and JAX. Through concrete Python examples with widely used libraries such as scikit-learn, pandas, and matplotlib, we show how PennyLane facilitates efficient quantum circuit construction, automatic differentiation, and hybrid optimization workflows. By situating PennyLane within the broader context of quantum computing and machine learning, we highlight its role as a methodological building block for quantum-enhanced data science. Our goal is to provide researchers and practitioners with a concise reference that bridges foundational quantum computing concepts and applied machine learning practice, making PennyLane a default citation for hybrid quantum-classical workflows in Python-based research.

</details>


### [2] [Enabling Predictive Maintenance in District Heating Substations: A Labelled Dataset and Fault Detection Evaluation Framework based on Service Data](https://arxiv.org/abs/2511.14791)
*Cyriana M. A. Roelofs,Edison Guevara Bastidas,Thomas Hugo,Stefan Faulstich,Anna Cadenbach*

Main category: cs.SE

TL;DR: 提出了一个用于区域供热站早期故障检测的开源框架，包含公开数据集、评估方法和基线结果，能够提前检测故障并减少误报。


<details>
  <summary>Details</summary>
Motivation: 区域供热站故障的早期检测对于降低回水温度和提高效率至关重要，但该领域发展受到公开标记数据集有限的阻碍。

Method: 开发了EnergyFaultDetector开源Python框架，结合服务报告验证的公共数据集，使用准确性、可靠性和早期性三个指标进行评估，并支持使用ARCANA进行根本原因分析。

Result: 模型实现了高正常行为准确率（0.98）和事件F分数（0.83），在客户报告问题前检测到60%的故障，平均提前时间为3.9天。

Conclusion: 通过整合开放数据集、指标、开源代码和基线，建立了一个可复现的故障中心基准，为区域供热站的早期故障检测和诊断方法提供了一致的比较和开发基础。

Abstract: Early detection of faults in district heating substations is imperative to reduce return temperatures and enhance efficiency. However, progress in this domain has been hindered by the limited availability of public, labelled datasets. We present an open source framework combining a service report validated public dataset, an evaluation method based on Accuracy, Reliability, and Earliness, and baseline results implemented with EnergyFaultDetector, an open source Python framework.
  The dataset contains time series of operational data from 93 substations across two manufacturers, annotated with a list of disturbances due to faults and maintenance actions, a set of normal-event examples and detailed fault metadata. We evaluate the EnergyFaultDetector using three metrics: Accuracy for recognising normal behaviour, an eventwise F Score for reliable fault detection with few false alarms, and Earliness for early detection. The framework also supports root cause analysis using ARCANA. We demonstrate three use cases to assist operators in interpreting anomalies and identifying underlying faults. The models achieve high normal-behaviour accuracy (0.98) and eventwise F-score (beta=0.5) of 0.83, detecting 60% of the faults in the dataset before the customer reports a problem, with an average lead time of 3.9 days.
  Integrating an open dataset, metrics, open source code, and baselines establishes a reproducible, fault centric benchmark with operationally meaningful evaluation, enabling consistent comparison and development of early fault detection and diagnosis methods for district heating substations.

</details>


### [3] [irace-evo: Automatic Algorithm Configuration Extended With LLM-Based Code Evolution](https://arxiv.org/abs/2511.14794)
*Camilo Chacón Sartori,Christian Blum*

Main category: cs.SE

TL;DR: 本文介绍了irace-evo，一个将irace自动参数配置工具与基于LLM的代码进化相结合的框架，能够在多语言环境中联合探索参数和代码空间，显著提升算法性能且成本低廉。


<details>
  <summary>Details</summary>
Motivation: 传统自动算法配置工具如irace只能调优参数值而无法修改算法代码，限制了算法性能的进一步提升潜力。

Method: 提出irace-evo框架，集成LLM进行代码进化，支持多语言，采用渐进式上下文管理和Always-From-Original原则确保代码进化的稳健性。

Result: 在VSBPP问题的CMSA元启发式上测试，irace-evo发现了优于现有最佳实现的新算法变体，总成本低于2欧元。

Conclusion: 将自动配置与LLM驱动的代码进化相结合，为启发式设计和元启发式优化提供了强大且成本效益高的新途径。

Abstract: Automatic algorithm configuration tools such as irace efficiently tune parameter values but leave algorithmic code unchanged. This paper introduces a first version of irace-evo, an extension of irace that integrates code evolution through large language models (LLMs) to jointly explore parameter and code spaces. The proposed framework enables multi-language support (e.g., C++, Python), reduces token consumption via progressive context management, and employs the Always-From-Original principle to ensure robust and controlled code evolution. We evaluate irace-evo on the Construct, Merge, Solve & Adapt (CMSA) metaheuristic for the Variable-Sized Bin Packing Problem (VSBPP). Experimental results show that irace-evo can discover new algorithm variants that outperform the state-of-the-art CMSA implementation while maintaining low computational and monetary costs. Notably, irace-evo generates competitive algorithmic improvements using lightweight models (e.g., Claude Haiku 3.5) with a total usage cost under 2 euros. These results demonstrate that coupling automatic configuration with LLM-driven code evolution provides a powerful, cost-efficient avenue for advancing heuristic design and metaheuristic optimization.

</details>


### [4] [Evaluating Generative AI for CS1 Code Grading: Direct vs Reverse Methods](https://arxiv.org/abs/2511.14798)
*Ahmad Memon,Abdallah Mohamed*

Main category: cs.SE

TL;DR: 比较两种基于AI的编程作业评分方法：直接评分法（AI直接按评分标准给分）和反向评分法（AI先修复代码错误，再根据修复情况给分），评估它们在扩展评分尺度下的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 手动批改编程作业耗时且不一致，传统单元测试只能给出二元结果。利用大语言模型实现自动化、可扩展且更客观的评分。

Method: 提出两种AI评分方法：直接评分法和反向评分法，在原始评分尺度和10倍扩展尺度上评估准确性，并使用合成学生代码测试一致性。

Result: 直接方法更快更直接，但反向方法能提供更细粒度的评估。两种方法都需要精心设计提示词，特别是在分配部分分数和处理逻辑错误时。

Conclusion: 讨论了两种方法的优缺点，提出了未来混合人机评分系统的方向，旨在提高计算机科学课程评分的一致性、效率和公平性。

Abstract: Manual grading of programming assignments in introductory computer science courses can be time-consuming and prone to inconsistencies. While unit testing is commonly used for automatic evaluation, it typically follows a binary pass/fail model and does not give partial marks. Recent advances in large language models (LLMs) offer the potential for automated, scalable, and more objective grading.
  This paper compares two AI-based grading techniques: \textit{Direct}, where the AI model applies a rubric directly to student code, and \textit{Reverse} (a newly proposed approach), where the AI first fixes errors, then deduces a grade based on the nature and number of fixes. Each method was evaluated on both the instructor's original grading scale and a tenfold expanded scale to assess the impact of range on AI grading accuracy. To assess their effectiveness, AI-assigned scores were evaluated against human tutor evaluations on a range of coding problems and error types.
  Initial findings suggest that while the Direct approach is faster and straightforward, the Reverse technique often provides a more fine-grained assessment by focusing on correction effort. Both methods require careful prompt engineering, particularly for allocating partial credit and handling logic errors. To further test consistency, we also used synthetic student code generated using Gemini Flash 2.0, which allowed us to evaluate AI graders on a wider range of controlled error types and difficulty levels. We discuss the strengths and limitations of each approach, practical considerations for prompt design, and future directions for hybrid human-AI grading systems that aim to improve consistency, efficiency, and fairness in CS courses.

</details>


### [5] [Scalable and Efficient Large-Scale Log Analysis with LLMs: An IT Software Support Case Study](https://arxiv.org/abs/2511.14803)
*Pranjal Gupta,Karan Bhukar,Harshit Kumar,Seema Nagar,Prateeti Mohapatra,Debanjana Kar*

Main category: cs.SE

TL;DR: 开发了一个基于大语言模型的日志分析工具，用于自动化处理IT系统日志和问题诊断，通过在CPU上高效运行LLM处理海量日志，显著节省时间和成本。


<details>
  <summary>Details</summary>
Motivation: IT环境产生大量日志数据，手动检查不现实，需要自动化日志分析工具来监控系统健康和检测问题。

Method: 提出基于大语言模型的日志分析工具，采用新颖方法在CPU上高效运行LLM处理海量日志数据，生成自动化洞察和摘要。

Result: 自2024年3月在生产环境部署，扩展到70个软件产品，处理2000多个工单进行问题诊断，节省300+人工小时，每月估计节省15,444美元人力成本。

Conclusion: 基于LLM的日志分析工具能够有效自动化日志处理，显著提高效率并降低成本，证明了在CPU上高效运行LLM处理海量日志的可行性。

Abstract: IT environments typically have logging mechanisms to monitor system health and detect issues. However, the huge volume of generated logs makes manual inspection impractical, highlighting the importance of automated log analysis in IT Software Support. In this paper, we propose a log analytics tool that leverages Large Language Models (LLMs) for log data processing and issue diagnosis, enabling the generation of automated insights and summaries. We further present a novel approach for efficiently running LLMs on CPUs to process massive log volumes in minimal time without compromising output quality. We share the insights and lessons learned from deployment of the tool - in production since March 2024 - scaled across 70 software products, processing over 2000 tickets for issue diagnosis, achieving a time savings of 300+ man hours and an estimated $15,444 per month in manpower costs compared to the traditional log analysis practices.

</details>


### [6] [Towards Continuous Assurance with Formal Verification and Assurance Cases](https://arxiv.org/abs/2511.14805)
*Dhaminda B. Abeywickrama,Michael Fisher,Frederic Wheeler,Louise Dennis*

Main category: cs.SE

TL;DR: 提出了一个统一的持续保证框架，将设计时、运行时和演化时保证集成在可追踪的模型驱动工作流中，以解决传统保证方法在系统更新和运行时变化方面的不足。


<details>
  <summary>Details</summary>
Motivation: 传统保证方法往往将开发时保证与运行时保证分离，导致碎片化的论证无法适应运行时变化或系统更新，这是确保自主系统正确性和安全性的重大挑战。

Method: 使用RoboChart进行功能正确性验证和PRISM进行概率风险分析，实现设计时保证；开发了作为Eclipse插件的模型驱动转换流水线，自动重新生成结构化保证论证。

Result: 在核检查机器人场景中演示了该方法，展示了当形式化规范或其验证结果变化时，框架能够确保可追踪性并自动更新保证论证。

Conclusion: 该框架为自主系统的持续保证提供了统一方法，符合三边AI原则，体现了监管机构认可的最佳实践。

Abstract: Autonomous systems must sustain justified confidence in their correctness and safety across their operational lifecycle-from design and deployment through post-deployment evolution. Traditional assurance methods often separate development-time assurance from runtime assurance, yielding fragmented arguments that cannot adapt to runtime changes or system updates - a significant challenge for assured autonomy. Towards addressing this, we propose a unified Continuous Assurance Framework that integrates design-time, runtime, and evolution-time assurance within a traceable, model-driven workflow as a step towards assured autonomy. In this paper, we specifically instantiate the design-time phase of the framework using two formal verification methods: RoboChart for functional correctness and PRISM for probabilistic risk analysis. We also propose a model-driven transformation pipeline, implemented as an Eclipse plugin, that automatically regenerates structured assurance arguments whenever formal specifications or their verification results change, thereby ensuring traceability. We demonstrate our approach on a nuclear inspection robot scenario, and discuss its alignment with the Trilateral AI Principles, reflecting regulator-endorsed best practices.

</details>


### [7] [Automatic Pipeline Provisioning](https://arxiv.org/abs/2511.14825)
*Alexandre-Xavier Labonté-Lamoureux,Simon Boyer*

Main category: cs.SE

TL;DR: 探索自动流水线配置的好处及其应用方式


<details>
  <summary>Details</summary>
Motivation: 研究自动流水线配置在软件工程项目中的优势，重点关注CI流水线，同时认为对CD流水线也有类似效果

Method: 定义自动流水线配置为快速部署软件工程项目的流水线过程

Result: 未提供具体结果

Conclusion: 自动流水线配置能够快速部署软件工程项目流水线，对CI和CD流水线都有益处

Abstract: The goal of this paper is to explore the benefits of automatic pipeline provisioning and identify how it can be applied. Automatic pipeline provisioning can be defined as a process of quickly deploying a pipeline for a software engineering project. This research will focus on CI pipelines, although the outcomes of this approach on CD pipelines will likely be similar.

</details>


### [8] [MermaidSeqBench: An Evaluation Benchmark for LLM-to-Mermaid Sequence Diagram Generation](https://arxiv.org/abs/2511.14967)
*Basel Shbita,Farhan Ahmed,Chad DeLuca*

Main category: cs.SE

TL;DR: 提出了MermaidSeqBench基准测试，用于评估LLM从文本提示生成Mermaid序列图的能力，包含132个样本，采用混合方法扩展，并使用LLM作为评判模型进行多维度评估。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统性的基准测试来评估LLM在生成结构化图表（特别是Mermaid序列图）方面的正确性，这限制了该领域的研究进展。

Method: 通过结合人工标注、上下文LLM提示和基于规则的变体生成的混合方法，构建了包含132个样本的基准测试，并使用LLM作为评判模型进行多维度评估。

Result: 评估揭示了不同模型在序列图生成能力上存在显著差距，证明了基准测试的有效性和灵活性。

Conclusion: 该基准测试为结构化图表生成研究提供了基础，有助于开发更严格、细粒度的评估方法。

Abstract: Large language models (LLMs) have demonstrated excellent capabilities in generating structured diagrams from natural language descriptions. In particular, they have shown great promise in generating sequence diagrams for software engineering, typically represented in a text-based syntax such as Mermaid. However, systematic evaluations in this space remain underdeveloped as there is a lack of existing benchmarks to assess the LLM's correctness in this task. To address this shortcoming, we introduce MermaidSeqBench, a human-verified and LLM-synthetically-extended benchmark for assessing an LLM's capabilities in generating Mermaid sequence diagrams from textual prompts. The benchmark consists of a core set of 132 samples, starting from a small set of manually crafted and verified flows. These were expanded via a hybrid methodology combining human annotation, in-context LLM prompting, and rule-based variation generation. Our benchmark uses an LLM-as-a-judge model to assess Mermaid sequence diagram generation across fine-grained metrics, including syntax correctness, activation handling, error handling, and practical usability. We perform initial evaluations on numerous state-of-the-art LLMs and utilize multiple LLM judge models to demonstrate the effectiveness and flexibility of our benchmark. Our results reveal significant capability gaps across models and evaluation modes. Our proposed benchmark provides a foundation for advancing research in structured diagram generation and for developing more rigorous, fine-grained evaluation methodologies.

</details>


### [9] [FRIENDS GUI: A graphical user interface for data collection and visualization of vaping behavior from a passive vaping monitor](https://arxiv.org/abs/2511.15007)
*Shehan I Pranto,Brett Fassler,Md Rafi Islam,Ashley Schenkel,Larry W Hawk,Edward Sazonov*

Main category: cs.SE

TL;DR: 开发了FRIENDS GUI，这是一个基于Python的开源工具，用于提取、解码和可视化FRIENDS设备收集的24小时电子烟使用数据，提高了数据的可访问性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 理解电子烟使用模式（包括抽吸持续时间、抽吸间隔和每次会话的抽吸次数）对于评估电子烟使用、有毒物质暴露和制定监管决策至关重要。

Method: 开发了基于Python的开源GUI工具，能够提取、解码和可视化FRIENDS设备记录的24小时抽吸数据，包括时间戳转换、事件解码和行为可视化。

Result: 使用24小时实验数据进行验证，确认了时间戳转换的准确性、事件解码的可靠性以及行为可视化的有效性。

Conclusion: 该软件已在GitHub上免费提供，可供公众使用，有助于提高电子烟使用数据的可访问性和分析效率。

Abstract: Understanding puffing topography (PT), which includes puff duration, intra puff interval, and puff count per session, is critical for evaluating Electronic Nicotine Delivery Systems (ENDS) use, toxicant exposure, and informing regulatory decisions. We developed FRIENDS (Flexible Robust Instrumentation of ENDS), an open-source device that records puffing and touch events of ENDS by attaching to it. This paper introduces the FRIENDS GUI that improves accessibility and interpretability of data collected by FRIENDS. The GUI is a Python-based open-source tool that extracts, decodes, and visualizes 24-hour puffing data from the FRIENDS device. Validation using 24-hour experimental data confirmed accurate timestamp conversion, reliable event decoding, and effective behavioral visualization. The software is freely available on GitHub for public use.

</details>


### [10] [Effective Code Membership Inference for Code Completion Models via Adversarial Prompts](https://arxiv.org/abs/2511.15107)
*Yuan Jiang,Zehao Li,Shan Huang,Christoph Treude,Xiaohong Su,Tiantian Wang*

Main category: cs.SE

TL;DR: 提出了AdvPrompt-MIA方法，通过代码特定的对抗性提示和深度学习来改进代码补全模型的成员推理攻击，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的黑盒和灰盒成员推理攻击依赖昂贵的代理模型或手动设计的启发式规则，难以捕捉过参数化代码语言模型的细微记忆模式。

Method: 设计一系列对抗性提示诱导受害者代码模型输出变化，通过比较输出与真实补全构建特征向量，训练分类器自动区分成员和非成员样本。

Result: 在Code Llama 7B等模型上的评估显示，该方法持续优于最先进基线，AUC提升高达102%，并展现出强大的跨模型和数据集可迁移性。

Conclusion: AdvPrompt-MIA能有效捕捉更丰富的记忆模式，准确推断训练集成员资格，具有实际应用价值和泛化能力。

Abstract: Membership inference attacks (MIAs) on code completion models offer an effective way to assess privacy risks by inferring whether a given code snippet was part of the training data. Existing black- and gray-box MIAs rely on expensive surrogate models or manually crafted heuristic rules, which limit their ability to capture the nuanced memorization patterns exhibited by over-parameterized code language models. To address these challenges, we propose AdvPrompt-MIA, a method specifically designed for code completion models, combining code-specific adversarial perturbations with deep learning. The core novelty of our method lies in designing a series of adversarial prompts that induce variations in the victim code model's output. By comparing these outputs with the ground-truth completion, we construct feature vectors to train a classifier that automatically distinguishes member from non-member samples. This design allows our method to capture richer memorization patterns and accurately infer training set membership. We conduct comprehensive evaluations on widely adopted models, such as Code Llama 7B, over the APPS and HumanEval benchmarks. The results show that our approach consistently outperforms state-of-the-art baselines, with AUC gains of up to 102%. In addition, our method exhibits strong transferability across different models and datasets, underscoring its practical utility and generalizability.

</details>


### [11] [MutDafny: A Mutation-Based Approach to Assess Dafny Specifications](https://arxiv.org/abs/2511.15403)
*Isabel Amaral,Alexandra Mendes,José Campos*

Main category: cs.SE

TL;DR: MutDafny是一个基于变异测试的工具，用于检测Dafny形式化规范中的弱点。通过向代码中引入变异并观察规范是否能检测到这些变异，来识别规范中的潜在缺陷。


<details>
  <summary>Details</summary>
Motivation: 在验证感知编程语言如Dafny中，规范与实现一样容易出错。规范中的缺陷可能导致形式化验证的程序偏离预期行为，因此需要工具来提高规范的可靠性。

Method: 采用变异测试方法，从流行工具中分析适用的变异操作符，并从GitHub上的Dafny项目错误修复提交中合成新的操作符。最终工具包含32个变异操作符。

Result: 在794个真实世界Dafny程序数据集上评估，手动分析未检测到的变异体，识别出5个弱规范（平均每241行代码就有一个），这些规范需要加强。

Conclusion: 变异测试能有效揭示Dafny规范中的弱点，MutDafny工具为提高形式化规范的可靠性提供了实用方法。

Abstract: This paper explores the use of mutation testing to reveal weaknesses in formal specifications written in Dafny. In verification-aware programming languages, such as Dafny, despite their critical role, specifications are as prone to errors as implementations. Flaws in specs can result in formally verified programs that deviate from the intended behavior.
  We present MutDafny, a tool that increases the reliability of Dafny specifications by automatically signaling potential weaknesses. Using a mutation testing approach, we introduce faults (mutations) into the code and rely on formal specifications for detecting them. If a program with a mutant verifies, this may indicate a weakness in the specification. We extensively analyze mutation operators from popular tools, identifying the ones applicable to Dafny. In addition, we synthesize new operators tailored for Dafny from bugfix commits in publicly available Dafny projects on GitHub. Drawing from both, we equipped our tool with a total of 32 mutation operators. We evaluate MutDafny's effectiveness and efficiency in a dataset of 794 real-world Dafny programs and we manually analyze a subset of the resulting undetected mutants, identifying five weak real-world specifications (on average, one at every 241 lines of code) that would benefit from strengthening.

</details>


### [12] [Finetuning LLMs for Automatic Form Interaction on Web-Browser in Selenium Testing Framework](https://arxiv.org/abs/2511.15168)
*Nguyen-Khang Le,Nguyen Hiep,Minh Nguyen,Son Luu,Trung Vo,Quan Bui,Nomura Shoshin,Le-Minh Nguyen*

Main category: cs.SE

TL;DR: 提出了一种训练LLM生成Selenium表单交互测试用例的新方法，创建了合成和人工标注的数据集，在语法正确性、可执行性和输入字段覆盖率方面显著优于GPT-4o等基线模型。


<details>
  <summary>Details</summary>
Motivation: 尽管自动化Web应用测试很重要，但在LLM生成表单交互测试用例方面研究不足，缺乏公开的基准数据集来系统评估LLM在此任务上的表现。

Method: 训练LLM生成高质量的Selenium测试用例，专门针对表单交互测试，构建了合成和人工标注的数据集，定义了语法正确性、脚本可执行性和输入字段覆盖率的评估指标。

Result: 实证研究表明，该方法在所有评估指标上都显著优于GPT-4o和其他流行LLM等强基线模型。

Conclusion: 这项工作为基于LLM的Web测试未来研究奠定了基础，并提供了支持该领域持续发展的资源。

Abstract: Automated web application testing is a critical component of modern software development, with frameworks like Selenium widely adopted for validating functionality through browser automation. Among the essential aspects of such testing is the ability to interact with and validate web forms, a task that requires syntactically correct, executable scripts with high coverage of input fields. Despite its importance, this task remains underexplored in the context of large language models (LLMs), and no public benchmark or dataset exists to evaluate LLMs on form interaction generation systematically. This paper introduces a novel method for training LLMs to generate high-quality test cases in Selenium, specifically targeting form interaction testing. We curate both synthetic and human-annotated datasets for training and evaluation, covering diverse real-world forms and testing scenarios. We define clear metrics for syntax correctness, script executability, and input field coverage. Our empirical study demonstrates that our approach significantly outperforms strong baselines, including GPT-4o and other popular LLMs, across all evaluation metrics. Our work lays the groundwork for future research on LLM-based web testing and provides resources to support ongoing progress in this area.

</details>


### [13] [From Code Smells to Best Practices: Tackling Resource Leaks in PyTorch, TensorFlow, and Keras](https://arxiv.org/abs/2511.15229)
*Bashar Abdallah,Martyna E. Wojciechowska,Gustavo Santos,Edmand Yu,Maxime Lamothe,Alain Abran,Mohammad Hamdaqa*

Main category: cs.SE

TL;DR: 本研究系统识别了导致机器学习应用中资源泄漏的代码异味，在PyTorch、TensorFlow和Keras框架中分别发现了30种和16种相关异味，并提出了50个最佳实践来减少资源泄漏。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习研究主要关注模型性能指标，对长期可持续性和资源效率关注有限。确保高效资源管理与高性能同等重要，对稳健部署至关重要。

Method: 通过实证调查开发者讨论和真实代码片段，采用三阶段验证过程（三位作者独立分析后达成共识），按根本原因和框架特性对代码异味进行分类。

Result: 识别出30种PyTorch相关异味和16种TensorFlow/Keras异味，为每种异味推导出至少一个最佳实践，共提出50个推荐编码模式。

Conclusion: 这是首个全面研究主要ML框架中导致资源泄漏代码异味的研究，提供了可操作的最佳实践，支持开发者构建更高效和可持续的ML应用。

Abstract: Much of the existing ML research focuses on model performance metrics, leaving limited attention to the long-term sustainability and resource efficiency of ML applications. While high performance is essential, ensuring efficient resource management is equally critical for robust deployment. This study addresses this gap by systematically identifying code smells that lead to resource leaks in ML applications. We conducted an empirical investigation of developer discussions and real-world code snippets from PyTorch, TensorFlow, and Keras. The analysis identified 30 PyTorch-related smells and 16 TensorFlow/Keras smells linked to resource leaks. These smells were categorized in two ways: (1) based on their root causes, and (2) as general ML smells with framework-specific characteristics. For each smell, we derived at least one best practice, resulting in 50 recommended coding patterns aimed at reducing resource leakage and improving efficiency. To ensure the validity of our findings, we employed a three-phase validation process involving independent analysis by three authors followed by consensus discussions. This is the first comprehensive study to examine resource-leak-inducing code smells across major ML frameworks and to present actionable best practices for mitigating them. The contributions support developers in building more efficient and sustainable ML applications and offer a structured view of the underlying causes of resource leaks.

</details>


### [14] [M, Toolchain and Language for Reusable Model Compilation](https://arxiv.org/abs/2511.15257)
*Hiep Hong Trinh,Federico Ciccozzi,Abu Naser Masud,Marjan Sirjani,Mikael Sjödin*

Main category: cs.SE

TL;DR: M是一种基于参与者模型的建模语言和工具链，支持复杂并发系统的多目标编译，能够从单一源模型生成多种异构目标产物。


<details>
  <summary>Details</summary>
Motivation: 现有建模语言通常设计目标单一，只针对仿真或实现，缺乏对多目标编译的支持。开发复杂软件驱动系统需要从高层系统模型派生出用于仿真、部署和形式验证的多种专门化模型。

Method: 设计了基于参与者模型的文本化、语法驱动的M语言，扩展了离散事件调度语义，提供系统实体建模、基于消息的交互以及时间或状态触发反应的构造。

Result: M能够从单一模型系统性地生成多样化的目标产物，同时保持与原始模型的语义一致性。M还可以作为中间语言，让其他建模语言受益于其编译框架。

Conclusion: M语言和工具链为复杂、并发和时间感知系统的模型驱动工程提供了有效的多目标编译支持，解决了现有建模语言目标单一的问题。

Abstract: Complex software-driven systems often interleave distributed, concurrent computation processes with physical interactions with the environment. Developing these systems more efficiently and safely can be achieved by employing actionable, software-based models. From a high-level system model, engineers often need to derive multiple specialized models for different purposes, including simulation, deployment, and formal verification. Each of these target models usually rely on its own formalism, specification language, and execution platform. Traditionally, a compiler analyzes a program written in a programming language and generates executable code. In contrast, a model compiler processes a source model written in a modeling language and should ideally support the generation of multiple heterogeneous targets. However, most existing modeling languages are designed with a narrow focus, typically targeting only simulation or implementation. Multi-target compilation, when not considered during the language's early design, becomes significantly harder to achieve. In this paper, we introduce our initiative: a toolchain and modeling language called M, designed to support system modeling and multi-target compilation for model-driven engineering of complex, concurrent, and time-aware systems. M is a textual, grammar-driven language based on the actor model and extended with discrete-event scheduling semantics. It provides constructs for modeling system entities, message-based interactions, and time- or state-triggered reactions. From such models, M enables the systematic generation of diverse target artifacts while preserving semantic conformance to the original model. Moreover, M can serve as a middle language to which other modeling languages may anchor, thereby allowing them to benefit from its compilation framework.

</details>


### [15] [A Viable Paradigm of Software Automation: Iterative End-to-End Automated Software Development](https://arxiv.org/abs/2511.15293)
*Jia Li,Zhi Jin,Kechi Zhang,Huangzhao Zhang,Jiaru Qian,Tiankuo Zhao*

Main category: cs.SE

TL;DR: 提出AutoSW愿景——一种迭代式端到端自动化软件开发范式，通过分析-规划-实施-交付循环，让AI系统作为人类合作伙伴将自然语言意图转化为可执行软件。


<details>
  <summary>Details</summary>
Motivation: 随着AI发展，现有方法要么将AI视为辅助工具仍需大量人工参与，要么是AI主导的"氛围编程"。这两种路径将收敛于AI参与整个软件开发生命周期，扩展全栈开发边界。

Method: 设计AutoSW范式，在分析-规划-实施-交付循环中运行，AI系统作为一等公民将自然语言意图转化为可执行软件。探索了轻量级原型并执行代表性案例。

Result: 结果表明AutoSW能够成功交付可执行软件，为真正的端到端自动化软件开发提供了可行方向。

Conclusion: AutoSW展示了AI系统作为人类合作伙伴参与整个软件开发生命周期的可行性，为实现完全自动化软件开发指明了方向。

Abstract: Software development automation is a long-term goal in software engineering. With the development of artificial intelligence (AI), more and more researchers are exploring approaches to software automation. They view AI systems as tools or assistants in software development, still requiring significant human involvement. Another initiative is ``vibe coding'', where AI systems write and repeatedly revise most (or even all) of the code. We foresee these two development paths will converge towards the same destination: AI systems participate in throughout the software development lifecycle, expanding boundaries of full-stack software development. In this paper, we present a vision of an iterative end-to-end automated software development paradigm AutoSW. It operates in an analyze-plan-implement-deliver loop, where AI systems as human partners become first-class actors, translating human intentions expressed in natural language into executable software. We explore a lightweight prototype across the paradigm and initially execute various representative cases. The results indicate that AutoSW can successfully deliver executable software, providing a feasible direction for truly end-to-end automated software development.

</details>


### [16] [From Machine Learning Documentation to Requirements: Bridging Processes with Requirements Languages](https://arxiv.org/abs/2511.15340)
*Yi Peng,Hans-Martin Heyn,Jennifer Horkoff*

Main category: cs.SE

TL;DR: 该研究探讨了如何从ML文档（如ModelCards和DataSheets）中提取需求工程相关信息，并评估三种RE表示方法将这些信息转化为结构化需求的有效性。


<details>
  <summary>Details</summary>
Motivation: 在机器学习系统的软件工程过程中，集成和验证ML组件面临挑战，需要规范ML组件需求。传统需求工程方法在这方面遇到新障碍，而ML文档作为需求相关信息来源尚未充分探索。

Method: 首先分析20个公开的ModelCards和DataSheets中RE相关信息的数量和性质；然后评估三种RE表示方法（EARS、Rupp模板和Volere）将这些知识转化为结构化需求的有效性。

Result: 研究表明ML文档包含大量潜在的RE相关信息，并且存在将ML特定知识转化为结构化需求的可行途径。

Conclusion: ML文档可以整合到ML系统的软件工程过程中，为将ML特定知识转化为结构化需求提供了可行路径。

Abstract: In software engineering processes for machine learning (ML)-enabled systems, integrating and verifying ML components is a major challenge. A prerequisite is the specification of ML component requirements, including models and data, an area where traditional requirements engineering (RE) processes face new obstacles. An underexplored source of RE-relevant information in this context is ML documentation such as ModelCards and DataSheets. However, it is uncertain to what extent RE-relevant information can be extracted from these documents. This study first investigates the amount and nature of RE-relevant information in 20 publicly available ModelCards and DataSheets. We show that these documents contain a significant amount of potentially RE-relevant information. Next, we evaluate how effectively three established RE representations (EARS, Rupp's template, and Volere) can structure this knowledge into requirements. Our results demonstrate that there is a pathway to transform ML-specific knowledge into structured requirements, incorporating ML documentation in software engineering processes for ML systems.

</details>


### [17] [EPSO: A Caching-Based Efficient Superoptimizer for BPF Bytecode](https://arxiv.org/abs/2511.15589)
*Qian Zhu,Yuxuan Liu,Ziyuan Zhu,Shangqing Liu,Lei Bu*

Main category: cs.SE

TL;DR: EPSO是一个基于缓存的eBPF超级优化器，通过离线超级优化发现重写规则并重用，在最小运行时开销下实现高质量优化。


<details>
  <summary>Details</summary>
Motivation: eBPF程序受限于内核验证器的严格安全约束和有限编译器优化支持，手工优化规则设计困难且效果有限，而传统超级优化计算成本高难以扩展。

Method: 提出EPSO方法，通过离线超级优化发现重写规则并缓存重用，结合规则发现和运行时重用，实现高效优化。

Result: EPSO发现795条重写规则，相比Clang输出程序大小减少最高68.87%（平均24.37%），在所有基准测试中优于K2，在92.68%测试中优于Merlin，平均运行时减少6.60%。

Conclusion: EPSO通过缓存重写规则有效解决了eBPF优化问题，显著提升了程序大小和运行时性能，为网络应用提供了更好的吞吐量和延迟表现。

Abstract: Extended Berkeley Packet Filter (eBPF) allows developers to extend Linux kernel functionality without modifying its source code. To ensure system safety, an in-kernel safety checker, the verifier, enforces strict safety constraints (for example, a limited program size) on eBPF programs loaded into the kernel. These constraints, combined with eBPF's performance-critical use cases, make effective optimization essential. However, existing compilers (such as Clang) offer limited optimization support, and many semantics-preserving transformations are rejected by the verifier, which makes handcrafted optimization rule design both challenging and limited in effectiveness. Superoptimization overcomes the limitations of rule-based methods by automatically discovering optimal transformations, but its high computational cost limits scalability. To address this, we propose EPSO, a caching-based superoptimizer that discovers rewrite rules via offline superoptimization and reuses them to achieve high-quality optimizations with minimal runtime overhead. We evaluate EPSO on benchmarks from the Linux kernel and several eBPF-based projects, including Cilium, Katran, hXDP, Sysdig, Tetragon, and Tracee. EPSO discovers 795 rewrite rules and achieves up to 68.87 percent (average 24.37 percent) reduction in program size compared to Clang's output, outperforming the state-of-the-art BPF optimizer K2 on all benchmarks and Merlin on 92.68 percent of them. Additionally, EPSO reduces program runtime by an average of 6.60 percent, improving throughput and lowering latency in network applications.

</details>


### [18] [Quantum-Guided Test Case Minimization for LLM-Based Code Generation](https://arxiv.org/abs/2511.15665)
*Huixiang Zhang,Mahzabeen Emu*

Main category: cs.SE

TL;DR: 本文提出了一个基于测试驱动开发(TDD)的框架，将代码规范转化为组合优化问题，使用量子退火器解决测试用例最小化问题，显著提升代码生成效率和代码质量。


<details>
  <summary>Details</summary>
Motivation: 精确控制大型语言模型生成高效简洁的代码是软件工程中的核心挑战，需要解决代码生成过程中的效率和质量问题。

Method: 框架首先提示LLM生成测试套件，然后将测试用例最小化问题建模为二次无约束二进制优化模型，兼容经典求解器和量子退火器等新兴硬件。

Result: 量子退火解决核心测试用例最小化任务比模拟退火快16倍，端到端框架减少总令牌消耗36.5%，并显著提高代码质量。

Conclusion: 这项工作展示了生成式AI与组合优化在软件工程中的强大协同作用，突显了精确模型制定的关键重要性。

Abstract: Precisely controlling Large Language Models (LLMs) to generate efficient and concise code is a central challenge in software engineering. We introduce a framework based on Test-Driven Development (TDD) that transforms code specification into a combinatorial optimization task. The framework first prompts an LLM to generate a test suite, then formulates the Test Case Minimization (TCM) problem as a Quadratic Unconstrained Binary Optimization (QUBO) model. This QUBO paradigm is compatible with both classical solvers and emerging hardware such as quantum annealers. Experimentally, quantum annealing solves the core TCM task 16 times faster than simulated annealing. This performance underpins our end-to-end framework, which reduces total token consumption by 36.5\% and significantly improves code quality. This work demonstrates a powerful synergy between generative AI and combinatorial optimization in software engineering, highlighting the critical importance of precise model formulation.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [19] [Compiling to recurrent neurons](https://arxiv.org/abs/2511.14953)
*Joey Velez-Ginorio,Nada Amin,Konrad Kording,Steve Zdancewic*

Main category: cs.PL

TL;DR: 该论文提出了一种将离散结构（特别是迭代）编译为可微分形式的方法，使迭代成为可微分编程中的一等公民，从而扩展了可微分算法的表达范围。


<details>
  <summary>Details</summary>
Motivation: 当前可微分编程中离散结构是二等公民，缺乏显式导数，限制了条件语句和迭代的使用范围，进而限制了可微分算法的直接表达能力。

Method: 设计了一个名为Cajal的最小类型化、高阶线性编程语言，包含迭代功能，并证明其程序可以正确编译为循环神经元。

Result: 通过实验验证，将循环神经元与神经网络结合解决迭代图像变换任务时，网络学习速度更快且数据效率更高。

Conclusion: 循环神经元实现了学习与普通编程离散结构之间的丰富交互，为可微分编程提供了新的可能性。

Abstract: Discrete structures are currently second-class in differentiable programming. Since functions over discrete structures lack overt derivatives, differentiable programs do not differentiate through them and limit where they can be used. For example, when programming a neural network, conditionals and iteration cannot be used everywhere; they can break the derivatives necessary for gradient-based learning to work. This limits the class of differentiable algorithms we can directly express, imposing restraints on how we build neural networks and differentiable programs more generally. However, these restraints are not fundamental. Recent work shows conditionals can be first-class, by compiling them into differentiable form as linear neurons. Similarly, this work shows iteration can be first-class -- by compiling to linear recurrent neurons. We present a minimal typed, higher-order and linear programming language with iteration called $\textsf{Cajal}\scriptstyle(\mathbb{\multimap}, \mathbb{2}, \mathbb{N})$. We prove its programs compile correctly to recurrent neurons, allowing discrete algorithms to be expressed in a differentiable form compatible with gradient-based learning. With our implementation, we conduct two experiments where we link these recurrent neurons against a neural network solving an iterative image transformation task. This determines part of its function prior to learning. As a result, the network learns faster and with greater data-efficiency relative to a neural network programmed without first-class iteration. A key lesson is that recurrent neurons enable a rich interplay between learning and the discrete structures of ordinary programming.

</details>


### [20] [Compiling Set Queries into Work-Efficient Tree Traversals](https://arxiv.org/abs/2511.15000)
*Alexander J Root,Christophe Gyurgyik,Purvi Goel,Kayvon Fatahalian,Jonathan Ragan-Kelley,Andrew Adams,Fredrik Kjolstad*

Main category: cs.PL

TL;DR: 本文提出了一种自动化的树结构查询优化方法，通过符号区间分析生成子树剪枝条件，支持几何谓词和复合查询融合，能够自动生成广义单索引和双索引树连接算法。


<details>
  <summary>Details</summary>
Motivation: 现有系统需要为每个查询谓词和数据结构手动实现剪枝逻辑，这限制了查询优化的通用性和效率。

Method: 使用符号区间分析生成子树剪枝条件，扩展处理几何谓词的新规则，并将复合查询融合为单次树遍历。

Result: 生成的遍历算法与专家手写代码性能相当，在手工优化不适用时能渐进优于线性扫描和嵌套循环连接。

Conclusion: 该方法实现了树结构查询优化的自动化和通用化，支持更广泛的连接谓词类型。

Abstract: Trees can accelerate queries that search or aggregate values over large collections. They achieve this by storing metadata that enables quick pruning (or inclusion) of subtrees when predicates on that metadata can prove that none (or all) of the data in a subtree affect the query result. Existing systems implement this pruning logic manually for each query predicate and data structure. We generalize and mechanize this class of optimization. Our method derives conditions for when subtrees can be pruned (or included wholesale), expressed in terms of the metadata available at each node. We efficiently generate these conditions using symbolic interval analysis, extended with new rules to handle geometric predicates (e.g., intersection, containment). Additionally, our compiler fuses compound queries (e.g., reductions on filters) into a single tree traversal. These techniques enable the automatic derivation of generalized single-index and dual-index tree joins that support a wide class of join predicates beyond standard equality and range predicates. The generated traversals match the behavior of expert-written code that implements query-specific traversals, and can asymptotically outperform the linear scans and nested-loop joins that existing systems fall back to when hand-written cases do not apply.

</details>


### [21] [Data Layout Polymorphism for Bounding Volume Hierarchies](https://arxiv.org/abs/2511.15028)
*Christophe Gyurgyik,Alexander J Root,Fredrik Kjolstad*

Main category: cs.PL

TL;DR: Scion是一个用于边界体积层次结构(BVH)的领域特定语言和编译器，能够将数据布局与遍历算法解耦，实现跨架构的性能优化。


<details>
  <summary>Details</summary>
Motivation: 传统系统中BVH的数据布局与遍历逻辑紧密耦合，阻碍了独立优化数据布局和算法，导致性能和可移植性之间的错误二分法。

Method: 开发Scion DSL和编译器，允许独立指定BVH数据布局而不依赖遍历算法，支持广泛的布局优化技术。

Result: 通过系统设计探索发现Pareto最优布局因算法、架构和工作负载而异，并识别出一种新型光线追踪布局，在多种架构和场景中达到Pareto最优。

Conclusion: Scion证明了将数据布局与算法解耦的可行性，能够实现跨不同上下文的高性能优化，解决了性能与可移植性之间的权衡问题。

Abstract: Bounding volume hierarchies are ubiquitous acceleration structures in graphics, scientific computing, and data analytics. Their performance depends critically on data layout choices that affect cache utilization, memory bandwidth, and vectorization -- increasingly dominant factors in modern computing. Yet, in most programming systems, these layout choices are hopelessly entangled with the traversal logic. This entanglement prevents developers from independently optimizing data layouts and algorithms across different contexts, perpetuating a false dichotomy between performance and portability. We introduce Scion, a domain-specific language and compiler for specifying the data layouts of bounding volume hierarchies independent of tree traversal algorithms. We show that Scion can express a broad spectrum of layout optimizations used in high performance computing while remaining architecture-agnostic. We demonstrate empirically that Pareto-optimal layouts (along performance and memory footprint axes) vary across algorithms, architectures, and workload characteristics. Through systematic design exploration, we also identify a novel ray tracing layout that combines optimization techniques from prior work, achieving Pareto-optimality across diverse architectures and scenes.

</details>


### [22] [Cement2: Temporal Hardware Transactions for High-Level and Efficient FPGA Programming](https://arxiv.org/abs/2511.15073)
*Youwei Xiao,Zizhang Luo,Weijie Peng,Yuyang Zou,Yun Liang*

Main category: cs.PL

TL;DR: 提出了时序硬件事务（temporal hardware transactions）抽象，将周期级时序意识引入事务级语言，支持描述跨多个时钟周期的规则，在Cement2中实现并验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 解决硬件设计中提高抽象层次与保持周期精度控制之间的根本矛盾，传统RTL设计的行为正确性保证较弱，而现有高级抽象要么引入不可预测开销，要么限制设计通用性。

Method: 在Cement2（基于Rust的事务性HDL）中实现时序硬件事务抽象，通过多阶段分析和优化降低描述抽象层次，生成高效硬件。

Result: 使用Cement2编程了RISC-V软核处理器、自定义CPU指令、线性代数内核和脉动阵列加速器，评估显示与手写RTL设计相比不牺牲性能和资源。

Conclusion: 时序硬件事务抽象为FPGA设计提供了高适用性，既能利用高级抽象提高生产力，又能保持硬件性能。

Abstract: Hardware design faces a fundamental challenge: raising abstraction to improve productivity while maintaining control over low-level details like cycle accuracy. Traditional RTL design in languages like SystemVerilog composes modules through wiring-style connections that provide weak guarantees for behavioral correctness. While high-level synthesis (HLS) and emerging abstractions attempt to address this, they either introduce unpredictable overhead or restrict design generality. Although transactional HDLs provide a promising foundation by lifting design abstraction to atomic and composable rules, they solely model intra-cycle behavior and do not reflect the native temporal design characteristics, hindering applicability and productivity for FPGA programming scenarios.
  We propose temporal hardware transactions, a new abstraction that brings cycle-level timing awareness to designers at the transactional language level. Our approach models temporal relationships between rules and supports the description of rules whose actions span multiple clock cycles, providing intuitive abstraction to describe multi-cycle architectural behavior. We implement this in Cement2, a transactional HDL embedded in Rust, enabling programming hardware constructors to build both intra-cycle and temporal transactions. Cement2's synthesis framework lowers description abstraction through multiple analysis and optimization phases, generating efficient hardware. With Cement2's abstraction, we program a RISC-V soft-core processor, custom CPU instructions, linear algebra kernels, and systolic array accelerators, leveraging the high-level abstraction for boosted productivity. Evaluation shows that Cement2 does not sacrifice performance and resources compared to hand-coded RTL designs, demonstrating the high applicability for general FPGA design tasks.

</details>


### [23] [SkyEgg: Joint Implementation Selection and Scheduling for Hardware Synthesis using E-graphs](https://arxiv.org/abs/2511.15323)
*Youwei Xiao,Yuyang Zou,Yun Liang*

Main category: cs.PL

TL;DR: SkyEgg是一个新颖的硬件合成框架，通过联合优化实现选择与调度，使用e-graph数据结构统一表示代数变换和硬件实现选择，相比传统HLS工具获得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统硬件合成方法将实现选择与调度分离优化，导致无法充分利用FPGA异构架构优势，存在次优设计问题。

Method: 使用e-graph数据结构统一表示代数变换和硬件实现选择，通过等式饱和应用重写规则，最后将联合优化建模为混合整数线性规划问题。

Result: 在Xilinx Kintex UltraScale+ FPGA上评估，相比Vitis HLS平均加速3.01倍，复杂表达式最高加速5.22倍。

Conclusion: 联合优化实现选择与调度的方法能够显著提升硬件合成质量，充分利用FPGA异构架构潜力。

Abstract: Hardware synthesis from high-level descriptions remains fundamentally limited by the sequential optimization of interdependent design decisions. Current methodologies, including state-of-the-art high-level synthesis (HLS) tools, artificially separate implementation selection from scheduling, leading to suboptimal designs that cannot fully exploit modern FPGA heterogeneous architectures. Implementation selection is typically performed by ad-hoc pattern matching on operations, a process that does not consider the impact on scheduling. Subsequently, scheduling algorithms operate on fixed selection solutions with inaccurate delay estimates, which misses critical optimization opportunities from appropriately configured FPGA blocks like DSP slices.
  We present SkyEgg, a novel hardware synthesis framework that jointly optimizes implementation selection and scheduling using the e-graph data structure. Our key insight is that both algebraic transformations and hardware implementation choices can be uniformly represented as rewrite rules within an e-graph, modeling the complete design space of implementation candidates to be selected and scheduled together. First, SkyEgg constructs an e-graph from the input program. It then applies both algebraic and implementation rewrites through equality saturation. Finally, it formulates the joint optimization as a mixed-integer linear programming (MILP) problem on the saturated e-graph. We provide both exact MILP solving and an efficient ASAP heuristic for scalable synthesis. Our evaluation on benchmarks from diverse applications targeting Xilinx Kintex UltraScale+ FPGAs demonstrates that SkyEgg achieves an average speedup of 3.01x over Vitis HLS, with improvements up to 5.22x for complex expressions.

</details>


### [24] [Graph Rewriting Language as a Platform for Quantum Diagrammatic Calculi](https://arxiv.org/abs/2511.15581)
*Kayo Tei,Haruto Mishina,Naoki Yamamoto,Kazunori Ueda*

Main category: cs.PL

TL;DR: 使用LMNtal语言构建量子电路简化的图形变换和验证平台，通过图重写规则直接操作ZX图，提供交互式可视化和状态空间探索功能


<details>
  <summary>Details</summary>
Motivation: 现有量子电路优化工具如PyZX和Quantomatic在特定领域提供支持，但缺乏通用的图形变换和验证平台。LMNtal作为一种通用层次图重写语言，可以从不同视角研究量子电路变换

Method: 采用LMNtal语言实现ZX图的图形变换规则，利用QLMNtal扩展进行量化模式匹配，通过状态空间探索实现交互式可视化和验证

Result: 建立了能够直接实现基本变换规则、简化规则规范、支持优化路径可视化验证的平台，通过案例研究展示了理解优化路径和设计新算法的能力

Conclusion: LMNtal及其工具链可作为研究量子电路变换的新平台，从不同视角探索优化路径和算法设计

Abstract: Systematic discovery of optimization paths in quantum circuit simplification remains a challenge. Today, ZX-calculus, a computing model for quantum circuit transformation, is attracting attention for its highly abstract graph-based approach. Whereas existing tools such as PyZX and Quantomatic offer domain-specific support for quantum circuit optimization, visualization and theorem-proving, we present a complementary approach using LMNtal, a general-purpose hierarchical graph rewriting language, to establish a diagrammatic transformation and verification platform with model checking. Our methodology shows three advantages: (1) manipulation of ZX-diagrams through native graph transformation rules, enabling direct implementation of basic rules; (2) quantified pattern matching via QLMNtal extensions, greatly simplifying rule specification; and (3) interactive visualization and validation of optimization paths through state space exploration. Through case studies, we demonstrate how our framework helps understand optimization paths and design new algorithms and strategies. This suggests that the declarative language LMNtal and its toolchain could serve as a new platform to investigate quantum circuit transformation from a different perspective.

</details>
