<div id=toc></div>

# Table of Contents

- [cs.LO](#cs.LO) [Total: 2]
- [cs.SE](#cs.SE) [Total: 14]
- [cs.PL](#cs.PL) [Total: 3]


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [1] [L-Mosaics and Bounded Join-Semilattices in Isabelle/HOL](https://arxiv.org/abs/2509.19854)
*Alessandro Linzi*

Main category: cs.LO

TL;DR: 在Isabelle/HOL中形式化验证了L-mosaic与有界并半格之间的等价关系，采用AI辅助方法将大型语言模型整合到证明开发过程中


<details>
  <summary>Details</summary>
Motivation: 验证Cangiotti等人关于超组合结构与正交模格和量子逻辑相关的研究结果，展示AI增强的交互式定理证明在处理复杂形式化项目中的潜力

Method: 使用Isabelle/HOL进行完整形式化，采用AI辅助方法将大型语言模型作为推理助手整合到证明开发过程中，验证等价关系及其互逆性质

Result: 成功形式化验证了L-mosaic与有界并半格之间的等价关系，证明了变换的互逆性质

Conclusion: 该工作既展示了多值代数运算的数学深度，也证明了AI增强的交互式定理证明在处理复杂形式化项目中的潜力

Abstract: We present a complete formalization in Isabelle/HOL of the object part of an
equivalence between L-mosaics and bounded join-semilattices, employing an
AI-assisted methodology that integrates large language models as reasoning
assistants throughout the proof development process. The equivalence was
originally established by Cangiotti, Linzi, and Talotti in their study of
hypercompositional structures related to orthomodular lattices and quantum
logic. Our formalization rigorously verifies the main theoretical result and
demonstrates the mutual inverse property of the transformations establishing
this equivalence. The development showcases both the mathematical depth of
multivalued algebraic operations and the potential for AI-enhanced interactive
theorem proving in tackling complex formalization projects.

</details>


### [2] [From Zonotopes to Proof Certificates: A Formal Pipeline for Safe Control Envelopes](https://arxiv.org/abs/2509.20301)
*Jonathan Hellwig,Lukas Schäfer,Long Qian,André Platzer,Matthias Althoff*

Main category: cs.LO

TL;DR: 提出了一种结合高性能可达性分析和形式化验证的控制包络验证方法，解决传统方法在安全关键系统中缺乏端到端正确性保证的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于zonotope的可达性方法虽然具有很好的可扩展性，但缺乏形式化验证，无法为安全关键系统提供足够的可信度保证。

Method: 开发了一个验证流程：首先使用高性能可达性算法计算控制包络，然后通过可证明正确的逻辑原则对每个中间结果进行认证，将计算密集的zonotope包含任务卸载给高效数值后端。

Result: 通过代表性案例研究展示了该方法的实际效用，能够有效结合可扩展性和形式化严谨性。

Conclusion: 该方法为控制包络验证提供了一种既具有可扩展性又具备形式化严谨性的解决方案，填补了现有方法的保证空白。

Abstract: Synthesizing controllers that enforce both safety and actuator constraints is
a central challenge in the design of cyber-physical systems. State-of-the-art
reachability methods based on zonotopes deliver impressive scalability, yet no
zonotope reachability tool has been formally verified and the lack of
end-to-end correctness undermines the confidence in their use for
safety-critical systems. Although deductive verification with the hybrid system
prover KeYmaera X could, in principle, resolve this assurance gap, the
high-dimensional set representations required for realistic control envelopes
overwhelm its reasoning based on quantifier elimination. To address this gap,
we formalize how control-invariant sets serve as sound safety certificates.
Building on that foundation, we develop a verification pipeline for control
envelopes that unites scalability and formal rigor. First, we compute control
envelopes with high-performance reachability algorithms. Second, we certify
every intermediate result using provably correct logical principles. To
accelerate this certification, we offload computationally intensive zonotope
containment tasks to efficient numerical backends, which return compact
witnesses that KeYmaera X validates rapidly. We show the practical utility of
our approach through representative case studies.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [Automated Insertion of Flushes and Fences for Persistency](https://arxiv.org/abs/2509.19459)
*Yutong Guo,Weiyu Luo,Brian Demsky*

Main category: cs.SE

TL;DR: PMRobust是一个编译器，能自动插入flush和fence操作，确保持久内存代码不会出现缺失flush和fence的bug，相比手动放置操作的基准测试仅有0.26%的平均开销。


<details>
  <summary>Details</summary>
Motivation: 持久内存和CXL共享内存需要在崩溃后保持数据，但正确使用flush和fence操作很困难。现有工具需要bug暴露的测试用例，且无法确保没有缺失flush的bug。

Method: PMRobust采用新颖的静态分析方法和针对新分配对象的优化，自动插入必要的flush和fence操作。

Result: 在持久内存库和多个持久内存数据结构上评估，相对于手动放置flush和fence操作的基准测试，几何平均开销仅为0.26%。

Conclusion: PMRobust能有效自动确保持久内存代码的正确性，且性能开销极小。

Abstract: CXL shared memory and persistent memory allow the contents of memory to
persist beyond crashes. Stores to persistent or CXL memory are typically not
immediately made persistent; developers must manually flush the corresponding
cache lines to force the data to be written to the underlying storage.
Correctly using flush and fence operations is known to be challenging. While
state-of-the-art tools can find missing flush instructions, they often require
bug-revealing test cases. No existing tools can ensure the absence of missing
flush bugs.
  In this paper, we present PMRobust, a compiler that automatically inserts
flush and fence operations to ensure that code using persistent memory is free
from missing flush and fence bugs. PMRobust employs a novel static analysis
with optimizations that target newly allocated objects. We have evaluated
PMRobust on persistent memory libraries and several persistent memory data
structures and measured a geometric mean overhead of 0.26% relative to the
original benchmarks with hand-placed flush and fence operations.

</details>


### [4] [Semantic-Aware Fuzzing: An Empirical Framework for LLM-Guided, Reasoning-Driven Input Mutation](https://arxiv.org/abs/2509.19533)
*Mengdi Lu,Steven Ding,Furkan Alaca,Philippe Charland*

Main category: cs.SE

TL;DR: 本文提出了一种将推理型大语言模型与AFL++模糊测试工具集成的开源微服务框架，旨在解决传统模糊测试工具缺乏语义推理能力的问题，并通过实验评估了不同LLM在模糊测试中的表现。


<details>
  <summary>Details</summary>
Motivation: 传统基于突变的模糊测试工具虽然能有效探索代码路径，但缺乏语义推理能力，无法处理复杂的协议逻辑和字段间依赖关系。而具备推理能力的大语言模型可以利用预训练知识理解输入格式和约束条件，但缺乏监督微调的真实数据，因此需要探索基于提示的少样本学习方法。

Method: 开发了一个开源微服务框架，将推理型LLM与AFL++集成在Google的FuzzBench平台上，解决了LLM和模糊测试器在异步执行和硬件需求（GPU vs CPU）方面的差异。评估了四个研究问题，包括LLM集成方法、少样本提示效果、提示工程改进以及不同开源LLM的性能比较。

Result: 实验使用Llama3.3、Deepseek-r1-Distill-Llama-70B、QwQ-32B和Gemma3等模型，发现Deepseek表现最佳。突变效果更多地取决于提示复杂度和模型选择，而非样本数量。响应延迟和吞吐量瓶颈仍是主要挑战。

Conclusion: 推理型LLM可以显著提升模糊测试的语义理解能力，但实际应用仍面临性能瓶颈。Deepseek模型在提示工程条件下表现最优，为未来工作提供了方向。

Abstract: Security vulnerabilities in Internet-of-Things devices, mobile platforms, and
autonomous systems remain critical. Traditional mutation-based fuzzers -- while
effectively explore code paths -- primarily perform byte- or bit-level edits
without semantic reasoning. Coverage-guided tools such as AFL++ use
dictionaries, grammars, and splicing heuristics to impose shallow structural
constraints, leaving deeper protocol logic, inter-field dependencies, and
domain-specific semantics unaddressed. Conversely, reasoning-capable large
language models (LLMs) can leverage pretraining knowledge to understand input
formats, respect complex constraints, and propose targeted mutations, much like
an experienced reverse engineer or testing expert. However, lacking ground
truth for "correct" mutation reasoning makes supervised fine-tuning
impractical, motivating explorations of off-the-shelf LLMs via prompt-based
few-shot learning. To bridge this gap, we present an open-source microservices
framework that integrates reasoning LLMs with AFL++ on Google's FuzzBench,
tackling asynchronous execution and divergent hardware demands (GPU- vs.
CPU-intensive) of LLMs and fuzzers. We evaluate four research questions: (R1)
How can reasoning LLMs be integrated into the fuzzing mutation loop? (R2) Do
few-shot prompts yield higher-quality mutations than zero-shot? (R3) Can prompt
engineering with off-the-shelf models improve fuzzing directly? and (R4) Which
open-source reasoning LLMs perform best under prompt-only conditions?
Experiments with Llama3.3, Deepseek-r1-Distill-Llama-70B, QwQ-32B, and Gemma3
highlight Deepseek as the most promising. Mutation effectiveness depends more
on prompt complexity and model choice than shot count. Response latency and
throughput bottlenecks remain key obstacles, offering directions for future
work.

</details>


### [5] [Reverse Engineering User Stories from Code using Large Language Models](https://arxiv.org/abs/2509.19587)
*Mohamed Ouf,Haoyu Li,Michael Zhang,Mariam Guizani*

Main category: cs.SE

TL;DR: 本文研究大型语言模型能否从源代码自动恢复用户故事，以及提示设计对输出质量的影响。使用1,750个带注释的C++代码片段评估五种最先进的LLM和六种提示策略。


<details>
  <summary>Details</summary>
Motivation: 用户故事在敏捷开发中至关重要，但在遗留系统和文档不完善的系统中经常缺失或过时。

Method: 使用1,750个带注释的C++代码片段，评估五种LLM在六种提示策略下的表现，包括单示例提示和链式推理等方法。

Result: 所有模型在不超过200行自然语言代码的情况下平均F1分数达到0.8。最小模型（8B）通过单个示例提示即可达到70B大模型的性能，而链式推理仅对较大模型有边际改善。

Conclusion: LLM能够有效从源代码恢复用户故事，简单的单示例提示策略足以让小型模型达到大型模型的性能，提示设计对结果质量有重要影响。

Abstract: User stories are essential in agile development, yet often missing or
outdated in legacy and poorly documented systems. We investigate whether large
language models (LLMs) can automatically recover user stories directly from
source code and how prompt design impacts output quality. Using 1,750 annotated
C++ snippets of varying complexity, we evaluate five state-of-the-art LLMs
across six prompting strategies. Results show that all models achieve, on
average, an F1 score of 0.8 for code up to 200 NLOC. Our findings show that a
single illustrative example enables the smallest model (8B) to match the
performance of a much larger 70B model. In contrast, structured reasoning via
Chain-of-Thought offers only marginal gains, primarily for larger models.

</details>


### [6] [Assertion Messages with Large Language Models (LLMs) for Code](https://arxiv.org/abs/2509.19673)
*Ahmed Aljohani,Anamul Haque Mollah,Hyunsook Do*

Main category: cs.SE

TL;DR: 本文评估了四种最先进的填充中间(FIM)大语言模型在生成Java测试断言消息方面的能力，发现Codestral-22B表现最佳，得分为2.76/5，接近人工编写的3.24分。研究表明包含描述性测试注释可进一步提升性能至2.97分。


<details>
  <summary>Details</summary>
Motivation: 断言消息能显著提升单元测试的可理解性，但开发者和自动化测试工具经常忽略编写。尽管LLMs有潜力，但尚未系统评估其在生成信息性断言消息方面的能力。

Method: 使用包含216个Java测试方法的数据集，评估四种FIM LLMs(Qwen2.5-Coder-32B、Codestral-22B、CodeLlama-13B、StarCoder)，采用类人评估方法，并进行消融研究分析测试注释的影响。

Result: Codestral-22B获得最高质量得分2.76/5，包含测试注释后提升至2.97分。所有模型都能复制开发者偏好的语言模式。传统文本评估指标在捕捉多样化断言消息结构方面存在局限性。

Conclusion: 研究为推进自动化、上下文感知的测试代码断言消息生成提供了重要基础，强调了上下文在生成清晰断言消息中的关键作用。

Abstract: Assertion messages significantly enhance unit tests by clearly explaining the
reasons behind test failures, yet they are frequently omitted by developers and
automated test-generation tools. Despite recent advancements, Large Language
Models (LLMs) have not been systematically evaluated for their ability to
generate informative assertion messages. In this paper, we introduce an
evaluation of four state-of-the-art Fill-in-the-Middle (FIM) LLMs -
Qwen2.5-Coder-32B, Codestral-22B, CodeLlama-13B, and StarCoder - on a dataset
of 216 Java test methods containing developer-written assertion messages. We
find that Codestral-22B achieves the highest quality score of 2.76 out of 5
using a human-like evaluation approach, compared to 3.24 for manually written
messages. Our ablation study shows that including descriptive test comments
further improves Codestral's performance to 2.97, highlighting the critical
role of context in generating clear assertion messages. Structural analysis
demonstrates that all models frequently replicate developers' preferred
linguistic patterns. We discuss the limitations of the selected models and
conventional text evaluation metrics in capturing diverse assertion message
structures. Our benchmark, evaluation results, and discussions provide an
essential foundation for advancing automated, context-aware generation of
assertion messages in test code. A replication package is available at
https://doi.org/10.5281/zenodo.15293133

</details>


### [7] [Intuition to Evidence: Measuring AI's True Impact on Developer Productivity](https://arxiv.org/abs/2509.19708)
*Anand Kumar,Vishal Khare,Deepak Sharma,Satyam Kumar,Vijay Saini,Anshul Yadav,Sachendra Jain,Ankit Rana,Pratham Verma,Vaibhav Meena,Avinash Edubilli*

Main category: cs.SE

TL;DR: 本文对在企业规模部署的AI辅助软件开发工具进行了为期一年的真实世界评估，展示了AI平台DeputyDev在代码生成和自动审查方面的显著生产力提升效果。


<details>
  <summary>Details</summary>
Motivation: 旨在通过实际生产环境中的纵向分析，提供AI工具在企业软件开发工作流程中整合的实证证据，揭示其变革潜力和实际部署挑战。

Method: 在一年时间内，让300名工程师跨多个团队集成内部AI平台DeputyDev到日常工作流程中，通过严格的队列分析进行评估。

Result: 统计显著的生产力改进：PR审查周期时间减少31.8%；开发者满意度85%，93%希望继续使用；代码推送量增加61%，约30-40%的生产代码通过该工具交付，整体代码交付量增加28%。

Conclusion: 研究表明AI辅助工具在企业软件开发中具有显著的变革潜力，能够有效提升生产力和代码交付效率，但同时也面临实际部署的挑战。

Abstract: We present a comprehensive real-world evaluation of AI-assisted software
development tools deployed at enterprise scale. Over one year, 300 engineers
across multiple teams integrated an in-house AI platform (DeputyDev) that
combines code generation and automated review capabilities into their daily
workflows. Through rigorous cohort analysis, our study demonstrates
statistically significant productivity improvements, including an overall 31.8%
reduction in PR review cycle time.
  Developer adoption was strong, with 85% satisfaction for code review features
and 93% expressing a desire to continue using the platform. Adoption patterns
showed systematic scaling from 4% engagement in month 1 to 83% peak usage by
month 6, stabilizing at 60% active engagement. Top adopters achieved a 61%
increase in code volume pushed to production, contributing to approximately 30
to 40% of code shipped to production through this tool, accounting for an
overall 28% increase in code shipment volume.
  Unlike controlled benchmark evaluations, our longitudinal analysis provides
empirical evidence from production environments, revealing both the
transformative potential and practical deployment challenges of integrating AI
into enterprise software development workflows.

</details>


### [8] [Beyond Language Barriers: Multi-Agent Coordination for Multi-Language Code Generation](https://arxiv.org/abs/2509.19918)
*Micheline Bénédicte Moumoula,Serge Lionel Nikiema,Albérick Euraste Djire,Abdoul Kader Kabore,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.SE

TL;DR: XL-CoGen是一个协调多智能体架构，通过数据驱动的桥接语言选择机制，显著提升多编程语言代码生成质量，在异构技术栈环境中实现跨语言知识共享和模式利用。


<details>
  <summary>Details</summary>
Motivation: 当前软件系统构建在异构技术栈上，需要高质量的多语言代码生成能力。现有LLMs在不同编程语言间的能力差异显著，特别是对于训练数据有限的Rust、Perl、OCaml和Erlang等语言，现有解决方案往往孤立处理每种目标语言，错过了知识共享和跨语言模式利用的机会。

Method: XL-CoGen采用协调多智能体架构，集成中间表示、代码生成、翻译和自动修复。其核心创新是数据驱动的桥接语言选择机制：通过经验推导的转移矩阵识别最佳中间语言（基于实际翻译成功率而非原始生成准确率），系统执行早期输出验证、迭代错误修正，并重用中间产物作为后续翻译的上下文支架。

Result: 广泛实验表明，XL-CoGen带来了显著改进，比最强的微调基线提高了13个百分点，比现有的单语言多智能体方法提高了多达30个百分点。消融研究进一步证明，兼容性引导的桥接显著优于基于LLM的启发式方法。

Conclusion: 该研究证实了累积跨语言知识转移的价值，XL-CoGen通过协调多智能体架构和数据驱动的桥接语言选择，有效解决了多编程语言代码生成中的挑战，为异构技术栈开发提供了有力支持。

Abstract: Producing high-quality code across multiple programming languages is
increasingly important as today's software systems are built on heterogeneous
stacks. Large language models (LLMs) have advanced the state of automated
programming, yet their proficiency varies sharply between languages, especially
those with limited training data such as Rust, Perl, OCaml, and Erlang. Many
current solutions including language-specific fine-tuning, multi-agent
orchestration, transfer learning, and intermediate-representation pipelines
still approach each target language in isolation, missing opportunities to
share knowledge or exploit recurring cross-language patterns.
  XL-CoGen tackles this challenge with a coordinated multi-agent architecture
that integrates intermediate representation, code generation, translation, and
automated repair. Its distinguishing feature is a data-driven mechanism for
selecting bridging languages: empirically derived transfer matrices identify
the best intermediate languages based on demonstrated translation success
rather than raw generation accuracy. The system performs early output
validation, iteratively corrects errors, and reuses intermediate artifacts as
contextual scaffolds for subsequent translations.
  Extensive experiments show that XL-CoGen yields notable improvements with 13
percentage-point gains over the strongest fine-tuned baseline and as much as 30
percentage points over existing single-language multi-agent methods. Ablation
studies further demonstrate that compatibility-guided bridging significantly
outperforms LLM-based heuristics, confirming the value of cumulative
cross-language knowledge transfer.

</details>


### [9] [Demystifying the Evolution of Neural Networks with BOM Analysis: Insights from a Large-Scale Study of 55,997 GitHub Repositories](https://arxiv.org/abs/2509.20010)
*Xiaoning Ren,Yuhang Ye,Xiongfei Wu,Yueming Wu,Yinxing Xue*

Main category: cs.SE

TL;DR: 本文提出了神经网络物料清单（NNBOM）的概念，用于分析神经网络软件的演化趋势，并基于55,997个PyTorch GitHub仓库构建了大规模数据集，进行了实证研究。


<details>
  <summary>Details</summary>
Motivation: 传统软件物料清单（SBOM）不适用于神经网络软件，而概念性AI物料清单（AIBOM）缺乏大规模演化分析的实际实现。需要专门的方法来分析神经网络软件的组件结构和重用模式。

Method: 引入NNBOM数据集构造方法，从55,997个精选的PyTorch GitHub仓库中收集第三方库（TPLs）、预训练模型（PTMs）和模块信息，进行软件规模、组件重用和跨领域依赖的全面实证研究。

Result: 构建了大规模NNBOM数据库，开发了两个原型应用：多仓库演化分析器和单仓库组件评估推荐器，展示了分析的实际价值。

Conclusion: NNBOM为神经网络软件的演化分析提供了有效工具，帮助开发者和维护者理解长期趋势，指导开发决策。

Abstract: Neural networks have become integral to many fields due to their exceptional
performance. The open-source community has witnessed a rapid influx of neural
network (NN) repositories with fast-paced iterations, making it crucial for
practitioners to analyze their evolution to guide development and stay ahead of
trends. While extensive research has explored traditional software evolution
using Software Bill of Materials (SBOMs), these are ill-suited for NN software,
which relies on pre-defined modules and pre-trained models (PTMs) with distinct
component structures and reuse patterns. Conceptual AI Bills of Materials
(AIBOMs) also lack practical implementations for large-scale evolutionary
analysis. To fill this gap, we introduce the Neural Network Bill of Material
(NNBOM), a comprehensive dataset construct tailored for NN software. We create
a large-scale NNBOM database from 55,997 curated PyTorch GitHub repositories,
cataloging their TPLs, PTMs, and modules. Leveraging this database, we conduct
a comprehensive empirical study of neural network software evolution across
software scale, component reuse, and inter-domain dependency, providing
maintainers and developers with a holistic view of its long-term trends.
Building on these findings, we develop two prototype applications,
\textit{Multi repository Evolution Analyzer} and \textit{Single repository
Component Assessor and Recommender}, to demonstrate the practical value of our
analysis.

</details>


### [10] [V-GameGym: Visual Game Generation for Code Large Language Models](https://arxiv.org/abs/2509.20136)
*Wei Zhang,Jack Yang,Renshuai Tao,Lingzheng Chai,Shawn Guo,Jiajun Wu,Xiaoming Chen,Ganqu Cui,Ning Ding,Xander Xu,Hu Wei,Bowen Zhou*

Main category: cs.SE

TL;DR: V-GameGym是一个针对视觉游戏开发的多模态代码生成基准测试，包含2,219个高质量样本，旨在填补当前代码大语言模型在算法问题解决与实际游戏开发需求之间的差距。


<details>
  <summary>Details</summary>
Motivation: 当前代码大语言模型基准测试主要关注语法正确性和执行准确性，忽视了游戏开发中关键的视觉美学、可玩性和用户参与度等指标，无法满足实际游戏开发需求。

Method: 采用基于聚类的筛选方法从真实世界仓库中提取100个主题簇的样本，构建多模态评估框架，使用完整的UI沙盒环境进行自动化LLM驱动的视觉代码合成。

Result: V-GameGym有效连接了代码生成准确性与实际游戏开发工作流程，为视觉编程和交互元素生成提供了可量化的质量指标。

Conclusion: 该基准测试为评估代码大语言模型在视觉游戏开发领域的综合能力提供了重要工具，推动了从单纯算法问题解决到实际应用场景的转变。

Abstract: Code large language models have demonstrated remarkable capabilities in
programming tasks, yet current benchmarks primarily focus on single modality
rather than visual game development. Most existing code-related benchmarks
evaluate syntax correctness and execution accuracy, overlooking critical
game-specific metrics such as playability, visual aesthetics, and user
engagement that are essential for real-world deployment. To address the gap
between current LLM capabilities in algorithmic problem-solving and competitive
programming versus the comprehensive requirements of practical game
development, we present V-GameGym, a comprehensive benchmark comprising 2,219
high-quality samples across 100 thematic clusters derived from real-world
repositories, adopting a novel clustering-based curation methodology to ensure
both diversity and structural completeness. Further, we introduce a multimodal
evaluation framework with an automated LLM-driven pipeline for visual code
synthesis using complete UI sandbox environments. Our extensive analysis
reveals that V-GameGym effectively bridges the gap between code generation
accuracy and practical game development workflows, providing quantifiable
quality metrics for visual programming and interactive element generation.

</details>


### [11] [Enhancing Requirement Traceability through Data Augmentation Using Large Language Models](https://arxiv.org/abs/2509.20149)
*Jianzhang Zhang,Jialong Zhou,Nan Niu,Chuang Liu*

Main category: cs.SE

TL;DR: 利用大型语言模型进行数据增强，解决需求追踪中的数据稀缺问题，通过提示技术生成需求到代码的追踪链接，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动化需求追踪方法受限于训练数据稀缺和需求与代码之间语义鸿沟的挑战，需要解决数据不足的问题。

Method: 提出基于提示技术的方法，使用四种LLM（Gemini 1.5 Pro、Claude 3、GPT-3.5、GPT-4）和零样本/少样本模板生成增强的追踪链接，并优化追踪模型的编码器组件。

Result: 实验结果显示该方法显著提升模型性能，F1分数最高提升28.59%。

Conclusion: 该方法有效解决了需求追踪中的数据稀缺问题，具有实际应用潜力。

Abstract: Requirements traceability is crucial in software engineering to ensure
consistency between requirements and code. However, existing automated
traceability methods are constrained by the scarcity of training data and
challenges in bridging the semantic gap between artifacts. This study aims to
address the data scarcity problem in requirements traceability by employing
large language models (LLMs) for data augmentation. We propose a novel approach
that utilizes prompt-based techniques with LLMs to generate augmented
requirement-to-code trace links, thereby enhancing the training dataset. Four
LLMs (Gemini 1.5 Pro, Claude 3, GPT-3.5, and GPT-4) were used, employing both
zero-shot and few-shot templates. Moreover, we optimized the encoder component
of the tracing model to improve its efficiency and adaptability to augmented
data. The key contributions of this paper are: (1) proposing and evaluating
four prompt templates for data augmentation; (2) providing a comparative
analysis of four LLMs for generating trace links; (3) enhancing the model's
encoder for improved adaptability to augmented datasets. Experimental results
show that our approach significantly enhances model performance, achieving an
F1 score improvement of up to 28.59%, thus demonstrating its effectiveness and
potential for practical application.

</details>


### [12] [Benchmarking Web API Integration Code Generation](https://arxiv.org/abs/2509.20172)
*Daniel Maninger,Leon Chemnitz,Amir Molzam Sharifloo,Jannis Brugger,Mira Mezini*

Main category: cs.SE

TL;DR: 评估大语言模型生成Web API集成代码能力的研究，发现现有开源模型在生成API调用代码时存在显著挑战，成功率低于40%


<details>
  <summary>Details</summary>
Motivation: API集成是数字基础设施的关键，但编写正确的API调用代码具有挑战性。尽管大语言模型在软件开发中日益流行，但其在自动化生成Web API集成代码方面的有效性尚未得到充分探索

Method: 提出了一个数据集和评估流程，用于评估大语言模型生成Web API调用代码的能力，并对多个开源大语言模型进行了实验

Result: 实验显示生成API调用代码存在重大挑战，包括虚构端点、参数使用错误等问题。所有评估的开源模型都无法解决超过40%的任务

Conclusion: 当前开源大语言模型在生成Web API集成代码方面能力有限，需要进一步改进

Abstract: API integration is a cornerstone of our digital infrastructure, enabling
software systems to connect and interact. However, as shown by many studies,
writing or generating correct code to invoke APIs, particularly web APIs, is
challenging. Although large language models~(LLMs) have become popular in
software development, their effectiveness in automating the generation of web
API integration code remains unexplored. In order to address this, we present a
dataset and evaluation pipeline designed to assess the ability of LLMs to
generate web API invocation code. Our experiments with several open-source LLMs
reveal that generating API invocations poses a significant challenge, resulting
in hallucinated endpoints, incorrect argument usage, and other errors. None of
the evaluated open-source models were able to solve more than 40% of the tasks.

</details>


### [13] [The Cream Rises to the Top: Efficient Reranking Method for Verilog Code Generation](https://arxiv.org/abs/2509.20215)
*Guang Yang,Wei Zheng,Xiang Chen,Yifan Sun,Fengji Zhang,Terry Yue Zhuo*

Main category: cs.SE

TL;DR: VCD-RNK是一个针对Verilog代码重排的判别模型，通过模拟专家在代码语义分析、测试用例生成和功能正确性评估三个维度的推理过程，解决LLMs在Verilog生成中因领域知识有限而面临的挑战。


<details>
  <summary>Details</summary>
Motivation: LLMs在Verilog代码生成方面面临领域特定知识有限的挑战，虽然采样技术提高了pass@k指标，但硬件工程师需要一个可信赖的解决方案而非不确定的候选方案。

Method: 将问题定义为需求与Verilog实现之间的语义对齐问题，提出VCD-RNK判别模型，通过三个维度（代码语义分析、测试用例生成、功能正确性评估）来蒸馏专家知识，在推理时显式模拟这些推理过程。

Result: VCD-RNK有效避免了现有方法中计算密集的测试执行过程。

Conclusion: VCD-RNK为Verilog代码生成提供了一个高效的重排解决方案，通过模拟专家推理过程提升了代码生成的可信度。

Abstract: LLMs face significant challenges in Verilog generation due to limited
domain-specific knowledge. While sampling techniques improve pass@k metrics,
hardware engineers need one trustworthy solution rather than uncertain
candidates. To bridge this gap, we formulate it as a semantic alignment problem
between requirements and Verilog implementations, and propose VCD-RNK, a
discriminator model tailored for efficient Verilog code reranking.
Specifically, VCD-RNKincorporates Verilog-specific reasoning by distilling
expert knowledge across three dimensions: code semantic analysis, test case
generation, and functional correctness assessment. By explicitly simulating the
above reasoning processes during inference, VCD-RNK effectively avoids
computationally intensive test execution in existing methods.

</details>


### [14] [Confidentiality-Preserving Verifiable Business Processes through Zero-Knowledge Proofs](https://arxiv.org/abs/2509.20300)
*Jannis Kiesel,Jonathan Heiss*

Main category: cs.SE

TL;DR: 本文提出了一种基于零知识证明（ZKP）的方法，用于在保护机密性的同时实现业务流程的可验证执行。通过将ZK虚拟机集成到业务流程管理引擎中，支持链式可验证计算，并以产品碳足迹为例展示了该方法的应用。


<details>
  <summary>Details</summary>
Motivation: 在跨组织业务流程中，确保业务流程完整性而不泄露商业机密信息是一个重大挑战。需要一种能够验证流程执行正确性同时保护敏感数据的方法。

Method: 将ZK虚拟机（zkVMs）集成到业务流程管理引擎中，通过系统架构和原型实现，支持证明组合的链式可验证计算。在碳足迹示例中建模顺序足迹活动。

Result: 实验评估表明，该方法能够在给定机密性约束下自动化流程验证，评估了不同ZKP证明变体在流程模型中的证明和验证效率。

Conclusion: 该方法成功实现了在保护机密性的前提下验证业务流程完整性，为ZKP在业务流程管理生命周期中的实际集成提供了实践指导。

Abstract: Ensuring the integrity of business processes without disclosing confidential
business information is a major challenge in inter-organizational processes.
This paper introduces a zero-knowledge proof (ZKP)-based approach for the
verifiable execution of business processes while preserving confidentiality. We
integrate ZK virtual machines (zkVMs) into business process management engines
through a comprehensive system architecture and a prototypical implementation.
Our approach supports chained verifiable computations through proof
compositions. On the example of product carbon footprinting, we model
sequential footprinting activities and demonstrate how organizations can prove
and verify the integrity of verifiable processes without exposing sensitive
information. We assess different ZKP proving variants within process models for
their efficiency in proving and verifying, and discuss the practical
integration of ZKPs throughout the Business Process Management (BPM) lifecycle.
Our experiment-driven evaluation demonstrates the automation of process
verification under given confidentiality constraints.

</details>


### [15] [Protocol Testing with I/O Grammars](https://arxiv.org/abs/2509.20308)
*Alexander Liggesmeyer,José Antonio Zamudio Amaya,Andreas Zeller*

Main category: cs.SE

TL;DR: 本文提出了一种基于I/O语法的协议测试新方法，将输入生成和输出检查统一在一个框架中，能够完整指定协议的语法和语义，并支持测试生成、模拟对象和验证功能。


<details>
  <summary>Details</summary>
Motivation: 协议测试面临两个基本问题：需要生成语法和语义正确且多样化的输入，以及需要验证测试用例正确性的机制。现有工具无法同时解决这两个问题。

Method: 引入I/O语法作为完整指定协议语法和语义的形式化方法，包括消息、状态和交互。基于FANDANGO框架实现，支持用户定义约束和k路径引导来系统覆盖协议特性。

Result: 在DNS、FTP、SMTP等协议上的评估表明，I/O语法能正确完整地指定高级协议特性，同时支持被测程序的输出验证。系统化覆盖I/O语法比随机方法能更快覆盖输入和响应空间。

Conclusion: I/O语法为协议测试提供了一个统一且完整的规范方法，其系统化覆盖策略在功能覆盖效率上优于现有的随机方法。

Abstract: Generating software tests faces two fundamental problems. First, one needs to
_generate inputs_ that are syntactically and semantically correct, yet
sufficiently diverse to cover behavior. Second, one needs an _oracle_ to _check
outputs_ whether a test case is correct or not. Both problems become apparent
in _protocol testing_, where inputs are messages exchanged between parties, and
outputs are the responses of these parties.
  In this paper, we propose a novel approach to protocol testing that combines
input generation and output checking in a single framework. We introduce _I/O
grammars_ as the first means to _completely_ specify the syntax and semantics
of protocols, including messages, states, and interactions. Our implementation,
based on the FANDANGO framework, takes a single I/O grammar, and can act as a
_test generator_, as a _mock object_, and as an _oracle_ for a _client_, a
_server_, or both (or actually any number of parties), a versatility not found
in any existing tool or formalism. User-defined _constraints}_can have the
generator focus on arbitrary protocol features; $k$-path guidance
systematically covers states, messages, responses, and value alternatives in a
unified fashion.
  We evaluate the effectiveness of our approach by applying it to several
protocols, including DNS, FTP, and SMTP. We demonstrate that I/O grammars can
specify advanced protocol features correctly and completely, while also
enabling output validation of the programs under test. In its evaluation, we
find that systematic coverage of the I/O grammar results in much quicker
coverage of the input and response spaces (and thus functionality) compared to
the random-based state-of-the-art approaches.

</details>


### [16] [Developer Productivity With and Without GitHub Copilot: A Longitudinal Mixed-Methods Case Study](https://arxiv.org/abs/2509.20353)
*Viktoria Stray,Elias Goldmann Brandtzæg,Viggo Tellefsen Wivestad,Astri Barbala,Nils Brede Moe*

Main category: cs.SE

TL;DR: 研究GitHub Copilot对开发者活动和感知生产力的实际影响，发现在代码提交活动指标上无显著变化，但用户主观感受与客观指标存在差异


<details>
  <summary>Details</summary>
Motivation: 评估生成式AI工具GitHub Copilot在真实工作环境中的实际效果，填补现有研究中对实际生产力影响的空白

Method: 混合方法案例研究：分析26,317个非合并提交数据（25名Copilot用户vs 14名非用户），结合问卷调查和13次访谈

Result: Copilot用户在使用工具前就比非用户更活跃；使用Copilot后提交活动指标无显著变化，仅有轻微增加；主观生产力感知与客观指标不一致

Conclusion: 生成式AI工具的实际影响需要结合客观指标和主观体验综合评估，仅靠代码提交数据可能无法全面反映生产力变化

Abstract: This study investigates the real-world impact of the generative AI (GenAI)
tool GitHub Copilot on developer activity and perceived productivity. We
conducted a mixed-methods case study in NAV IT, a large public sector agile
organization. We analyzed 26,317 unique non-merge commits from 703 of NAV IT's
GitHub repositories over a two-year period, focusing on commit-based activity
metrics from 25 Copilot users and 14 non-users. The analysis was complemented
by survey responses on their roles and perceived productivity, as well as 13
interviews. Our analysis of activity metrics revealed that individuals who used
Copilot were consistently more active than non-users, even prior to Copilot's
introduction. We did not find any statistically significant changes in
commit-based activity for Copilot users after they adopted the tool, although
minor increases were observed. This suggests a discrepancy between changes in
commit-based metrics and the subjective experience of productivity.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [17] [Macro-embedding Compiler Intermediate Languages in Racket](https://arxiv.org/abs/2509.19607)
*William J. Bowman*

Main category: cs.PL

TL;DR: 本文介绍了一种在Racket中嵌入编译器中间语言家族的宏嵌入设计，用于编译器课程测试框架，实现从Scheme类语言到x86-64的完整编译链。


<details>
  <summary>Details</summary>
Motivation: 展示面向语言的技术和抽象方法，实现大量语言家族以及高低级语言间的互操作性，强调代码复用和模块化设计。

Method: 通过局部宏展开到单一宿主语言（Racket），实现开放语言特征的模块化和组合性，而非预定义封闭特征集的语言。

Result: 实现了高度代码复用和互操作性，简化了中间语言语义开发，支持扩展和重定义个别语言特征。

Conclusion: 该方法通过宏嵌入技术成功实现了编译器中间语言家族的高效实现，展示了语言导向编程在编译器教育中的实用价值。

Abstract: We present the design and implementation of a macro-embedding of a family of
compiler intermediate languages, from a Scheme-like language to x86-64, into
Racket. This embedding is used as part of a testing framework for a compilers
course to derive interpreters for all the intermediate languages. The embedding
implements features including safe, functional abstractions as well as unsafe
assembly features, and the interactions between the two at various intermediate
stages.
  This paper aims to demonstrate language-oriented techniques and abstractions
for implementing (1) a large family of languages and (2) interoperability
between low- and high-level languages. The primary strength of this approach is
the high degree of code reuse and interoperability compared to implementing
each interpreter separately. The design emphasizes modularity and
compositionality of an open set of language features by local macro expansion
into a single host language, rather than implementing a language pre-defined by
a closed set of features. This enables reuse from both the host language
(Racket) and between intermediate languages, and enables interoperability
between high- and low-level features, simplifying development of the
intermediate language semantics. It also facilitates extending or redefining
individual language features in intermediate languages, and exposing multiple
interfaces to the embedded languages.

</details>


### [18] [Compilation as Multi-Language Semantics](https://arxiv.org/abs/2509.19613)
*William J. Bowman*

Main category: cs.PL

TL;DR: 提出一种统一建模编译器的方法，将编译器完全建模为多语言语义中开放项的归约系统，而不是语法翻译，从而同时定义编译器和互操作性语义。


<details>
  <summary>Details</summary>
Motivation: 现有使用多语言语义的编译模型需要为每个编译通道定义两个变体：语法翻译和运行时翻译，存在重复工作。

Method: 将编译器建模为多语言语义中开放项的归约系统，通过跨语言redex的归一化实现AOT编译，通过多语言评估实现JIT编译。

Result: 该方法减少了重复定义，提供了语义洞察，并通过多语言归约的合流性证明编译器正确性和安全编译的部分证明。

Conclusion: 该方法统一了编译器和互操作性语义的建模，简化了证明过程，并使类型保持性证明更加直接。

Abstract: Modeling interoperability between programs in different languages is a key
problem when modeling verified and secure compilation, which has been
successfully addressed using multi-language semantics. Unfortunately, existing
models of compilation using multi-language semantics define two variants of
each compiler pass: a syntactic translation on open terms to model compilation,
and a run-time translation of closed terms at multi-language boundaries to
model interoperability.
  In this talk, I discuss work-in-progress approach to uniformly model a
compiler entirely as a reduction system on open term in a multi-language
semantics, rather than as a syntactic translation. This simultaneously defines
the compiler and the interoperability semantics, reducing duplication. It also
provides interesting semantic insights. Normalization of the cross-language
redexes performs ahead-of-time (AOT) compilation. Evaluation in the
multi-language models just-in-time (JIT) compilation. Confluence of
multi-language reduction implies compiler correctness, and part of the secure
compilation proof (full abstraction), enabling focus on the difficult part of
the proof. Subject reduction of the multi-language reduction implies
type-preservation of the compiler.

</details>


### [19] [The Syntax and Semantics of einsum](https://arxiv.org/abs/2509.20020)
*Maurice Wenig,Paul G. Rump,Mark Blacher,Joachim Giesen*

Main category: cs.PL

TL;DR: 本文为einsum符号提供了理论基础，定义了einsum语言的形式化语义，并证明了张量表达式的重要等价规则。


<details>
  <summary>Details</summary>
Motivation: 尽管einsum符号在机器学习、量子电路模拟等领域取得了实际成功，但缺乏坚实的理论基础，且在不同框架中不统一，限制了形式化推理和系统优化的机会。

Method: 讨论了张量表达式的术语，提供了einsum语言的形式化定义，基于此定义形式化并证明了张量表达式的重要等价规则。

Result: 建立了einsum符号的理论基础，证明了等价规则，并展示了这些规则在实际应用中的相关性。

Conclusion: 这项工作为einsum符号提供了形式化理论基础，有助于促进形式化推理和系统优化。

Abstract: In 2011, einsum was introduced to NumPy as a practical and convenient
notation for tensor expressions in machine learning, quantum circuit
simulation, and other fields. It has since been implemented in additional
Python frameworks such as PyTorch and TensorFlow, as well as in other
programming languages such as Julia. Despite its practical success, the einsum
notation still lacks a solid theoretical basis, and is not unified across the
different frameworks, limiting opportunities for formal reasoning and
systematic optimization. In this work, we discuss the terminology of tensor
expressions and provide a formal definition of the einsum language. Based on
this definition, we formalize and prove important equivalence rules for tensor
expressions and highlight their relevance in practical applications.

</details>
