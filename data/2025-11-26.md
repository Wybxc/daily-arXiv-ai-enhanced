<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 9]
- [cs.LO](#cs.LO) [Total: 6]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Mechanizing a Proof-Relevant Logical Relation for Timed Message-Passing Protocols](https://arxiv.org/abs/2511.19521)
*Tesla Zhang,Asher Kornfeld,Rui Li,Sonya Simkin,Yue Yao,Stephanie Balzer*

Main category: cs.PL

TL;DR: 本文实现了Yao等人提出的语义类型系统在Rocq定理证明器中的机械化，包括逻辑关系、可计算轨迹的代数和基本定理。


<details>
  <summary>Details</summary>
Motivation: 语义类型化在程序验证中很强大，但Yao等人关于定时消息传递协议的验证系统缺乏机械化实现。机械化不仅能提供机器证明，还能促进未来扩展和应用的可扩展性。

Method: 在Rocq定理证明器中机械化Yao等人的结果，包括逻辑关系、可计算轨迹代数及其支持引理，以及逻辑关系的基本定理。

Result: 成功实现了语义类型系统的机械化，支持轨迹的交错、分区和连接操作，并在证明助手中处理了轨迹图相等的复杂等式问题。

Conclusion: 这项工作为定时消息传递协议的语义类型验证提供了机械化的基础，解决了在内涵类型理论证明助手中处理轨迹等式时面临的挑战。

Abstract: Semantic typing has become a powerful tool for program verification, applying the technique of logical relations as not only a proof method, but also a device for prescribing program behavior. In recent work, Yao et al. scaled semantic typing to the verification of timed message-passing protocols, which are prevalent in, e.g., IoT and real-time systems applications. The appeal of semantic typing in this context is precisely because of its ability to support typed and untyped program components alike -- including physical objects -- which caters to the heterogeneity of these applications. Another demand inherent to these applications is timing: constraining the time or time window within which a message exchange must happen. Yao et al. equipped their logical relation not only with temporal predicates, but also with computable trajectories, to supply the evidence that an inhabitant can step from one time point to another one. While Yao et al. provide the formalization for such a verification tool, it lacks a mechanization. Mechanizing the system would not only provide a machine proof for it, but also facilitate scalability for future extensions and applications.
  This paper tackles the challenge of mechanizing the resulting proof-relevant logical relation in a proof assistant. allowing trajectories to be interleaved, partitioned, and concatenated, while the intended equality on trajectories is the equality of their graphs when seen as processes indexed by time. Unfortunately, proof assistants based on intensional type theory only have modest support for such equations, forcing a prolific use of transports. This paper reports on the process of mechanizing Yao et al.'s results, comprising the logical relation, the algebra of computable trajectories with supporting lemmas, and the fundamental theorem of the logical relation, in the Rocq theorem prover.

</details>


### [2] [Understanding Accelerator Compilers via Performance Profiling](https://arxiv.org/abs/2511.19764)
*Ayaka Yorihiro,Griffin Berlstein,Pedro Pontes García,Kevin Laeufer,Adrian Sampson*

Main category: cs.PL

TL;DR: Petal是一个针对Calyx中间语言的周期级分析工具，通过探测和跟踪分析来帮助加速器设计者理解编译器决策对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 加速器设计语言(ADL)编译器存在性能不可预测性，需要工具来帮助程序员理解编译器决策如何影响性能。

Method: Petal在Calyx代码中插入探针，分析寄存器传输级仿真的跟踪数据，将跟踪事件映射回高级控制结构来跟踪每个结构活跃的时钟周期。

Result: 案例研究表明Petal能够识别现有加速器设计中的性能问题，并指导开发者进行编译器无法自动执行的优化，其中一个应用的总周期数减少了46.9%。

Conclusion: Petal为ADL程序员提供了理解编译器决策对性能影响的工具，弥补了编译器不完美带来的性能不可预测性问题。

Abstract: Accelerator design languages (ADLs), high-level languages that compile to hardware units, help domain experts quickly design efficient application-specific hardware. ADL compilers optimize datapaths and convert software-like control flow constructs into control paths. Such compilers are necessarily complex and often unpredictable: they must bridge the wide semantic gap between high-level semantics and cycle-level schedules, and they typically rely on advanced heuristics to optimize circuits. The resulting performance can be difficult to control, requiring guesswork to find and resolve performance problems in the generated hardware. We conjecture that ADL compilers will never be perfect: some performance unpredictability is endemic to the problem they solve.
  In lieu of compiler perfection, we argue for compiler understanding tools that give ADL programmers insight into how the compiler's decisions affect performance. We introduce Petal, a cycle-level Petal for the Calyx intermediate language (IL). Petal instruments the Calyx code with probes and then analyzes the trace from a register-transfer-level simulation. It maps the events in the trace back to high-level control constructs in the Calyx code to track the clock cycles when each construct was active. Using case studies, we demonstrate that Petal's cycle-level profiles can identify performance problems in existing accelerator designs. We show that these insights can also guide developers toward optimizations that the compiler was unable to perform automatically, including a reduction by 46.9\% of total cycles for one application.

</details>


### [3] [The Ghosts of Empires: Extracting Modularity from Interleaving-Based Proofs (Extended Version)](https://arxiv.org/abs/2511.20369)
*Frank Schüssele,Matthias Zumkeller,Miriam Lagunes-Rochin,Dominik Klumpp*

Main category: cs.PL

TL;DR: 将基于交错执行的正确性证明转换为线程模块化证明，通过自动合成幽灵变量来捕获相关交错信息，生成紧凑的正确性证书


<details>
  <summary>Details</summary>
Motivation: 算法软件验证器中的实现错误威胁其可靠性，为正确程序生成正确性证书可以独立验证结果，但并发程序的正确性证明面临指数爆炸问题

Method: 将基于交错执行的正确性证明转换为Owicki-Gries风格的线程模块化证明，自动合成幽灵变量来捕获相关交错信息，抽象掉无关细节

Result: 评估表明该方法在实践中高效，相比基线生成更紧凑的证明

Conclusion: 提出的方法能够高效生成并发程序的紧凑正确性证书，有助于验证验证器的可靠性

Abstract: Implementation bugs threaten the soundness of algorithmic software verifiers. Generating correctness certificates for correct programs allows for efficient independent validation of verification results, and thus helps to reveal such bugs. Automatic generation of small, compact correctness proofs for concurrent programs is challenging, as the correctness arguments may depend on the particular interleaving, which can lead to exponential explosion. We present an approach that converts an interleaving-based correctness proof, as generated by many algorithmic verifiers, into a thread-modular correctness proof in the style of Owicki and Gries. We automatically synthesize ghost variables that capture the relevant interleaving information, and abstract away irrelevant details. Our evaluation shows that the approach is efficient in practice and generates compact proofs, compared to a baseline.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [4] [Softmax Transformers are Turing-Complete](https://arxiv.org/abs/2511.20038)
*Hongjian Jiang,Michael Hahn,Georg Zetzsche,Anthony Widjaja Lin*

Main category: cs.FL

TL;DR: 本文证明了长度可泛化的softmax注意力链式思维（CoT）变换器是图灵完备的，通过扩展计数RASP（C-RASP）模型实现，并在单字母表和字母有界语言上验证了图灵完备性。


<details>
  <summary>Details</summary>
Motivation: 解决软注意力链式思维变换器是否图灵完备的开放性问题，特别关注长度可泛化的情况。

Method: 通过链式思维扩展的计数RASP（CoT C-RASP）模型，结合因果掩码和相对位置编码，证明其在特定语言上的图灵完备性。

Result: 证明了CoT C-RASP在单字母表和字母有界语言上是图灵完备的，且通过相对位置编码扩展后可在任意语言上实现图灵完备性。

Conclusion: 长度可泛化的softmax注意力CoT变换器是图灵完备的，这为复杂算术推理任务提供了理论基础，并通过实验验证了其有效性。

Abstract: Hard attention Chain-of-Thought (CoT) transformers are known to be Turing-complete. However, it is an open problem whether softmax attention Chain-of-Thought (CoT) transformers are Turing-complete. In this paper, we prove a stronger result that length-generalizable softmax CoT transformers are Turing-complete. More precisely, our Turing-completeness proof goes via the CoT extension of the Counting RASP (C-RASP), which correspond to softmax CoT transformers that admit length generalization. We prove Turing-completeness for CoT C-RASP with causal masking over a unary alphabet (more generally, for letter-bounded languages). While we show this is not Turing-complete for arbitrary languages, we prove that its extension with relative positional encoding is Turing-complete for arbitrary languages. We empirically validate our theory by training transformers for languages requiring complex (non-linear) arithmetic reasoning.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [5] [Building Browser Agents: Architecture, Security, and Practical Solutions](https://arxiv.org/abs/2511.19477)
*Aram Vardanyan*

Main category: cs.SE

TL;DR: 生产级浏览器代理面临可靠性和安全挑战，研究发现模型能力不是限制因素，架构决策决定成败。安全分析显示提示注入攻击使通用自主操作不安全，应开发具有程序约束的专用工具而非通用浏览智能。


<details>
  <summary>Details</summary>
Motivation: 解决浏览器代理在生产环境中的可靠性和安全性问题，分析当前方法失败的原因和阻碍安全自主操作的因素。

Method: 采用混合上下文管理（结合可访问性树快照和选择性视觉）、全面浏览器工具匹配人类交互能力、智能提示工程。

Result: 在WebGames基准测试的53个多样化挑战中达到约85%成功率，优于之前浏览器代理的约50%，接近人类基线的95.7%。

Conclusion: 反对开发通用浏览智能，主张开发具有程序约束的专用工具，通过代码而非LLM推理来强制执行安全边界。

Abstract: Browser agents enable autonomous web interaction but face critical reliability and security challenges in production. This paper presents findings from building and operating a production browser agent. The analysis examines where current approaches fail and what prevents safe autonomous operation. The fundamental insight: model capability does not limit agent performance; architectural decisions determine success or failure. Security analysis of real-world incidents reveals prompt injection attacks make general-purpose autonomous operation fundamentally unsafe. The paper argues against developing general browsing intelligence in favor of specialized tools with programmatic constraints, where safety boundaries are enforced through code instead of large language model (LLM) reasoning. Through hybrid context management combining accessibility tree snapshots with selective vision, comprehensive browser tooling matching human interaction capabilities, and intelligent prompt engineering, the agent achieved approximately 85% success rate on the WebGames benchmark across 53 diverse challenges (compared to approximately 50% reported for prior browser agents and 95.7% human baseline).

</details>


### [6] [Translating Large-Scale C Repositories to Idiomatic Rust](https://arxiv.org/abs/2511.20617)
*Saman Dehghan,Tianran Sun,Tianxiang Wu,Zihan Li,Reyhaneh Jabbarvand*

Main category: cs.SE

TL;DR: Rustine是一个自动化C到Rust翻译管道，平衡了质量与可扩展性，相比现有方法生成更安全、更地道的Rust代码。


<details>
  <summary>Details</summary>
Motivation: 现有C到Rust翻译技术无法平衡质量与可扩展性：基于转译的方法可扩展但代码安全性和可读性差，而基于LLM的方法成本过高且依赖前沿模型。

Method: 提出Rustine全自动管道，用于仓库级别的C到地道安全Rust翻译。

Result: 在23个C程序上评估，Rustine能为所有程序生成完全可编译的Rust代码，达到87%功能等价性（通过1,063,099/1,221,192个断言），平均函数和行覆盖率为74.7%和72.2%。

Conclusion: Rustine翻译比六种现有技术更安全、更地道、更可读。当翻译无法通过所有测试时，开发者平均只需4.5小时即可完成调试。

Abstract: Existing C to Rust translation techniques fail to balance quality and scalability: transpilation-based approaches scale to large projects but produce code with poor safety, idiomaticity, and readability. In contrast, LLM-based techniques are prohibitively expensive due to their reliance on frontier models (without which they cannot reliably generate compilable translations), thus limiting scalability. This paper proposes Rustine, a fully automated pipeline for effective and efficient repository-level C to idiomatic safe Rust translation. Evaluating on a diverse set of 23 C programs, ranging from 27 to 13,200 lines of code, Rustine can generate fully compilable Rust code for all and achieve 87% functional equivalence (passing 1,063,099 assertions out of 1,221,192 in test suites with average function and line coverage of 74.7% and 72.2%). Compared to six prior repository-level C to Rust translation techniques, the translations by Rustine are overall safer (fewer raw pointers, pointer arithmetic, and unsafe constructs), more idiomatic (fewer Rust linter violations), and more readable. When the translations cannot pass all tests to fulfill functional equivalence, human developers were able to complete the task in 4.5 hours, on average, using Rustine as debugging support.

</details>


### [7] [Z-Space: A Multi-Agent Tool Orchestration Framework for Enterprise-Grade LLM Automation](https://arxiv.org/abs/2511.19483)
*Qingsong He,Jing Nan,Jiayu Jiao,Liangjie Tang,Xiaodong Xu,Mengmeng Sun,Qingyao Wang,Minghui Yan*

Main category: cs.SE

TL;DR: Z-Space是一个面向数据生成的多智能体协作工具调用框架，通过意图解析、工具过滤和执行代理解决大规模MCP服务中工具匹配的挑战，显著提升智能测试数据生成系统的效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着企业级MCP服务的快速增长，在数千个异构工具中高效准确地匹配目标功能成为限制系统实用性的核心挑战。现有方法依赖全提示注入或静态语义检索，面临用户查询与工具描述语义脱节、LLM输入上下文膨胀和高推理延迟等问题。

Method: 建立多智能体协作架构和工具过滤算法：(1)通过意图解析模型实现用户查询的结构化语义理解；(2)基于融合子空间加权算法的工具过滤模块(FSWW)实现意图与工具的细粒度语义对齐；(3)构建推理执行代理支持多步任务的动态规划和容错执行。

Result: 系统在饿了么平台技术部部署，服务于淘天、高德、盒马等多个业务单元的大规模测试数据生成场景。生产数据显示，系统将工具推理的平均token消耗降低96.26%，同时达到92%的工具调用准确率。

Conclusion: Z-Space框架显著提升了智能测试数据生成系统的效率和可靠性，为大规模MCP服务中的工具匹配问题提供了有效解决方案。

Abstract: Large Language Models can break through knowledge and timeliness limitations by invoking external tools within the Model Context Protocol framework to achieve automated execution of complex tasks. However, with the rapid growth of enterprise-scale MCP services, efficiently and accurately matching target functionalities among thousands of heterogeneous tools has become a core challenge restricting system practicality. Existing approaches generally rely on full-prompt injection or static semantic retrieval, facing issues including semantic disconnection between user queries and tool descriptions, context inflation in LLM input, and high inference latency. To address these challenges, this paper proposes Z-Space, a data-generation-oriented multi-agent collaborative tool invocation framework Z-Space. The Z-Space framework establishes a multi-agent collaborative architecture and tool filtering algorithm: (1) A structured semantic understanding of user queries is achieved through an intent parsing model; (2) A tool filtering module (FSWW) based on fused subspace weighted algorithm realizes fine-grained semantic alignment between intents and tools without parameter tuning; (3) An inference execution agent is constructed to support dynamic planning and fault-tolerant execution for multi-step tasks. This framework has been deployed in the Eleme platform's technical division, serving large-scale test data generation scenarios across multiple business units including Taotian, Gaode, and Hema. Production data demonstrates that the system reduces average token consumption in tool inference by 96.26\% while achieving a 92\% tool invocation accuracy rate, significantly enhancing the efficiency and reliability of intelligent test data generation systems.

</details>


### [8] [stable-pretraining-v1: Foundation Model Research Made Simple](https://arxiv.org/abs/2511.19484)
*Randall Balestriero,Hugues Van Assel,Sami BuGhanem,Lucas Maes*

Main category: cs.SE

TL;DR: stable-pretraining是一个模块化、可扩展的自监督学习库，旨在降低基础模型研究的工程负担，提供统一的SSL工具和全面日志记录，加速研究迭代。


<details>
  <summary>Details</summary>
Motivation: 当前自监督学习和基础模型研究面临代码库复杂、重复实现、工程负担重等问题，阻碍了研究进展。

Method: 基于PyTorch、Lightning、Hugging Face和TorchMetrics构建模块化库，统一SSL工具（探针、崩溃检测、数据增强、评估流程），采用全面日志记录原则。

Result: 验证了库的有效性，能够以最小开销生成新研究见解，包括深度表示探针和CLIP在合成数据微调下的退化分析。

Conclusion: stable-pretraining通过降低入门门槛并保持大规模实验的可扩展性，旨在加速基础模型研究的发现进程。

Abstract: Foundation models and self-supervised learning (SSL) have become central to modern AI, yet research in this area remains hindered by complex codebases, redundant re-implementations, and the heavy engineering burden of scaling experiments. We present stable-pretraining, a modular, extensible, and performance-optimized library built on top of PyTorch, Lightning, Hugging Face, and TorchMetrics. Unlike prior toolkits focused narrowly on reproducing state-of-the-art results, stable-pretraining is designed for flexibility and iteration speed: it unifies essential SSL utilities--including probes, collapse detection metrics, augmentation pipelines, and extensible evaluation routines--within a coherent and reliable framework. A central design principle is logging everything, enabling fine-grained visibility into training dynamics that makes debugging, monitoring, and reproducibility seamless. We validate the library by demonstrating its ability to generate new research insights with minimal overhead, including depthwise representation probing and the analysis of CLIP degradation under synthetic data finetuning. By lowering barriers to entry while remaining scalable to large experiments, stable-pretraining aims to accelerate discovery and expand the possibilities of foundation model research.

</details>


### [9] [Evolution without an Oracle: Driving Effective Evolution with LLM Judges](https://arxiv.org/abs/2511.19489)
*Zhe Zhao,Yuheng Yang,Haibin Wen,Xiaojie Qiu,Zaixi Zhang,Qingfu Zhang*

Main category: cs.SE

TL;DR: MADE框架通过将模糊指令分解为可验证的子需求，利用LLM主观评价替代传统可计算适应度函数，在无真实标签的开放领域实现进化优化


<details>
  <summary>Details</summary>
Motivation: 打破传统进化计算对可计算适应度函数的依赖，探索在纯主观评价下的进化优化可能性

Method: 提出MADE框架，通过问题规约将模糊指令分解为具体可验证的子需求，利用多智能体分解进化来驯服LLM主观评价的噪声

Result: 在DevAI和InfoBench基准测试中，软件需求满足率从39.9%提升至61.9%，复杂指令遵循完美通过率达到95%

Conclusion: 实现了从优化"可计算指标"到"可描述质量"的范式转变，为无真实标签的开放领域解锁了进化优化能力

Abstract: The integration of Large Language Models (LLMs) with Evolutionary Computation (EC) has unlocked new frontiers in scientific discovery but remains shackled by a fundamental constraint: the reliance on an Oracle--an objective, machine-computable fitness function. This paper breaks this barrier by asking: Can evolution thrive in a purely subjective landscape governed solely by LLM judges? We introduce MADE (Multi-Agent Decomposed Evolution), a framework that tames the inherent noise of subjective evaluation through "Problem Specification." By decomposing vague instructions into specific, verifiable sub-requirements, MADE transforms high-variance LLM feedback into stable, precise selection pressure. The results are transformative: across complex benchmarks like DevAI and InfoBench, MADE outperforms strong baselines by over 50% in software requirement satisfaction (39.9% to 61.9%) and achieves a 95% perfect pass rate on complex instruction following. This work validates a fundamental paradigm shift: moving from optimizing "computable metrics" to "describable qualities," thereby unlocking evolutionary optimization for the vast open-ended domains where no ground truth exists.

</details>


### [10] [CodeR3: A GenAI-Powered Workflow Repair and Revival Ecosystem](https://arxiv.org/abs/2511.19510)
*Asif Zaman,Kallol Naha,Khalid Belhajjame,Hasan M. Jamil*

Main category: cs.SE

TL;DR: CodeR³系统利用生成式AI将过时的Taverna工作流迁移到Snakemake和VisFlow等现代技术，结合可视化分析和人机协作验证，显著减少手动工作量但关键任务仍需人工监督。


<details>
  <summary>Details</summary>
Motivation: 科学工作流包含重要领域知识，但大量已发布工作流随时间衰败，特别是Taverna等遗留系统因服务终止、依赖过时等问题导致工作流失效。

Method: 开发CodeR³系统，使用生成式AI分析衰败工作流特征，将其转换为现代工作流技术，集成逐步可视化分析、自动服务替换和人机协作验证。

Result: 案例研究表明该方法可行，自动化显著减少了工作流解析和服务识别的手动工作，但服务替换和数据验证仍需领域专家参与。

Conclusion: 提出了一个平衡自动化效率和必要人工判断的工作流复兴框架，将开发众包平台供社区协作复兴和验证工作流。

Abstract: Scientific workflows encode valuable domain expertise and computational methodologies. Yet studies consistently show that a significant proportion of published workflows suffer from decay over time. This problem is particularly acute for legacy workflow systems like Taverna, where discontinued services, obsolete dependencies, and system retirement render previously functional workflows unusable. We present a novel legacy workflow migration system, called CodeR$^3$ (stands for Code Repair, Revival and Reuse), that leverages generative AI to analyze the characteristics of decayed workflows, reproduce them into modern workflow technologies like Snakemake and VisFlow. Our system additionally integrates stepwise workflow analysis visualization, automated service substitution, and human-in-the-loop validation. Through several case studies of Taverna workflow revival, we demonstrate the feasibility of this approach while identifying key challenges that require human oversight. Our findings reveal that automation significantly reduces manual effort in workflow parsing and service identification. However, critical tasks such as service substitution and data validation still require domain expertise. Our result will be a crowdsourcing platform that enables the community to collaboratively revive decayed workflows and validate the functionality and correctness of revived workflows. This work contributes a framework for workflow revival that balances automation efficiency with necessary human judgment.

</details>


### [11] [Agint: Agentic Graph Compilation for Software Engineering Agents](https://arxiv.org/abs/2511.19635)
*Abhi Chivukula,Jay Somasundaram,Vijay Somasundaram*

Main category: cs.SE

TL;DR: Agint是一个基于图的智能编码代理系统，通过分层编译将自然语言指令转换为类型化代码DAG，提供混合LLM和函数JIT运行时，支持动态图优化、可重现执行和与现有开发工具集成。


<details>
  <summary>Details</summary>
Motivation: 解决当前LLM编码代理在上下文管理、延迟、可靠性、可重现性和可扩展性方面的挑战，构建可组合、团队协作的编码代理系统。

Method: 采用显式类型层次（文本→数据→规范→代码）的语义图转换，结合混合LLM和函数JIT运行时，实现增量分层编译和动态图优化。

Result: 提高了可靠性，支持并发代码库组合，使用更小更快的模型，降低延迟，提高上下文利用效率和吞吐量。

Conclusion: Agint通过连接自然语言、编译器方法和开发工具，实现了可组合、团队导向的编码代理规模化部署，支持从原型到生产代码的无缝过渡。

Abstract: LLM-based coding agents are increasingly common but still face challenges in context management, latency, reliability, reproducibility, and scalability. We present Agint, an agentic graph compiler, interpreter, and runtime that incrementally and hierarchically converts natural-language instructions into typed, effect-aware code DAGs. Agint introduces explicit type floors (text to data to spec to code) grounded in semantic graph transformations and a hybrid LLM and function-based JIT runtime. This enables dynamic graph refinement, reproducible and optimizable execution, speculative evaluation, and interoperability with existing developer tools. Agint's typed graph bindings improve reliability and allow concurrent composition of concurrent codebases by construction, supporting accelerated development with smaller and faster models, lower latency, efficient context utilization, and higher throughput. Hierarchical compilation allows scalable graph edits, while the graph structure supports reproducibility and efficient parallel generation. Agint provides a composable unix-style toolchain: dagify (DAG compiler), dagent (hybrid JIT runtime), schemagin (schema generator), and datagin (data transformer) for realtime, low-latency code and dataflow creation. Human developers and coding agents refine graphs through the Agint CLI, while non-technical users use Agint Flow GUI for visual editing, conversational refinement, and debugging to promote prototype agentic workflows to production code. This continuous co-creation model allows teams to prototype quickly, refine seamlessly, and deploy reliably, bridging natural language, compiler methods, and developer tooling to enable a new generation of composable, team-centric coding agents at scale.

</details>


### [12] [CodeFuse-CommitEval: Towards Benchmarking LLM's Power on Commit Message and Code Change Inconsistency Detection](https://arxiv.org/abs/2511.19875)
*Qingyu Zhang,Puzhuo Liu,Peng Di,Chenxiong Qian*

Main category: cs.SE

TL;DR: 提出了CODEFUSE-COMMITEVAL，这是首个专门用于评估大语言模型检测提交消息与代码不一致性的基准，通过规则化变异生成七种不一致消息类型，并评估了六种开源LLM在不同增强策略下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的提交消息往往质量低且与代码变更不一致，这会误导代码审查、阻碍维护、污染研究数据集，甚至可能掩盖安全补丁，但目前缺乏专门评估MCI检测模型的基准。

Method: 基于ApacheCM数据集，通过规则引导的变异生成七种类型的不一致消息，采用双重验证确保正负样本质量，评估六种开源LLM在普通设置和三种增强策略下的表现。

Result: 模型检测不一致提交比一致提交更可靠；gpt-oss-20B表现最佳但token消耗是其他模型的两倍多；增强策略效果各异：相邻上下文对大模型有帮助但对小模型增加噪声，few-shot提高准确率但增加错误预测，chain-of-thought提升精确度但降低召回率。

Conclusion: CODEFUSE-COMMITEVAL为MCI检测提供了严谨的评估基础，揭示了需要更丰富的上下文和平衡数据来捕捉高层次语义差距，不同不一致类型的检测难度存在差异。

Abstract: Version control relies on commit messages to convey the rationale for code changes, but these messages are often low quality and, more critically, inconsistent with their diffs-known as message-code inconsistency (MCI). MCIs mislead reviewers, hinder maintenance, contaminate research datasets, and may obscure security patches. Yet, no dedicated benchmark exists to evaluate models for MCI detection. We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models (LLMs). Built on the ApacheCM dataset for diversity and quality, we generate seven types of inconsistent messages through rule-guided mutations of originally consistent commits and apply two-fold validation to verify both positive and negative samples. Using this labeled dataset of message-diff pairs, we evaluate six state-of-the-art open-source LLMs under a vanilla setting and with three augmentation strategies: few-shot prompting, chain-of-thought, and extended context. Results show models detect inconsistent commits more reliably than consistent ones (average Recall 85.95%, Precision 80.28%, Specificity 63.8%); gpt-oss-20B performs best overall but uses over twice the tokens of others. Augmentation effects vary: adjacent context helps larger models but adds noise for smaller ones; few-shot improves accuracy and reduces token use, yet increases universally incorrect predictions; chain-of-thought boosts precision and specificity at the cost of recall and higher token consumption. Type-wise analysis reveals higher detectability for component, file-path, and operation inconsistencies, but lower accuracy and higher token cost for intent-level "purpose" inconsistencies. CODEFUSE-COMMITEVAL provides a rigorous foundation for measuring, comparing, and advancing MCI detection, highlighting the need for richer context and balanced data to capture high-level semantic gaps.

</details>


### [13] [LLMs for Automated Unit Test Generation and Assessment in Java: The AgoneTest Framework](https://arxiv.org/abs/2511.20403)
*Andrea Lops,Fedelucio Narducci,Azzurra Ragone,Michelantonio Trizio,Claudio Barto*

Main category: cs.SE

TL;DR: AgoneTest是一个用于评估LLM生成的Java单元测试的自动化框架，包含Classes2Test数据集和综合评估指标，实验显示LLM生成的测试在编译成功的情况下可以达到或超过人工编写测试的质量。


<details>
  <summary>Details</summary>
Motivation: 单元测试是软件开发中重要但资源密集的环节，需要标准化框架来评估不同LLM和提示策略在真实条件下的测试生成能力。

Method: 提出AgoneTest评估框架和Classes2Test数据集，集成变异分数和测试异味等高级评估指标，建立端到端的标准化评估流程。

Result: 对于能够编译的测试子集，LLM生成的测试在覆盖率和缺陷检测方面能够匹配或超过人工编写的测试，增强的提示策略有助于提高测试质量。

Conclusion: AgoneTest阐明了LLM在软件测试中的潜力，为未来模型设计、提示工程和测试实践的改进提供了见解。

Abstract: Unit testing is an essential but resource-intensive step in software development, ensuring individual code units function correctly. This paper introduces AgoneTest, an automated evaluation framework for Large Language Model-generated (LLM) unit tests in Java. AgoneTest does not aim to propose a novel test generation algorithm; rather, it supports researchers and developers in comparing different LLMs and prompting strategies through a standardized end-to-end evaluation pipeline under realistic conditions. We introduce the Classes2Test dataset, which maps Java classes under test to their corresponding test classes, and a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment. Experimental results show that, for the subset of tests that compile, LLM-generated tests can match or exceed human-written tests in terms of coverage and defect detection. Our findings also demonstrate that enhanced prompting strategies contribute to test quality. AgoneTest clarifies the potential of LLMs in software testing and offers insights for future improvements in model design, prompt engineering, and testing practices.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [14] [Separating the Wheat from the Chaff: Understanding (In-)Completeness of Proof Mechanisms for Separation Logic with Inductive Definitions](https://arxiv.org/abs/2511.20193)
*Neta Elad,Adithya Murali,Sharon Shoham*

Main category: cs.LO

TL;DR: 本文研究了分离逻辑与归纳定义(SLID)的不完备性，提出了弱分离逻辑(WSL)作为更完整的框架，证明了WSL在量化蕴含问题上的完备性，并开发了工具来发现反例模型。


<details>
  <summary>Details</summary>
Motivation: 分离逻辑虽然流行但存在不完备性，特别是在自动证明机制中经常失败。需要深入理解这些失败的原因，并建立更完整的逻辑框架。

Method: 提出弱分离逻辑(WSL)作为SLID的扩展，通过归约到一阶逻辑证明WSL的完备性，使用符号结构表示和发现非标准模型，并开发原型工具实现FOL编码。

Result: 证明了WSL在量化蕴含问题上的完备性，工具在700多个问题上成功找到反例模型，揭示了现实世界证明失败的原因。

Conclusion: WSL比SLID更完整，非标准模型是证明失败的主要原因，通过符号结构可以自动发现这些反例模型，为改进自动证明机制提供了理论基础。

Abstract: For over two decades Separation Logic has been arguably the most popular framework for reasoning about heap-manipulating programs, as well as reasoning about shared resources and permissions. Separation Logic is often extended to include inductively-defined predicates, interpreted as least fixpoints, forming Separation Logic with Inductive Definitions (SLID). Many theoretical and practical advances have been made in developing automated proof mechanisms for SLID, but these mechanisms are imperfect, and a deeper understanding of their failures is desired. As expressive as Separation Logic is, it is not surprising that it is incomplete, and in fact, it contains several sources of incompleteness that defy automated reasoning.
  In this paper we study these sources of incompleteness and how they relate to failures of proof mechanisms. We place SLID within a larger logic, that we call Weak Separation Logic (WSL). We prove that unlike SLID, WSL is complete for a non-trivial fragment of quantified entailments with background theories and inductive definitions, via a reduction to first-order logic (FOL). Moreover, we show that the ubiquitous fold/unfold proof mechanism is sound and complete for theory-free, quantifier-free WSL entailments with inductive definitions. Through this, we understand proof failures as stemming from nonstandard models present in WSL, but not allowed in SLID. These rogue models are typically infinite, and we use the formalism of symbolic structures to represent and automatically find them.
  We present a prototype tool that implements the FOL encoding of WSL and test it on an existing benchmark, which contains over 700 quantified entailment problems with inductive definitions. Our tool is able to find counter-models to many of the examples, and we provide a partial taxonomy of the rogue models, shedding some light on real-world proof failures.

</details>


### [15] [Deductive Systems for Logic Programs with Counting](https://arxiv.org/abs/2511.19565)
*Jorge Fandinno,Vladimir Lifschitz*

Main category: cs.LO

TL;DR: 本文展示了如何将证明强等价性的方法扩展到包含计数聚合的程序中。


<details>
  <summary>Details</summary>
Motivation: 在回答集编程中，强等价性是指两个规则组在任何上下文中具有相同含义。传统上可以通过适当的演绎系统来证明强等价性，但这种方法需要扩展到包含计数聚合的程序。

Method: 通过扩展演绎系统，使得能够从另一个程序的规则推导出每个程序的规则，从而证明包含计数聚合的程序的强等价性。

Result: 开发了一种方法来证明包含计数聚合的程序的强等价性，扩展了传统的证明方法。

Conclusion: 该方法成功地将强等价性证明技术扩展到包含计数聚合的程序，为回答集编程中的等价性验证提供了更广泛的适用性。

Abstract: In answer set programming, two groups of rules are considered strongly equivalent if they have the same meaning in any context. Strong equivalence of two programs can be sometimes established by deriving rules of each program from rules of the other in an appropriate deductive system. This paper shows how to extend this method of proving strong equivalence to programs containing the counting aggregate.

</details>


### [16] [Parameterized Verification of Quantum Circuits (Technical Report)](https://arxiv.org/abs/2511.19897)
*Parosh Aziz Abdulla,Yu-Fang Chen,Michal Hečko,Lukáš Holík,Ondřej Lengál,Jyun-Ao Lin,Ramanathan Srinivasan Thinniyam*

Main category: cs.LO

TL;DR: 提出了首个自动验证参数化量子程序关系性质的框架，核心是同步加权树自动机(SWTA)模型，能够紧凑精确地表示参数化程序产生的无限量子态族。


<details>
  <summary>Details</summary>
Motivation: 需要验证参数化量子程序的输入输出正确性和等价性，这类程序根据输入大小生成对应的量子电路，传统方法难以处理无限状态族。

Method: 使用同步加权树自动机(SWTA)捕获量子态族，引入量子门语义的转换器类，开发参数化电路的组合算法，将验证问题转化为SWTA的功能包含或等价检查。

Result: 实现验证了多种代表性参数化量子程序，验证时间从毫秒到秒级，证明了框架的表达能力和实际效率。

Conclusion: 该框架首次实现了参数化量子程序的自动关系性质验证，SWTA模型和转换器方法为量子程序验证提供了有效解决方案。

Abstract: We present the first fully automatic framework for verifying relational properties of parameterized quantum programs, i.e., a program that, given an input size, generates a corresponding quantum circuit. We focus on verifying input-output correctness as well as equivalence. At the core of our approach is a new automata model, synchronized weighted tree automata (SWTAs), which compactly and precisely captures the infinite families of quantum states produced by parameterized programs. We introduce a class of transducers to model quantum gate semantics and develop composition algorithms for constructing transducers of parameterized circuits. Verification is reduced to functional inclusion or equivalence checking between SWTAs, for which we provide decision procedures. Our implementation demonstrates both the expressiveness and practical efficiency of the framework by verifying a diverse set of representative parameterized quantum programs with verification times ranging from milliseconds to seconds.

</details>


### [17] [Chopping More Finely: Finite Countermodels in Modal Logic via the Subdivision Construction](https://arxiv.org/abs/2511.19747)
*Tenyo Takahashi*

Main category: cs.LO

TL;DR: 提出了一种名为细分构造的新方法，用于证明模态逻辑和模态规则系统的有限模型性质，该方法基于稳定规范规则框架构建有限模态代数/空间作为规则的反模型。


<details>
  <summary>Details</summary>
Motivation: 为广泛类别的模态逻辑和模态规则系统提供有限模型性质的证明方法，特别是针对由有限高度有限模态代数的稳定规范公式和规则公理化的系统。

Method: 细分构造方法，基于稳定规范规则框架，构建有限模态代数或有限模态空间作为有限反模型。

Result: 成功证明了由有限高度有限模态代数的稳定规范公式和规则公理化的逻辑和规则系统具有有限模型性质，并发现这些系统在相应格中是并分裂。

Conclusion: 细分构造是证明模态逻辑和模态规则系统有限模型性质的有效方法，并识别了在NExt(K4)中具有Kripke不完全度为1的并分裂类。

Abstract: We present a new method, the Subdivision Construction, for proving the finite model property (the fmp) for broad classes of modal logics and modal rule systems. The construction builds on the framework of stable canonical rules, and produces a finite modal algebra (finite modal space) that will be a finite countermodel of such rules, yielding the fmp. We apply the Subdivision Construction for proving the fmp for logics and rule systems axiomatized by stable canonical formulas and rules of finite modal algebras of finite height. We also observe that these logics and rule systems are union-splittings in corresponding lattices. As a consequence, we identify a class of union-splittings in $\mathsf{NExt}(\mathsf{K4})$ with the degree of Kripke incompleteness 1.

</details>


### [18] [Proceedings Twentieth Conference on Theoretical Aspects of Rationality and Knowledge](https://arxiv.org/abs/2511.20540)
*Adam Bjorndahl*

Main category: cs.LO

TL;DR: TARK会议是关于理性和知识理论方面的跨学科会议，汇集了计算机科学、人工智能、博弈论等多个领域的研究者，旨在促进对理性和知识推理的理解。


<details>
  <summary>Details</summary>
Motivation: 促进不同学科领域研究者之间的交流，进一步理解理性和知识推理的跨学科问题。

Method: 通过两年一次的会议形式，汇集来自计算机科学、人工智能、博弈论、决策理论、哲学、逻辑学、语言学和认知科学等领域的研究者进行学术交流。

Result: 成功举办了自1986年以来的系列会议，2025年将在德国杜塞尔多夫大学举办第20届会议，收录了相关领域的研究论文。

Conclusion: TARK会议作为一个长期运行的跨学科平台，持续推动着理性和知识理论领域的研究发展，促进了不同学科之间的思想交流。

Abstract: The TARK conference (Theoretical Aspects of Rationality and Knowledge) is a conference that aims to bring together researchers from a wide variety of fields, including computer science, artificial intelligence, game theory, decision theory, philosophy, logic, linguistics, and cognitive science. Its goal is to further our understanding of interdisciplinary issues involving reasoning about rationality and knowledge.
  Previous conferences have been held biennially around the world since 1986, on the initiative of Joe Halpern (Cornell University). Topics of interest include, but are not limited to, semantic models for knowledge, belief, uncertainty, awareness, bounded rationality, common sense epistemic reasoning, epistemic logic, epistemic game theory, knowledge and action, applications of reasoning about knowledge and other mental states, belief revision, computational social choice, algorithmic game theory, and foundations of multi-agent systems.
  Information about TARK is available at http://www.tark.org/.
  These proceedings contain the papers that have been accepted for presentation at the Twentieth Conference on Theoretical Aspects of Rationality and Knowledge (TARK 2025), held July 14--16, 2025, at Heinrich-Heine-Universität, Düsseldorf, Germany. The conference website can be found at https://ccc.cs.uni-duesseldorf.de/tark-2025/.

</details>


### [19] [Verifying Numerical Methods with Isabelle/HOL](https://arxiv.org/abs/2511.20550)
*Dustin Bryant,Jonathan Julian Huerta y Munive,Simon Foster*

Main category: cs.LO

TL;DR: 提出了一个基于Isabelle/HOL和ITrees的验证数值方法框架，支持从形式化规范到可执行代码的端到端路径，并应用于二分法和不动点迭代法。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习管道依赖数值算法，可靠的数值方法是可信机器学习和网络物理系统的先决条件。

Method: 基于ITrees的Isabelle/HOL框架，提供用户友好的规范语言，可直接声明数值程序并标注变体和不变式，验证条件可通过自动化证明方法和HOL-Analysis库引理解决。

Result: 成功验证了二分法和不动点迭代法，扩展了形式化数学库（高阶导数和泰勒定理），提供了从形式规范到可执行代码的完整路径。

Conclusion: 该框架为数值方法提供了机器检查的保证，展示了在验证数值算法方面的有效性，并扩展了必要的数学基础库。

Abstract: Modern machine learning pipelines are built on numerical algorithms. Reliable numerical methods are thus a prerequisite for trustworthy machine learning and cyber-physical systems. Therefore, we contribute a framework for verified numerical methods in Isabelle/HOL based on ITrees. Our user-friendly specification language enables the direct declaration of numerical programs that can be annotated with variants and invariants for reasoning about correctness specifications. The generated verification conditions can be discharged via automated proof methods and lemmas from the HOL-Analysis library. The ITrees foundation interacts with Isabelle's code generator to export source code. This provides an end-to-end path from formal specifications with machine-checked guarantees to executable sources. We illustrate the process of modelling numerical methods and demonstrate the effectiveness of the verification by focusing on two well-known methods, the bisection method and the fixed-point iteration method. We also contribute crucial extensions to the libraries of formalised mathematics required for this objective: higher-order derivatives and Taylor's theorem in Peano form. Finally, we qualitatively evaluate the use of the framework for verifying numerical methods.

</details>
