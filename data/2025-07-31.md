<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.SE](#cs.SE) [Total: 21]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [A Compute-Matched Re-Evaluation of TroVE on MATH](https://arxiv.org/abs/2507.22069)
*Tobias Sesterhenn,Ian Berlot-Attwell,Janis Zenkner,Christian Bartelt*

Main category: cs.PL

TL;DR: TroVE声称通过工具箱方法在MATH基准上优于直接生成代码的PRIMITIVE，但分析表明其优势主要来自更高的计算预算，而非工具箱机制。修正后，TroVE仅比PRIMITIVE高1%的准确率。


<details>
  <summary>Details</summary>
Motivation: 验证TroVE工具箱方法的实际效果，分析其性能提升是否源于工具箱机制或计算预算。

Method: 重新评估TroVE在MATH基准上的表现，分析其三种模式（直接生成代码、创建工具、重用工具）的影响，并修正其选择机制。

Result: TroVE的改进主要来自更高的计算预算，修正后仅比PRIMITIVE高1%的准确率。

Conclusion: 工具箱方法在MATH基准上未提供显著优势，性能提升主要源于计算预算而非机制本身。

Abstract: Reusing established theorems and formulas is central to mathematical problem
solving, serving as essential building blocks for tackling increasingly complex
challenges. Recent work, TroVE, argues that code-generating Large Language
Models (LLMs) can benefit similarly on the MATH benchmark by inducing and
reusing higher-level toolboxes. By allocating computational budget across an
ensemble of three modes -- directly generating code, creating tools, and
reusing tools -- TroVE claims to outperform a PRIMITIVE baseline that only
performs direct generation. However, recent analysis (Berlot-Attwell et al.,
2024) casts doubt on these gains, noting that the tools created are often
trivial or rarely reused, suggesting that improvements may stem from
self-consistency or self-correction. In this work, we re-evaluate TroVE on
MATH, analyze the impact of each of its modes, and show that its benefit does
not come from these mechanisms, but simply from a higher computational budget
spent for TroVE compared to PRIMITIVE. To this end, we also perform a small
correction in the original implementation of TroVE's selection mechanism,
boosting TroVE's performance on MATH by 3\% in accuracy. After matching for
compute, the benefit of TroVE reduces to a marginal improvement of 1\%,
suggesting that this toolbox approach does not provide a significant benefit on
MATH.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [2] [Infinite Traces by Finality: a Sheaf-Theoretic Approach](https://arxiv.org/abs/2507.22536)
*Marco Peressotti*

Main category: cs.LO

TL;DR: 提出了一种基于层论框架的方法，在Kleisli范畴中通过最终余代数捕获无限迹语义。


<details>
  <summary>Details</summary>
Motivation: Kleisli范畴中缺乏对无限迹语义的系统性描述，现有方法仅适用于有限迹语义。

Method: 结合Kleisli范畴、序数上的层以及守卫（co）递归，通过融合有限逼近构造无限行为。

Result: 引入守卫行为函子概念，证明在温和条件下其最终余代数可直接表征无限迹。

Conclusion: 该框架为Kleisli范畴中的无限迹语义提供了系统性解决方案。

Abstract: Kleisli categories have long been recognised as a setting for modelling the
linear behaviour of various types of systems. However, the final coalgebra in
such settings does not, in general, correspond to a fixed notion of linear
semantics. While there are well-understood conditions under which final
coalgebras capture finite trace semantics, a general account of infinite trace
semantics via finality has remained elusive. In this work, we present a
sheaf-theoretic framework for infinite trace semantics in Kleisli categories
that systematically constructs final coalgebras capturing infinite traces. Our
approach combines Kleisli categories, sheaves over ordinals, and guarded
(co)recursion, enabling infinite behaviours to emerge from coherent families of
finite approximations via amalgamation. We introduce the notion of guarded
behavioural functor and show that, under mild conditions, their final
coalgebras directly characterise infinite traces.

</details>


### [3] [Concrete Security Bounds for Simulation-Based Proofs of Multi-Party Computation Protocols](https://arxiv.org/abs/2507.22705)
*Kristina Sojakova,Mihai Codescu,Joshua Gancher*

Main category: cs.LO

TL;DR: 论文提出了一种新方法，用于自动计算多方计算（MPC）协议的具体安全性边界，填补了现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: 传统渐近安全性无法提供实际安全性的精确边界，而具体安全性边界难以手动推导。

Method: 受IPDL启发，提出了一种新方法，支持协议运行时间和对手优势的推理，并在Maude中实现。

Result: 通过四个案例研究验证了方法的有效性，首次实现了GMW MPC协议的形式化验证，并显著减少了证明代码量。

Conclusion: 该方法为MPC协议提供了高效的形式化验证工具，大幅简化了证明过程。

Abstract: The concrete security paradigm aims to give precise bounds on the probability
that an adversary can subvert a cryptographic mechanism. This is in contrast to
asymptotic security, where the probability of subversion may be eventually
small, but large enough in practice to be insecure. Fully satisfactory concrete
security bounds for Multi-Party Computation (MPC) protocols are difficult to
attain, as they require reasoning about the running time of cryptographic
adversaries and reductions. In this paper we close this gap by introducing a
new foundational approach that allows us to automatically compute concrete
security bounds for MPC protocols. We take inspiration from the meta-theory of
IPDL, a prior approach for formally verified distributed cryptography, to
support reasoning about the runtime of protocols and adversarial advantage. For
practical proof developments, we implement our approach in Maude, an extensible
logic for equational rewriting. We carry out four case studies of concrete
security for simulation-based proofs. Most notably, we deliver the first formal
verification of the GMW MPC protocol over N parties. To our knowledge, this is
the first time that formally verified concrete security bounds are computed for
a proof of an MPC protocol in the style of Universal Composability. Our tool
provides a layer of abstraction that allows the user to write proofs at a high
level, which drastically simplifies the proof size. For comparison, a case
study that in prior works required 2019 LoC only takes 567 LoC, thus reducing
proof size by 72%

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [4] [Fuzzing: Randomness? Reasoning! Efficient Directed Fuzzing via Large Language Models](https://arxiv.org/abs/2507.22065)
*Xiaotao Feng,Xiaogang Zhu,Kun Hu,Jincheng Wang,Yingjie Cao,Guang Gong,Jianfeng Pan*

Main category: cs.SE

TL;DR: 论文提出RandLuzz，利用大语言模型（LLMs）减少模糊测试中的随机性，通过生成可达种子和构建针对特定错误的变异器，显著提高错误检测效率。


<details>
  <summary>Details</summary>
Motivation: 模糊测试中的随机性虽然有助于发现错误，但降低了效率。即使定向模糊测试减少了随机性，种子和变异器的随机性仍影响效率。

Method: 利用LLMs生成可达种子和构建针对特定错误的变异器，结合定向模糊测试工具RandLuzz。

Result: RandLuzz在四种定向模糊测试工具上平均提速2.1×至4.8×，并在60秒内暴露8个错误。

Conclusion: RandLuzz通过LLMs减少随机性，显著提高了模糊测试的效率和错误检测能力。

Abstract: Fuzzing is highly effective in detecting bugs due to the key contribution of
randomness. However, randomness significantly reduces the efficiency of
fuzzing, causing it to cost days or weeks to expose bugs. Even though directed
fuzzing reduces randomness by guiding fuzzing towards target buggy locations,
the dilemma of randomness still challenges directed fuzzers. Two critical
components, which are seeds and mutators, contain randomness and are closely
tied to the conditions required for triggering bugs. Therefore, to address the
challenge of randomness, we propose to use large language models (LLMs) to
remove the randomness in seeds and reduce the randomness in mutators. With
their strong reasoning and code generation capabilities, LLMs can be used to
generate reachable seeds that target pre-determined locations and to construct
bug-specific mutators tailored for specific bugs. We propose RandLuzz, which
integrates LLMs and directed fuzzing, to improve the quality of seeds and
mutators, resulting in efficient bug exposure. RandLuzz analyzes function call
chain or functionality to guide LLMs in generating reachable seeds. To
construct bug-specific mutators, RandLuzz uses LLMs to perform bug analysis,
obtaining information such as bug causes and mutation suggestions, which
further help generate code that performs bug-specific mutations. We evaluate
RandLuzz by comparing it with four state-of-the-art directed fuzzers, AFLGo,
Beacon, WindRanger, and SelectFuzz. With RandLuzz-generated seeds, the fuzzers
achieve an average speedup ranging from 2.1$\times$ to 4.8$\times$ compared to
using widely-used initial seeds. Additionally, when evaluated on individual
bugs, RandLuzz achieves up to a 2.7$\times$ speedup compared to the
second-fastest exposure. On 8 bugs, RandLuzz can even expose them within 60
seconds.

</details>


### [5] [Automated Test Data Generation for Enterprise Protobuf Systems: A Metaclass-Enhanced Statistical Approach](https://arxiv.org/abs/2507.22070)
*Y. Du*

Main category: cs.SE

TL;DR: 提出了一种基于Python元类系统和生产日志统计分析的测试数据生成框架，显著提升了企业级protobuf系统的测试效率和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 企业级protobuf系统的复杂嵌套数据结构对性能测试提出了挑战，传统测试数据生成方法难以应对。

Method: 结合自动模式内省、统计值分布分析和递归下降算法，动态增强类型并提取真实值域。

Result: 在三个实际企业系统中，测试数据准备时间减少95%，测试覆盖率提高80%，支持15层嵌套结构并快速生成10万+测试用例。

Conclusion: 该框架有效解决了复杂protobuf结构的测试数据生成问题，显著提升了测试效率。

Abstract: Large-scale enterprise systems utilizing Protocol Buffers (protobuf) present
significant challenges for performance testing, particularly when targeting
intermediate business interfaces with complex nested data structures.
Traditional test data generation approaches are inadequate for handling the
intricate hierarchical and graph-like structures inherent in enterprise
protobuf schemas. This paper presents a novel test data generation framework
that leverages Python's metaclass system for dynamic type enhancement and
statistical analysis of production logs for realistic value domain extraction.
Our approach combines automatic schema introspection, statistical value
distribution analysis, and recursive descent algorithms for handling deeply
nested structures. Experimental evaluation on three real-world enterprise
systems demonstrates up to 95\% reduction in test data preparation time and
80\% improvement in test coverage compared to existing approaches. The
framework successfully handles protobuf structures with up to 15 levels of
nesting and generates comprehensive test suites containing over 100,000 test
cases within seconds.

</details>


### [6] [TypyBench: Evaluating LLM Type Inference for Untyped Python Repositories](https://arxiv.org/abs/2507.22086)
*Honghua Dong,Jiacheng Yang,Xun Deng,Yuhe Jiang,Gennady Pekhimenko,Fan Long,Xujie Si*

Main category: cs.SE

TL;DR: TypyBench是一个评估大型语言模型（LLMs）在Python代码库中进行类型推断能力的基准测试，提出了TypeSim和TypeCheck两个新指标，发现LLMs在复杂嵌套类型和类型一致性方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 动态语言（如Python）的类型推断是一个持续挑战，LLMs在此领域的能力尚未充分探索。

Method: 引入TypyBench基准测试，包含TypeSim（捕捉预测类型与真实类型间的语义关系）和TypeCheck（评估代码库中的类型一致性）。

Result: LLMs在TypeSim上表现尚可，但在复杂嵌套类型和类型一致性上存在显著问题。

Conclusion: 未来研究应关注代码库级别的类型一致性，而非仅改进类型相似性。TypyBench为此提供了基础。

Abstract: Type inference for dynamic languages like Python is a persistent challenge in
software engineering. While large language models (LLMs) have shown promise in
code understanding, their type inference capabilities remain underexplored. We
introduce TypyBench, a benchmark designed to evaluate LLMs' type inference
across entire Python repositories. TypyBench features two novel metrics:
TypeSim, which captures nuanced semantic relationships between predicted and
ground truth types, and TypeCheck, which assesses type consistency across
codebases. Our evaluation of various LLMs on a curated dataset of 50
high-quality Python repositories reveals that, although LLMs achieve decent
TypeSim scores, they struggle with complex nested types and exhibit significant
type consistency errors. These findings suggest that future research should
shift focus from improving type similarity to addressing repository-level
consistency. TypyBench provides a foundation for this new direction, offering
insights into model performance across different type complexities and usage
contexts. Our code and data are available at
https://github.com/typybench/typybench.

</details>


### [7] [RedCoder: Automated Multi-Turn Red Teaming for Code LLMs](https://arxiv.org/abs/2507.22063)
*Wenjie Jacky Mo,Qin Liu,Xiaofei Wen,Dongwon Jung,Hadi Askari,Wenxuan Zhou,Zhe Zhao,Muhao Chen*

Main category: cs.SE

TL;DR: RedCoder是一个多轮对话红队代理，通过模拟对抗交互和多轮对话诱导代码生成模型生成漏洞代码，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有红队方法依赖人工且忽略多轮交互，难以扩展和实用。

Method: 通过多代理游戏模拟对抗交互生成原型对话和攻击策略，微调LLM作为RedCoder核心，动态检索策略诱导漏洞。

Result: 实验表明RedCoder在多轮对话中诱导漏洞的效果优于现有方法。

Conclusion: RedCoder为评估代码生成系统的安全性提供了可扩展且有效的工具。

Abstract: Large Language Models (LLMs) for code generation (i.e., Code LLMs) have
demonstrated impressive capabilities in AI-assisted software development and
testing. However, recent studies have shown that these models are prone to
generating vulnerable or even malicious code under adversarial settings.
Existing red-teaming approaches rely on extensive human effort, limiting their
scalability and practicality, and generally overlook the interactive nature of
real-world AI-assisted programming, which often unfolds over multiple turns. To
bridge these gaps, we present RedCoder, a red-teaming agent that engages victim
models in multi-turn conversation to elicit vulnerable code. The pipeline to
construct RedCoder begins with a multi-agent gaming process that simulates
adversarial interactions, yielding a set of prototype conversations and an
arsenal of reusable attack strategies. We then fine-tune an LLM on these
prototype conversations to serve as the backbone of RedCoder. Once deployed,
RedCoder autonomously engages Code LLMs in multi-turn conversations,
dynamically retrieving relevant strategies from the arsenal to steer the
dialogue toward vulnerability-inducing outputs. Experiments across multiple
Code LLMs show that our approach outperforms prior single-turn and multi-turn
red-team methods in inducing vulnerabilities in code generation, offering a
scalable and effective tool for evaluating the security boundaries of modern
code-generation systems.

</details>


### [8] [Machine Learning Experiences: A story of learning AI for use in enterprise software testing that can be used by anyone](https://arxiv.org/abs/2507.22064)
*Michael Cohoon,Debbie Furman*

Main category: cs.SE

TL;DR: 本文描述了一组专注于软件测试的团队在机器学习（ML）项目中的工作流程，类似于CRISP-DM过程，包括数据收集、清洗、特征工程、数据分割、模型选择、训练、测试和性能评估。


<details>
  <summary>Details</summary>
Motivation: 分享团队在软件测试中应用ML的经验，为其他项目提供可复用的工作流程。

Method: 采用类似CRISP-DM的ML工作流程，涵盖从数据准备到模型评估的完整步骤。

Result: 通过遵循此工作流程，团队成功将ML技术应用于软件测试项目。

Conclusion: 该工作流程具有普适性，可帮助任何人在ML项目中高效应用相关技术。

Abstract: This paper details the machine learning (ML) journey of a group of people
focused on software testing. It tells the story of how this group progressed
through a ML workflow (similar to the CRISP-DM process). This workflow consists
of the following steps and can be used by anyone applying ML techniques to a
project: gather the data; clean the data; perform feature engineering on the
data; splitting the data into two sets, one for training and one for testing;
choosing a machine learning model; training the model; testing the model and
evaluating the model performance. By following this workflow, anyone can
effectively apply ML to any project that they are doing.

</details>


### [9] [CodableLLM: Automating Decompiled and Source Code Mapping for LLM Dataset Generation](https://arxiv.org/abs/2507.22066)
*Dylan Manuel,Paul Rad*

Main category: cs.SE

TL;DR: CodableLLM是一个Python框架，用于自动化生成和整理数据集，通过将反编译的函数映射到原始源代码函数，提升反编译代码与源代码的对齐质量，支持多语言并集成现有工具。


<details>
  <summary>Details</summary>
Motivation: 解决代码理解和生成领域中高质量数据集生成的挑战，特别是反编译二进制文件与原始源代码的对齐问题。

Method: 设计并实现CodableLLM框架，支持多语言，集成现有反编译器和解析器，自动化数据集生成。

Result: CodableLLM在数据集生成中表现高效且稳健，优于现有工具。

Conclusion: CodableLLM为代码导向的大型语言模型提供了高效的数据集生成解决方案。

Abstract: The generation of large, high-quality datasets for code understanding and
generation remains a significant challenge, particularly when aligning
decompiled binaries with their original source code. To address this, we
present CodableLLM, a Python framework designed to automate the creation and
curation of datasets by mapping decompiled functions to their corresponding
source functions. This process enhances the alignment between decompiled and
source code representations, facilitating the development of large language
models (LLMs) capable of understanding and generating code across multiple
abstraction levels. CodableLLM supports multiple programming languages and
integrates with existing decompilers and parsers to streamline dataset
generation. This paper presents the design and implementation of CodableLLM,
evaluates its performance in dataset creation, and compares it to existing
tools in the field. The results demonstrate that CodableLLM offers a robust and
efficient solution for generating datasets tailored for code-focused LLMS.

</details>


### [10] [Analyzing and Evaluating the Behavior of Git Diff and Merge](https://arxiv.org/abs/2507.22071)
*Niels Glodny*

Main category: cs.SE

TL;DR: 论文探讨了Git中diff和merge算法的功能及其意外行为，包括单行更改导致整个文件标记为更改、默认merge策略可能导致指数时间消耗等问题。


<details>
  <summary>Details</summary>
Motivation: 尽管Git被广泛使用，但其协作算法（如diff和merge）的具体行为并未被充分理解，这些算法在其他场景中也有潜在应用。

Method: 通过文档化Git的主要功能（如diff计算、merge运行方式）并分析其行为，揭示了算法中的多个意外现象。

Result: 发现Git的histogram diff算法存在极端情况，默认merge策略（ort）可能消耗指数时间，merge和rebase不满足交换律，且merge结果依赖于diff算法。

Conclusion: Git的diff和merge算法存在未预期的行为，这些发现对理解和改进Git及其他类似工具具有重要意义。

Abstract: Despite being widely used, the algorithms that enable collaboration with Git
are not well understood. The diff and merge algorithms are particularly
interesting, as they could be applied in other contexts. In this thesis, I
document the main functionalities of Git: how diffs are computed, how they are
used to run merges, and how merges enable more complex operations. In the
process, I show multiple unexpected behaviors in Git, including the following:
The histogram diff algorithm has pathological cases where a single-line change
can cause the entire rest of the file to be marked as changed. The default
merge strategy (ort) can result in merges requiring exponential time in the
number of commits in the history. Merges and rebases are not commutative, and
even when merges do not result in a conflict, the result is not specified but
depends on the diff algorithm used. And finally, sometimes when two sides of a
merge add different lines at the same position, the result is not a conflict,
but a merge containing both changes after each other, in arbitrary order.

</details>


### [11] [CodeEvo: Interaction-Driven Synthesis of Code-centric Data through Hybrid and Iterative Feedback](https://arxiv.org/abs/2507.22080)
*Qiushi Sun,Jinyang Gong,Lei Li,Qipeng Guo,Fei Yuan*

Main category: cs.SE

TL;DR: CodeEvo框架通过两个LLM代理（Coder和Reviewer）的迭代交互合成高质量代码数据，结合编译器确定性和生成灵活性，显著提升代码生成模型的性能。


<details>
  <summary>Details</summary>
Motivation: 手动获取高质量的指令-代码对成本高且规模有限，现有方法缺乏严格的数据验证，导致合成数据质量不佳。

Method: CodeEvo框架通过Coder生成候选代码和测试用例，Reviewer提供新指令和反馈，结合混合反馈机制进行质量控制。

Result: 实验表明，基于CodeEvo数据微调的模型在多种代码生成基准测试中显著优于基线。

Conclusion: CodeEvo为代码中心数据合成提供了有效方法，并通过多角度分析提供了深入见解。

Abstract: Acquiring high-quality instruction-code pairs is essential for training Large
Language Models (LLMs) for code generation. Manually curated data is expensive
and inherently limited in scale, motivating the development of code-centric
synthesis methods. Yet, current approaches either focus on augmenting existing
code or rely on predefined heuristics, both lacking rigorous data validation,
which results in synthetic data that is ungrounded, repetitive, or overly
simplistic. Inspired by collaborative programming practices, we propose
CodeEvo, a framework that synthesizes code data through iterative interactions
between two LLM agents: a Coder, which generates candidate code and test cases
based on given instructions, and a Reviewer, which guides the synthesis process
by producing new instructions and feedback. We further introduce a hybrid
feedback mechanism that combines compiler determinism with the generative
flexibility of agents, enabling automatic quality control throughout synthesis.
Extensive experiments demonstrate that models fine-tuned on CodeEvo data
significantly outperform established baselines across code generation
benchmarks with various difficulties. In-depth analyses further provide
insights from multiple perspectives into effective code-centric data synthesis.

</details>


### [12] [BOOP: Write Right Code](https://arxiv.org/abs/2507.22085)
*Vaani Goenka,Aalok D. Thakkar*

Main category: cs.SE

TL;DR: BOOP框架通过强制四个阶段（规范、算法开发、实现和证明）来改善新手程序员的系统化推理能力，减少试错调试。


<details>
  <summary>Details</summary>
Motivation: 新手程序员常依赖语法和测试驱动的方法，而AI工具可能提供语法正确但概念错误的解决方案，导致缺乏系统化推理。

Method: BOOP框架包含四个强制阶段：形式化规范、语言无关算法开发、实现和正确性证明，通过VS Code扩展强制执行。

Result: 初步评估显示学生算法推理能力提升，试错减少，对边界条件和问题分解的理解增强。

Conclusion: BOOP框架有效提升学生的基础技能，尽管初期可能显得冗长，但效果优于传统方法。

Abstract: Novice programmers frequently adopt a syntax-specific and test-case-driven
approach, writing code first and adjusting until programs compile and test
cases pass, rather than developing correct solutions through systematic
reasoning. AI coding tools exacerbate this challenge by providing syntactically
correct but conceptually flawed solutions. In this paper, we introduce BOOP
(Blueprint, Operations, OCaml, Proof), a structured framework requiring four
mandatory phases: formal specification, language-agnostic algorithm
development, implementation, and correctness proof. This shifts focus from
``making code work'' to understanding why code is correct.
  BOOP was implemented at our institution using a VS Code extension and
preprocessor that enforces constraints and identifies counterproductive
patterns. Initial evaluation shows improved algorithmic reasoning and reduced
trial-and-error debugging. Students reported better edge case understanding and
problem decomposition, though some initially found the format verbose.
Instructors observed stronger foundational skills compared to traditional
approaches.

</details>


### [13] [Secure coding for web applications: Frameworks, challenges, and the role of LLMs](https://arxiv.org/abs/2507.22223)
*Kiana Kiashemshaki,Mohammad Jalili Torkamani,Negin Mahmoudi*

Main category: cs.SE

TL;DR: 本文综述了安全编码实践，比较了主要框架和领域，探讨了LLMs在安全代码评估中的作用，并提供了实际案例研究。


<details>
  <summary>Details</summary>
Motivation: 尽管安全编码至关重要，但其实际应用仍因组织、教育和技术障碍而不一致。

Method: 通过框架比较、威胁分类（基于OWASP Top 10）和LLMs的应用案例研究。

Result: 提供了可复现的案例研究，涵盖四种主要漏洞类型，为研究人员和开发者提供实用见解。

Conclusion: 本文为将安全编码整合到实际开发流程中提供了实用指导。

Abstract: Secure coding is a critical yet often overlooked practice in software
development. Despite extensive awareness efforts, real-world adoption remains
inconsistent due to organizational, educational, and technical barriers. This
paper provides a comprehensive review of secure coding practices across major
frameworks and domains, including web development, DevSecOps, and cloud
security. It introduces a structured framework comparison and categorizes
threats aligned with the OWASP Top 10. Additionally, we explore the rising role
of Large Language Models (LLMs) in evaluating and recommending secure code,
presenting a reproducible case study across four major vulnerability types.
This paper offers practical insights for researchers, developers, and educators
on integrating secure coding into real-world development processes.

</details>


### [14] [From Articles to Code: On-Demand Generation of Core Algorithms from Scientific Publications](https://arxiv.org/abs/2507.22324)
*Cameron S. Movassaghi,Amanda Momenzadeh,Jesse G. Meyer*

Main category: cs.SE

TL;DR: 论文提出利用科学文献中的方法描述作为LLM的独立规范，实现按需代码生成，可能替代人工维护的软件包。


<details>
  <summary>Details</summary>
Motivation: 减少软件包维护成本（如依赖管理、错误修复和版本控制），探索LLM在代码生成中的潜力。

Method: 通过让先进LLM（如GPT-4-mini-high、Gemini Pro 2.5、Claude Sonnet 4）根据科学文献实现核心算法，并进行性能测试。

Result: 当前LLM能可靠生成功能与常规库无异的代码，性能相当。

Conclusion: 未来可能转向灵活的按需代码生成，减少对静态人工维护包的依赖，降低维护成本。

Abstract: Maintaining software packages imposes significant costs due to dependency
management, bug fixes, and versioning. We show that rich method descriptions in
scientific publications can serve as standalone specifications for modern large
language models (LLMs), enabling on-demand code generation that could supplant
human-maintained libraries. We benchmark state-of-the-art models
(GPT-o4-mini-high, Gemini Pro 2.5, Claude Sonnet 4) by tasking them with
implementing a diverse set of core algorithms drawn from original publications.
Our results demonstrate that current LLMs can reliably reproduce package
functionality with performance indistinguishable from conventional libraries.
These findings foreshadow a paradigm shift toward flexible, on-demand code
generation and away from static, human-maintained packages, which will result
in reduced maintenance overhead by leveraging published articles as sufficient
context for the automated implementation of analytical workflows.

</details>


### [15] [AutoCodeSherpa: Symbolic Explanations in AI Coding Agents](https://arxiv.org/abs/2507.22414)
*Sungmin Kang,Haifeng Ruan,Abhik Roychoudhury*

Main category: cs.SE

TL;DR: LLM代理通过程序分析工具提升软件工程任务中的补丁质量，并提供符号公式形式的解释以增强开发者理解和自动化工作流程。


<details>
  <summary>Details</summary>
Motivation: 为了在生产环境中部署LLM代理，需要高置信度的软件工件（如补丁）及其解释。

Method: 提出一种工作流，代理通过属性测试（PBT）和程序内部符号表达式提供错误解释（输入、感染和输出条件）。

Result: 解释可执行，支持开发者生成具体输入以重现问题，并用于自动化补丁验证。

Conclusion: 程序分析驱动的解释可提升LLM修复技术的输出质量，适用于自动化环境。

Abstract: Large Language Model (LLM) agents autonomously use external tools on top of
one or more LLMs to accomplish specific tasks. Lately LLM agents for software
engineering tasks have become popular. These agents can benefit from the use of
program analysis tools working on program representations. This is demonstrated
by existing agentic AI solutions such as AutoCodeRover or SpecRover which
perform automated program repair. Specifically the goal of these works is to
use program analysis to improve the patch quality. These agents are currently
being used to automatically fix static analysis issues from the widely used
SonarQube static analyzer.
  Nevertheless, for the agents to be deployed in a production environment,
agents need to suggest software artifacts, such as patches, with evidence and
with high confidence. In this work, we provide a workflow where an agent
provides explanations of the bug in the form of symbolic formulae. The
explanations are in the form of input conditions, infection conditions and
output conditions, implemented as property based tests (PBT) and
program-internal symbolic expressions. These can help in human developer
cognition of the agent outputs as well as in achieving completely automated
agentic workflows for software. The human developer can benefit from the input
condition, represented as a PBT, to generate various concrete inputs showing a
given issue. Furthermore, since the PBTs are executable, our explanations are
executable as well. We can thus also use the explanations in a completely
automated issue resolution environment for accepting or rejecting the patches
that are suggested by patching agents such as AutoCodeRover. Finally, as
agentic AI approaches continue to develop, the program analysis driven
explanations can be provided to other LLM-based repair techniques such as
Agentless to improve their output.

</details>


### [16] [Ensemble Fuzzing with Dynamic Resource Scheduling and Multidimensional Seed Evaluation](https://arxiv.org/abs/2507.22442)
*Yukai Zhao,Shaohua Wang,Jue Wang,Xing Hu,Xin Xia*

Main category: cs.SE

TL;DR: Legion是一个新型的集成模糊测试框架，通过动态资源调度和多维种子评估策略，显著提升了模糊测试的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有集成模糊测试技术在资源调度和性能评估方面存在不足，导致资源浪费，需要更高效的解决方案。

Method: 提出基于上置信界算法的资源调度算法和多维种子评估策略，动态优化资源分配。

Result: 在Google模糊测试套件和实际开源项目中，Legion检测到20个漏洞，包括5个未知漏洞和3个CVE漏洞，优于现有技术。

Conclusion: Legion通过动态资源调度和全面性能评估，显著提升了集成模糊测试的效果，具有实际应用价值。

Abstract: Fuzzing is widely used for detecting bugs and vulnerabilities, with various
techniques proposed to enhance its effectiveness. To combine the advantages of
multiple technologies, researchers proposed ensemble fuzzing, which integrates
multiple base fuzzers. Despite promising results, state-of-the-art ensemble
fuzzing techniques face limitations in resource scheduling and performance
evaluation, leading to unnecessary resource waste. In this paper, we propose
Legion, a novel ensemble fuzzing framework that dynamically schedules resources
during the ensemble fuzzing campaign. We designed a novel resource scheduling
algorithm based on the upper confidence bound algorithm to reduce the resource
consumption of ineffective base fuzzers. Additionally, we introduce a
multidimensional seed evaluation strategy, which considers multiple metrics to
achieve more comprehensive fine-grained performance evaluation. We implemented
Legion as a prototype tool and evaluated its effectiveness on Google's
fuzzer-test-suite as well as real-world open-source projects. Results show that
Legion outperforms existing state-of-the-art base fuzzers and ensemble fuzzing
techniques, detecting 20 vulnerabilities in real-world open-source
projects-five previously unknown and three classified as CVEs.

</details>


### [17] [Inside madupite: Technical Design and Performance](https://arxiv.org/abs/2507.22538)
*Matilde Gargiani,Robin Sieber,Philip Pawlowsky,John Lygeros*

Main category: cs.SE

TL;DR: Madupite是一种新型高性能求解器，专为大规模折扣无限时域马尔可夫决策过程设计，具有分布式计算和定制化算法的特点。


<details>
  <summary>Details</summary>
Motivation: 解决现有求解器无法高效处理大规模马尔可夫决策过程的问题，尤其是在内存受限和接近无折扣设置的情况下。

Method: 基于数学优化方法，采用分布式计算技术，支持用户定制算法以加速收敛。

Result: Madupite能够高效计算精确解，适用于大规模问题，并在流行病学和控制等领域展示了卓越性能。

Conclusion: Madupite在解决大规模马尔可夫决策过程方面具有显著优势，提供了前所未有的可扩展性和灵活性。

Abstract: In this work, we introduce and benchmark madupite, a newly proposed
high-performance solver designed for large-scale discounted infinite-horizon
Markov decision processes with finite state and action spaces. After a brief
overview of the class of mathematical optimization methods on which madupite
relies, we provide details on implementation choices, technical design and
deployment. We then demonstrate its scalability and efficiency by showcasing
its performance on the solution of Markov decision processes arising from
different application areas, including epidemiology and classical control.
Madupite sets a new standard as, to the best of our knowledge, it is the only
solver capable of efficiently computing exact solutions for large-scale Markov
decision processes, even when these exceed the memory capacity of modern
laptops and operate in near-undiscounted settings. This is possible as madupite
can work in a fully distributed manner and therefore leverage the memory
storage and computation capabilities of modern high-performance computing
clusters. This key feature enables the solver to efficiently handle problems of
medium to large size in an exact manner instead of necessarily resorting to
function approximations. Moreover, madupite is unique in allowing users to
customize the solution algorithm to better exploit the specific structure of
their problem, significantly accelerating convergence especially in
large-discount factor settings. Overall, madupite represents a significant
advancement, offering unmatched scalability and flexibility in solving
large-scale Markov decision processes.

</details>


### [18] [RePaCA: Leveraging Reasoning Large Language Models for Static Automated Patch Correctness Assessment](https://arxiv.org/abs/2507.22580)
*Marcos Fuster-Pena,David de-Fitero-Dominguez,Antonio Garcia-Cabot,Eva Garcia-Lopez*

Main category: cs.SE

TL;DR: RePaCA是一种基于大型语言模型（LLM）的静态APCA技术，通过分析代码差异和推理补丁的正确性，显著提高了准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有静态APCA技术在可靠性、灵活性和透明度方面存在不足，需要一种更有效的方法来识别过拟合补丁。

Method: 利用专门用于思考任务的LLM，通过提示输入错误和修复后的代码片段，生成思维链分析补丁，并使用强化学习进行微调。

Result: 在标准测试中达到83.1%的准确率和84.8%的F1分数，并展示了优越的泛化能力。

Conclusion: 微调后的LLM在静态APCA任务中表现出色，显著提升了准确性、泛化能力和可解释性。

Abstract: Automated Program Repair (APR) seeks to automatically correct software bugs
without requiring human intervention. However, existing tools tend to generate
patches that satisfy test cases without fixing the underlying bug, those are
known as overfitting patches. To address this issue, Automated Patch
Correctness Assessment (APCA) attempts to identify overfitting patches
generated by APR tools. It can be solved as a static approach, meaning that no
additional information is needed beyond the original and fixed code snippets.
Current static techniques often struggle with reliability, flexibility and
transparency. To address these issues, we introduce RePaCA, a novel static APCA
technique that leverages Large Language Models (LLMs) specialized in thinking
tasks. Our model is prompted with both buggy and fixed code snippets and guided
to generate a Chain of Thought that analyses code differences, reasons about
how the patch addresses the root cause, and ultimately provides a binary
classification: correct or overfitting. To enhance these reasoning capabilities
for the APCA task specifically, the LLM is finetuned using Reinforcement
Learning with the Group Relative Policy Optimization algorithm. When evaluated
on a standard Defects4J-derived test, our approach achieves state-of-the-art
performance, with 83.1% accuracy and an 84.8% F1-score. Furthermore, our model
demonstrates superior generalization capabilities when trained on different
datasets, outperforming the leading technique. This reasoning capability also
provides enhanced explainability for the patch assessment. These findings
underscore the considerable promise of finetuned, reasoning LLMs to advance
static APCA by enhancing accuracy, generalization, and explainability.

</details>


### [19] [Metamorphic Testing of Deep Code Models: A Systematic Literature Review](https://arxiv.org/abs/2507.22610)
*Ali Asgari,Milan de Koning,Pouria Derakhshanfar,Annibale Panichella*

Main category: cs.SE

TL;DR: 本文综述了变形测试在深度代码模型中的应用，分析了45篇论文，总结了当前的研究现状、挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 深度代码模型在软件工程中表现优异，但其鲁棒性仍需验证，尤其是在对抗性条件下。变形测试成为评估模型鲁棒性的重要方法。

Method: 通过系统文献综述，分析45篇论文中的变形测试技术、评估方法和应用场景。

Result: 总结了常用的模型、编程任务、数据集、目标语言和评估指标，并识别了当前研究中的关键挑战。

Conclusion: 变形测试是评估深度代码模型鲁棒性的有效方法，未来研究需进一步解决现有挑战并拓展应用场景。

Abstract: Large language models and deep learning models designed for code intelligence
have revolutionized the software engineering field due to their ability to
perform various code-related tasks. These models can process source code and
software artifacts with high accuracy in tasks such as code completion, defect
detection, and code summarization; therefore, they can potentially become an
integral part of modern software engineering practices. Despite these
capabilities, robustness remains a critical quality attribute for deep-code
models as they may produce different results under varied and adversarial
conditions (e.g., variable renaming). Metamorphic testing has become a widely
used approach to evaluate models' robustness by applying semantic-preserving
transformations to input programs and analyzing the stability of model outputs.
While prior research has explored testing deep learning models, this systematic
literature review focuses specifically on metamorphic testing for deep code
models. By studying 45 primary papers, we analyze the transformations,
techniques, and evaluation methods used to assess robustness. Our review
summarizes the current landscape, identifying frequently evaluated models,
programming tasks, datasets, target languages, and evaluation metrics, and
highlights key challenges and future directions for advancing the field.

</details>


### [20] [A Systematic Literature Review on Detecting Software Vulnerabilities with Large Language Models](https://arxiv.org/abs/2507.22659)
*Sabrina Kaniewski,Fabian Schmidt,Markus Enzweiler,Michael Menth,Tobias Heer*

Main category: cs.SE

TL;DR: 该论文通过系统文献综述分析了基于LLM的软件漏洞检测研究，总结了227项研究，提出了分类框架，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 由于LLM在软件工程中的广泛应用，相关研究碎片化严重，难以比较和总结现状。

Method: 对2020年至2025年间的227项研究进行系统文献综述，按任务、输入、架构和技术分类，并分析数据集。

Result: 提出了漏洞检测方法的细粒度分类，识别了关键限制，并提出了未来研究方向。

Conclusion: 通过结构化综述，提高了研究透明度，为后续研究提供了实用指南，并公开了所有资源。

Abstract: The increasing adoption of Large Language Models (LLMs) in software
engineering has sparked interest in their use for software vulnerability
detection. However, the rapid development of this field has resulted in a
fragmented research landscape, with diverse studies that are difficult to
compare due to differences in, e.g., system designs and dataset usage. This
fragmentation makes it difficult to obtain a clear overview of the
state-of-the-art or compare and categorize studies meaningfully. In this work,
we present a comprehensive systematic literature review (SLR) of LLM-based
software vulnerability detection. We analyze 227 studies published between
January 2020 and June 2025, categorizing them by task formulation, input
representation, system architecture, and adaptation techniques. Further, we
analyze the datasets used, including their characteristics, vulnerability
coverage, and diversity. We present a fine-grained taxonomy of vulnerability
detection approaches, identify key limitations, and outline actionable future
research opportunities. By providing a structured overview of the field, this
review improves transparency and serves as a practical guide for researchers
and practitioners aiming to conduct more comparable and reproducible research.
We publicly release all artifacts and maintain a living repository of LLM-based
software vulnerability detection studies.

</details>


### [21] [RobEthiChor: Automated Context-aware Ethics-based Negotiation for Autonomous Robots](https://arxiv.org/abs/2507.22664)
*Mashal Afzal Memon,Gianluca Filippone,Gian Luca Scoccia,Marco Autili,Paola Inverardi*

Main category: cs.SE

TL;DR: 论文提出RobEthiChor方法，使自主系统能通过基于伦理的协商整合用户伦理偏好和情境因素，提升决策个性化。


<details>
  <summary>Details</summary>
Motivation: 自主系统缺乏整合用户伦理偏好的能力，影响用户信任和个性化决策。

Method: 提出RobEthiChor架构及其实现在ROS中的RobEthiChor-Ros，支持伦理协商。

Result: 实验显示73%场景下机器人能达成协议，平均协商时间0.67秒，方法具有可扩展性。

Conclusion: RobEthiChor有效实现了基于伦理的协商，提升了自主系统的伦理决策能力。

Abstract: The presence of autonomous systems is growing at a fast pace and it is
impacting many aspects of our lives. Designed to learn and act independently,
these systems operate and perform decision-making without human intervention.
However, they lack the ability to incorporate users' ethical preferences, which
are unique for each individual in society and are required to personalize the
decision-making processes. This reduces user trust and prevents autonomous
systems from behaving according to the moral beliefs of their end-users. When
multiple systems interact with differing ethical preferences, they must
negotiate to reach an agreement that satisfies the ethical beliefs of all the
parties involved and adjust their behavior consequently. To address this
challenge, this paper proposes RobEthiChor, an approach that enables autonomous
systems to incorporate user ethical preferences and contextual factors into
their decision-making through ethics-based negotiation. RobEthiChor features a
domain-agnostic reference architecture for designing autonomous systems capable
of ethic-based negotiating. The paper also presents RobEthiChor-Ros, an
implementation of RobEthiChor within the Robot Operating System (ROS), which
can be deployed on robots to provide them with ethics-based negotiation
capabilities. To evaluate our approach, we deployed RobEthiChor-Ros on real
robots and ran scenarios where a pair of robots negotiate upon resource
contention. Experimental results demonstrate the feasibility and effectiveness
of the system in realizing ethics-based negotiation. RobEthiChor allowed robots
to reach an agreement in more than 73\% of the scenarios with an acceptable
negotiation time (0.67s on average). Experiments also demonstrate that the
negotiation approach implemented in RobEthiChor is scalable.

</details>


### [22] [The Multi-Agent Fault Localization System Based on Monte Carlo Tree Search Approach](https://arxiv.org/abs/2507.22800)
*Rui Ren*

Main category: cs.SE

TL;DR: 论文提出KnowledgeMind，一种基于蒙特卡洛树搜索和知识库奖励机制的LLM多智能体系统，用于解决微服务系统中根因分析的挑战。


<details>
  <summary>Details</summary>
Motivation: 微服务系统的灵活性和解耦性增加了可靠性挑战，现有LLM方法因幻觉和异常传播导致定位不准，且上下文窗口限制大。

Method: 结合蒙特卡洛树搜索和知识库奖励机制，实现标准化服务级推理，减少上下文窗口负担并抑制幻觉。

Result: 相比现有方法，上下文窗口需求减少90%，根因定位准确率提升49.29%至128.35%。

Conclusion: KnowledgeMind通过多智能体系统和奖励机制，显著提升了微服务系统中根因分析的效率和准确性。

Abstract: In real-world scenarios, due to the highly decoupled and flexible nature of
microservices, it poses greater challenges to system reliability. The more
frequent occurrence of incidents has created a demand for Root Cause
Analysis(RCA) methods that enable rapid identification and recovery of
incidents. Large language model (LLM) provides a new path for quickly locating
and recovering from incidents by leveraging their powerful generalization
ability combined with expert experience. Current LLM for RCA frameworks are
based on ideas like ReAct and Chain-of-Thought, but the hallucination of LLM
and the propagation nature of anomalies often lead to incorrect localization
results. Moreover, the massive amount of anomalous information generated in
large, complex systems presents a huge challenge for the context window length
of LLMs. To address these challenges, we propose KnowledgeMind, an innovative
LLM multi-agent system based on Monte Carlo Tree Search and a knowledge base
reward mechanism for standardized service-by-service reasoning. Compared to
State-Of-The-Art(SOTA) LLM for RCA methods, our service-by-service exploration
approach significantly reduces the burden on the maximum context window length,
requiring only one-tenth of its size. Additionally, by incorporating a
rule-based real-time reward mechanism, our method effectively mitigates
hallucinations during the inference process. Compared to the SOTA LLM for RCA
framework, our method achieves a 49.29% to 128.35% improvement in root cause
localization accuracy.

</details>


### [23] [Repair-R1: Better Test Before Repair](https://arxiv.org/abs/2507.22853)
*Haichuan Hu,Xiaochen Xie,Quanjun Zhang*

Main category: cs.SE

TL;DR: Repair-R1是一种改进的自动程序修复（APR）方法，通过在训练阶段引入测试用例并优先生成测试，提高了修复效果。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-based APR方法在训练阶段未利用测试用例，且测试验证滞后，限制了修复效果。

Method: Repair-R1在训练阶段引入测试用例，先生成区分性测试用例，再进行修复，并使用RL协同优化测试生成和修复。

Result: 在四个基准测试中，Repair-R1显著提高了修复成功率（2.68%至48.29%）、测试生成成功率（16.38%至53.28%）和测试覆盖率（0.78%至53.96%）。

Conclusion: Repair-R1通过优化测试用例的使用顺序和方式，显著提升了APR的效果。

Abstract: APR (Automated Program Repair) aims to automatically locate program defects,
generate patches and validate the repairs. Existing techniques for APR are
often combined with LLMs (Large Language Models), which leverages the
code-related knowledge of LLMs to improve repair effectiveness. Current
LLM-based APR methods typically utilize test cases only during the inference
stage, adopting an iterative approach that performs repair first and validates
it through test execution afterward. This conventional paradigm neglects two
important aspects: the potential contribution of test cases in the training
phase, and the possibility of leveraging testing prior to repair. To address
this, we propose Repair-R1, which introduces test cases into the model's
training phase and shifts test generation to precede repair. The model is
required to first generate discriminative test cases that can distinguish
defective behaviors, and then perform repair based on these tests. This enables
the model to better locate defects and understand the underlying causes of
defects, thereby improving repair effectiveness. We implement Repair-R1 with
three different backbone models, using RL (reinforcement learning) to
co-optimize test generation and bug repair. Experimental results on four widely
adopted benchmarks demonstrate the superiority of Repair-R1. Specially,
compared to vanilla models, Repair-R1 improves repair success rate by 2.68\% to
48.29\%, test generation success rate by 16.38\% to 53.28\%, and test coverage
by 0.78\% to 53.96\%. We publish the code and weights at
https://github.com/Tomsawyerhu/APR-RL and
https://huggingface.co/tomhu/Qwen3-4B-RL-5000-step.

</details>


### [24] [Tracking research software outputs in the UK](https://arxiv.org/abs/2507.22871)
*Domhnall Carlin,Austen Rainer*

Main category: cs.SE

TL;DR: 研究探讨了英国学术机构如何存储和注册研究软件，发现软件作为研究成果的报告比例较低，且共享率不高，多数链接无效或缺失。


<details>
  <summary>Details</summary>
Motivation: 开放科学的发展凸显了研究软件的重要性，但如何将其作为研究产出进行记录和共享仍存在问题。

Method: 通过分析英国研究创新署（UKRI）的Gateway to Research（GtR）元数据，调查英国公共资助的研究软件的存储和注册情况。

Result: 软件作为研究成果的报告比例低，45%的链接无效或缺失，GitHub是主要托管平台。

Conclusion: 缺乏共享导致研究软件可能沦为短期工具，影响科学的长期发展。

Abstract: Research software is crucial in the research process and the growth of Open
Science underscores the importance of accessing research artifacts, like data
and code, raising traceability challenges among outputs. While it is a clear
principle that research code, along with other essential outputs, should be
recognised as artifacts of the research process, the how of this principle
remains variable. This study examines where UK academic institutions store and
register software as a unique research output, searching the UKRI's Gateway to
Research (GtR) metadata for publicly funded research software in the UK. The
quantity of software reported as research outcomes remains low in proportion to
other categories. Artifact sharing appears low, with one-quarter of the
reported software having no links and 45% having either a missing or erroneous
URL. Of the valid URLs, we find the single largest category is Public
Commercial Code Repository, with GitHub being the host of 18% of all publicly
funded research software listed. These observations are contrasted with past
findings from 2023 and finally, we discuss the lack of artifact sharing in UK
research, with resulting implications for the maintenance and evolution of
research software. Without dissemination, research software risks demotion to a
transient artifact, useful only to meet short term research demands but
ultimately lost to the broader enterprise of science.

</details>
