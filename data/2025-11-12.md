<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 15]
- [cs.LO](#cs.LO) [Total: 6]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Dynamic Stability of LLM-Generated Code](https://arxiv.org/abs/2511.07463)
*Prateek Rajput,Abdoul Aziz Bonkoungou,Yewei Song,Abdoul Kader Kabore,Iyiola E. Olatunji,Jacques Klein,Tegewende Bissyande*

Main category: cs.PL

TL;DR: 提出了一个评估代码生成动态稳定性的框架，通过SCTD和DCTD指标来衡量算法结构和运行时行为的多样性，发现LLM在生成功能正确代码时存在显著的算法方差。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代码生成评估只关注功能正确性，忽略了算法复杂度差异。功能正确的解决方案可能在算法结构上差异很大，导致生产环境中性能成本显著不同。

Method: 提出了基于操作码分布的两个指标：静态规范轨迹差异(SCTD)捕获算法结构多样性，动态规范轨迹差异(DCTD)量化运行时行为方差。它们的比值行为表达因子(BEF)作为诊断信号。

Result: 在BigOBench和CodeContests上的实验表明，最先进的LLM即使在功能正确的输出中也表现出显著的算法方差。提高采样温度会改善pass@1率但降低稳定性。

Conclusion: 需要在代码生成中考虑稳定性感知目标，并建立包含渐近测试用例的新基准，以实现更稳健的LLM评估。

Abstract: Current evaluations of LLMs for code generation emphasize functional correctness, overlooking the fact that functionally correct solutions can differ significantly in algorithmic complexity. For instance, an $(O(n^2))$ versus $(O(n \log n))$ sorting algorithm may yield similar output but incur vastly different performance costs in production. This discrepancy reveals a critical limitation in current evaluation methods: they fail to capture the behavioral and performance diversity among correct solutions. To address this, we introduce a principled framework for evaluating the dynamic stability of generated code. We propose two metrics derived from opcode distributions: Static Canonical Trace Divergence (SCTD), which captures algorithmic structure diversity across generated solutions, and Dynamic Canonical Trace Divergence (DCTD), which quantifies runtime behavioral variance. Their ratio, the Behavioral Expression Factor (BEF), serves as a diagnostic signal: it indicates critical runtime instability when BEF $\ll$ 1 and functional redundancy when BEF $\gg$ 1. Empirical results on BigOBench and CodeContests show that state-of-the-art LLMs exhibit significant algorithmic variance even among functionally correct outputs. Notably, increasing sampling temperature improves pass@1 rates but degrades stability, revealing an unrecognized trade-off: searching for correct solutions in diverse output spaces introduces a "penalty of instability" between correctness and behavioral consistency. Our findings call for stability-aware objectives in code generation and new benchmarks with asymptotic test cases for robust, real-world LLM evaluation.

</details>


### [2] [Streaming Tensor Program: A streaming abstraction for dynamic parallelism](https://arxiv.org/abs/2511.07776)
*Gina Sohn,Genghan Zhang,Konstantin Hossfeld,Jungwoo Kim,Nathan Sobotka,Nathan Zhang,Olivia Hsu,Kunle Olukotun*

Main category: cs.PL

TL;DR: STeP是一个新的流式抽象，用于在空间数据流加速器上高效运行动态张量工作负载，通过引入灵活路由操作符、显式内存层次和符号形状语义来支持动态行为。


<details>
  <summary>Details</summary>
Motivation: 当前空间数据流加速器的编程抽象表达能力有限，无法有效处理动态形状张量和数据依赖控制流等动态行为，导致性能受限。

Method: 提出STeP流式抽象，包含灵活路由操作符、显式内存层次和符号形状语义，支持动态分块、动态并行化和配置时分复用等优化技术。

Result: 在代表性LLM层上，动态分块减少片上内存需求2.18倍，动态并行化提升延迟1.5倍，配置时分复用提升计算利用率2.57倍。

Conclusion: STeP能够有效适应动态行为，同时保持数据流效率，为动态张量工作负载在空间数据流加速器上的高效执行提供了解决方案。

Abstract: Dynamic behaviors are becoming prevalent in many tensor applications. In machine learning, for example, the input tensors are dynamically shaped or ragged, and data-dependent control flow is widely used in many models. However, the limited expressiveness of prior programming abstractions for spatial dataflow accelerators forces the dynamic behaviors to be implemented statically or lacks the visibility for performance-critical decisions. To address these challenges, we present the Streaming Tensor Program (STeP), a new streaming abstraction that enables dynamic tensor workloads to run efficiently on spatial dataflow accelerators. STeP introduces flexible routing operators, an explicit memory hierarchy, and symbolic shape semantics that expose dynamic data rates and tensor dimensions. These capabilities unlock new optimizations-dynamic tiling, dynamic parallelization, and configuration time-multiplexing-that adapt to dynamic behaviors while preserving dataflow efficiency. Using a cycle-approximate simulator on representative LLM layers with real-world traces, dynamic tiling reduces on-chip memory requirement by 2.18x, dynamic parallelization improves latency by 1.5x, and configuration time-multiplexing improves compute utilization by 2.57x over implementations available in prior abstractions.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [3] [Parikh Automata on Finite and Infinite Words](https://arxiv.org/abs/2307.07238)
*Mario Grobler,Leif Sabellek,Sebastian Siebertz*

Main category: cs.FL

TL;DR: 本文研究了有限和无限单词上的Parikh自动机，建立了有限单词的结果，提出了无限单词的多种定义，研究了确定性/非确定性变体、闭包性质、表达能力、决策问题及其在模型检查中的应用，并与其他计数机制模型进行了比较。


<details>
  <summary>Details</summary>
Motivation: 研究Parikh自动机在无限单词上的扩展，探索其在模型检查等领域的应用潜力，并与其他计数模型进行对比分析。

Method: 首先建立有限单词上Parikh自动机的结果，然后提出无限单词上Parikh自动机的多种定义，研究确定性/非确定性变体的性质，分析闭包性质、表达能力和决策问题。

Result: 建立了Parikh自动机在有限和无限单词上的理论框架，获得了关于闭包性质、表达能力和决策问题的结果，展示了在模型检查中的应用价值。

Conclusion: Parikh自动机可以成功扩展到无限单词，具有丰富的理论性质和实践应用价值，为模型检查等领域的计数约束提供了新的形式化工具。

Abstract: We study Parikh automata on finite and infinite words. First we establish some results for Parikh automata on finite words. Following, we present several definitions of Parikh automata on infinite words. We consider the deterministic as well as the non-deterministic variants and study closure properties, expressiveness, and common decision problems with applications to model checking. Furthermore, we compare our models to other models with counting mechanisms operating on infinite words.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [4] [Can Large Language Models Simulate Symbolic Execution Output Like KLEE?](https://arxiv.org/abs/2511.08530)
*Rong Feng,Vanisha Gupta,Vivek Patel,Viroopaksh Reddy Ernampati,Suman Saha*

Main category: cs.SE

TL;DR: 研究探索使用GPT-4o模拟KLEE符号执行工具的输出，特别是识别程序中最受约束的执行路径，在100个C程序数据集上达到约20%的准确率。


<details>
  <summary>Details</summary>
Motivation: 符号执行工具如KLEE在复杂程序中运行缓慢且资源消耗大，希望探索LLM是否能替代部分符号执行过程以节省时间和资源。

Method: 使用GPT-4o预测KLEE输出和识别最受约束路径（具有最多符号条件的执行路径），在100个C程序数据集上进行测试。

Result: GPT-4o在生成KLEE类输出和识别最受约束路径方面达到约20%的准确率，虽然不高但显示了LLM在模拟符号执行方面的潜力。

Conclusion: 当前LLM在模拟符号执行方面能力有限，但这项早期工作有助于了解LLM在这类任务中的能力和局限性。

Abstract: Symbolic execution helps check programs by exploring different paths based on symbolic inputs. Tools like KLEE are commonly used because they can automatically detect bugs and create test cases. But one of KLEE's biggest issues is how slow it can get when programs have lots of branching paths-it often becomes too resource-heavy to run on large or complex code. In this project, we wanted to see if a large language model like GPT-4o could simulate the kinds of outputs that KLEE generates. The idea was to explore whether LLMs could one day replace parts of symbolic execution to save time and resources.
  One specific goal was to have GPT-4o identify the most constrained path in a program, this is the execution path with the most symbolic conditions. These paths are especially important because they often represent edge cases that are harder to test and more likely to contain deep bugs. However, figuring this out usually requires fully running KLEE, which can be expensive. So, we tested whether GPT-4o could predict the KLEE outputs and the most complex path using a dataset of 100 C programs. Our results showed about 20% accuracy in generating KLEE-like outputs and identifying the most constrained path. While not highly accurate, this early work helps show what current LLMs can and can't do when it comes to simulating symbolic execution.

</details>


### [5] [A Service Suite for Specifying Digital Twins for Industry 5.0](https://arxiv.org/abs/2511.07506)
*Izaque Esteves,Regina Braga,José Maria David,Victor Stroele*

Main category: cs.SE

TL;DR: 提出了一个名为DT-Create的数字孪生服务套件，专注于预测性维护中的决策支持，基于智能技术、语义数据处理和自适应性。


<details>
  <summary>Details</summary>
Motivation: 预测性维护面临基于数据做出敏捷和准确决策的挑战，数字孪生可以处理信息并支持决策制定。

Method: 使用设计科学研究方法通过两个开发周期开发DT-Create套件，并通过案例研究进行评估，结合传感器数据收集、存储和智能处理，机器学习与本体论的信息丰富，智能技术选择预测模型，以及决策支持和自适应性。

Result: 结果表明DT-Create在指定数字孪生方面的可行性，包括传感器数据的收集、存储和智能处理，通过机器学习和本体论丰富信息，使用智能技术选择适合可用数据集的预测模型，以及决策支持和自适应性。

Conclusion: DT-Create套件在预测性维护中指定数字孪生是可行的，能够有效支持决策制定。

Abstract: One of the challenges of predictive maintenance is making decisions based on data in an agile and assertive way. Connected sensors and operational data favor intelligent processing techniques to enrich information and enable decision-making. Digital Twins (DTs) can be used to process information and support decision-making. DTs are a real-time representation of physical machines and generate data that predictive maintenance can use to make assertive and quick decisions. The main contribution of this work is the specification of a suite of services for specifying DTs, called DT-Create, focused on decision support in predictive maintenance. DT-Create suite is based on intelligent techniques, semantic data processing, and self-adaptation. This suite was developed using the Design Science Research (DSR) methodology through two development cycles and evaluated through case studies. The results demonstrate the feasibility of using DT-Create in specifying DTs considering the following aspects: (i) collection, storage, and intelligent processing of data generated by sensors, (ii) enrichment of information through machine learning and ontologies, (iii) use of intelligent techniques to select predictive models that adhere to the available data set, and (iv) decision support and self-adaptation.

</details>


### [6] [SemanticForge: Repository-Level Code Generation through Semantic Knowledge Graphs and Constraint Satisfaction](https://arxiv.org/abs/2511.07584)
*Wuyang Zhang,Chenkai Zhang,Zhen Luo,Jianming Ma,Wangming Yuan,Chuqiao Gu,Chenwei Feng*

Main category: cs.SE

TL;DR: SemanticForge提出了四种算法创新来解决LLM代码生成中的逻辑幻觉和模式幻觉问题，通过双静态-动态知识图谱、结构化查询生成、集成SMT求解的束搜索和增量维护算法，实现语义感知的代码生成。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在代码生成中常见的系统错误，特别是逻辑幻觉（控制/数据流推理错误）和模式幻觉（类型不匹配、签名违规、架构不一致），这些错误源于缺乏可查询的仓库级语义表示。

Method: 1) 双静态-动态知识图谱自动协调算法；2) 从自然语言生成结构化图查询的神经方法；3) 集成SMT求解的束搜索算法；4) 增量维护算法。

Result: 结构化查询生成精度达到73%，相比传统检索方法的51%有显著提升；知识图谱增量维护在O(|ΔR|·log n)时间内完成。

Conclusion: SemanticForge通过语义感知的代码生成方法，有效解决了LLM的系统性错误，为实际部署提供了可靠的技术基础。

Abstract: Large language models (LLMs) have transformed software development by enabling automated code generation, yet they frequently suffer from systematic errors that limit practical deployment. We identify two critical failure modes: \textit{logical hallucination} (incorrect control/data-flow reasoning) and \textit{schematic hallucination} (type mismatches, signature violations, and architectural inconsistencies). These errors stem from the absence of explicit, queryable representations of repository-wide semantics.
  This paper presents \textbf{SemanticForge}, which introduces four fundamental algorithmic advances for semantically-aware code generation: (1) a novel automatic reconciliation algorithm for dual static-dynamic knowledge graphs, unifying compile-time and runtime program semantics; (2) a neural approach that learns to generate structured graph queries from natural language, achieving 73\% precision versus 51\% for traditional retrieval; (3) a novel beam search algorithm with integrated SMT solving, enabling real-time constraint verification during generation rather than post-hoc validation; and (4) an incremental maintenance algorithm that updates knowledge graphs in $O(|ΔR| \cdot \log n)$ time while maintaining semantic equivalence.

</details>


### [7] [An Exploratory Eye Tracking Study on How Developers Classify and Debug Python Code in Different Paradigms](https://arxiv.org/abs/2511.07612)
*Samuel W. Flint,Jigyasa Chauhan,Niloofar Mansoor,Bonita Sharif,Robert Dyer*

Main category: cs.SE

TL;DR: 本研究通过眼动追踪实验探索Python多范式语言特性对代码理解和调试的影响，发现开发者对函数式和过程式范式的分类存在混淆，函数式代码完成时间最长，但范式变化不影响调试能力。


<details>
  <summary>Details</summary>
Motivation: 现代编程语言如Python支持多种范式特性，但尚无研究探讨特定范式语言特性如何影响代码理解。本研究旨在揭示范式特定特性如何影响代码理解和调试能力。

Method: 采用探索性眼动追踪实证研究，招募29名开发者进行4个分类任务和4个调试任务，记录眼动数据，分析范式分类准确性和调试表现。

Result: 结果显示：1) 函数式和过程式范式分类存在混淆，面向对象范式分类准确；2) 函数式代码完成时间最长；3) 范式变化不影响调试能力，但开发者对函数式代码自信心较低；4) 调试时阅读模式存在显著差异，分类时开发者不一定会阅读范式相关标记。

Conclusion: Python多范式特性确实影响开发者对代码的理解，特别是函数式范式带来更多挑战，但调试能力不受范式影响。眼动数据揭示了范式特定的阅读模式差异。

Abstract: Modern programming languages, such as Python, support language features from several paradigms, such as object-oriented, procedural, and functional. Research has shown that code written in some paradigms can be harder to comprehend, but to date, no research has looked at which paradigm-specific language features impact comprehension. To this end, this study seeks to uncover which paradigm-specific features impactcomprehension and debugging of code or how multi-paradigm code might affect a developer's ability to do so. We present an exploratory empirical eye-tracking study to investigate 1) how developers classify the predominant paradigm in Python code and 2) how the paradigm affects their ability to debug Python code. The goal is to uncover if specific language features are looked at more often while classifying and debugging code with a predominant paradigm. Twenty-nine developers (primarily students) were recruited for the study and were each given four classification and four debugging tasks in Python. Eye movements were recorded during all the tasks. The results indicate confusion in labeling Functional and Procedural paradigms, but not Object-Oriented. The code with predominantly functional paradigms also took the longest to complete. Changing the predominant paradigm did not affect the ability to debug the code, though developers did rate themselves with lower confidence for Functional code. We report significant differences in reading patterns during debugging, especially in the Functional code. During classification, results show that developers do not necessarily read paradigm-relevant token types.

</details>


### [8] [A Self-Improving Architecture for Dynamic Safety in Large Language Models](https://arxiv.org/abs/2511.07645)
*Tyler Slater*

Main category: cs.SE

TL;DR: 提出了自改进安全框架(SISF)，这是一个运行时架构，通过动态反馈循环让AI系统能够自主持续地适应和改进安全协议。


<details>
  <summary>Details</summary>
Motivation: 现有软件架构模式是静态的，而当前的安全保障方法不可扩展，使得系统容易受到新型对抗性威胁的攻击。

Method: 将未受保护的基础LLM与动态反馈循环耦合，包括用于违规检测的AI仲裁器和自主生成新安全策略的策略合成模块。

Result: 在AdvBench数据集上，SISF从零策略开始，检测到237次违规，自主合成了234个新策略，将攻击成功率从100%降低到45.58%，同时在良性提示上实现0.00%的误报率。

Conclusion: 基于自适应原则的AI安全架构方法是可行且有效的策略，为构建更强大、有弹性和可扩展的AI驱动系统提供了实用路径。

Abstract: Context: The integration of Large Language Models (LLMs) into core software systems is accelerating. However, existing software architecture patterns are static, while current safety assurance methods are not scalable, leaving systems vulnerable to novel adversarial threats.
  Objective: To design, implement, and evaluate a novel software architecture that enables an AI-driven system to autonomously and continuously adapt its own safety protocols at runtime.
  Method: We propose the Self-Improving Safety Framework (SISF), a runtime architecture that couples an unprotected, unaligned base LLM (mistralai/Mistral-7B-v0.1) with a dynamic feedback loop. This loop consists of an AI Adjudicator (GPT-4o) for breach detection and a Policy Synthesis Module (GPT-4 Turbo) that autonomously generates new, generalized safety policies (both heuristic and semantic) in response to failures.
  Results: We conducted a dynamic learning evaluation using the 520-prompt AdvBench dataset. The unprotected model was 100% vulnerable. Our SISF, starting from zero policies, demonstrated a clear learning curve: it detected 237 breaches, autonomously synthesized 234 new policies, and reduced the overall Attack Success Rate (ASR) to 45.58%. In a subsequent test on 520 benign prompts, the SISF achieved a 0.00% False Positive Rate (FPR), proving its ability to adapt without compromising user utility.
  Conclusion: An architectural approach to AI safety, based on the principles of self-adaptation, is a viable and effective strategy. Our framework demonstrates a practical path towards building more robust, resilient, and scalable AI-driven systems, shifting safety assurance from a static, pre-deployment activity to an automated, runtime process.

</details>


### [9] [Smart but Costly? Benchmarking LLMs on Functional Accuracy and Energy Efficiency](https://arxiv.org/abs/2511.07698)
*Mohammadjavad Mehditabar,Saurabhsingh Rajput,Antonio Mastropaolo,Tushar Sharma*

Main category: cs.SE

TL;DR: 提出了BRACE框架，用于在统一尺度上评估代码语言模型(CLMs)的能效和功能正确性，包含两种评级方法：CIRC和OTER。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏系统框架来评估代码语言模型的准确性与能耗权衡，需要平衡AI技术的环境影响与功能正确性。

Method: 开发BRACE框架，包含CIRC（基于欧几里得距离的确定性排名）和OTER（趋势感知评估）两种评级方法，对22个最先进模型在代码生成和摘要任务上进行基准测试。

Result: 模型在代码摘要任务中表现更好，因为它们不需要生成语法正确的输出；模型大小对评级影响不显著，表明参数利用效率更重要。

Conclusion: BRACE框架使从业者能够基于证据选择模型，平衡可持续性与任务需求，根据部署优先级选择CIRC或OTER评级方法。

Abstract: The rapid advancement of AI technologies and their accelerated adoption in software development necessitates a systematic evaluation of their environmental impact alongside functional correctness. While prior studies have examined sustainability in large language models, existing approaches lack systematic frameworks for evaluating accuracy-energy trade-offs in Code Language Models (CLMs). In this paper, we present a framework, BRACE, to benchmark CLMs on a unified scale of energy efficiency and functional correctness (referred to as accuracy). We benchmark 22 state-of-the-art models on code generation and summarization tasks, proposing two rating methods: Concentric Incremental Rating Circles (CIRC) and Observation to Expectation Rating (OTER). CIRC provides deterministic Euclidean-based rankings with static trade-offs that are robust to outliers, and OTER offers trend-aware evaluation with dynamic trade-offs that capture the complex correlation between energy and accuracy, each offering a distinct perspective and addressing the problem in a unique way. These rating methods enable us to rate LLMs on a 1-5 scale reflecting their combined capabilities in terms of energy efficiency and functional correctness. Our analysis reveals models generally perform better in the code summarization tasks as they are not enforced to generate a grammar-based and syntactically correct output. Also, we find that models' size does not have a significant impact on their ratings, indicating that if models utilize their parameters efficiently, they can be ranked higher on these scales. The proposed BRACE framework empowers practitioners to make evidence-based model selections that balance sustainability with task requirements, guiding rating choice -- CIRC for deterministic comparisons or OTER for trend-aware evaluation -- based on deployment priorities.

</details>


### [10] [Post Processing Graphical User Interface for Heat Flow Visualization](https://arxiv.org/abs/2511.07709)
*Lars Olt,Luis Diego Fonseca Flores,Ian Mckinley*

Main category: cs.SE

TL;DR: 开发了一个基于MATLAB和C++的GUI工具，利用Thermal Desktop的OpenTD API和自定义解析器，通过CSR文件高效提取和可视化热流相关指标，显著提升热分析效率。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏能够有效提取和可视化Thermal Desktop中热流相关指标的软件，阻碍了热工程师快速分析结果。

Method: 使用MATLAB和C++构建GUI，结合OpenTD API和自定义解析器，利用CSR文件的副作用高效加载温度、电导和子模型指标。

Result: 该方法可将模型节点和导体与子模型ID关联的运行时间减少几个数量级。

Conclusion: 讨论了该数据读取方法的局限性，考虑了GUI的未来发展，并为后续OpenTD版本提供了改进建议。

Abstract: Thermal Desktop (TD) is an industry-standard thermal analysis tool used to create and analyze thermal models for landers, rovers, spacecraft, and instrument payloads. Currently, limited software exists to extract and visualize metrics relevant to heat flow within TD, impeding thermal engineers from analyzing their results quickly. This paper discusses a graphical user interface (GUI) built in MATLAB and C++ which uses TDs application programming interface (API), OpenTD, and a custom parser to address this void. Specifically, we present a method for efficiently loading temperature, conductance, and submodel metrics using a side effect of TDs Compressed Solution Results (CSR) files. This approach can reduce the runtime for correlating model nodes and conductors with submodel IDs by orders of magnitude. Lastly, we reflect on the shortcomings of this method for reading data, consider the future of the GUI, and provide recommendations for subsequent OpenTD releases.

</details>


### [11] [Event-Driven Inconsistency Detection Between UML Class and Sequence Diagrams](https://arxiv.org/abs/2511.07742)
*Luan Lazzari,Kleinner Farias*

Main category: cs.SE

TL;DR: Harmony Validator是一个集成在Papyrus建模环境中的插件工具，能够自动检测和报告UML模型中的不一致性，支持实时监控建模操作，提升模型完整性意识。


<details>
  <summary>Details</summary>
Motivation: 软件工程建模需要抽象、一致性维护和精确沟通等技能，这些技能难以掌握和教授。教育者和学生在建模过程中经常难以理解和处理出现的不一致性问题。

Method: 采用事件驱动架构，持续监控建模操作，实时通知用户出现的不一致性。工具集成在Papyrus建模环境中，支持类图和序列图的检测。

Result: 通过在软件工程课程中进行的案例研究表明，Harmony Validator能够促进对模型一致性的更好理解，并支持反思性学习实践。

Conclusion: Harmony Validator增强了模型完整性意识，支持设计工件的迭代精化，在软件建模教育中具有积极的教学价值。

Abstract: Modeling is a central and demanding activity in software engineering that requires skills such as abstraction, consistency maintenance, and precise communication. These skills are difficult to master and even harder to teach effectively. Educators and students often struggle to understand and manage inconsistencies that arise during the modeling process. To address this challenge, we present \texttt{Harmony Validator}, a tool integrated as a plugin for the Papyrus modeling environment, designed to automatically detect and report inconsistencies in UML models, including class and sequence diagrams. The tool adopts an event-driven architecture that continuously monitors modeling actions and notifies users of emerging inconsistencies in real time. This approach enhances awareness of model integrity and supports the iterative refinement of design artifacts. The paper describes the architecture, detection mechanisms, and usage scenarios of Harmony Validator. It also includes a case study conducted with students in a software engineering course to evaluate the perceived usefulness and benefits of UML modeling in teaching and learning. Our results indicate that Harmony Validator fosters a better understanding of model consistency and promotes reflective learning practices in software modeling education.

</details>


### [12] [Uncovering Scientific Software Sustainability through Community Engagement and Software Quality Metrics](https://arxiv.org/abs/2511.07851)
*Sharif Ahmed,Addi Malviya Thakur,Gregory R. Watson,Nasir U. Eisty*

Main category: cs.SE

TL;DR: 本文研究了GitHub上科学开源软件的可持续性，通过分析社区参与度和软件质量两个因素，开发了新的可视化技术来展示软件指标随时间的变化，并发现即使相似领域的项目也以不同方式维持可持续性。


<details>
  <summary>Details</summary>
Motivation: 科学开源软件项目对研究进展至关重要，但长期维持这些项目仍然是一个重大挑战。需要探索如何通过社区参与和软件质量来确保这些项目的可持续发展。

Method: 使用文献中的存储库指标和从十个知名Sci-OSS项目中挖掘的数据，进行多模态分析，开发了新的可视化技术来展示软件指标的演变，并进行统计分析和自然语言分析。

Result: 开发了一种新颖的可视化技术，能够在一个视图中显示当前和演变的软件指标；统计分析显示相似领域的项目以不同方式维持可持续性；自然语言分析证实项目特定反馈对维护软件质量至关重要。

Conclusion: 所开发的可视化和分析方法为研究人员、资助者和开发者提供了关于长期软件可持续性的关键见解，有助于更好地理解和促进科学开源软件的可持续发展。

Abstract: Scientific open-source software (Sci-OSS) projects are critical for advancing research, yet sustaining these projects long-term remains a major challenge. This paper explores the sustainability of Sci-OSS hosted on GitHub, focusing on two factors drawn from stewardship organizations: community engagement and software quality. We map sustainability to repository metrics from the literature and mined data from ten prominent Sci-OSS projects. A multimodal analysis of these projects led us to a novel visualization technique, providing a robust way to display both current and evolving software metrics over time, replacing multiple traditional visualizations with one. Additionally, our statistical analysis shows that even similar-domain projects sustain themselves differently. Natural language analysis supports claims from the literature, highlighting that project-specific feedback plays a key role in maintaining software quality. Our visualization and analysis methods offer researchers, funders, and developers key insights into long-term software sustainability.

</details>


### [13] [LLM-Powered Fully Automated Chaos Engineering: Towards Enabling Anyone to Build Resilient Software Systems at Low Cost](https://arxiv.org/abs/2511.07865)
*Daisuke Kikuta,Hiroki Ikeuchi,Kengo Tajiri*

Main category: cs.SE

TL;DR: ChaosEater是一个基于大语言模型的自动化混沌工程系统，用于Kubernetes软件系统，能够自动完成整个混沌工程周期，显著降低时间和金钱成本。


<details>
  <summary>Details</summary>
Motivation: 传统混沌工程工具虽然能自动化执行实验，但实验规划和基于结果的系统改进仍需人工操作，这些过程劳动密集且需要多领域专业知识。

Method: ChaosEater使用大语言模型自动化整个混沌工程周期，预定义基于系统混沌工程周期的代理工作流，将工作流中的细分过程分配给LLM完成，包括需求定义、代码生成、测试和调试等软件工程任务。

Result: 在小型和大型Kubernetes系统上的案例研究表明，ChaosEater能够以显著较低的时间和金钱成本持续完成合理的混沌工程周期，其周期质量得到了人类工程师和LLM的验证。

Conclusion: ChaosEater成功实现了混沌工程周期的全自动化，使任何人都能以低成本构建弹性系统，解决了传统方法劳动密集和需要专业知识的问题。

Abstract: Chaos Engineering (CE) is an engineering technique aimed at improving the resilience of distributed systems. It involves intentionally injecting faults into a system to test its resilience, uncover weaknesses, and address them before they cause failures in production. Recent CE tools automate the execution of predefined CE experiments. However, planning such experiments and improving the system based on the experimental results still remain manual. These processes are labor-intensive and require multi-domain expertise. To address these challenges and enable anyone to build resilient systems at low cost, this paper proposes ChaosEater, a system that automates the entire CE cycle with Large Language Models (LLMs). It predefines an agentic workflow according to a systematic CE cycle and assigns subdivided processes within the workflow to LLMs. ChaosEater targets CE for software systems built on Kubernetes. Therefore, the LLMs in ChaosEater complete CE cycles through software engineering tasks, including requirement definition, code generation, testing, and debugging. We evaluate ChaosEater through case studies on small- and large-scale Kubernetes systems. The results demonstrate that it consistently completes reasonable CE cycles with significantly low time and monetary costs. Its cycles are also qualitatively validated by human engineers and LLMs.

</details>


### [14] [Testing Question Answering Software with Context-Driven Question Generation](https://arxiv.org/abs/2511.07924)
*Shuang Liu,Zhirun Zhang,Jinhao Dong,Zan Wang,Qingchao Shen,Junjie Chen,Wei Lu,Xiaoyong Du*

Main category: cs.SE

TL;DR: CQ^2A是一种基于上下文的问题生成方法，用于测试问答系统。它从上下文中提取实体和关系形成真实答案，并利用大语言模型基于这些答案和上下文生成问题，提高了问题自然度和错误检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有问答系统测试方法生成的问题不自然且缺乏多样性，无法有效识别真实场景中的错误。需要一种能生成更自然、相关且多样问题的方法来提升测试效果。

Method: CQ^2A从上下文中提取实体和关系形成真实答案，利用大语言模型基于真实答案和上下文生成问题，并通过一致性验证和约束检查提高输出可靠性。

Result: 在三个数据集上的实验表明，CQ^2A在错误检测能力、生成问题自然度和上下文覆盖方面优于现有方法，且生成的测试用例能降低问答系统的错误率。

Conclusion: CQ^2A通过上下文驱动的问题生成方法，有效提升了问答系统测试的质量和效果，生成的测试用例可用于优化问答系统性能。

Abstract: Question-answering software is becoming increasingly integrated into our daily lives, with prominent examples including Apple Siri and Amazon Alexa. Ensuring the quality of such systems is critical, as incorrect answers could lead to significant harm. Current state-of-the-art testing approaches apply metamorphic relations to existing test datasets, generating test questions based on these relations. However, these methods have two key limitations. First, they often produce unnatural questions that humans are unlikely to ask, reducing the effectiveness of the generated questions in identifying bugs that might occur in real-world scenarios. Second, these questions are generated from pre-existing test datasets, ignoring the broader context and thus limiting the diversity and relevance of the generated questions.
  In this work, we introduce CQ^2A, a context-driven question generation approach for testing question-answering systems. Specifically, CQ^2A extracts entities and relationships from the context to form ground truth answers, and utilizes large language models to generate questions based on these ground truth answers and the surrounding context. We also propose the consistency verification and constraint checking to increase the reliability of LLM's outputs. Experiments conducted on three datasets demonstrate that CQ^2A outperforms state-of-the-art approaches on the bug detection capability, the naturalness of the generated questions as well as the coverage of the context. Moreover, the test cases generated by CQ^2A reduce error rate when utilized for fine-tuning the QA software under test

</details>


### [15] ["I need to learn better searching tactics for privacy policy laws.'' Investigating Software Developers' Behavior When Using Sources on Privacy Issues](https://arxiv.org/abs/2511.08059)
*Stefan Albert Horstmann,Sandy Hong,Maziar Niazian,Cristiana Santos,Alena Naiakshina*

Main category: cs.SE

TL;DR: 研究评估了开发者在隐私敏感场景中使用个人知识、网络资源和AI助手时的决策过程，发现现有支持存在严重不足


<details>
  <summary>Details</summary>
Motivation: 随着GDPR和CCPA等法规的实施，开发者需要在系统设计和实现中做出隐私相关决策，但缺乏法律专业知识，需要了解现有信息源的有效性

Method: 对30名开发者进行定性研究，通过思考出声会话和后续访谈，观察他们在隐私敏感场景中使用知识、网络资源和AI助手时的决策过程

Result: 参与者在使用所有三种信息源时都遇到困难：个人知识不足、网络内容过于复杂、AI助手虽然提供清晰回答但缺乏情境相关性和无法识别场景特定问题

Conclusion: 现有隐私相关开发任务支持存在重大缺陷，需要更易访问、易理解和可操作的隐私资源

Abstract: Since the introduction of the European General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA), software developers increasingly have to make privacy-related decisions during system design and implementation. However, past research showed that they often lack legal expertise and struggle with privacy-compliant development. To shed light on how effective current information sources are in supporting them with privacy-sensitive implementation, we conducted a qualitative study with 30 developers. Participants were presented with a privacy-sensitive scenario and asked to identify privacy issues and suggest measures using their knowledge, online resources, and an AI assistant. We observed developers' decision-making in think-aloud sessions and discussed it in follow-up interviews. We found that participants struggled with all three sources: personal knowledge was insufficient, web content was often too complex, and while AI assistants provided clear and user-tailored responses, they lacked contextual relevance and failed to identify scenario-specific issues. Our study highlights major shortcomings in existing support for privacy-related development tasks. Based on our findings, we discuss the need for more accessible, understandable, and actionable privacy resources for developers.

</details>


### [16] [A Small Leak Sinks All: Exploring the Transferable Vulnerability of Source Code Models](https://arxiv.org/abs/2511.08127)
*Weiye Li,Wenyi Tang*

Main category: cs.SE

TL;DR: 本文系统研究了传统源代码模型和LLM4Code的内在可转移漏洞，提出了HABITAT方法生成受害者无关的对抗样本，揭示了传统SCM与LLM4Code之间的漏洞相关性。


<details>
  <summary>Details</summary>
Motivation: 现有研究既未提供生成有效对抗样本的实用方法（需要访问SCM的下游分类器），也未关注现代软件开发平台中广泛使用的LLM4Code，导致可转移漏洞这一基础安全问题研究不足。

Method: 设计了HABITAT方法，包含定制的扰动插入机制和分层强化学习框架，能够自适应选择最优扰动，无需访问SCM的下游分类器。

Result: 实验评估显示，基于传统SCM构建的对抗样本对LLM4Code的攻击成功率高达64%，比现有技术高出15%以上。

Conclusion: 揭示了SCM漏洞的内在可转移性，为未来开发鲁棒防御提供了关键焦点，强调了传统SCM与LLM4Code之间的潜在漏洞相关性。

Abstract: Source Code Model learn the proper embeddings from source codes, demonstrating significant success in various software engineering or security tasks. The recent explosive development of LLM extends the family of SCMs,bringing LLMs for code that revolutionize development workflows. Investigating different kinds of SCM vulnerability is the cornerstone for the security and trustworthiness of AI-powered software ecosystems, however, the fundamental one, transferable vulnerability, remains critically underexplored. Existing studies neither offer practical ways, i.e. require access to the downstream classifier of SCMs, to produce effective adversarial samples for adversarial defense, nor give heed to the widely used LLM4Code in modern software development platforms and cloud-based integrated development environments. Therefore, this work systematically studies the intrinsic vulnerability transferability of both traditional SCMs and LLM4Code, and proposes a victim-agnostic approach to generate practical adversarial samples. We design HABITAT, consisting of a tailored perturbation-inserting mechanism and a hierarchical Reinforcement Learning framework that adaptively selects optimal perturbations without requiring any access to the downstream classifier of SCMs. Furthermore, an intrinsic transferability analysis of SCM vulnerabilities is conducted, revealing the potential vulnerability correlation between traditional SCMs and LLM4Code, together with fundamental factors that govern the success rate of victim-agnostic transfer attacks. These findings of SCM vulnerabilities underscore the critical focal points for developing robust defenses in the future. Experimental evaluation demonstrates that our constructed adversarial examples crafted based on traditional SCMs achieve up to 64% success rates against LLM4Code, surpassing the state-of-the-art by over 15%.

</details>


### [17] [OWLAPY: A Pythonic Framework for OWL Ontology Engineering](https://arxiv.org/abs/2511.08232)
*Alkid Baci,Luke Friedrichs,Caglar Demir,Axel-Cyrille Ngonga Ngomo*

Main category: cs.SE

TL;DR: OWLAPY是一个用于OWL本体工程的Python框架，支持本机Python推理器和外部Java推理器，提供本体创建、修改、序列化以及OWL类表达式与多种格式的转换功能，还支持利用大语言模型从自然语言生成本体。


<details>
  <summary>Details</summary>
Motivation: 为OWL本体工程提供一个灵活的Python库，特别是为从Java环境过渡的用户提供支持，简化OWL 2本体的创建、修改和序列化过程。

Method: 开发一个综合的Python框架，集成本机Python推理器和外部Java推理器，实现核心本体组件的多种实现，并提供OWL类表达式与描述逻辑、曼彻斯特语法、SPARQL等格式的转换能力。

Result: OWLAPY已成为一个经过充分测试的软件框架，在GitHub和PyPI上公开可用，下载量超过50,000次，证明了其在实际应用中的价值。

Conclusion: OWLAPY为高级本体工程提供了一个灵活且功能丰富的Python库，特别适合需要从Java环境迁移到Python的用户，支持自定义工作流程和大语言模型集成。

Abstract: In this paper, we introduce OWLAPY, a comprehensive Python framework for OWL ontology engineering. OWLAPY streamlines the creation, modification, and serialization of OWL 2 ontologies. It uniquely integrates native Python-based reasoners with support for external Java reasoners, offering flexibility for users. OWLAPY facilitates multiple implementations of core ontology components and provides robust conversion capabilities between OWL class expressions and formats such as Description Logics, Manchester Syntax, and SPARQL. It also allows users to define custom workflows to leverage large language models (LLMs) in ontology generation from natural language text. OWLAPY serves as a well-tested software framework for users seeking a flexible Python library for advanced ontology engineering, including those transitioning from Java-based environments. The project is publicly available on GitHub at https://github.com/dice-group/owlapy and on the Python Package Index (PyPI) at https://pypi.org/project/owlapy/ , with over 50,000 downloads at the time of writing.

</details>


### [18] [Designing LLM-based Multi-Agent Systems for Software Engineering Tasks: Quality Attributes, Design Patterns and Rationale](https://arxiv.org/abs/2511.08475)
*Yangxiao Cai,Ruiyin Li,Peng Liang,Mojtaba Shahin,Zengyang Li*

Main category: cs.SE

TL;DR: 本研究系统分析了94篇关于基于LLM的多智能体系统在软件工程任务中的应用，识别了主要关注的软件工程任务、质量属性、设计模式和设计理念。


<details>
  <summary>Details</summary>
Motivation: 随着软件工程任务复杂性增加，基于LLM的多智能体系统因其自主性和可扩展性受到关注，但目前缺乏对其设计模式的系统研究。

Method: 收集94篇相关论文作为数据源，分析其中基于LLM的多智能体系统在软件工程任务中的应用情况。

Result: 发现代码生成是最常见的应用任务，功能性适用性是最受关注的质量属性，基于角色的协作是最常用的设计模式，提高生成代码质量是最主要的设计理念。

Conclusion: 研究结果为设计基于LLM的多智能体系统以支持软件工程任务提供了重要启示。

Abstract: As the complexity of Software Engineering (SE) tasks continues to escalate, Multi-Agent Systems (MASs) have emerged as a focal point of research and practice due to their autonomy and scalability. Furthermore, through leveraging the reasoning and planning capabilities of Large Language Models (LLMs), the application of LLM-based MASs in the field of SE is garnering increasing attention. However, there is no dedicated study that systematically explores the design of LLM-based MASs, including the Quality Attributes (QAs) on which the designers mainly focus, the design patterns used by the designers, and the rationale guiding the design of LLM-based MASs for SE tasks. To this end, we conducted a study to identify the QAs that LLM-based MASs for SE tasks focus on, the design patterns used in the MASs, and the design rationale for the MASs. We collected 94 papers on LLM-based MASs for SE tasks as the source. Our study shows that: (1) Code Generation is the most common SE task solved by LLM-based MASs among ten identified SE tasks, (2) Functional Suitability is the QA on which designers of LLM-based MASs pay the most attention, (3) Role-Based Cooperation is the design pattern most frequently employed among 16 patterns used to construct LLM-based MASs, and (4) Improving the Quality of Generated Code is the most common rationale behind the design of LLM-based MASs. Based on the study results, we presented the implications for the design of LLM-based MASs to support SE tasks.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [19] [TurboSAT: Gradient-Guided Boolean Satisfiability Accelerated on GPU-CPU Hybrid System](https://arxiv.org/abs/2511.07737)
*Steve Dai,Cunxi Yu,Kalyan Krishnamani,Brucek Khailany*

Main category: cs.LO

TL;DR: 提出了一种混合GPU-CPU系统，将SAT问题编码为可微优化问题，在GPU上并行求解，在CPU上使用传统冲突驱动搜索，相比纯CPU求解器获得200倍加速。


<details>
  <summary>Details</summary>
Motivation: 当前SAT求解器主要依赖顺序冲突驱动搜索算法，限制了并行性。受神经网络训练启发，希望利用GPU并行计算能力加速SAT求解。

Method: 将SAT问题编码为二值化矩阵乘法层，使用可微目标函数优化。GPU负责并行可微求解，CPU负责冲突驱动顺序搜索，形成混合系统。

Result: 在NVIDIA DGX GB200节点上测试，相比最先进的CPU求解器，在可满足基准问题上获得高达200倍的运行时间加速。

Conclusion: 结合并行可微优化和顺序搜索的混合方法能显著加速SAT求解，证明了GPU在逻辑推理问题中的潜力。

Abstract: While accelerated computing has transformed many domains of computing, its impact on logical reasoning, specifically Boolean satisfiability (SAT), remains limited. State-of-the-art SAT solvers rely heavily on inherently sequential conflict-driven search algorithms that offer powerful heuristics but limit the amount of parallelism that could otherwise enable significantly more scalable SAT solving. Inspired by neural network training, we formulate the SAT problem as a binarized matrix-matrix multiplication layer that could be optimized using a differentiable objective function. Enabled by this encoding, we combine the strengths of parallel differentiable optimization and sequential search to accelerate SAT on a hybrid GPU-CPU system. In this system, the GPUs leverage parallel differentiable solving to rapidly evaluate SAT clauses and use gradients to stochastically explore the solution space and optimize variable assignments. Promising partial assignments generated by the GPUs are post-processed on many CPU threads which exploit conflict-driven sequential search to further traverse the solution subspaces and identify complete assignments. Prototyping the hybrid solver on an NVIDIA DGX GB200 node, our solver achieves runtime speedups up to over 200x when compared to a state-of-the-art CPU-based solver on public satisfiable benchmark problems from the SAT Competition.

</details>


### [20] [Revisiting Conjunctive Query Entailment for $\mathcal S$](https://arxiv.org/abs/2511.07933)
*Yazmín Ibáñez-García,Jean Christoph Jung,Vincent Michielini,Filip Murlak*

Main category: cs.LO

TL;DR: 该论文澄清了在描述逻辑$\mathcal S$（$\mathcal{ALC}$加上传递角色的扩展）上回答合取查询并集的复杂性，证明该问题是2ExpTime-完全的，并展示了在某些限制条件下的复杂性下界。


<details>
  <summary>Details</summary>
Motivation: 现有部分结果表明该问题的复杂性可能较低，但作者发现实际复杂性更高，需要明确其真实复杂性边界。

Method: 通过理论分析和复杂性证明，展示了在传递角色存在的情况下查询回答的复杂性特征。

Result: 证明该问题是2ExpTime-完全的，即使只有两个传递角色和布尔合取查询；但在查询是根查询或只使用一个传递角色时，问题保持在coNExpTime内。

Conclusion: $\mathcal S$知识库上合取查询并集的回答具有较高的计算复杂性，但在特定限制条件下复杂性会显著降低。

Abstract: We clarify the complexity of answering unions of conjunctive queries over knowledge bases formulated in the description logic $\mathcal S$, the extension of $\mathcal{ALC}$ with transitive roles. Contrary to what existing partial results suggested, we show that the problem is in fact 2ExpTime-complete; hardness already holds in the presence of two transitive roles and for Boolean conjunctive queries. We complement this result by showing that the problem remains in coNExpTime when the input query is rooted or is restricted to use at most one transitive role (but may use arbitrarily many non-transitive roles).

</details>


### [21] [Semi-Algebraic Proof Systems for QBF](https://arxiv.org/abs/2511.08050)
*Olaf Beyersdorff,Ilario Bonacina,Kaspar Kasche,Meena Mahajan,Luc Nicolas Spachmann*

Main category: cs.LO

TL;DR: 本文为量化布尔公式(QBF)引入了新的半代数证明系统，包括Nullstellensatz、Sherali-Adams和Sum-of-Squares的QBF版本，并获得了这些系统的强下界和分离结果。


<details>
  <summary>Details</summary>
Motivation: 将命题逻辑中的半代数证明系统扩展到量化布尔公式领域，以研究QBF的证明复杂性。

Method: 结合QBF文献中的策略提取技术和命题证明复杂性中的大小-度关系和伪期望方法。

Result: 获得了这些QBF半代数证明系统的强下界，即使忽略命题难度也能实现系统间的分离。

Conclusion: 成功将命题证明复杂性的技术迁移到QBF领域，为QBF证明系统建立了新的分析框架和重要结果。

Abstract: We introduce new semi-algebraic proof systems for Quantified Boolean Formulas (QBF) analogous to the propositional systems Nullstellensatz, Sherali-Adams and Sum-of-Squares. We transfer to this setting techniques both from the QBF literature (strategy extraction) and from propositional proof complexity (size-degree relations and pseudo-expectation). We obtain a number of strong QBF lower bounds and separations between these systems, even when disregarding propositional hardness.

</details>


### [22] [Constrained and Robust Policy Synthesis with Satisfiability-Modulo-Probabilistic-Model-Checking](https://arxiv.org/abs/2511.08078)
*Linus Heck,Filip Macák,Milan Češka,Sebastian Junges*

Main category: cs.LO

TL;DR: 本文提出了首个有效计算满足任意结构约束的鲁棒策略的方法，通过结合可满足性求解器和概率模型检查算法，实现了灵活高效的框架。


<details>
  <summary>Details</summary>
Motivation: 在规划、控制器合成和验证等应用中，需要策略既能在MDP扰动下保持良好性能（鲁棒性），又能满足表示或实现成本等结构约束。但计算这类鲁棒且受约束的策略在计算上更具挑战性。

Method: 使用一阶理论在MDP集合上表达约束，紧密集成可满足性求解器处理组合性质问题，结合概率模型检查算法分析MDP。

Result: 在数百个基准测试上的实验表明，该方法在约束和鲁棒策略合成方面具有可行性，并在问题的各种片段上与最先进方法具有竞争力。

Conclusion: 该框架为计算鲁棒且受约束的策略提供了首个有效方法，通过可满足性求解器和概率模型检查的集成实现了灵活性和效率的平衡。

Abstract: The ability to compute reward-optimal policies for given and known finite Markov decision processes (MDPs) underpins a variety of applications across planning, controller synthesis, and verification. However, we often want policies (1) to be robust, i.e., they perform well on perturbations of the MDP and (2) to satisfy additional structural constraints regarding, e.g., their representation or implementation cost. Computing such robust and constrained policies is indeed computationally more challenging. This paper contributes the first approach to effectively compute robust policies subject to arbitrary structural constraints using a flexible and efficient framework. We achieve flexibility by allowing to express our constraints in a first-order theory over a set of MDPs, while the root for our efficiency lies in the tight integration of satisfiability solvers to handle the combinatorial nature of the problem and probabilistic model checking algorithms to handle the analysis of MDPs. Experiments on a few hundred benchmarks demonstrate the feasibility for constrained and robust policy synthesis and the competitiveness with state-of-the-art methods for various fragments of the problem.

</details>


### [23] [An abstract fixed-point theorem for Horn formula equations](https://arxiv.org/abs/2511.08162)
*Stefan Hetzl,Johannes Kloibhofer*

Main category: cs.LO

TL;DR: 本文提出了一个关于一阶逻辑中Horn公式方程的抽象不动点定理，该定理适用于推广了标准语义的抽象语义。


<details>
  <summary>Details</summary>
Motivation: Horn公式方程在计算机科学的许多应用中扮演重要角色，需要建立统一的不动点理论框架。

Method: 在一阶逻辑中加入最小不动点算子，针对Horn公式方程陈述并证明抽象不动点定理。

Result: 证明了适用于抽象语义的Horn公式方程不动点定理，该定理在计算逻辑的多个领域具有应用价值。

Conclusion: 该不动点定理为程序验证的逻辑基础和归纳定理证明等计算逻辑领域提供了理论基础和应用工具。

Abstract: We consider a class of formula equations in first-order logic, Horn formula equations, which are defined by a syntactic restriction on the occurrences of predicate variables. Horn formula equations play an important role in many applications in computer science. We state and prove a fixed-point theorem for Horn formula equations in first-order logic with a least fixed-point operator. Our fixed-point theorem is abstract in the sense that it applies to an abstract semantics which generalises standard semantics. We describe several corollaries of this fixed-point theorem in various areas of computational logic, ranging from the logical foundations of program verification to inductive theorem proving.

</details>


### [24] [Proof Minimization in Neural Network Verification](https://arxiv.org/abs/2511.08198)
*Omri Isac,Idan Refaeli,Haoze Wu,Clark Barrett,Guy Katz*

Main category: cs.LO

TL;DR: 该论文提出了一种最小化DNN验证器产生的不可满足性证明的方法，通过移除验证过程中学习到但对证明不必要的事实，显著减小证明大小和验证时间。


<details>
  <summary>Details</summary>
Motivation: DNN验证器可能存在bug，影响验证的可靠性。虽然可以使用证明来确保正确性，但传统证明往往过于庞大，限制了实际应用。

Method: 提出了分析证明中事实依赖关系的算法，移除对推导UNSAT结果不必要的事实，并进一步通过两种替代程序消除剩余的不必要依赖。

Result: 最佳算法将证明大小减少了37%-82%，证明检查时间减少了30%-88%，验证过程本身仅增加7%-20%的运行时间开销。

Conclusion: 该方法有效解决了DNN验证证明过大的问题，在保持验证可靠性的同时显著提升了证明的实用性。

Abstract: The widespread adoption of deep neural networks (DNNs) requires efficient techniques for verifying their safety. DNN verifiers are complex tools, which might contain bugs that could compromise their soundness and undermine the reliability of the verification process. This concern can be mitigated using proofs: artifacts that are checkable by an external and reliable proof checker, and which attest to the correctness of the verification process. However, such proofs tend to be extremely large, limiting their use in many scenarios. In this work, we address this problem by minimizing proofs of unsatisfiability produced by DNN verifiers. We present algorithms that remove facts which were learned during the verification process, but which are unnecessary for the proof itself. Conceptually, our method analyzes the dependencies among facts used to deduce UNSAT, and removes facts that did not contribute. We then further minimize the proof by eliminating remaining unnecessary dependencies, using two alternative procedures. We implemented our algorithms on top of a proof producing DNN verifier, and evaluated them across several benchmarks. Our results show that our best-performing algorithm reduces proof size by 37%-82% and proof checking time by 30%-88%, while introducing a runtime overhead of 7%-20% to the verification process itself.

</details>
