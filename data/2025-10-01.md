<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 38]
- [cs.LO](#cs.LO) [Total: 7]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Black-box Context-free Grammar Inference for Readable & Natural Grammars](https://arxiv.org/abs/2509.26616)
*Mohammad Rifat Arefin,Shanto Rahman,Christoph Csallner*

Main category: cs.SE

TL;DR: NatGI是一个基于LLM引导的黑盒上下文无关文法推断框架，通过括号引导的探索、LLM驱动的生成和非终端标记以及分层增量调试，显著提升了语法推断的准确性、可扩展性和可读性。


<details>
  <summary>Details</summary>
Motivation: 现有的语法推断工具（如Arvada、TreeVada、Kedavra）在处理大型复杂语言时存在可扩展性、可读性和准确性不足的问题，需要更有效的解决方案。

Method: 1) 括号引导的气泡探索：利用括号等语法线索提出结构良好的语法片段；2) LLM驱动的气泡生成和非终端标记：生成有意义的非终端名称并选择更有前景的合并；3) 分层增量调试：系统性地简化语法树，使语法更紧凑和可解释。

Result: 在从小型语言到大型语言（如lua、c、mysql）的综合基准测试中，NatGI平均F1分数达到0.57，比最佳基线TreeVada高出25个百分点，在可解释性方面显著优于现有方法。

Conclusion: NatGI通过LLM引导的方法在语法推断任务上实现了更高的准确性和可解释性，生成的语法具有有意义的非终端名称和紧凑结构，便于开发者和研究人员检查、验证和推理。

Abstract: Black-box context-free grammar inference is crucial for program analysis,
reverse engineering, and security, yet existing tools such as Arvada, TreeVada,
and Kedavra struggle with scalability, readability, and accuracy on large,
complex languages. We present NatGI, a novel LLM-guided grammar inference
framework that extends TreeVada's parse tree recovery with three key
innovations: bracket-guided bubble exploration, LLM-driven bubble generation
and non-terminal labeling, and hierarchical delta debugging (HDD) for
systematic tree simplification. Bracket-guided exploration leverages syntactic
cues such as parentheses to propose well-structured grammar fragments, while
LLM guidance produces meaningful non-terminal names and selects more promising
merges. Finally, HDD incrementally reduces unnecessary rules, which makes the
grammars both compact and interpretable. In our experiments, we evaluate NatGI
on a comprehensive benchmark suite ranging from small languages to larger ones
such as lua, c, and mysql. Our results show that NatGI consistently outperforms
strong baselines in terms of F1 score. On average, NatGI achieves an F1 score
of 0.57, which is 25pp (percentage points) higher than the best-performing
baseline, TreeVada. In the case of interpretability, our generated grammars
perform significantly better than those produced by existing approaches.
Leveraging LLM-based node renaming and bubble exploration, NatGI produces rules
with meaningful non-terminal names and compact structures that align more
closely with human intuition. As a result, developers and researchers can
achieve higher accuracy while still being able to easily inspect, verify, and
reason about the structure and semantics of the induced grammars.

</details>


### [2] [APRIL: API Synthesis with Automatic Prompt Optimization and Reinforcement Learning](https://arxiv.org/abs/2509.25196)
*Hua Zhong,Shan Jiang,Sarfraz Khurshid*

Main category: cs.SE

TL;DR: APRIL结合LLM合成、自动提示优化(APO)和基于可验证奖励的强化学习(RLVR)，显著提升了大型库中API合成的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统组件合成方法搜索空间大、成本高，而LLM生成的代码存在幻觉问题且缺乏最新上下文信息，需要更可靠的API合成方案。

Method: 使用APO迭代优化提示词，同时通过RLVR对策略进行微调以实现功能正确性，构建高效的合成流水线。

Result: 在81个真实世界科学Python库API上评估，相比专家提示指导的指令调优LLM，APRIL取得显著改进。

Conclusion: APO和RLVR的集成为大型库中的组件式API合成提供了稳健、可扩展的路径。

Abstract: APIs are central to modern software development, yet composing new APIs from
large libraries is difficult due to the exponential search space; traditional
component-based synthesis relies on costly exploration and hand-crafted
specifications. While large language models (LLMs) can generate implementations
from natural language, hallucinations and limited access to up-to-date
contextual information often yield incorrect code. In this paper, we present
APRIL, an approach that combines LLM-based synthesis with Automatic Prompt
Optimization (APO) and Reinforcement Learning from Verifiable Rewards (RLVR):
APO iteratively refines prompts for a frozen model, while RLVR fine-tunes the
policy toward functional correctness, producing an efficient synthesis
pipeline. Evaluated on 81 real-world APIs from widely used scientific Python
libraries and benchmarked against instruction-tuned but unfine-tuned LLMs
guided by expert prompts, APRIL achieves substantial improvements. These
results indicate that integrating APO and RLVR provides a robust, scalable path
for component-based API synthesis in large libraries.

</details>


### [3] [WARP -- Web-Augmented Real-time Program Repairer: A Real-Time Compilation Error Resolution using LLMs and Web-Augmented Synthesis](https://arxiv.org/abs/2509.25192)
*Anderson de Lima Luiz*

Main category: cs.SE

TL;DR: WARP是一个利用大语言模型和动态网络增强合成技术实时修复编译错误的系统，通过监控开发者终端、智能检测错误，并结合微调代码LLM与网络资源来提供解决方案。


<details>
  <summary>Details</summary>
Motivation: 编译错误严重影响了软件开发效率，需要一种能够实时自动修复这些错误的系统来提高开发生产力。

Method: WARP系统主动监控开发者终端，智能检测编译错误，并将微调的代码LLM的理解与从开发者论坛和官方文档等最新网络资源检索到的相关解决方案、解释和代码片段相结合。

Result: 在CGP基准测试（包含C/C++、Python和Go错误）上，WARP实现了72.5%的正确编译修复率，在语义正确性方面优于基线LLM方法和传统IDE快速修复。

Conclusion: WARP通过结合LLM和动态网络增强合成技术，有效解决了编译错误修复问题，但处理噪声网络数据以实现高精度合成仍面临技术挑战。

Abstract: Compilation errors represent a significant bottleneck in software development
productivity. This paper introduces WARP (Web-Augmented Real-time Program
Repairer), a novel system that leverages Large Language Models (LLMs) and
dynamic web-augmented synthesis for real-time resolution of these errors. WARP
actively monitors developer terminals, intelligently detects compilation
errors, and synergistically combines the understanding of a fine-tuned Code-LLM
with relevant solutions, explanations, and code snippets retrieved from
up-to-date web sources like developer forums and official documentation.
Experimental results on our curated benchmark, CGP (featuring C/C++, Python,
and Go errors), demonstrate WARP achieves a superior fix rate (72.5 % Compiles
correctly) and higher semantic correctness compared to baseline LLM-only
approaches and traditional IDE quick-fixes. Key technical challenges in
achieving high-accuracy synthesis from noisy web data.

</details>


### [4] [Towards Repository-Level Program Verification with Large Language Models](https://arxiv.org/abs/2509.25197)
*Si Cheng Zhong,Xujie Si*

Main category: cs.SE

TL;DR: RVBench是首个专门为仓库级验证设计的基准，RagVerus框架结合检索增强生成和上下文感知提示，显著提高了多模块仓库的证明合成能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法主要关注孤立的函数级验证任务，忽略了跨模块依赖和全局上下文等关键挑战，难以扩展到真实项目。

Method: 引入RVBench基准，并开发RagVerus框架，结合检索增强生成和上下文感知提示来自动化多模块仓库的证明合成。

Result: RagVerus在现有基准上将证明通过率提高了三倍，在更难的RVBench基准上实现了27%的相对改进。

Conclusion: 该方法提供了一个可扩展且样本高效的验证解决方案，能够处理整个软件仓库的验证挑战。

Abstract: Recent advancements in large language models (LLMs) suggest great promises in
code and proof generations. However, scaling automated formal verification to
real-world projects requires resolving cross-module dependencies and global
contexts, which are crucial challenges overlooked by existing LLM-based methods
with a special focus on targeting isolated, function-level verification tasks.
To systematically explore and address the significant challenges of verifying
entire software repositories, we introduce RVBench, the first verification
benchmark explicitly designed for repository-level evaluation, constructed from
four diverse and complex open-source Verus projects.
  We further introduce RagVerus, an extensible framework that synergizes
retrieval-augmented generation with context-aware prompting to automate proof
synthesis for multi-module repositories. RagVerus triples proof pass rates on
existing benchmarks under constrained model inference budgets, and achieves a
27% relative improvement on the more challenging RVBench benchmark,
demonstrating a scalable and sample-efficient verification solution.

</details>


### [5] [Devstral: Fine-tuning Language Models for Coding Agent Applications](https://arxiv.org/abs/2509.25193)
*Abhinav Rastogi,Adam Yang,Albert Q. Jiang,Alexander H. Liu,Alexandre Sablayrolles,Amélie Héliou,Amélie Martin,Anmol Agarwal,Andy Ehrenberg,Andy Lo,Antoine Roux,Arthur Darcet,Arthur Mensch,Baptiste Bout,Baptiste Rozière,Baudouin De Monicault,Chris Bamford,Christian Wallenwein,Christophe Renaudin,Clémence Lanfranchi,Clément Denoix,Corentin Barreau,Darius Dabert Devon Mizelle,Diego de las Casas,Elliot Chane-Sane,Emilien Fugier,Emma Bou Hanna,Gabrielle Berrada,Gauthier Delerce,Gauthier Guinet,Georgii Novikov,Graham Neubig,Guillaume Lample,Guillaume Martin,Himanshu Jaju,Jan Ludziejewski,Jason Rute,Jean-Malo Delignon,JeanHadrien Chabran,Joachim Studnia,Joep Barmentlo,Jonas Amar,Josselin Somerville Roberts,Julien Denize,Karan Saxena,Karmesh Yadav,Kartik Khandelwal,Khyathi Raghavi Chandu,Kush Jain,Lélio Renard Lavaud,Léonard Blier,Lingxiao Zhao,Louis Martin,Lucile Saulnier,Luyu Gao,Marie Pellat,Mathilde Guillaumin,Mathis Felardos,Matthieu Dinot,Maxime Darrin,Maximilian Augustin,Mickaël Seznec,Neha Gupta,Nikhil Raghuraman,Olivier Duchenne,Patricia Wang,Patrick von Platen,Patryk Saffer,Paul Jacob,Paul Wambergue,Paula Kurylowicz,Philomène Chagniot,Pierre Stock,Pravesh Agrawal,Rémi Delacourt,Roman Soletskyi,Romain Sauvestre,Sagar Vaze,Sanchit Gandhi,Sandeep Subramanian,Shashwat Dalal,Siddharth Gandhi,Soham Ghosh,Srijan Mishra,Sumukh Aithal,Szymon Antoniak,Teven Le Scao,Thibaut Lavril,Thibault Schueller,Thomas Foubert,Thomas Robert,Thomas Wang,Timothée Lacroix,Tom Bewley,Valeriia Nemychnikova,Victor Paltz,Virgile Richard,Wen-Ding Li,William Marshall,Xingyao Wang,Xuanyu Zhang,Yihan Wan,Yunhao Tang*

Main category: cs.SE

TL;DR: Devstral-Small是一个24B参数的轻量级开源代码代理模型，在100B以下模型中性能最佳，具有快速部署和竞争力的性能表现。


<details>
  <summary>Details</summary>
Motivation: 开发一个轻量级但性能优异的代码代理模型，解决大模型部署困难和服务成本高的问题。

Method: 通过专门的设计和开发方法，专注于智能软件代理的专业化能力构建。

Result: Devstral-Small在保持24B小规模的同时，性能可与大一个数量级的大型模型相媲美。

Conclusion: Devstral-Small证明了轻量级模型在代码代理任务中也能达到出色的性能，为实际部署提供了高效解决方案。

Abstract: We introduce Devstral-Small, a lightweight open source model for code agents
with the best performance among models below 100B size. In this technical
report, we give an overview of how we design and develop a model and craft
specializations in agentic software development. The resulting model,
Devstral-Small is a small 24B model, fast and easy to serve. Despite its size,
Devstral-Small still attains competitive performance compared to models more
than an order of magnitude larger.

</details>


### [6] [BuildBench: Benchmarking LLM Agents on Compiling Real-World Open-Source Software](https://arxiv.org/abs/2509.25248)
*Zehua Zhang,Ati Priya Bajaj,Divij Handa,Siyu Liu,Arvind S Raj,Hongkai Chen,Hulin Wang,Yibo Liu,Zion Leonahenahe Basque,Souradip Nath,Vishal Juneja,Nikhil Chapre,Yan Shoshitaishvili,Adam Doupé,Chitta Baral,Ruoyu Wang*

Main category: cs.SE

TL;DR: 提出了一个更挑战性的基准BUILD-BENCH和强基线LLM智能体OSS-BUILD-AGENT，用于自动编译开源软件项目。


<details>
  <summary>Details</summary>
Motivation: 现有的方法依赖手动制定的规则和工作流，无法适应需要定制配置或环境设置的开源软件。最近使用大语言模型的尝试只评估了高度评价的开源软件子集，低估了开源软件编译的现实挑战。

Method: 提出了OSS-BUILD-AGENT，一个具有增强构建指令检索模块的有效系统，在BUILD-BENCH上实现了最先进的性能，并能适应异构的开源软件特性。

Result: OSS-BUILD-AGENT在BUILD-BENCH基准上实现了最先进的性能，该基准包含了质量、规模和特性更加多样化的开源软件。

Conclusion: BUILD-BENCH的性能能够真实反映智能体处理编译这一复杂软件工程任务的能力，该基准将推动创新，对软件开发和软件安全领域的下游应用产生重大影响。

Abstract: Automatically compiling open-source software (OSS) projects is a vital,
labor-intensive, and complex task, which makes it a good challenge for LLM
Agents. Existing methods rely on manually curated rules and workflows, which
cannot adapt to OSS that requires customized configuration or environment
setup. Recent attempts using Large Language Models (LLMs) used selective
evaluation on a subset of highly rated OSS, a practice that underestimates the
realistic challenges of OSS compilation. In practice, compilation instructions
are often absent, dependencies are undocumented, and successful builds may even
require patching source files or modifying build scripts. We propose a more
challenging and realistic benchmark, BUILD-BENCH, comprising OSS that are more
diverse in quality, scale, and characteristics. Furthermore, we propose a
strong baseline LLM-based agent, OSS-BUILD-AGENT, an effective system with
enhanced build instruction retrieval module that achieves state-of-the-art
performance on BUILD-BENCH and is adaptable to heterogeneous OSS
characteristics. We also provide detailed analysis regarding different
compilation method design choices and their influence to the whole task,
offering insights to guide future advances. We believe performance on
BUILD-BENCH can faithfully reflect an agent's ability to tackle compilation as
a complex software engineering tasks, and, as such, our benchmark will spur
innovation with a significant impact on downstream applications in the fields
of software development and software security.

</details>


### [7] [Automated Code Development for PDE Solvers Using Large Language Models](https://arxiv.org/abs/2509.25194)
*Haoyang Wu,Xinxin Zhang,Lailai Zhu*

Main category: cs.SE

TL;DR: LLM-PDEveloper是一个零样本多智能体LLM框架，专门为偏微分方程(PDE)库的二次开发者自动化代码开发，能够将数学和算法描述直接转换为源代码。


<details>
  <summary>Details</summary>
Motivation: 利用基础模型（特别是大语言模型）的跨领域知识、文本处理和推理能力，为软件开发（如偏微分方程数值库）提供支持，现有研究主要面向终端用户自动化案例设置和执行，而本研究针对二次开发者。

Method: 采用零样本多智能体LLM框架，通过端到端的数学到代码方法，生成新的求解器/模块并适配现有组件，实现自增强的代码库扩展管道。

Result: 在三个任务上展示了有效性：1)为新PDE构建求解器，2)为给定PDE实现新边界条件，3)修改现有求解器以包含附加项，取得了中等成功率。分析了LLM产生的语法错误并提出有效修复方法。

Conclusion: LLM-PDEveloper能够持续扩展PDE库的代码库、增强其能力并拓宽应用范围，同时识别了某些语义错误的机制，为未来研究提供指导。

Abstract: Foundation models -- large language models (LLMs) in particular -- have
become ubiquitous, shaping daily life and driving breakthroughs across science,
engineering, and technology. Harnessing their broad cross-domain knowledge,
text-processing, and reasoning abilities for software development, e.g.,
numerical libraries for solving partial differential equations (PDEs), is
therefore attracting growing interest. Yet existing studies mainly automate
case setup and execution for end users. We introduce LLM-PDEveloper, a
zero-shot, multi-agent LLM framework that automates code development for PDE
libraries, specifically targeting secondary developers. By translating
mathematical and algorithmic descriptions directly into source code,
LLM-PDEveloper generates new solvers/modules and adapts existing ones. This
end-to-end math-to-code approach enables a self-augmenting pipeline that
continuously expands the codebase of a library, extends its capacities, and
broadens its scope. We demonstrate LLM-PDEveloper on three tasks: 1) build a
solver for a new PDE, 2) implement new BCs for a given PDE, and 3) modify an
existing solver to incorporate additional terms, achieving moderate success
rates. Failures due to syntactic errors made by LLMs are analyzed and we
propose effective fixes. We also identify the mechanisms underlying certain
semantic errors, guiding future research.

</details>


### [8] [Understanding Practitioners Perspectives on Monitoring Machine Learning Systems](https://arxiv.org/abs/2509.25195)
*Hira Naveed,John Grundy,Chetan Arora,Hourieh Khalajzadeh,Omar Haggag*

Main category: cs.SE

TL;DR: 该论文通过调查91名ML从业者，探讨了机器学习系统监控的策略、挑战和改进机会，发现从业者面临模型性能下降、延迟超时和安全违规等运行时问题，期望未来监控工具能提供自动化监控生成、性能公平性监控支持以及问题解决建议。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习系统的非确定性特性，其在生产环境中的行为可能导致不可预见且危险的结果。为了及时检测不良行为并防止组织遭受财务和声誉损失，监控这些系统至关重要。

Method: 通过全球调查91名ML从业者，收集关于当前ML系统监控实践的多样化见解，进行定性和定量分析，重点关注运行时问题、工业监控和缓解实践、关键挑战以及未来监控工具的期望改进。

Result: 从业者经常面临模型性能下降、延迟超时和安全违规等运行时问题。虽然大多数偏好自动化监控以提高效率，但由于复杂性或缺乏合适的自动化解决方案，许多人仍依赖手动方法。监控工具的初始设置和配置通常复杂且具有挑战性，监控增加了额外工作负担、消耗资源并导致警报疲劳。

Conclusion: 从业者期望的改进包括：监控器的自动生成和部署、改进的性能和公平性监控支持、以及解决运行时问题的建议。这些见解为未来开发更符合从业者需求的ML监控工具提供了宝贵指导。

Abstract: Given the inherent non-deterministic nature of machine learning (ML) systems,
their behavior in production environments can lead to unforeseen and
potentially dangerous outcomes. For a timely detection of unwanted behavior and
to prevent organizations from financial and reputational damage, monitoring
these systems is essential. This paper explores the strategies, challenges, and
improvement opportunities for monitoring ML systems from the practitioners
perspective. We conducted a global survey of 91 ML practitioners to collect
diverse insights into current monitoring practices for ML systems. We aim to
complement existing research through our qualitative and quantitative analyses,
focusing on prevalent runtime issues, industrial monitoring and mitigation
practices, key challenges, and desired enhancements in future monitoring tools.
Our findings reveal that practitioners frequently struggle with runtime issues
related to declining model performance, exceeding latency, and security
violations. While most prefer automated monitoring for its increased
efficiency, many still rely on manual approaches due to the complexity or lack
of appropriate automation solutions. Practitioners report that the initial
setup and configuration of monitoring tools is often complicated and
challenging, particularly when integrating with ML systems and setting alert
thresholds. Moreover, practitioners find that monitoring adds extra workload,
strains resources, and causes alert fatigue. The desired improvements from the
practitioners perspective are: automated generation and deployment of monitors,
improved support for performance and fairness monitoring, and recommendations
for resolving runtime issues. These insights offer valuable guidance for the
future development of ML monitoring tools that are better aligned with
practitioners needs.

</details>


### [9] [CircInspect: Integrating Visual Circuit Analysis, Abstraction, and Real-Time Development in Quantum Debugging](https://arxiv.org/abs/2509.25199)
*Mushahid Khan,Prashant J. Nair,Olivia Di Matteo*

Main category: cs.SE

TL;DR: CircInspect是一个用于调试Python和PennyLane中量子程序的交互式工具，通过断点和实时开发功能帮助用户分析量子电路组件、监控输出、可视化结构变化。


<details>
  <summary>Details</summary>
Motivation: 量子软件开发面临独特挑战，包括量子计算的概率性质、不同的算法原语和硬件噪声，需要专门的调试工具来应对这些复杂性。

Method: 开发了CircInspect工具，利用断点和实时软件开发功能，支持分析隔离的量子电路组件、监控程序输出、可视化结构变化和信息抽象。

Result: CircInspect为量子程序调试提供了专门的交互式解决方案，能够增强用户对量子程序的理解。

Conclusion: CircInspect是针对量子软件开发挑战的有效调试工具，通过其交互式特性提升了量子程序的分析和调试能力。

Abstract: Software bugs typically result from errors in specifications or code
translation. While classical software engineering has evolved with various
tools and methodologies to tackle such bugs, the emergence of quantum computing
presents unique challenges. Quantum software development introduces
complexities due to the probabilistic nature of quantum computing, distinct
algorithmic primitives, and potential hardware noise. In this paper, we
introduce CircInspect, an interactive tool tailored for debugging quantum
programs in Python and PennyLane. By leveraging breakpoints and real-time
software development features, \toolname~empowers users to analyze isolated
quantum circuit components, monitor program output, visualize structural
changes, and abstract information to enhance comprehension.

</details>


### [10] [Generating High-Quality Datasets for Code Editing via Open-Source Language Models](https://arxiv.org/abs/2509.25203)
*Zekai Zhang,Mingwei Liu,Zhenxi Chen,Linxi Liang,Yuxuan Chen,Guangsheng Ou,Yanlin Wang,Dan Li,Xin Peng,Zibin Zheng*

Main category: cs.SE

TL;DR: 提出了CanItEdit开源流水线，使用多个LLM合成真实的代码编辑三元组，构建了OCEDataFT数据集，通过微调显著提升了代码编辑性能，接近闭源系统表现。


<details>
  <summary>Details</summary>
Motivation: 现有的基于提交的数据集存在噪声大、多样性不足、无法反映真实编辑指令风格的问题，需要更好的数据集来支持代码编辑任务。

Method: 开发CanItEdit流水线，利用多个LLM合成代码编辑三元组，生成简洁和详细两种指令，通过差异和主题过滤保证数据质量和多样性，构建OCEDataFT数据集并微调基础模型。

Result: 在CanItEdit基准测试中，微调后的模型pass@1相对提升4.50%-20.79%，性能接近闭源系统，与GPT-4的差距缩小到仅3.54%。

Conclusion: CanItEdit流水线能够有效生成高质量的代码编辑数据集，通过微调可以显著提升模型性能，无需依赖专有资源或人工标注就能达到接近顶级闭源系统的效果。

Abstract: Code editing plays a vital role in software engineering, requiring developers
to adjust existing code according to natural language instructions while
keeping functionality intact and avoiding unnecessary modifications. However,
commit-based datasets commonly used for this task are often noisy, lack
diversity, and fail to reflect the style of real-world edit instructions. To
address this, we introduce CanItEdit, an open-source pipeline that leverages
multiple LLMs to synthesize realistic code-edit triplets. The pipeline produces
both concise "lazy" instructions and more detailed "descriptive" ones, and
applies filtering based on diffs and topics to guarantee data quality and
variety. Using this process, we construct OCEDataFT, a curated dataset of 20K
samples. Fine-tuning three advanced base models on OCEDataFT leads to
significant performance boosts on the CanItEdit benchmark, with relative pass@1
improvements ranging from 4.50% to 20.79%. Notably, the resulting models
achieve performance close to closed-source systems, narrowing the gap to GPT-4
to just 3.54%, without relying on proprietary resources or manual annotation.

</details>


### [11] [A Benchmark for Localizing Code and Non-Code Issues in Software Projects](https://arxiv.org/abs/2509.25242)
*Zejun Zhang,Jian Wang,Qingyun Yang,Yifan Pan,Yi Tang,Yi Li,Zhenchang Xing,Tian Zhang,Xuandong Li,Guoan Zhang*

Main category: cs.SE

TL;DR: 提出了MULocBench数据集，包含1100个来自46个Python项目的issue，用于评估软件维护中的项目定位问题，相比现有基准更具多样性。


<details>
  <summary>Details</summary>
Motivation: 现有issue定位基准如SWE-Bench和LocBench主要关注pull-request问题和代码位置，忽略了其他证据和非代码文件，需要更全面的评估基准。

Method: 构建包含1100个issue的MULocBench数据集，涵盖多种issue类型、根本原因、定位范围和文件类型，并使用该基准评估最先进的定位方法和5种基于LLM的提示策略。

Result: 当前技术存在显著局限性：即使在文件级别，性能指标（Acc@5, F1）仍低于40%，表明在现实多层面问题解决中泛化能力不足。

Conclusion: MULocBench为issue解决中的项目定位研究提供了更现实的测试平台，揭示了当前技术的挑战，并公开了数据集以促进未来研究。

Abstract: Accurate project localization (e.g., files and functions) for issue
resolution is a critical first step in software maintenance. However, existing
benchmarks for issue localization, such as SWE-Bench and LocBench, are limited.
They focus predominantly on pull-request issues and code locations, ignoring
other evidence and non-code files such as commits, comments, configurations,
and documentation. To address this gap, we introduce MULocBench, a
comprehensive dataset of 1,100 issues from 46 popular GitHub Python projects.
Comparing with existing benchmarks, MULocBench offers greater diversity in
issue types, root causes, location scopes, and file types, providing a more
realistic testbed for evaluation. Using this benchmark, we assess the
performance of state-of-the-art localization methods and five LLM-based
prompting strategies. Our results reveal significant limitations in current
techniques: even at the file level, performance metrics (Acc@5, F1) remain
below 40%. This underscores the challenge of generalizing to realistic,
multi-faceted issue resolution. To enable future research on project
localization for issue resolution, we publicly release MULocBench at
https://huggingface.co/datasets/somethingone/MULocBench.

</details>


### [12] [Reinforcement Learning-Guided Chain-of-Draft for Token-Efficient Code Generation](https://arxiv.org/abs/2509.25243)
*Xunzhu Tang,Iyiola Emmanuel Olatunji,Tiezhu Sun,Jacques Klein,Tegawende F. Bissyande*

Main category: cs.SE

TL;DR: MultiCoD是一个强化学习框架，通过从Chain-of-Draft生成的多个候选解决方案中选择最优解，在保持代码生成质量的同时显著降低用户成本。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在代码生成中虽然表面流畅，但在需要正确性和语义对齐的结构化推理任务中表现不佳。Chain-of-Thought提示存在冗长和效率低的问题，而Chain-of-Draft的随机性导致解决方案质量不稳定，难以选择最优解。

Method: 提出MultiCoD强化学习框架，使用策略引导提示鼓励多样化推理风格，将解决方案选择建模为上下文多臂老虎机问题。通过奖励函数优化代码复杂度、推理结构和战略元数据等可解释特征。

Result: 在MBPP、BigCodeBench、SWE-bench Verified和Defects4J等基准测试中，MultiCoD优于或与标准提示、CoT和CoD基线相当，同时通过多候选设计将用户账单减少50%以上，提高了LLM响应质量。

Conclusion: MultiCoD通过有效的候选选择和成本优化，为现实世界部署提供了更可持续和可扩展的代码生成解决方案。

Abstract: LLMs demonstrate surface-level fluency in code generation but struggle with
structured reasoning tasks requiring correctness and semantic alignment. While
Chain-of-Thought (CoT) prompting enhances reasoning through intermediate steps,
it suffers from verbosity and inefficiency. Chain-of-Draft (CoD) prompting
offers more concise reasoning, but the stochastic nature of LLMs produces
varying solution quality, making optimal selection challenging. We propose
\multicod, a reinforcement learning framework that learns to select the most
promising candidate from CoD-generated solutions. Our approach uses
strategy-guided prompting to encourage diverse reasoning styles and models
solution selection as a contextual bandit problem. The framework optimizes
interpretable features including code complexity, reasoning structure, and
strategic metadata through a reward function balancing correctness, efficiency,
and clarity. Experiments on MBPP, BigCodeBench, SWE-bench Verified, and
Defects4J show \multicod~outperforms and in some cases, on par with standard
prompting, CoT, and CoD baselines while achieving cost and token efficiency
from the user's perspective through a multi-candidate design that charges only
for the selected output, reducing user billing by over 50\% and improving LLM
response quality, making \multicod~more sustainable and scalable for real-world
deployment. Our code is available: https://anonymous.4open.science/r/MultiCoD.

</details>


### [13] [Protocode: Prototype-Driven Interpretability for Code Generation in LLMs](https://arxiv.org/abs/2509.25247)
*Krishna Vamshi Bodla,Haizhao Yang*

Main category: cs.SE

TL;DR: 本文提出了一种自动采样上下文学习(ICL)演示的方法，通过AST分析识别代码中受演示影响最大的区域，以提高大语言模型代码生成的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在代码生成中的广泛应用，虽然提高了开发效率，但也带来了次优解决方案和不安全代码的风险增加。本文旨在通过优化ICL演示选择来改善模型性能和代码可解释性。

Method: 基于MBPP测试集的输出进行AST分析，识别代码中受演示影响最大的区域，自动采样高质量的ICL演示。

Result: 高质量ICL演示不仅使输出更易解释，还在pass@10指标上带来正向性能提升；而低质量演示则会对模型性能产生负面影响。

Conclusion: 高效的ICL采样策略对模型性能至关重要，合适的演示选择能显著影响模型在特定任务上的表现。

Abstract: Since the introduction of Large Language Models (LLMs), they have been widely
adopted for various tasks such as text summarization, question answering,
speech-to-text translation, and more. In recent times, the use of LLMs for code
generation has gained significant attention, with tools such as Cursor and
Windsurf demonstrating the ability to analyze massive code repositories and
recommend relevant changes. Big tech companies have also acknowledged the
growing reliance on LLMs for code generation within their codebases. Although
these advances significantly improve developer productivity, increasing
reliance on automated code generation can proportionally increase the risk of
suboptimal solutions and insecure code. Our work focuses on automatically
sampling In-Context Learning (ICL) demonstrations which can improve model
performance and enhance the interpretability of the generated code. Using
AST-based analysis on outputs from the MBPP test set, we identify regions of
code most influenced by the chosen demonstrations. In our experiments, we show
that high-quality ICL demonstrations not only make outputs easier to interpret
but also yield a positive performance improvement on the pass@10 metric.
Conversely, poorly chosen ICL demonstrations affected the LLM performance on
the pass@10 metric negatively compared to the base model. Overall, our approach
highlights the importance of efficient sampling strategies for ICL, which can
affect the performance of the model on any given task.

</details>


### [14] [RANGER -- Repository-Level Agent for Graph-Enhanced Retrieval](https://arxiv.org/abs/2509.25257)
*Pratik Shah,Rajat Ghosh,Aryan Singhal,Debojyoti Dutta*

Main category: cs.SE

TL;DR: RANGER是一个仓库级代码检索代理，能够处理代码实体查询和自然语言查询，通过构建知识图谱和双阶段检索流程，在多个代码检索任务中表现优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的代码检索系统主要关注代码实体查询，但缺乏对自然语言查询的支持。需要一种能够同时处理两种查询类型的通用代码检索系统来支持自动化软件工程任务。

Method: 首先构建包含层次结构和跨文件依赖关系的知识图谱，节点包含文本描述和嵌入表示。然后采用双阶段检索流程：代码实体查询使用Cypher查询，自然语言查询使用MCTS引导的图探索。

Result: 在CodeSearchNet和RepoQA上优于使用Qwen3-8B等强大模型嵌入的检索基线；在RepoBench上实现更好的跨文件依赖检索；在CrossCodeEval上与BM25结合获得最高的代码补全精确匹配率。

Conclusion: RANGER通过结合知识图谱和双阶段检索策略，有效解决了仓库级代码检索问题，在多种自动化软件工程任务中表现出色，填补了现有方法主要关注代码实体查询的空白。

Abstract: General-purpose automated software engineering (ASE) includes tasks such as
code completion, retrieval, repair, QA, and summarization. These tasks require
a code retrieval system that can handle specific queries about code entities,
or code entity queries (for example, locating a specific class or retrieving
the dependencies of a function), as well as general queries without explicit
code entities, or natural language queries (for example, describing a task and
retrieving the corresponding code). We present RANGER, a repository-level code
retrieval agent designed to address both query types, filling a gap in recent
works that have focused primarily on code-entity queries. We first present a
tool that constructs a comprehensive knowledge graph of the entire repository,
capturing hierarchical and cross-file dependencies down to the variable level,
and augments graph nodes with textual descriptions and embeddings to bridge the
gap between code and natural language. RANGER then operates on this graph
through a dual-stage retrieval pipeline. Entity-based queries are answered
through fast Cypher lookups, while natural language queries are handled by
MCTS-guided graph exploration. We evaluate RANGER across four diverse
benchmarks that represent core ASE tasks including code search, question
answering, cross-file dependency retrieval, and repository-level code
completion. On CodeSearchNet and RepoQA it outperforms retrieval baselines that
use embeddings from strong models such as Qwen3-8B. On RepoBench, it achieves
superior cross-file dependency retrieval over baselines, and on CrossCodeEval,
pairing RANGER with BM25 delivers the highest exact match rate in code
completion compared to other RAG methods.

</details>


### [15] [Automatically Generating Web Applications from Requirements Via Multi-Agent Test-Driven Development](https://arxiv.org/abs/2509.25297)
*Yuxuan Wan,Tingshuo Liang,Jiakai Xu,Jingyu Xiao,Yintong Huo,Michael R. Lyu*

Main category: cs.SE

TL;DR: TDDev是首个基于测试驱动开发(TDD)的LLM代理框架，能够从自然语言描述或设计图像自动生成端到端的全栈Web应用，通过自动生成测试用例、前后端代码和迭代优化来实现功能完整的高质量应用。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型虽然能从视觉输入生成网页，但仅限于前端任务，无法交付功能完整的全栈应用。全栈开发复杂度高，需要跨多种技术栈的熟练技能。

Method: 采用测试驱动开发(TDD)方法，自动生成可执行测试用例，生成前后端代码，模拟用户交互，并通过迭代优化直到满足所有需求。解决了用户需求不明确、多文件复杂依赖关系等关键挑战。

Result: 在多样化应用场景的广泛实验中，TDDev相比最先进基线方法在整体准确率上提升了14.4%，能够无需人工干预生成可靠、高质量的Web应用。

Conclusion: TDDev框架有效解决了全栈Web应用自动化的关键挑战，能够生成功能正确且视觉保真的完整应用，显著优于现有方法。

Abstract: Developing full-stack web applications is complex and time-intensive,
demanding proficiency across diverse technologies and frameworks. Although
recent advances in multimodal large language models (MLLMs) enable automated
webpage generation from visual inputs, current solutions remain limited to
front-end tasks and fail to deliver fully functional applications. In this
work, we introduce TDDev, the first test-driven development (TDD)-enabled
LLM-agent framework for end-to-end full-stack web application generation. Given
a natural language description or design image, TDDev automatically derives
executable test cases, generates front-end and back-end code, simulates user
interactions, and iteratively refines the implementation until all requirements
are satisfied. Our framework addresses key challenges in full-stack automation,
including underspecified user requirements, complex interdependencies among
multiple files, and the need for both functional correctness and visual
fidelity. Through extensive experiments on diverse application scenarios, TDDev
achieves a 14.4% improvement on overall accuracy compared to state-of-the-art
baselines, demonstrating its effectiveness in producing reliable, high-quality
web applications without requiring manual intervention.

</details>


### [16] [Detecting and Fixing API Misuses of Data Science Libraries Using Large Language Models](https://arxiv.org/abs/2509.25378)
*Akalanka Galappaththi,Francisco Ribeiro,Sarah Nadi*

Main category: cs.SE

TL;DR: DSCHECKER是一个基于LLM的方法，用于检测和修复数据科学库中的API误用。通过整合API指令和数据信息，该方法在检测和修复API误用方面表现良好，最佳模型检测F1分数达61.18%，修复率达51.28%。


<details>
  <summary>Details</summary>
Motivation: 数据科学库（如scikit-learn和pandas）的数据中心特性使得API误用检测更具挑战性，需要专门的方法来解决这一问题。

Method: 提出DSCHECKER方法，识别API指令和数据信息两个关键信息，使用三个LLM和五种数据科学库的误用进行实验，并实现具有自适应函数调用机制的DSCHECKER代理。

Result: 最佳模型检测F1分数为61.18%，修复率为51.28%；DSCHECKER代理检测F1分数为48.65%，修复率为39.47%。

Conclusion: 基于LLM的API误用检测和修复在真实场景中具有潜力，整合API指令和数据特定细节能显著提升性能。

Abstract: Data science libraries, such as scikit-learn and pandas, specialize in
processing and manipulating data. The data-centric nature of these libraries
makes the detection of API misuse in them more challenging. This paper
introduces DSCHECKER, an LLM-based approach designed for detecting and fixing
API misuses of data science libraries. We identify two key pieces of
information, API directives and data information, that may be beneficial for
API misuse detection and fixing. Using three LLMs and misuses from five data
science libraries, we experiment with various prompts. We find that
incorporating API directives and data-specific details enhances Dschecker's
ability to detect and fix API misuses, with the best-performing model achieving
a detection F1-score of 61.18 percent and fixing 51.28 percent of the misuses.
Building on these results, we implement Dschecker agent which includes an
adaptive function calling mechanism to access information on demand, simulating
a real-world setting where information about the misuse is unknown in advance.
We find that Dschecker agent achieves 48.65 percent detection F1-score and
fixes 39.47 percent of the misuses, demonstrating the promise of LLM-based API
misuse detection and fixing in real-world scenarios.

</details>


### [17] [A Cartography of Open Collaboration in Open Source AI: Mapping Practices, Motivations, and Governance in 14 Open Large Language Model Projects](https://arxiv.org/abs/2509.25397)
*Johan Linåker,Cailean Osborne,Jennifer Ding,Ben Burtenshaw*

Main category: cs.SE

TL;DR: 本文通过访谈14个开源大语言模型项目的开发者，系统分析了开源LLM项目在开发和重用生命周期中的协作模式、动机和组织结构。


<details>
  <summary>Details</summary>
Motivation: 目前对开源LLM项目在发布前后的协作方式缺乏全面研究，限制了我们对这些项目如何启动、组织和治理的理解，以及如何进一步促进这个生态系统的发展。

Method: 采用探索性分析方法，通过对来自草根项目、研究机构、初创公司和大型科技公司的14个开源LLM项目开发者进行半结构化访谈。

Result: 发现开源LLM协作不仅限于模型本身，还包括数据集、基准测试、开源框架等多个方面；开发者具有多样化的社会、经济和技术动机；项目呈现五种不同的组织模式，在控制集中度和社区参与策略上存在差异。

Conclusion: 为寻求支持全球社区构建更开放AI未来的利益相关者提供了实用建议。

Abstract: The proliferation of open large language models (LLMs) is fostering a vibrant
ecosystem of research and innovation in artificial intelligence (AI). However,
the methods of collaboration used to develop open LLMs both before and after
their public release have not yet been comprehensively studied, limiting our
understanding of how open LLM projects are initiated, organized, and governed
as well as what opportunities there are to foster this ecosystem even further.
We address this gap through an exploratory analysis of open collaboration
throughout the development and reuse lifecycle of open LLMs, drawing on
semi-structured interviews with the developers of 14 open LLMs from grassroots
projects, research institutes, startups, and Big Tech companies in North
America, Europe, Africa, and Asia. We make three key contributions to research
and practice. First, collaboration in open LLM projects extends far beyond the
LLMs themselves, encompassing datasets, benchmarks, open source frameworks,
leaderboards, knowledge sharing and discussion forums, and compute
partnerships, among others. Second, open LLM developers have a variety of
social, economic, and technological motivations, from democratizing AI access
and promoting open science to building regional ecosystems and expanding
language representation. Third, the sampled open LLM projects exhibit five
distinct organizational models, ranging from single company projects to
non-profit-sponsored grassroots projects, which vary in their centralization of
control and community engagement strategies used throughout the open LLM
lifecycle. We conclude with practical recommendations for stakeholders seeking
to support the global community building a more open future for AI.

</details>


### [18] [PIPer: On-Device Environment Setup via Online Reinforcement Learning](https://arxiv.org/abs/2509.25455)
*Alexander Kovrigin,Aleksandra Eliseeva,Konstantin Grotov,Egor Bogomolov,Yaroslav Zharov*

Main category: cs.SE

TL;DR: 本文提出了一种结合监督微调和强化学习的专门模型，用于自动化软件环境设置任务，使较小的模型在性能上能与大型模型相媲美。


<details>
  <summary>Details</summary>
Motivation: 软件环境设置是软件工程中的持续挑战，现有的大语言模型在此任务上表现有限，需要专门的方法来提高自动化环境配置的成功率。

Method: 结合监督微调生成正确的Bash脚本，并使用带可验证奖励的强化学习来适应环境设置任务。

Result: 在EnvBench-Python基准测试中，该方法使Qwen3-8B模型的表现与Qwen3-32B和GPT-4o等大型模型相当。

Conclusion: 通过专门的任务调优，较小的模型也能在环境设置任务上达到与大型模型相当的性能，为软件工程自动化提供了可行方案。

Abstract: Environment setup-the process of configuring the system to work with a
specific software project-represents a persistent challenge in Software
Engineering (SE). Automated environment setup methods could assist developers
by providing fully configured environments for arbitrary repositories without
manual effort. This also helps SE researchers to scale execution-based
benchmarks. However, recent studies reveal that even state-of-the-art Large
Language Models (LLMs) achieve limited success in automating this task. To
address this limitation, we tune a specialized model for environment setup. We
combine supervised fine-tuning for generating correct Bash scripts and
Reinforcement Learning with Verifiable Rewards (RLVR) to adapt it to the task
of environment setup. On EnvBench-Python, our method enables Qwen3-8B (a model
runnable on consumer hardware) to perform on par with larger models-Qwen3-32B
and GPT-4o. The training code and model checkpoints are available online:
https://github.com/JetBrains-Research/PIPer.

</details>


### [19] [BloomAPR: A Bloom's Taxonomy-based Framework for Assessing the Capabilities of LLM-Powered APR Solutions](https://arxiv.org/abs/2509.25465)
*Yinghang Ma,Jiho Shin,Leuson Da Silva,Zhen Ming,Jiang,Song Wang,Foutse Khomh,Shin Hwei Tan*

Main category: cs.SE

TL;DR: 提出了BloomAPR评估框架，基于布鲁姆分类法动态评估LLM驱动的自动程序修复能力，发现现有方法在记忆层面表现良好但高级推理能力不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于静态基准的评估存在数据污染风险，且无法评估LLM在动态多样化环境中的程序修复能力。

Method: 基于布鲁姆分类法构建动态评估框架，使用Defects4J作为案例研究，评估三种LLM在两个最先进APR解决方案上的表现。

Result: LLM驱动的APR解决方案在记忆层面修复了81.57%的bug，但在应用层面仅修复43.32%，在分析层面仅修复13.46%-41.34%的bug。

Conclusion: 需要发展更先进的基准测试，为LLM驱动的软件工程解决方案提供更可靠的评估基础。

Abstract: Recent advances in large language models (LLMs) have accelerated the
development of AI-driven automated program repair (APR) solutions. However,
these solutions are typically evaluated using static benchmarks such as
Defects4J and SWE-bench, which suffer from two key limitations: (1) the risk of
data contamination, potentially inflating evaluation results due to overlap
with LLM training data, and (2) limited ability to assess the APR capabilities
in dynamic and diverse contexts. In this paper, we introduced BloomAPR, a novel
dynamic evaluation framework grounded in Bloom's Taxonomy. Our framework offers
a structured approach to assess the cognitive capabilities of LLM-powered APR
solutions across progressively complex reasoning levels. Using Defects4J as a
case study, we evaluated two state-of-the-art LLM-powered APR solutions,
ChatRepair and CigaR, under three different LLMs: GPT-3.5-Turbo, Llama-3.1, and
StarCoder-2. Our findings show that while these solutions exhibit basic
reasoning skills and effectively memorize bug-fixing patterns (fixing up to
81.57% of bugs at the Remember layer), their performance increases with
synthetically generated bugs (up to 60.66% increase at the Understand layer).
However, they perform worse on minor syntactic changes (fixing up to 43.32% at
the Apply layer), and they struggle to repair similar bugs when injected into
real-world projects (solving only 13.46% to 41.34% bugs at the Analyze layer).
These results underscore the urgent need for evolving benchmarks and provide a
foundation for more trustworthy evaluation of LLM-powered software engineering
solutions.

</details>


### [20] [AGNOMIN -- Architecture Agnostic Multi-Label Function Name Prediction](https://arxiv.org/abs/2509.25514)
*Yonatan Gizachew Achamyeleh,Tongtao Zhang,Joshua Hyunki Kim,Gabriel Garcia,Shih-Yuan Yu,Anton Kocheturov,Mohammad Abdullah Al Faruque*

Main category: cs.SE

TL;DR: AGNOMIN是一种架构无关的多标签函数名预测方法，通过构建特征增强层次图和使用层次图神经网络，在剥离二进制文件中实现跨架构的函数名预测，显著提升安全评估效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在软件逆向工程中面临架构特定限制、数据稀缺和命名规范多样的问题，而函数名预测对于后续漏洞分析和补丁至关重要。

Method: 构建特征增强层次图（FEHGs），结合控制流图、函数调用图和动态学习的PCode特征，使用层次图神经网络生成跨架构一致的函数表示，并采用Renée启发式解码器进行函数名预测。

Result: 在9,000个ELF可执行二进制文件上评估，相比最先进方法，精确度提升高达27.17%，召回率提升55.86%，在未见架构上召回率比最接近基线高5.89%。

Conclusion: AGNOMIN在安全黑客马拉松中验证了其实际效用，成功帮助逆向工程师分析和修补不同架构的易受攻击二进制文件，展示了良好的泛化能力和实用性。

Abstract: Function name prediction is crucial for understanding stripped binaries in
software reverse engineering, a key step for \textbf{enabling subsequent
vulnerability analysis and patching}. However, existing approaches often
struggle with architecture-specific limitations, data scarcity, and diverse
naming conventions. We present AGNOMIN, a novel architecture-agnostic approach
for multi-label function name prediction in stripped binaries. AGNOMIN builds
Feature-Enriched Hierarchical Graphs (FEHGs), combining Control Flow Graphs,
Function Call Graphs, and dynamically learned \texttt{PCode} features. A
hierarchical graph neural network processes this enriched structure to generate
consistent function representations across architectures, vital for
\textbf{scalable security assessments}. For function name prediction, AGNOMIN
employs a Ren\'ee-inspired decoder, enhanced with an attention-based head layer
and algorithmic improvements.
  We evaluate AGNOMIN on a comprehensive dataset of 9,000 ELF executable
binaries across three architectures, demonstrating its superior performance
compared to state-of-the-art approaches, with improvements of up to 27.17\% in
precision and 55.86\% in recall across the testing dataset. Moreover, AGNOMIN
generalizes well to unseen architectures, achieving 5.89\% higher recall than
the closest baseline. AGNOMIN's practical utility has been validated through
security hackathons, where it successfully aided reverse engineers in analyzing
and patching vulnerable binaries across different architectures.

</details>


### [21] [M&SCheck: Towards a Checklist to Support Software Engineering Newcomers to the Modeling and Simulation Area](https://arxiv.org/abs/2509.25625)
*Luiza Martins de Freitas Cintra,Philipp Zech,Mohamad Kassab,Eliomar Araújo Lima,Sofia Larissa da Costa Paiva,Valdemar Vicente Graciano Neto*

Main category: cs.SE

TL;DR: 本文提出了一个初步检查清单，帮助初学者在建模与仿真中选择最合适的范式（DEVS、系统动力学和基于代理的仿真），以解决复杂生态系统中的问题。


<details>
  <summary>Details</summary>
Motivation: 随着数字孪生、智慧城市和工业4.0/5.0等复杂动态生态系统的出现，需要在软件开发周期中融入建模与仿真。但软件工程师特别是新手在选择合适的形式化方法时面临困难。

Method: 基于DEVS、系统动力学和基于代理仿真三种主要形式化方法，建立了一个初步检查清单，并通过试点研究和专家咨询进行评估。

Result: 初步结果显示：(i) 检查清单的建议与原始研究中选用的形式化方法一致；(ii) 专家反馈积极。

Conclusion: 该检查清单为建模与仿真初学者提供了有效的指导工具，有助于选择合适的仿真范式来解决实际问题。

Abstract: The advent of increasingly complex and dynamic ecosystems, such as digital
twins (DT), smart cities and Industry 4.0 and 5.0, has made evident the need to
include modeling and simulation (M&S) in the software development life cycle.
Such disruptive systems include simulation models in their own architecture
(such as DT) or require the use of simulation models to represent the high
degree of movement and the multiplicity of interactions that occur between the
involved systems. However, when software engineers (particularly the newcomers)
need to use M&S in their projects, they often pose themselves an important
question: which formalism should I use? In this direction, the main
contribution of this paper is the establishment of a preliminary checklist with
questions to assist beginners in M&S in choosing the most appropriate paradigm
to solve their problems. The checklist is based on three main formalisms: DEVS,
System Dynamics and Agent-Based Simulation. A pilot study was carried out and
an expert was consulted. The preliminary results show (i) conformance between
the suggestion given by the checklist and the formalism selected in the
original studies used as input for evaluating the checklist, and (ii) a
positive feedback from the expert.

</details>


### [22] [Explainable Fault Localization for Programming Assignments via LLM-Guided Annotation](https://arxiv.org/abs/2509.25676)
*Fang Liu,Tianze Wang,Li Zhang,Zheyu Yang,Jing Jiang,Zian Sun*

Main category: cs.SE

TL;DR: FLAME是一个针对编程作业的细粒度、可解释的故障定位方法，通过LLM引导的标注和模型集成来提高故障定位的准确性和教育价值。


<details>
  <summary>Details</summary>
Motivation: 现有故障定位技术在教育环境中面临挑战：大多数方法在方法级别操作且缺乏解释性反馈，粒度太粗；而一些行级方法直接预测行号，不适合LLM。需要为编程作业提供细粒度、可解释的故障定位。

Method: FLAME利用编程作业的丰富上下文信息引导LLM识别错误代码行，通过标注而非直接预测行号的方式，结合详细解释。采用加权多模型投票策略聚合多个LLM的结果来确定每行代码的可疑度。

Result: 在编程作业上，FLAME优于最先进的故障定位基线方法，在top-1位置成功定位了比最佳基线多207个故障。在Defects4J基准测试中也优于所有基线方法。

Conclusion: FLAME为编程作业提供了有效的细粒度故障定位，不仅在教育环境中表现出色，还能有效泛化到通用软件代码库。

Abstract: Providing timely and personalized guidance for students' programming
assignments, offers significant practical value for helping students complete
assignments and enhance their learning. In recent years, various automated
Fault Localization (FL) techniques have demonstrated promising results in
identifying errors in programs. However, existing FL techniques face challenges
when applied to educational contexts. Most approaches operate at the method
level without explanatory feedback, resulting in granularity too coarse for
students who need actionable insights to identify and fix their errors. While
some approaches attempt line-level fault localization, they often depend on
predicting line numbers directly in numerical form, which is ill-suited to
LLMs. To address these challenges, we propose FLAME, a fine-grained,
explainable Fault Localization method tailored for programming assignments via
LLM-guided Annotation and Model Ensemble. FLAME leverages rich contextual
information specific to programming assignments to guide LLMs in identifying
faulty code lines. Instead of directly predicting line numbers, we prompt the
LLM to annotate faulty code lines with detailed explanations, enhancing both
localization accuracy and educational value. To further improve reliability, we
introduce a weighted multi-model voting strategy that aggregates results from
multiple LLMs to determine the suspiciousness of each code line. Extensive
experimental results demonstrate that FLAME outperforms state-of-the-art fault
localization baselines on programming assignments, successfully localizing 207
more faults at top-1 over the best-performing baseline. Beyond educational
contexts, FLAME also generalizes effectively to general-purpose software
codebases, outperforming all baselines on the Defects4J benchmark.

</details>


### [23] [DeepCodeSeek: Real-Time API Retrieval for Context-Aware Code Generation](https://arxiv.org/abs/2509.25716)
*Esakkivel Esakkiraja,Denis Akhiyarov,Aditya Shanmugham,Chitra Ganapathy*

Main category: cs.SE

TL;DR: 提出了一种新颖的代码生成技术，通过扩展代码索引来预测所需API，解决了现有代码基准数据集中的API泄露问题，并开发了高效的0.6B重排器，在保持低延迟的同时超越更大模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前搜索技术仅限于标准RAG查询-文档应用，无法有效处理代码生成中API使用意图不明确的问题，且现有基准数据集存在API泄露问题。

Method: 引入基于真实ServiceNow脚本的新数据集，开发包含合成数据集生成、监督微调和强化学习的完整后训练流程，优化紧凑的0.6B重排器。

Result: 达到87.86%的top-40检索准确率，紧凑重排器性能超越8B模型，同时延迟降低2.5倍，有效处理企业特定代码的细微差别。

Conclusion: 该方法成功解决了API使用意图不明确的问题，为自动补全和智能AI应用提供了高质量的端到端代码生成能力，且计算开销显著降低。

Abstract: Current search techniques are limited to standard RAG query-document
applications. In this paper, we propose a novel technique to expand the code
and index for predicting the required APIs, directly enabling high-quality,
end-to-end code generation for auto-completion and agentic AI applications. We
address the problem of API leaks in current code-to-code benchmark datasets by
introducing a new dataset built from real-world ServiceNow Script Includes that
capture the challenge of unclear API usage intent in the code. Our evaluation
metrics show that this method achieves 87.86% top-40 retrieval accuracy,
allowing the critical context with APIs needed for successful downstream code
generation. To enable real-time predictions, we develop a comprehensive
post-training pipeline that optimizes a compact 0.6B reranker through synthetic
dataset generation, supervised fine-tuning, and reinforcement learning. This
approach enables our compact reranker to outperform a much larger 8B model
while maintaining 2.5x reduced latency, effectively addressing the nuances of
enterprise-specific code without the computational overhead of larger models.

</details>


### [24] [Are Classical Clone Detectors Good Enough For the AI Era?](https://arxiv.org/abs/2509.25754)
*Ajmain Inqiad Alam,Palash Roy,Farouq Al-omari,Chanchal Roy,Banani Roy,Kevin Schneider*

Main category: cs.SE

TL;DR: 评估9种经典代码克隆检测工具在AI生成代码上的有效性，发现经过有效规范化技术增强的工具仍能保持相当效果，但与传统基准相比存在性能差异。


<details>
  <summary>Details</summary>
Motivation: AI生成代码的普及引入了语法和语义变体，对传统主要针对人工编写代码优化的代码克隆检测工具提出了新挑战。

Method: 使用GPTCloneBench基准测试9种广泛使用的CCD工具，并在BigCloneBench和SemanticCloneBench等传统基准上进行对比验证。

Result: 经典CCD工具特别是经过规范化技术增强的工具对AI生成克隆仍保持相当有效性，但与传统基准相比性能存在差异。

Conclusion: 规范化技术在提高检测准确性中发挥重要作用，研究为实际CCD工具选择提供了详细的可扩展性和执行时间分析。

Abstract: The increasing adoption of AI-generated code has reshaped modern software
development, introducing syntactic and semantic variations in cloned code.
Unlike traditional human-written clones, AI-generated clones exhibit systematic
syntactic patterns and semantic differences learned from large-scale training
data. This shift presents new challenges for classical code clone detection
(CCD) tools, which have historically been validated primarily on human-authored
codebases and optimized to detect syntactic (Type 1-3) and limited semantic
clones. Given that AI-generated code can produce both syntactic and complex
semantic clones, it is essential to evaluate the effectiveness of classical CCD
tools within this new paradigm. In this paper, we systematically evaluate nine
widely used CCD tools using GPTCloneBench, a benchmark containing
GPT-3-generated clones. To contextualize and validate our results, we further
test these detectors on established human-authored benchmarks, BigCloneBench
and SemanticCloneBench, to measure differences in performance between
traditional and AI-generated clones. Our analysis demonstrates that classical
CCD tools, particularly those enhanced by effective normalization techniques,
retain considerable effectiveness against AI-generated clones, while some
exhibit notable performance variation compared to traditional benchmarks. This
paper contributes by (1) evaluating classical CCD tools against AI-generated
clones, providing critical insights into their current strengths and
limitations; (2) highlighting the role of normalization techniques in improving
detection accuracy; and (3) delivering detailed scalability and execution-time
analyses to support practical CCD tool selection.

</details>


### [25] [LogPilot: Intent-aware and Scalable Alert Diagnosis for Large-scale Online Service Systems](https://arxiv.org/abs/2509.25874)
*Zhihan Jiang,Jinyang Liu,Yichen Li,Haiyu Huang,Xiao He,Tieying Zhang,Jianjun Chen,Yi Li,Rui Shi,Michael R. Lyu*

Main category: cs.SE

TL;DR: LogPilot是一个基于大语言模型的自动化日志诊断框架，通过意图感知和可扩展的方法解决大规模在线服务系统中的告警诊断问题，显著提升了根因定位的准确性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化工具在告警诊断中存在局限性：告警无关的日志范围界定不准确，无法有效组织复杂数据以支持推理。值班工程师需要手动检查大量日志来定位根因，负担沉重。

Method: LogPilot采用意图感知方法解析告警定义逻辑，精确识别因果相关的日志和请求。通过将请求执行重构为时空日志链，聚类相似链以识别重复执行模式，并为LLM提供代表性样本进行诊断。

Result: 在火山引擎云的真实告警评估中，LogPilot将根因总结的有用性提升了50.34%，精确定位准确率提升了54.79%。诊断时间低于1分钟，每个告警成本仅0.074美元，已成功部署到生产环境。

Conclusion: LogPilot提供了一个自动化且实用的服务告警诊断解决方案，通过聚类方法确保输入既包含丰富的诊断细节又足够紧凑以适应LLM的上下文窗口。

Abstract: Effective alert diagnosis is essential for ensuring the reliability of
large-scale online service systems. However, on-call engineers are often
burdened with manually inspecting massive volumes of logs to identify root
causes. While various automated tools have been proposed, they struggle in
practice due to alert-agnostic log scoping and the inability to organize
complex data effectively for reasoning. To overcome these limitations, we
introduce LogPilot, an intent-aware and scalable framework powered by Large
Language Models (LLMs) for automated log-based alert diagnosis. LogPilot
introduces an intent-aware approach, interpreting the logic in alert
definitions (e.g., PromQL) to precisely identify causally related logs and
requests. To achieve scalability, it reconstructs each request's execution into
a spatiotemporal log chain, clusters similar chains to identify recurring
execution patterns, and provides representative samples to the LLMs for
diagnosis. This clustering-based approach ensures the input is both rich in
diagnostic detail and compact enough to fit within the LLM's context window.
Evaluated on real-world alerts from Volcano Engine Cloud, LogPilot improves the
usefulness of root cause summarization by 50.34% and exact localization
accuracy by 54.79% over state-of-the-art methods. With a diagnosis time under
one minute and a cost of only $0.074 per alert, LogPilot has been successfully
deployed in production, offering an automated and practical solution for
service alert diagnosis.

</details>


### [26] [Red Teaming Program Repair Agents: When Correct Patches can Hide Vulnerabilities](https://arxiv.org/abs/2509.25894)
*Simin Chen,Yixin He,Suman Jana,Baishakhi Ray*

Main category: cs.SE

TL;DR: SWExploit是一种针对LLM驱动的自动程序修复代理的攻击方法，通过生成对抗性问题描述，诱导代理生成功能正确但存在漏洞的补丁。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注自动程序修复生成补丁的功能正确性，而忽视了潜在的安全风险。由于GitHub等平台的开放性，恶意用户可能提交有效问题误导LLM代理生成功能正确但易受攻击的补丁。

Method: SWExploit包含三个主要步骤：程序分析识别漏洞注入点；对抗性问题生成，在保持原始问题语义的同时提供误导性重现和错误信息；基于APR代理输出的迭代优化。

Result: 在三个代理流水线和五个后端LLM上的实证评估显示，SWExploit能生成功能正确且存在漏洞的补丁，攻击成功率可达0.91，而基线攻击成功率均低于0.20。

Conclusion: 该研究首次挑战了传统假设——通过所有测试的补丁天生可靠安全，突显了当前APR代理评估范式的关键局限性。

Abstract: LLM-based agents are increasingly deployed for software maintenance tasks
such as automated program repair (APR). APR agents automatically fetch GitHub
issues and use backend LLMs to generate patches that fix the reported bugs.
However, existing work primarily focuses on the functional correctness of
APR-generated patches, whether they pass hidden or regression tests, while
largely ignoring potential security risks. Given the openness of platforms like
GitHub, where any user can raise issues and participate in discussions, an
important question arises: Can an adversarial user submit a valid issue on
GitHub that misleads an LLM-based agent into generating a functionally correct
but vulnerable patch? To answer this question, we propose SWExploit, which
generates adversarial issue statements designed to make APR agents produce
patches that are functionally correct yet vulnerable. SWExploit operates in
three main steps: (1) program analysis to identify potential injection points
for vulnerable payloads; (2) adversarial issue generation to provide misleading
reproduction and error information while preserving the original issue
semantics; and (3) iterative refinement of the adversarial issue statements
based on the outputs of the APR agents. Empirical evaluation on three agent
pipelines and five backend LLMs shows that SWExploit can produce patches that
are both functionally correct and vulnerable (the attack success rate on the
correct patch could reach 0.91, whereas the baseline ASRs are all below 0.20).
Based on our evaluation, we are the first to challenge the traditional
assumption that a patch passing all tests is inherently reliable and secure,
highlighting critical limitations in the current evaluation paradigm for APR
agents.

</details>


### [27] [R-Log: Incentivizing Log Analysis Capability in LLMs via Reasoning-based Reinforcement Learning](https://arxiv.org/abs/2509.25987)
*Yilun Liu,Ziang Chen,Song Xu,Minggui He,Shimin Tao,Weibin Meng,Yuming Xie,Tao Han,Chunguang Zhao,Jingzhou Du,Daimeng Wei,Shenglin Zhang,Yongqian Sun*

Main category: cs.SE

TL;DR: R-Log是一个基于推理的日志分析新范式，通过模拟人类工程师的结构化分析过程，结合强化学习来提升泛化能力并减少幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于监督微调的方法存在领域差异导致的过拟合问题，以及长上下文淹没关键细节造成的幻觉问题，需要新的解决方案。

Method: 提出R-Log推理范式，首先在2000+推理轨迹数据集上冷启动，基于13种运维策略建立初始推理能力，然后通过强化学习在模拟运维环境中优化模型。

Result: 在真实日志上的实证评估显示，R-Log在五个日志分析任务中优于现有方法，特别是在未见场景下性能提升228.05%。R-Log-fast版本实现了5倍加速同时保持93%效能。

Conclusion: R-Log通过推理式分析和强化学习有效解决了传统方法的局限性，在日志分析任务中表现出优异的泛化性能和准确性。

Abstract: The growing complexity of log data in modern software systems has prompted
the use of Large Language Models (LLMs) for automated log analysis. Current
approaches typically rely on direct supervised fine-tuning (SFT) on log-label
pairs. However, this exacerbates the domain discrepancy between general-purpose
LLMs and specialized log data, causing overfitting. Furthermore, SFT's
imbalanced loss computation often allows lengthy contexts to overwhelm
critical, concise details in model answers, leading to hallucinations. To
address these limitations, we propose R-Log, a novel reasoning-based paradigm
that mirrors the structured, step-by-step analytical process of human
engineers. This approach enhances generalizability by learning the underlying
rules behind conclusions. We further employ Reinforcement Learning (RL) to
optimize the model within a simulated O&M environment, thereby reducing
hallucinations by directly rewarding correct outcomes. R-Log is first
cold-started on a curated dataset of 2k+ reasoning trajectories, guided by 13
strategies from manual O&M practices, to establish an initial reasoning
capability. This ability is then refined via RL using a joint reward function.
Empirical evaluations on real-world logs show that R-Log outperforms existing
methods across five log analysis tasks, particularly in unseen scenarios (by
228.05%). We also designed R-Log-fast with 5x speedup while keeping 93% of the
efficacy.

</details>


### [28] [Using GPT to build a Project Management assistant for Jira environments](https://arxiv.org/abs/2509.26014)
*Joel Garcia-Escribano,Arkaitz Carbajo,Mikel Egaña Aranguren,Unai Lopez-Novoa*

Main category: cs.SE

TL;DR: JiraGPT Next是一个基于GPT大语言模型的Jira插件，为项目经理提供自然语言界面来简化数据检索过程。


<details>
  <summary>Details</summary>
Motivation: 项目管理工具通常学习曲线陡峭且需要复杂编程语言来获取数据，项目经理在处理大量数据时面临挑战。

Method: 开发Jira作为插件的软件，利用GPT大语言模型提供自然语言查询接口，评估不同提示对任务完成准确性的影响。

Result: 该工具能够简化项目经理的数据检索过程，通过自然语言界面提高数据访问效率。

Conclusion: JiraGPT Next通过集成GPT模型为项目管理工具提供了更易用的自然语言交互方式，降低了数据检索的技术门槛。

Abstract: In the domain of Project Management, the sheer volume of data is a challenge
that project managers continually have to deal with. Effectively steering
projects from inception to completion requires handling of diverse information
streams, including timelines, budgetary considerations, and task dependencies.
To navigate this data-driven landscape with precision and agility, project
managers must rely on efficient and sophisticated tools. These tools have
become essential, as they enable project managers to streamline communication,
optimize resource allocation, and make informed decisions in real-time.
However, many of these tools have steep learning curves and require using
complex programming languages to retrieve the exact data that project managers
need. In this work we present JiraGPT Next, a software that uses the GPT Large
Language Model to ease the process by which project managers deal with large
amounts of data. It is conceived as an add-on for Jira, one of the most popular
Project Management tools, and provides a natural language interface to retrieve
information. This work presents the design decisions behind JiraGPT Next and an
evaluation of the accuracy of GPT in this context, including the effects of
providing different prompts to complete a particular task.

</details>


### [29] [Evaluating the impact of code smell refactoring on the energy consumption of Android applications](https://arxiv.org/abs/2509.26031)
*Hina Anwar,Dietmar Pfahl,Satish N. Srirama*

Main category: cs.SE

TL;DR: 研究探讨了Android应用中常见代码重构对性能和能耗的影响，发现重复代码和类型检查重构可降低能耗达10.8%，但能耗减少与执行时间变化无直接关系。


<details>
  <summary>Details</summary>
Motivation: 移动应用能耗问题受到广泛关注，研究表明通过提高应用质量可改善移动设备能耗，频繁重构是实现这一目标的方法之一。

Method: 通过实验分析Android应用中几种常见代码重构的性能和能耗影响，包括重复代码和类型检查等代码坏味重构。

Result: 某些代码坏味重构对Android应用能耗有积极影响，重复代码和类型检查重构可降低能耗达10.8%，但能耗减少与执行时间变化无直接相关性。

Conclusion: 需要进一步研究软件应用大小、年龄、开发者经验和贡献者数量等因素如何与代码坏味数量类型以及重构后的能耗性能影响相关联。

Abstract: Energy consumption of mobile apps is a domain that is receiving a lot of
attention from researchers. Recent studies indicate that the energy consumption
of mobile devices could be improved by improving the quality of mobile apps.
Frequent refactoring is one way of achieving this goal. In this paper, we
explore the performance and energy impact of several common code refactorings
in Android apps. Experimental results indicate that some code smell
refactorings positively impact the energy consumption of Android apps.
Refactoring of the code smells "Duplicated code" and "Type checking" reduce
energy consumption by up to 10.8%. Significant reduction in energy consumption,
however, does not seem to be directly related to the increase or decrease of
execution time. In addition, the energy impact over permutations of code smell
refactorings in the selected Android apps was small. When analyzing the order
in which refactorings were made across code smell types, it turned out that
some permutations resulted in a reduction and some in an increase of energy
consumption for the analyzed apps. More research needs to be done to
investigate how factors like size and age of software apps, experience, and
number of contributors to app development correlate with (a) the number and
type of code smells found and (b) the impact of energy consumption and
performance after refactoring.

</details>


### [30] [Agent-based code generation for the Gammapy framework](https://arxiv.org/abs/2509.26110)
*Dmitriy Kostunin,Vladimir Sotnikov,Sergo Golovachev,Abhay Mehta,Tim Lukas Holch,Elisa Jones*

Main category: cs.SE

TL;DR: 开发了一个针对Gammapy科学库的代码生成代理，能够在受控环境中编写、执行和验证代码，以解决专业科学库因缺乏资源和API不稳定导致的LLM代码生成困难。


<details>
  <summary>Details</summary>
Motivation: 专业科学库（如Gammapy）通常缺乏文档、示例和社区支持，且API不稳定，这使得基于有限或过时数据训练的LLM难以有效生成代码。

Method: 开发了一个能够编写、执行和验证代码的代理系统，在受控环境中运行，并提供了网页演示和基准测试套件。

Result: 成功构建了针对Gammapy库的代码生成代理系统，能够处理专业科学库特有的挑战。

Conclusion: 该工作展示了针对专业科学库的代码生成解决方案，并规划了后续发展步骤。

Abstract: Software code generation using Large Language Models (LLMs) is one of the
most successful applications of modern artificial intelligence. Foundational
models are very effective for popular frameworks that benefit from
documentation, examples, and strong community support. In contrast, specialized
scientific libraries often lack these resources and may expose unstable APIs
under active development, making it difficult for models trained on limited or
outdated data. We address these issues for the Gammapy library by developing an
agent capable of writing, executing, and validating code in a controlled
environment. We present a minimal web demo and an accompanying benchmarking
suite. This contribution summarizes the design, reports our current status, and
outlines next steps.

</details>


### [31] [A Multi-Language Object-Oriented Programming Benchmark for Large Language Models](https://arxiv.org/abs/2509.26111)
*Shuai Wang,Liang Ding,Li Shen,Yong Luo,Han Hu,Lefei Zhang,Fu Lin*

Main category: cs.SE

TL;DR: 提出了MultiOOP基准测试，这是一个多语言面向对象编程基准，覆盖6种流行编程语言，包含267个任务/语言，旨在解决现有代码生成基准测试中的语言单一性、任务粒度不足和测试用例过少的问题。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成基准测试存在三个主要不平衡：85.7%专注于单一编程语言；94.3%仅针对函数级或语句级任务；超过80%平均包含少于10个测试用例。这些限制阻碍了对LLM代码生成能力的全面评估。

Method: 设计了一个翻译器将现有的单语言OOP基准扩展到多语言环境，提出了pass@o指标，并开发了自动测试用例增强框架以确保评估结果的可靠性。

Result: 评估了14个主流LLM，发现：1）性能显著下降：与函数级任务相比，MultiOOP上的pass@1分数下降高达65.6个百分点；2）跨语言变异性：GPT-4o mini在Python中达到48.06%的pass@1，但在其他语言中仅为0.12%-15.26%；3）概念差距：pass@o分数始终比pass@k低1.1-19.2分。

Conclusion: MultiOOP基准测试、指标扩展和评估脚本将公开发布，以促进对LLM在面向对象代码生成中更平衡和全面的评估。

Abstract: Establishing fair and robust benchmarks is essential for evaluating
intelligent code generation by large language models (LLMs). Our survey of 35
existing benchmarks uncovers three major imbalances: 85.7% focus on a single
programming language; 94.3% target only function-level or statement-level
tasks; and over 80% include fewer than ten test cases on average. To address
these gaps, we propose MultiOOP, a multi-language object-oriented programming
benchmark covering six popular languages (Python, PHP, C++, C#, Java,
JavaScript) with 267 tasks per language. We design a translator that extends an
existing single-language OOP benchmark and the pass@o metric to a multilingual
setting. Moreover, we propose an automated framework for augmenting test cases
to ensure the reliability of the evaluation results. We evaluate 14 mainstream
LLMs under zero-shot prompting and report three key findings: 1) Substantial
performance degradation: pass@1 scores on MultiOOP drop by up to 65.6
percentage points compared to function-level tasks (e.g., HumanEval). 2)
Cross-language variability: GPT-4o mini achieves pass@1 of 48.06% in Python but
only 0.12%-15.26% in other languages, indicating limited multilingual
generalization. 3) Conceptual gaps: pass@o scores are consistently 1.1-19.2
points lower than pass@k, demonstrating that LLMs often generate executable
code without fully capturing core OOP concepts. Our benchmark, metric
extensions, and evaluation scripts will be publicly released to foster a more
balanced and comprehensive assessment of LLMs in object-oriented code
generation. Our code and data will be released at
https://github.com/alphadl/OOP-eval and
https://huggingface.co/datasets/codeai-dteam/MultiOOP respectively.

</details>


### [32] [Understanding Collective Social Behavior in OSS Communities: A Co-editing Network Analysis of Activity Cascades](https://arxiv.org/abs/2509.26173)
*Lisi Qarkaxhija,Maximilian Carparo,Stefan Menzel,Bernhard Sendhoff,Ingo Scholtes*

Main category: cs.SE

TL;DR: 该论文通过分析开源软件社区中开发者的时间活动模式，揭示了提交贡献的突发性特征，并基于共同编辑网络建立了活动级联模型来解释这种现象。研究发现活动级联在超过一半的项目中具有统计显著性，并开发了预测开发者流失的实用方法。


<details>
  <summary>Details</summary>
Motivation: 理解软件开发者的集体社会行为对于建模和预测开源软件社区的长期动态和可持续性至关重要。研究者希望揭示提交贡献突发性现象背后的社会机制。

Method: 采用基于网络的建模框架，通过共同编辑网络捕捉开发者互动，建立活动级联模型来模拟开发者活动在网络中的传播。使用50个主要开源社区的大数据集，开发了识别活动级联的方法。

Result: 研究结果显示活动级联在超过一半的研究项目中是统计显著现象。基于这些洞察开发了一个简单实用的流失预测方法，能够预测哪些开发者可能离开项目。

Conclusion: 这项工作揭示了开源软件社区中涌现的集体社会动态，强调了活动级联对于理解协作软件项目中开发者流失和保留的重要性。

Abstract: Understanding the collective social behavior of software developers is
crucial to model and predict the long-term dynamics and sustainability of Open
Source Software (OSS) communities. To this end, we analyze temporal activity
patterns of developers, revealing an inherently ``bursty'' nature of commit
contributions. To investigate the social mechanisms behind this phenomenon, we
adopt a network-based modelling framework that captures developer interactions
through co-editing networks. Our framework models social interactions, where a
developer editing the code of other developers triggers accelerated activity
among collaborators. Using a large data set on 50 major OSS communities, we
further develop a method that identifies activity cascades, i.e. the
propagation of developer activity in the underlying co-editing network. Our
results suggest that activity cascades are a statistically significant
phenomenon in more than half of the studied projects. We further show that our
insights can be used to develop a simple yet practical churn prediction method
that forecasts which developers are likely to leave a project. Our work sheds
light on the emergent collective social dynamics in OSS communities and
highlights the importance of activity cascades to understand developer churn
and retention in collaborative software projects.

</details>


### [33] [Hamster: A Large-Scale Study and Characterization of Developer-Written Tests](https://arxiv.org/abs/2509.26204)
*Rangeet Pan,Tyler Stennett,Raju Pavuluri,Nate Levin,Alessandro Orso,Saurabh Sinha*

Main category: cs.SE

TL;DR: 本文通过分析170万个Java开发者编写的测试用例，揭示了当前自动化测试生成工具与开发者实际测试实践之间的差距，并提出了改进方向。


<details>
  <summary>Details</summary>
Motivation: 尽管自动化测试生成技术研究已有数十年，但对开发者实际编写的测试特征理解仍存在空白，无法准确评估ATG工具生成测试的质量和代表性。

Method: 对开源仓库中的170万个Java测试用例进行实证研究，分析测试范围、测试装置、断言、输入类型和模拟使用等被现有文献忽视的方面。

Result: 研究发现绝大多数开发者编写的测试具有当前ATG工具无法复现的特征，现有工具生成的测试与开发者实际测试存在显著差异。

Conclusion: 基于研究洞察，识别了有前景的研究方向，旨在缩小工具能力与开发者测试实践之间的差距，推动ATG工具生成更接近开发者编写的测试类型。

Abstract: Automated test generation (ATG), which aims to reduce the cost of manual test
suite development, has been investigated for decades and has produced countless
techniques based on a variety of approaches: symbolic analysis, search-based,
random and adaptive-random, learning-based, and, most recently,
large-language-model-based approaches. However, despite this large body of
research, there is still a gap in our understanding of the characteristics of
developer-written tests and, consequently, in our assessment of how well ATG
techniques and tools can generate realistic and representative tests. To bridge
this gap, we conducted an extensive empirical study of developer-written tests
for Java applications, covering 1.7 million test cases from open-source
repositories. Our study is the first of its kind in studying aspects of
developer-written tests that are mostly neglected in the existing literature,
such as test scope, test fixtures and assertions, types of inputs, and use of
mocking. Based on the characterization, we then compare existing tests with
those generated by two state-of-the-art ATG tools. Our results highlight that a
vast majority of developer-written tests exhibit characteristics that are
beyond the capabilities of current ATG tools. Finally, based on the insights
gained from the study, we identify promising research directions that can help
bridge the gap between current tool capabilities and more effective tool
support for developer testing practices. We hope that this work can set the
stage for new advances in the field and bring ATG tools closer to generating
the types of tests developers write.

</details>


### [34] [UniSage: A Unified and Post-Analysis-Aware Sampling for Microservices](https://arxiv.org/abs/2509.26336)
*Zhouruixing Zhu,Zhihan Jiang,Tianyi Yang,Pinjia He*

Main category: cs.SE

TL;DR: UniSage是首个统一框架，采用后分析感知范式对追踪和日志进行采样，通过轻量级多模态异常检测和根因分析指导双支柱采样策略，在2.5%采样率下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有追踪和日志采样方法采用先采样后分析范式，会不可避免地丢弃故障相关信息，阻碍系统行为诊断的透明度。

Method: UniSage首先对完整数据流执行轻量级多模态异常检测和根因分析，然后使用双支柱采样策略：分析引导采样器优先处理RCA涉及的数据，边缘案例采样器确保捕获罕见但关键的行为。

Result: 在2.5%采样率下，UniSage捕获了56.5%的关键追踪和96.25%的相关日志，下游根因分析准确率(AC@1)提高了42.45%，处理10分钟遥测数据仅需不到5秒。

Conclusion: UniSage通过后分析感知采样范式，在保持高效处理的同时显著提升了关键信号的覆盖范围和下游诊断准确性，适用于生产环境。

Abstract: Traces and logs are essential for observability and fault diagnosis in modern
distributed systems. However, their ever-growing volume introduces substantial
storage overhead and complicates troubleshooting. Existing approaches typically
adopt a sample-before-analysis paradigm: even when guided by data heuristics,
they inevitably discard failure-related information and hinder transparency in
diagnosing system behavior. To address this, we introduce UniSage, the first
unified framework to sample both traces and logs using a post-analysis-aware
paradigm. Instead of discarding data upfront, UniSagefirst performs lightweight
and multi-modal anomaly detection and root cause analysis (RCA) on the complete
data stream. This process yields fine-grained, service-level diagnostic
insights that guide a dual-pillar sampling strategy for handling both normal
and anomalous scenarios: an analysis-guided sampler prioritizes data implicated
by RCA, while an edge-case-based sampler ensures rare but critical behaviors
are captured. Together, these pillars ensure comprehensive coverage of critical
signals without excessive redundancy. Extensive experiments demonstrate that
UniSage significantly outperforms state-of-the-art baselines. At a 2.5%
sampling rate, it captures 56.5% of critical traces and 96.25% of relevant
logs, while improving the accuracy (AC@1) of downstream root cause analysis by
42.45%. Furthermore, its efficient pipeline processes 10 minutes of telemetry
data in under 5 seconds, demonstrating its practicality for production
environments.

</details>


### [35] [Institutional Policy Pathways for Supporting Research Software: Global Trends and Local Practices](https://arxiv.org/abs/2509.26422)
*Michelle Barker,Jeremy Cohen,Pedro Hernández Serrano,Daniel S. Katz,Kim Martin,Dan Rudmann,Hugh Shanahan*

Main category: cs.SE

TL;DR: 研究机构需要制定稳健的政策来支持研究软件的开发、使用和可持续性，以应对现代科研环境中研究软件日益重要但政策支持不足的问题。


<details>
  <summary>Details</summary>
Motivation: 随着研究软件在现代科学中的核心地位日益增强，研究机构需要确保对人员、技能和基础设施的投资能够产生可持续和可维护的软件，从而提升研究质量、机构声誉和资金竞争力。然而，目前研究机构对研究软件及其人员的认可和支持大多处于临时状态，缺乏系统性的政策支持。

Method: PRO4RS工作组（ReSA和RDA的联合倡议）通过审查和推进全球机构的研究软件政策发展，分析了研究软件政策制定的理论基础，并利用工作组产出和分析来突出关键政策缺口。

Result: 研究发现，尽管资助者和出版商在FAIR和开放科学原则方面势头增长，但专门针对研究软件的机构层面政策仍然有限或缺乏广度，特别是在研究评估改革中考虑研究软件人员方面存在显著政策缺口。

Conclusion: 研究机构必须实施和采用稳健的政策来支持研究软件的开发、使用和可持续性，以解决现代研究环境中的这一基本问题，特别是在研究评估改革中应充分考虑研究软件人员的作用。

Abstract: As research software becomes increasingly central to modern science,
research-performing organisations (RPOs) need to ensure that their investment
in people, skills and infrastructure around research software produces
sustainable and maintainable software that improves the research they perform,
which in turn improves the overall institution and its reputation and funding,
for example, by competing with peers who lack this approach. However, research
institution management and recognition of research software and its personnel
has mostly often developed in an ad hoc manner. RPO training infrastructures,
recognition and reward structures, have not developed at a sufficient rate to
support and encourage both the widespread use of research software best
practices and the long-term support for technical roles that is required. To
begin to address this fundamental problem for modern research environments,
RPOs must implement and adopt robust policies to support research software
development, use, and sustainability. Despite growing momentum from funders and
publishers around FAIR and open science principles, research
institutional-level policies specifically addressing research software remain
limited or lacking in breadth.
  This article outlines the work of the Policies in Research Organisations for
Research Software (PRO4RS) Working Group (WG), a joint initiative of the
Research Software Alliance (ReSA) and the Research Data Alliance (RDA), which
examined and advanced research software policy development across institutions
worldwide. After consideration of the rationale for institutional policies on
research software, the PRO4RS WG outputs and analysis are utilised to highlight
critical policy gaps, particularly related to consideration of research
software personnel in policy work focused on reform of research assessment.

</details>


### [36] [EQ-Robin: Generating Multiple Minimal Unique-Cause MC/DC Test Suites](https://arxiv.org/abs/2509.26458)
*Robin Lee,Youngho Nam*

Main category: cs.SE

TL;DR: EQ-Robin是一个轻量级管道，通过生成多个语义等价的布尔表达式变体，为每个变体应用Robin's Rule算法，从而产生多个最小MC/DC测试套件，以解决单个测试套件可能因约束条件而无法实现100%覆盖率的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的Robin's Rule算法虽然能生成理论最小N+1个测试用例，但只产生单一测试套件。如果其中某个必需的'independence pair'测试用例违反了系统约束，整个套件就无法实现100%覆盖率，这在安全关键软件验证中存在严重风险。

Method: 通过代数重排在抽象语法树(AST)上系统生成语义等价的奇异布尔表达式(SBEs)变体，然后对每个结构变体应用Robin's Rule算法，产生多样化的最小测试套件集合。

Result: 论文提出了EQ-Robin方法，能够生成多个最小Unique-Cause MC/DC测试套件，在保持N+1最小性保证的同时，提供了发现有效测试套件的弹性路径。

Conclusion: EQ-Robin为在现实约束条件下确保鲁棒的MC/DC覆盖率提供了实用解决方案，通过在TCAS-II衍生的SBEs上的评估计划来验证其有效性。

Abstract: Modified Condition/Decision Coverage (MC/DC), particularly its strict
Unique-Cause form, is a cornerstone of safety-critical software verification. A
recent algorithm, "Robin's Rule," introduced a deterministic method to
construct the theoretical minimum of N+1 test cases for Singular Boolean
Expressions (SBEs). However, this approach yields only a single test suite,
introducing a critical risk: if a test case forming a required 'independence
pair' is an illegal input forbidden by system constraints, the suite fails to
achieve 100% coverage. This paper proposes EQ-Robin, a lightweight pipeline
that systematically generates a family of minimal Unique-Cause MC/DC suites to
mitigate this risk. We introduce a method for systematically generating
semantically equivalent SBEs by applying algebraic rearrangements to an
Abstract Syntax Tree (AST) representation of the expression. By applying
Robin's Rule to each structural variant, a diverse set of test suites can be
produced. This provides a resilient path to discovering a valid test suite that
preserves the N+1 minimality guarantee while navigating real-world constraints.
We outline an evaluation plan on TCAS-II-derived SBEs to demonstrate how
EQ-Robin offers a practical solution for ensuring robust MC/DC coverage.

</details>


### [37] [ErrorPrism: Reconstructing Error Propagation Paths in Cloud Service Systems](https://arxiv.org/abs/2509.26463)
*Junsong Pu,Yichen Li,Zhuangbin Chen,Jinyang Liu,Zhihan Jiang,Jianjun Chen,Rui Shi,Zibin Zheng,Tieying Zhang*

Main category: cs.SE

TL;DR: ErrorPrism：通过静态分析和LLM代理自动重建微服务系统中错误传播路径的工具，在字节跳动67个生产微服务上达到97.0%的准确率


<details>
  <summary>Details</summary>
Motivation: 云服务系统中由于错误级联效应导致可靠性管理困难，错误包装实践虽然丰富了错误上下文，但也带来了从最终日志消息回溯完整错误传播路径的追踪问题

Method: 首先对服务代码库进行静态分析构建函数调用图并映射日志字符串到相关候选函数，然后使用LLM代理进行迭代反向搜索以重建完整的多跳错误路径

Result: 在字节跳动67个生产微服务上评估，对102个真实世界错误重建路径达到97.0%的准确率，优于现有的静态分析和基于LLM的方法

Conclusion: ErrorPrism为工业微服务系统中的根本原因分析提供了有效实用的工具

Abstract: Reliability management in cloud service systems is challenging due to the
cascading effect of failures. Error wrapping, a practice prevalent in modern
microservice development, enriches errors with context at each layer of the
function call stack, constructing an error chain that describes a failure from
its technical origin to its business impact. However, this also presents a
significant traceability problem when recovering the complete error propagation
path from the final log message back to its source. Existing approaches are
ineffective at addressing this problem. To fill this gap, we present ErrorPrism
in this work for automated reconstruction of error propagation paths in
production microservice systems. ErrorPrism first performs static analysis on
service code repositories to build a function call graph and map log strings to
relevant candidate functions. This significantly reduces the path search space
for subsequent analysis. Then, ErrorPrism employs an LLM agent to perform an
iterative backward search to accurately reconstruct the complete, multi-hop
error path. Evaluated on 67 production microservices at ByteDance, ErrorPrism
achieves 97.0% accuracy in reconstructing paths for 102 real-world errors,
outperforming existing static analysis and LLM-based approaches. ErrorPrism
provides an effective and practical tool for root cause analysis in industrial
microservice systems.

</details>


### [38] [Towards Verified Code Reasoning by LLMs](https://arxiv.org/abs/2509.26546)
*Meghana Sistla,Gogul Balakrishnan,Pat Rondon,José Cambronero,Michele Tufano,Satish Chandra*

Main category: cs.SE

TL;DR: 提出了一种自动验证代码推理代理答案的方法，通过形式化验证和程序分析工具来检查代理的推理步骤，提高代码推理的可信度。


<details>
  <summary>Details</summary>
Motivation: LLM代理在代码推理中答案不总是正确，这限制了其在需要高精度场景的应用，如代码理解、代码审查和自动化代码生成验证。手动验证代理答案耗费人力且降低开发效率。

Method: 提取代理响应的形式化表示，然后使用形式化验证和程序分析工具来验证代理的推理步骤。

Result: 在20个未初始化变量错误检测中，形式化验证成功验证了13/20个例子的代理推理；在20个程序等价性查询中，成功捕获了6/8个代理的错误判断。

Conclusion: 该方法能有效自动验证代码推理代理的答案，提高代理的可信度和实用性，减少人工验证负担。

Abstract: While LLM-based agents are able to tackle a wide variety of code reasoning
questions, the answers are not always correct. This prevents the agent from
being useful in situations where high precision is desired: (1) helping a
software engineer understand a new code base, (2) helping a software engineer
during code review sessions, and (3) ensuring that the code generated by an
automated code generation system meets certain requirements (e.g. fixes a bug,
improves readability, implements a feature).
  As a result of this lack of trustworthiness, the agent's answers need to be
manually verified before they can be trusted. Manually confirming responses
from a code reasoning agent requires human effort and can result in slower
developer productivity, which weakens the assistance benefits of the agent. In
this paper, we describe a method to automatically validate the answers provided
by a code reasoning agent by verifying its reasoning steps. At a very high
level, the method consists of extracting a formal representation of the agent's
response and, subsequently, using formal verification and program analysis
tools to verify the agent's reasoning steps.
  We applied this approach to a benchmark set of 20 uninitialized variable
errors detected by sanitizers and 20 program equivalence queries. For the
uninitialized variable errors, the formal verification step was able to
validate the agent's reasoning on 13/20 examples, and for the program
equivalence queries, the formal verification step successfully caught 6/8
incorrect judgments made by the agent.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [39] [Monoid Structures on Indexed Containers](https://arxiv.org/abs/2509.25879)
*Michele De Pascalis,Tarmo Uustalu,Niccolò Veltrì*

Main category: cs.LO

TL;DR: 本文研究了索引容器的幺半群结构，给出了该幺半范畴中幺半元的组合特征，并证明了这些幺半元与Set^I上的单子一一对应。


<details>
  <summary>Details</summary>
Motivation: 索引容器是依赖类型编程中的重要构造，本文旨在研究其幺半群结构及其与单子的对应关系。

Method: 通过分析索引容器的幺半范畴结构，给出幺半元的组合特征，并建立与Set^I上单子的对应关系。

Result: 成功刻画了索引容器幺半范畴中的幺半元，证明了它们与Set^I上单子的等价性，并提供了多个实例。

Conclusion: 索引容器的幺半群结构为理解单子提供了新的组合视角，该结果已在Cubical Agda中形式化验证。

Abstract: Containers represent a wide class of type constructions relevant for
functional programming and (co)inductive reasoning. Indexed containers
generalize this notion to better fit the scope of dependently typed
programming. When interpreting types to be sets, a container describes an
endofunctor on the category of sets while an I-indexed container describes an
endofunctor on the category Set^I of I-indexed families of sets.
  We consider the monoidal structure on the category of I-indexed containers
whose tensor product of containers describes the composition of the respective
induced endofunctors. We then give a combinatorial characterization of monoids
in this monoidal category, and we show how these monoids correspond precisely
to monads on the induced endofunctors on Set^I. Lastly, we conclude by
presenting some examples of monads on Set^I that fall under our
characterization, including the product of two monads, indexed variants of the
state and the writer monads and an example of a free monad. The technical
results of this work are accompanied by a formalization in the proof assistant
Cubical Agda.

</details>


### [40] [A Function-Set Framework: General Properties and Applications to Modal Logic](https://arxiv.org/abs/2509.25880)
*Luke Bayzid,Alexandre Madeira,Manuel A. Martins*

Main category: cs.LO

TL;DR: 提出一种基于集合论概念的通用结构，用于容纳逻辑和语义框架中的多种情境，特别展示了模态逻辑的替代构造方法。


<details>
  <summary>Details</summary>
Motivation: 不同表示方法各有优势，但在不同框架间比较结果往往很困难，需要一种通用结构来统一处理各种逻辑和语义框架。

Method: 基于集合论概念构建通用结构，能够容纳多种逻辑和语义框架，特别展示了模态逻辑在该框架中的替代构造方法。

Result: 所有模态逻辑都可以在该框架中表示，证明了该方法的通用性和灵活性。

Conclusion: 提出的集合论基础结构为比较不同表示方法提供了统一框架，特别适用于逻辑和语义框架的研究。

Abstract: Representations are essential to mathematically model phenomena, but there
are many options available. While each of those options provides useful
properties with which to solve problems related to the phenomena in study,
comparing results between these representations can be non-trivial, as
different frameworks are used for different contexts. We present a general
structure based on set-theoretic concepts that accommodates many situations
related to logical and semantic frameworks. We show the versatility of this
approach by presenting alternative constructions of modal logic; in particular,
all modal logics can be represented within the framework.

</details>


### [41] [Characterization of Lattice Properties Within Modal Extensions](https://arxiv.org/abs/2509.25882)
*Alfredo R. Freire,Manuel A. Martins*

Main category: cs.LO

TL;DR: 本文研究将基于格的逻辑扩展到模态语言的方法，探讨了必然算子的多种解释方式及其逻辑性质。


<details>
  <summary>Details</summary>
Motivation: 由于基于格的逻辑在扩展到模态语言时，必然算子的解释不是由底层格结构唯一确定的，因此需要研究不同的解释方式及其逻辑后果。

Method: 主要考察了两种解释：自然解释（将必然性定义为公式在所有可达世界中真值的交）以及仅当公式在所有可达世界中成立时才赋予必然性的情况。

Result: 分析了在这些不同解释下产生的逻辑性质，包括满足公理K和其他常见模态有效性的条件。

Conclusion: 基于格的模态逻辑扩展存在多种合理的解释方式，每种方式都产生不同的逻辑性质，需要根据具体应用场景选择适当的解释。

Abstract: This paper investigates the extension of lattice-based logics into modal
languages. We observe that such extensions admit multiple approaches, as the
interpretation of the necessity operator is not uniquely determined by the
underlying lattice structure. The most natural interpretation defines necessity
as the meet of the truth values of a formula across all accessible worlds -- an
approach we refer to as the \textitnormal interpretation. We examine the
logical properties that emerge under this and other interpretations, including
the conditions under which the resulting modal logic satisfies the axiom K and
other common modal validities. Furthermore, we consider cases in which
necessity is attributed exclusively to formulas that hold in all accessible
worlds.

</details>


### [42] [Nominal Sets in Rocq](https://arxiv.org/abs/2509.25883)
*Fabrício Sanches Paranhos,Daniel Ventura*

Main category: cs.LO

TL;DR: 在Rocq中形式化构造名义集合，包括新鲜度、α等价、名称抽象等概念，通过类型类层次结构简化定义和证明。


<details>
  <summary>Details</summary>
Motivation: 名义技术能够更接近非正式开发的方式形式化带绑定结构的语法，但其核心概念名义集合需要形式化构造实现。

Method: 使用Rocq的类型类层次结构和广义重写机制，形式化名义集合、新鲜度、α等价、名称抽象和有限支持函数等概念。

Result: 实现了简洁的定义和证明，缓解了"setoid hell"问题，并获得了构造性的α结构递归和归纳组合子。

Conclusion: 为名义框架提供了形式化基础，展示了类型类层次结构在名义集合形式化中的有效性。

Abstract: Nominal techniques have been praised for their ability to formalize grammars
with binding structures closer to their informal developments. At its core,
there lies the definition of nominal sets, which capture the notion of name
(in)dependence through a simple, and uniform, metatheory based on name
permutations. We present a formal constructive development of nominal sets in
Rocq (formerly known as Coq), with its main design and project decisions.
Furthermore, we formalize the concepts of freshness, nominal alpha-equivalence,
name abstraction, and finitely supported functions. Our implementation relies
on a type class hierarchy which, combined with Rocq generalized rewriting
mechanism, achieves concise definitions and proofs, whilst easing the
well-known "setoid hell" scenario. We conclude with a discussion on how to
obtain the constructive alpha-structural recursion and induction combinators,
towards a nominal framework.

</details>


### [43] [Demystifying Codensity Monads via Duality](https://arxiv.org/abs/2509.26197)
*Fabian Lenke,Nico Wittrock,Stefan Milius,Henning Urbat*

Main category: cs.LO

TL;DR: 提出了一种基于对偶性的统一方法来生成密度单子，简化了现有证明并推导出新的密度单子表示。


<details>
  <summary>Details</summary>
Motivation: 密度单子为从简单函子生成复杂单子提供了通用方法，但现有证明复杂且缺乏统一框架。

Method: 基于范畴对偶思想，通过将表示函子与稠密函子相关联，建立通用的密度表示结果。

Result: 证明了大多数文献中的密度表示都源于这种对偶设置，简化了证明过程，并推导出滤波器单子、下Vietoris单子等新表示。

Conclusion: 基于对偶性的方法为密度单子表示提供了统一框架，显著简化证明并扩展了应用范围。

Abstract: Codensity monads provide a universal method to generate complex monads from
simple functors. Recently, a wide range of important monads in logic,
denotational semantics, and probabilistic computation, such as several
incarnations of the ultrafilter monad, the Vietoris monad, and the Giry monad,
have been presented as codensity monads, using complex arguments. We propose a
unifying categorical approach to codensity presentations of monads, based on
the idea of relating the presenting functor to a dense functor via a suitable
duality between categories. We prove a general presentation result applying to
every such situation and demonstrate that most codensity presentations known in
the literature emerge from this strikingly simple duality-based setup,
drastically alleviating the complexity of their proofs and in many cases
completely reducing them to standard duality results. Additionally, we derive a
number of novel codensity presentations using our framework, including the
first non-trivial codensity presentations for the filter monads on sets and
topological spaces, the lower Vietoris monad on topological spaces, and the
expectation monad on sets.

</details>


### [44] [Logical Approaches to Non-deterministic Polynomial Time over Semirings](https://arxiv.org/abs/2509.26214)
*Timon Barlag,Nicolas Fröhlich,Teemu Hankala,Miika Hannula,Minna Hirvonen,Vivian Holzapfel,Juha Kontinen,Arne Meier,Laura Strieker*

Main category: cs.LO

TL;DR: 该论文为BSS机器在半环上定义的非确定性多项式时间提供了逻辑特征化，通过Gr"adel和Tannen开发的半环语义中的存在二阶逻辑。证明了命题逻辑在半环语义中的可满足性问题是该NP版本的典型完全问题，并且半环的真存在一阶理论是该NP版本布尔部分的完全问题。


<details>
  <summary>Details</summary>
Motivation: 研究BSS机器在半环上定义的非确定性多项式时间的逻辑特征化，类似于经典计算复杂性理论中的NP类逻辑特征化，以建立半环语义下的计算复杂性理论框架。

Method: 使用Gr"adel和Tannen开发的半环语义，通过存在二阶逻辑来特征化BSS机器在半环上定义的非确定性多项式时间，并分析命题逻辑在半环语义中的可满足性问题。

Result: 成功建立了BSS机器在半环上定义的非确定性多项式时间的逻辑特征化，证明了命题逻辑在半环语义中的可满足性问题是该NP版本的典型完全问题，半环的真存在一阶理论是该NP版本布尔部分的完全问题。

Conclusion: 该研究为半环语义下的计算复杂性理论提供了重要的逻辑基础，将经典NP类的逻辑特征化推广到半环语义中，建立了相应的完全问题理论。

Abstract: We provide a logical characterization of non-deterministic polynomial time
defined by BSS machines over semirings via existential second-order logic
interpreted in the semiring semantics developed by Gr\"adel and Tannen.
Furthermore, we show that, similarly to the classical setting, the
satisfiability problem of propositional logic in the semiring semantics is the
canonical complete problem for this version of NP. Eventually, we prove that
the true existential first-order theory of the semiring is a complete problem
for the so-called Boolean part of this version of NP.

</details>


### [45] [Transporting Theorems about Typeability in LF Across Schematically Defined Contexts](https://arxiv.org/abs/2509.26362)
*Chase Johnson,Gopalan Nadathur*

Main category: cs.LO

TL;DR: 本文为在依赖类型lambda演算LF中，将涉及上下文量化定理从一个模式描述转移到另一个模式描述提供了逻辑证明，开发了基于类型从属关系的上下文模式蕴含概念和传输证明规则。


<details>
  <summary>Details</summary>
Motivation: 在LF编码的对象系统证明中，需要处理涉及上下文量化的公式，但实际推理中经常需要将定理从一个上下文模式描述转移到另一个模式描述，这需要逻辑上的正当性证明。

Method: 利用逻辑$\mathcal{L}_{LF}$，开发基于类型从属关系的上下文模式蕴含概念，构建传输证明规则，并在Adelfa证明助手中实现该规则。

Result: 证明了传输证明规则相对于$\mathcal{L}_{LF}$语义的可靠性，并在实际推理示例中验证了其有效性。

Conclusion: 提出的上下文模式蕴含和传输证明规则为在LF规范中处理上下文量化提供了理论基础，增强了形式化推理的能力。

Abstract: The dependently-typed lambda calculus LF is often used as a vehicle for
formalizing rule-based descriptions of object systems. Proving properties of
object systems encoded in this fashion requires reasoning about formulas over
LF typing judgements. An important characteristic of LF is that it supports a
higher-order abstract syntax representation of binding structure. When such an
encoding is used, the typing judgements include contexts that assign types to
bound variables and formulas must therefore allow for quantification over
contexts. The possible instantiations of such quantifiers are usually governed
by schematic descriptions that must also be made explicit for effectiveness in
reasoning. In practical reasoning tasks, it is often necessary to transport
theorems involving universal quantification over contexts satisfying one
schematic description to those satisfying another description. We provide here
a logical justification for this ability. Towards this end, we utilize the
logic $\mathcal{L}_{LF}$, which has previously been designed for formalizing
properties of LF specifications. We develop a transportation proof rule and
show it to be sound relative to the semantics of $\mathcal{L}_{LF}$. Key to
this proof rule is a notion of context schema subsumption that uses the
subordination relation between types as a means for determining the equivalence
of contexts relative to individual LF typing judgements. We discuss the
incorporation of this rule into the Adelfa proof assistant and its use in
actual reasoning examples.

</details>
