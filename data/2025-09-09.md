<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 23]
- [cs.LO](#cs.LO) [Total: 3]
- [cs.FL](#cs.FL) [Total: 2]
- [cs.PL](#cs.PL) [Total: 7]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Adversarial Bug Reports as a Security Risk in Language Model-Based Automated Program Repair](https://arxiv.org/abs/2509.05372)
*Piotr Przymus,Andreas Happe,Jürgen Cito*

Main category: cs.SE

TL;DR: 这篇论文研究了基于大语言模型的自动程序修复系统的安全风险，发现对抗性故障报告攻击的防御措施目前存在显著缺口，建议提升APR系统的验证和防御能力。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的自动程序修复系统在软件开发中的普及，依赖不可信用户输入引入了新的攻击面，需要研究对抗性故障报告对APR系统的安全威胁。

Method: 构建了全面的威胁模型，通过手工精选和全自动流程生成51个对抗性故障报告，对现有领先APR系统进行实验研究，评估修复前后的防御措施效果。

Result: 当前防御措施显著不足：90%的制造故障报告触发了攻击者对齐的补丁，最佳修复前过滤器仅阻挡了47%，修复后分析效果为58%。

Conclusion: 生成对抗性输入成本低而检测和缓解成本高且容错，需要提升APR系统的验证和防御能力，并为可信自动修复研究提供方向。

Abstract: Large Language Model (LLM) - based Automated Program Repair (APR) systems are
increasingly integrated into modern software development workflows, offering
automated patches in response to natural language bug reports. However, this
reliance on untrusted user input introduces a novel and underexplored attack
surface. In this paper, we investigate the security risks posed by adversarial
bug reports -- realistic-looking issue submissions crafted to mislead APR
systems into producing insecure or harmful code changes. We develop a
comprehensive threat model and conduct an empirical study to evaluate the
vulnerability of state-of-the-art APR systems to such attacks. Our
demonstration comprises 51 adversarial bug reports generated across a spectrum
of strategies, from manual curation to fully automated pipelines. We test these
against leading APR model and assess both pre-repair defenses (e.g., LlamaGuard
variants, PromptGuard variants, Granite-Guardian, and custom LLM filters) and
post-repair detectors (GitHub Copilot, CodeQL). Our findings show that current
defenses are insufficient: 90\% of crafted bug reports triggered
attacker-aligned patches. The best pre-repair filter blocked only 47\%, while
post-repair analysis-often requiring human oversight-was effective in just 58\%
of cases. To support scalable security testing, we introduce a prototype
framework for automating the generation of adversarial bug reports. Our
analysis exposes a structural asymmetry: generating adversarial inputs is
inexpensive, while detecting or mitigating them remains costly and error-prone.
We conclude with practical recommendations for improving the robustness of APR
systems against adversarial misuse and highlight directions for future work on
trustworthy automated repair.

</details>


### [2] [Reverse Browser: Vector-Image-to-Code Generator](https://arxiv.org/abs/2509.05394)
*Zoltan Toth-Czifra*

Main category: cs.SE

TL;DR: 使用矢量图像而非位图作为输入，通过创建大型数据集和新的多尺度质量评估指标，训练了一个大型开放权重模型来解决图像到代码转换的保真度问题


<details>
  <summary>Details</summary>
Motivation: 现有的图像到代码转换解决方案在保真度方面表现不佳，无法准确还原原始设计，需要新的方法来提高转换质量

Method: 采用矢量图像作为模型输入，创建多个大型数据集，评估现有图像质量评估算法并引入新的多尺度指标，训练大型开放权重模型

Result: 开发了一个新的图像到代码转换方法，使用矢量图像输入和多尺度质量评估指标，训练出了大型模型

Conclusion: 矢量图像输入和多尺度质量评估指标的组合为图像到代码转换提供了新的解决方案，但模型仍存在一些局限性

Abstract: Automating the conversion of user interface design into code (image-to-code
or image-to-UI) is an active area of software engineering research. However,
the state-of-the-art solutions do not achieve high fidelity to the original
design, as evidenced by benchmarks. In this work, I approach the problem
differently: I use vector images instead of bitmaps as model input. I create
several large datasets for training machine learning models. I evaluate the
available array of Image Quality Assessment (IQA) algorithms and introduce a
new, multi-scale metric. I then train a large open-weights model and discuss
its limitations.

</details>


### [3] [Combining TSL and LLM to Automate REST API Testing: A Comparative Study](https://arxiv.org/abs/2509.05540)
*Thiago Barradas,Aline Paes,Vânia de Oliveira Neves*

Main category: cs.SE

TL;DR: RestTSLLM使用测试规范语言(TSL)和大语言模型(LLM)自动生成REST API测试案例，解决测试场景创建和输入数据定义挑战。Claude 3.5 Sonnet在各项指标上表现最优。


<details>
  <summary>Details</summary>
Motivation: REST API测试面临分布式系统复杂性、测试场景多样性和时间压力挑战，传统测试方法导致测试覆盖率低、手动工作量大且容易漏测故障。

Method: 整合提示工程技术和自动化流水线，评估多种LLM根据OpenAPI规范生成测试案例的能力，采用成功率、测试覆盖率和变异分数等指标进行系统性比较。

Result: Claude 3.5 Sonnet、Deepseek R1、Qwen 2.5 32b和Sabia 3都能产生稳健且上下文一致的REST API测试。其中Claude 3.5 Sonnet在所有指标上都表现最优，被评为最适合此任务的模型。

Conclusion: LLM基于API规范自动生成测试案例具有很大潜力，Claude 3.5 Sonnet在该领域表现出色，为REST API测试自动化提供了有效解决方案。

Abstract: The effective execution of tests for REST APIs remains a considerable
challenge for development teams, driven by the inherent complexity of
distributed systems, the multitude of possible scenarios, and the limited time
available for test design. Exhaustive testing of all input combinations is
impractical, often resulting in undetected failures, high manual effort, and
limited test coverage. To address these issues, we introduce RestTSLLM, an
approach that uses Test Specification Language (TSL) in conjunction with Large
Language Models (LLMs) to automate the generation of test cases for REST APIs.
The approach targets two core challenges: the creation of test scenarios and
the definition of appropriate input data. The proposed solution integrates
prompt engineering techniques with an automated pipeline to evaluate various
LLMs on their ability to generate tests from OpenAPI specifications. The
evaluation focused on metrics such as success rate, test coverage, and mutation
score, enabling a systematic comparison of model performance. The results
indicate that the best-performing LLMs - Claude 3.5 Sonnet (Anthropic),
Deepseek R1 (Deepseek), Qwen 2.5 32b (Alibaba), and Sabia 3 (Maritaca) -
consistently produced robust and contextually coherent REST API tests. Among
them, Claude 3.5 Sonnet outperformed all other models across every metric,
emerging in this study as the most suitable model for this task. These findings
highlight the potential of LLMs to automate the generation of tests based on
API specifications.

</details>


### [4] [Natural Language-Programming Language Software Traceability Link Recovery Needs More than Textual Similarity](https://arxiv.org/abs/2509.05585)
*Zhiyuan Zou,Bangchao Wang,Peng Liang,Tingting Bi,Huan Jin*

Main category: cs.SE

TL;DR: 本文通过大规模实证分析发现文本相似性在自然语言与编程语言(NL-PL)追踪链接恢复中存在局限性，提出多策略集成方法，在HGT和Gemini 2.5 Pro模型中整合领域特定辅助策略，显著提升了需求到代码追踪任务的性能。


<details>
  <summary>Details</summary>
Motivation: 软件追踪链接恢复中长期依赖文本相似性，但在NL-PL场景下存在语义鸿沟问题，需要探索更有效的方法来克服这一局限性。

Method: 通过实证分析识别多个领域特定辅助策略，将其集成到两种模型中：通过边类型整合到异构图变换器(HGT)，以及通过额外输入信息整合到基于提示的Gemini 2.5 Pro模型。

Result: 多策略HGT和Gemini 2.5 Pro模型均优于原始版本，相比当前最先进方法HGNNLink，在12个开源项目中分别实现了3.68%和8.84%的平均F1分数提升。

Conclusion: 多策略集成能有效提升NL-PL追踪链接恢复任务的整体模型性能，为解决文本相似性在跨语言场景中的局限性提供了有效解决方案。

Abstract: In the field of software traceability link recovery (TLR), textual similarity
has long been regarded as the core criterion. However, in tasks involving
natural language and programming language (NL-PL) artifacts, relying solely on
textual similarity is limited by their semantic gap. To this end, we conducted
a large-scale empirical evaluation across various types of TLR tasks, revealing
the limitations of textual similarity in NL-PL scenarios. To address these
limitations, we propose an approach that incorporates multiple domain-specific
auxiliary strategies, identified through empirical analysis, into two models:
the Heterogeneous Graph Transformer (HGT) via edge types and the prompt-based
Gemini 2.5 Pro via additional input information. We then evaluated our approach
using the widely studied requirements-to-code TLR task, a representative case
of NL-PL TLR. Experimental results show that both the multi-strategy HGT and
Gemini 2.5 Pro models outperformed their original counterparts without strategy
integration. Furthermore, compared to the current state-of-the-art method
HGNNLink, the multi-strategy HGT and Gemini 2.5 Pro models achieved average
F1-score improvements of 3.68% and 8.84%, respectively, across twelve
open-source projects, demonstrating the effectiveness of multi-strategy
integration in enhancing overall model performance for the requirements-code
TLR task.

</details>


### [5] [Verifying Correctness of PLC Software during System Evolution using Model Containment Approach](https://arxiv.org/abs/2509.05596)
*Soumyadip Bandyopadhyay,Santonu Sarkar*

Main category: cs.SE

TL;DR: 通过将旧版和新版PLC程序转换为Petri网模型，使用符号路径等价算法验证升级正确性，在实际案例中实现4倍性能提升


<details>
  <summary>Details</summary>
Motivation: PLC软件升级时确保现有功能正确性的验证挑战

Method: 将旧新版SFC转换为Petri网模型，使用符号路径等价基础的含蓄检查算法验证模型含蓄关系

Result: 在80个实际OSCAT库案例中验证了框架的可扩展性和有效性，相比verifAPS工具实现了近4倍性能提升

Conclusion: 该方法为PLC软件升级提供了高效的正确性验证方案，具有良好的应用前景

Abstract: Upgradation of Programmable Logic Controller (PLC) software is quite common
to accommodate evolving industrial requirements. Verifying the correctness of
such upgrades remains a significant challenge. In this paper, we propose a
verification-based approach to ensure the correctness of the existing
functionality in the upgraded version of a PLC software. The method converts
the older and the newer versions of the sequential function chart (SFC) into
two Petri net models. We then verify whether one model is contained within
another, based on a novel containment checking algorithm grounded in symbolic
path equivalence. For this purpose, we have developed a home-grown Petri
net-based containment checker. Experimental evaluation on 80 real-world
benchmarks from the OSCAT library highlights the scalability and effectiveness
of the framework. We have compared our approach with verifAPS, a popular tool
used for software upgradation, and observed nearly 4x performance improvement.

</details>


### [6] [Automating API Documentation with LLMs: A BERTopic Approach](https://arxiv.org/abs/2509.05749)
*AmirHossein Naghshzan*

Main category: cs.SE

TL;DR: 自动化摘要Stack Overflow社区讨论内容，为Android API提供简洁的实践摘要和代码片段，提高开发者生产力


<details>
  <summary>Details</summary>
Motivation: 官方API文档经常长缘、复杂或不完整，开发者需要从Stack Overflow等社区论坛获取实践经验

Method: 使用BERTopic从360万个Stack Overflow帖子中提取主题，应用摘要技术生成简洁摘要（包括代码片段）

Result: 通过30名Android开发者的用户研究评估，摘要在连贯性、相关性、信息量和满意度方面表现良好，显示出提高了开发生产力

Conclusion: 结合形式API知识与社区生成内容可以改善文档质量，使API资源更易访问和可操作

Abstract: Developers rely on API documentation, but official sources are often lengthy,
complex, or incomplete. Many turn to community-driven forums like Stack
Overflow for practical insights. We propose automating the summarization of
informal sources, focusing on Android APIs. Using BERTopic, we extracted
prevalent topics from 3.6 million Stack Overflow posts and applied extractive
summarization techniques to generate concise summaries, including code
snippets. A user study with 30 Android developers assessed the summaries for
coherence, relevance, informativeness, and satisfaction, showing improved
productivity. Integrating formal API knowledge with community-generated content
enhances documentation, making API resources more accessible and actionable
work.

</details>


### [7] [IoT Miner: Intelligent Extraction of Event Logs from Sensor Data for Process Mining](https://arxiv.org/abs/2509.05769)
*Edyta Brzychczy,Urszula Jessen,Krzysztof Kluza,Sridhar Sriram,Manuel Vargas Nettelnstroth*

Main category: cs.SE

TL;DR: IoT Miner是一个从工业传感器数据自动生成高级事件日志的框架，通过数据预处理、无监督聚类、LLM标注和日志构建四阶段流程，解决了传统流程挖掘中事件日志缺失的问题。


<details>
  <summary>Details</summary>
Motivation: 在采矿、制造等工业场景中，标准事件日志通常不可用，原始传感器数据缺乏结构化和语义信息，无法直接用于流程挖掘分析。

Method: 采用四阶段管道：数据预处理、无监督聚类、基于大语言模型（LLM）的标注（使用领域特定提示生成有意义的标签）、事件日志构建。关键创新是利用LLM从聚类统计中生成活动标签。

Result: 在装载-运输-卸载（LHD）采矿机械传感器数据上评估，提出相似性加权精度新指标评估标注质量。结果显示更丰富的提示词能产生更准确一致的标签。

Conclusion: IoT Miner通过结合AI和领域感知数据处理，提供了一种可扩展且可解释的方法，能够在传统日志缺失的场景下从IoT数据生成事件日志，支持流程挖掘应用。

Abstract: This paper presents IoT Miner, a novel framework for automatically creating
high-level event logs from raw industrial sensor data to support process
mining. In many real-world settings, such as mining or manufacturing, standard
event logs are unavailable, and sensor data lacks the structure and semantics
needed for analysis. IoT Miner addresses this gap using a four-stage pipeline:
data preprocessing, unsupervised clustering, large language model (LLM)-based
labeling, and event log construction. A key innovation is the use of LLMs to
generate meaningful activity labels from cluster statistics, guided by
domain-specific prompts. We evaluate the approach on sensor data from a
Load-Haul-Dump (LHD) mining machine and introduce a new metric,
Similarity-Weighted Accuracy, to assess labeling quality. Results show that
richer prompts lead to more accurate and consistent labels. By combining AI
with domain-aware data processing, IoT Miner offers a scalable and
interpretable method for generating event logs from IoT data, enabling process
mining in settings where traditional logs are missing.

</details>


### [8] [GeoAnalystBench: A GeoAI benchmark for assessing large language models for spatial analysis workflow and code generation](https://arxiv.org/abs/2509.05881)
*Qianheng Zhang,Song Gao,Chen Wei,Yibo Zhao,Ying Nie,Ziru Chen,Shijie Chen,Yu Su,Huan Sun*

Main category: cs.SE

TL;DR: 这篇论文提出了GeoAnalystBench基准，用于严格评估大语言模型在地理信息系统自动化处理任务中的能力，发现专有模型表现更好但所有模型在空间推理任务上都遇到困难。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在地理信息系统自动化方面引起了广泛关注，但其实际能力仍不确定，需要通过严格的评估来确定其真正能力和局限性。

Method: 构建了GeoAnalystBench基准，包含50个来自真实地理问题的Python任务，每个任务都有最低可交付产品，并通过工作流有效性、结构对齐、语义相似性和代码质量(CodeBLEU)进行评估。

Result: 专有模型如ChatGPT-4o-mini表现更好(有效性95%，CodeBLEU 0.39)，而小型开源模型如DeepSeek-R1-7B生成的工作流往往不完整或不一致(48.5%有效性，0.272 CodeBLEU)。需要深度空间推理的任务对所有模型都极具挑战性。

Conclusion: 当前的大语言模型在GIS自动化方面既有希望也有限制，需要人类参与的支持，本研究提供了可复现的研究框架来推进GeoAI领域的发展。

Abstract: Recent advances in large language models (LLMs) have fueled growing interest
in automating geospatial analysis and GIS workflows, yet their actual
capabilities remain uncertain. In this work, we call for rigorous evaluation of
LLMs on well-defined geoprocessing tasks before making claims about full GIS
automation. To this end, we present GeoAnalystBench, a benchmark of 50
Python-based tasks derived from real-world geospatial problems and carefully
validated by GIS experts. Each task is paired with a minimum deliverable
product, and evaluation covers workflow validity, structural alignment,
semantic similarity, and code quality (CodeBLEU). Using this benchmark, we
assess both proprietary and open source models. Results reveal a clear gap:
proprietary models such as ChatGPT-4o-mini achieve high validity 95% and
stronger code alignment (CodeBLEU 0.39), while smaller open source models like
DeepSeek-R1-7B often generate incomplete or inconsistent workflows (48.5%
validity, 0.272 CodeBLEU). Tasks requiring deeper spatial reasoning, such as
spatial relationship detection or optimal site selection, remain the most
challenging across all models. These findings demonstrate both the promise and
limitations of current LLMs in GIS automation and provide a reproducible
framework to advance GeoAI research with human-in-the-loop support.

</details>


### [9] [Code2MCP: A Multi-Agent Framework for Automated Transformation of Code Repositories into Model Context Protocol Services](https://arxiv.org/abs/2509.05941)
*Chaoqian Ouyang,Ling Yue,Shimin Di,Libin Zheng,Shaowu Pan,Min-Ling Zhang*

Main category: cs.SE

TL;DR: Code2MCP是一个自动化框架，能够将GitHub仓库转换为MCP兼容服务，解决LLM集成中的N×M问题，通过多阶段工作流和自主调试循环实现最小人工干预。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型(LLMs)集成中的N×M问题，即N个模型需要为M个工具定制集成，这种碎片化阻碍了创新并增加了开发负担。虽然MCP协议作为标准出现，但其采用受到将现有软件转换为MCP兼容服务所需手动工作的阻碍。

Method: 采用多阶段工作流自动化整个过程，包括代码分析、环境配置、服务生成和部署。关键创新是LLM驱动的闭环"运行-审查-修复"循环，使系统能够自主调试和修复生成的代码。

Result: Code2MCP不仅生成可部署的服务，还提供全面的技术文档，系统性地解锁世界上最大的开源代码仓库，自动化工具集成的关键最后一步。

Conclusion: Code2MCP作为催化剂加速MCP生态系统发展，通过高度自动化框架解决了LLM工具集成的核心挑战，代码已在GitHub开源。

Abstract: The proliferation of Large Language Models (LLMs) has created a significant
integration challenge in the AI agent ecosystem, often called the "$N \times M$
problem," where N models require custom integrations for M tools. This
fragmentation stifles innovation and creates substantial development overhead.
While the Model Context Protocol (MCP) has emerged as a standard to resolve
this, its adoption is hindered by the manual effort required to convert the
vast universe of existing software into MCP-compliant services. This is
especially true for the millions of open-source repositories on GitHub, the
world's largest collection of functional code. This paper introduces Code2MCP,
a highly automated, agentic framework designed to transform any GitHub
repository into a functional MCP service with minimal human intervention. Our
system employs a multi-stage workflow that automates the entire process, from
code analysis and environment configuration to service generation and
deployment. A key innovation of our framework is an LLM-driven, closed-loop
"Run--Review--Fix" cycle, which enables the system to autonomously debug and
repair the code it generates. Code2MCP produces not only deployable services
but also comprehensive technical documentation, acting as a catalyst to
accelerate the MCP ecosystem by systematically unlocking the world's largest
open-source code repository and automating the critical last mile of tool
integration. The code is open-sourced at
https://github.com/DEFENSE-SEU/MCP-Github-Agent.

</details>


### [10] [GRACE: Graph-Guided Repository-Aware Code Completion through Hierarchical Code Fusion](https://arxiv.org/abs/2509.05980)
*Xingliang Wang,Baoyi Wang,Chen Zhi,Junxiao Han,Xinkui Zhao,Jianwei Yin,Shuiguang Deng*

Main category: cs.SE

TL;DR: GRACE是一个基于图结构的代码检索增强生成框架，通过构建多层级代码图来捕捉代码库中的结构和语义依赖关系，显著提升了仓库级代码任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于RAG的代码补全方法过度依赖文本相似性检索，忽略了代码的结构关系（如调用链、继承层次），且检索到的代码片段以简单文本拼接方式输入LLM，导致关键结构信息丢失。

Method: GRACE构建统一的多层级代码图（包含文件结构、AST、函数调用图、类层次结构和数据流图），采用混合图检索器结合图神经网络结构相似性和文本检索，并通过图注意力网络重排序，最后通过结构融合机制将检索到的子图与本地代码上下文合并。

Result: 在公开仓库级基准测试中，GRACE在所有指标上显著优于最先进方法。使用DeepSeek-V3作为骨干LLM，GRACE比最强的基于图的RAG基线在EM和ES指标上分别提升8.19%和7.51%。

Conclusion: GRACE通过整合结构信息和语义信息，有效解决了仓库级代码任务中的上下文稀缺和结构依赖问题，为代码检索增强生成提供了新的解决方案。

Abstract: LLMs excel in localized code completion but struggle with repository-level
tasks due to limited context windows and complex semantic and structural
dependencies across codebases. While Retrieval-Augmented Generation (RAG)
mitigates context scarcity by retrieving relevant code snippets, current
approaches face significant limitations. They overly rely on textual similarity
for retrieval, neglecting structural relationships such as call chains and
inheritance hierarchies, and lose critical structural information by naively
concatenating retrieved snippets into text sequences for LLM input. To address
these shortcomings, GRACE constructs a multi-level, multi-semantic code graph
that unifies file structures, abstract syntax trees, function call graphs,
class hierarchies, and data flow graphs to capture both static and dynamic code
semantics. For retrieval, GRACE employs a Hybrid Graph Retriever that
integrates graph neural network-based structural similarity with textual
retrieval, refined by a graph attention network-based re-ranker to prioritize
topologically relevant subgraphs. To enhance context, GRACE introduces a
structural fusion mechanism that merges retrieved subgraphs with the local code
context and preserves essential dependencies like function calls and
inheritance. Extensive experiments on public repository-level benchmarks
demonstrate that GRACE significantly outperforms state-of-the-art methods
across all metrics. Using DeepSeek-V3 as the backbone LLM, GRACE surpasses the
strongest graph-based RAG baselines by 8.19% EM and 7.51% ES points on every
dataset. The code is available at
https://anonymous.4open.science/r/grace_icse-C3D5.

</details>


### [11] [Students' Perception of LLM Use in Requirements Engineering Education: An Empirical Study Across Two Universities](https://arxiv.org/abs/2509.05995)
*Sharon Guardado,Risha Parveen,Zheying Zhang,Maruf Rayhan,Nirnaya Tripathi*

Main category: cs.SE

TL;DR: 本研究实证评估了在需求工程课程中集成大语言模型的影响，发现LLMs能提高学生对RE概念的理解，特别是在需求获取和文档化任务中，但也存在学术诚信、AI过度依赖等挑战。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在需求工程教育中的整合如何重塑教学方法，旨在提高学生参与度和动机，同时为他们的专业未来提供实用工具支持。

Method: 通过调查收集了来自两所大学两门RE课程的179名学生的数据，LLMs通过不同教学形式（个人作业vs团队敏捷项目）整合到课程作业中。

Result: LLMs提高了学生对RE概念的理解，特别是在需求获取和文档化任务中。但学生担忧学术诚信、AI过度依赖等问题。个人作业学生比团队作业学生感知到更多益处。

Conclusion: 研究为LLMs在RE教育中的有效整合提供了建议，并提出了未来研究方向，以平衡AI辅助学习与RE课程中的批判性思维和协作实践。

Abstract: The integration of Large Language Models (LLMs) in Requirements Engineering
(RE) education is reshaping pedagogical approaches, seeking to enhance student
engagement and motivation while providing practical tools to support their
professional future. This study empirically evaluates the impact of integrating
LLMs in RE coursework. We examined how the guided use of LLMs influenced
students' learning experiences, and what benefits and challenges they perceived
in using LLMs in RE practices. The study collected survey data from 179
students across two RE courses in two universities. LLMs were integrated into
coursework through different instructional formats, i.e., individual
assignments versus a team-based Agile project. Our findings indicate that LLMs
improved students' comprehension of RE concepts, particularly in tasks like
requirements elicitation and documentation. However, students raised concerns
about LLMs in education, including academic integrity, overreliance on AI, and
challenges in integrating AI-generated content into assignments. Students who
worked on individual assignments perceived that they benefited more than those
who worked on team-based assignments, highlighting the importance of contextual
AI integration. This study offers recommendations for the effective integration
of LLMs in RE education. It proposes future research directions for balancing
AI-assisted learning with critical thinking and collaborative practices in RE
courses.

</details>


### [12] [A Rapid Review Regarding the Concept of Legal Requirements in Requirements Engineering](https://arxiv.org/abs/2509.06012)
*Jukka Ruohonen*

Main category: cs.SE

TL;DR: 对法律需求在需求工程研究中的概念进行快速审查，发现存在概念混乱、缺乏定义和实证证据


<details>
  <summary>Details</summary>
Motivation: 出于个人困惑、同行评审评论和现有文献中的明显混乱，需要对法律需求概念进行系统性审查

Method: 采用快速审查方法，对需求工程领域关于法律需求的研究文献进行综合分析

Result: 发现法律需求存在规范性理解但缺乏明确定义，被视为功能性或非功能性需求，具有模糊、复杂、变化快、重叠等特征，且实施励忙

Conclusion: 本文提出了关于知识空白的批判性论点，包括缺乏实证证据支撑观察结论和持续的概念混乱问题

Abstract: Out of a personal puzzlement, recent peer review comments, and demonstrable
confusion in the existing literature, the paper presents a rapid review of the
concept of legal requirements (LRs) in requirements engineering (RE) research.
According to reviewing results, a normative understanding of LRs has often been
present, although proper definitions and conceptual operationalizations are
lacking. Some papers also see LRs as functional and others as non-functional
requirements. Legal requirements are often characterized as being vague and
complex, requiring a lot of effort to elicit, implement, and validate. These
characterizations supposedly correlate with knowledge gaps among requirements
engineers. LRs are also seen to often change and overlap. They may be also
prioritized. According to the literature, they seem to be also reluctantly
implemented, often providing only a minimal baseline for other requirements.
With these and other observations, the review raises critical arguments about
apparent knowledge gaps, including a lack of empirical evidence backing the
observations and enduring conceptual confusion.

</details>


### [13] [Empirical Study of Code Large Language Models for Binary Security Patch Detection](https://arxiv.org/abs/2509.06052)
*Qingyuan Li,Binchang Li,Cuiyun Gao,Shuzheng Gao,Zongjie Li*

Main category: cs.SE

TL;DR: 研究了代码大语言模型在二进制安全补丁检测任务中的性能，构建了大规模二进制补丁数据集，并通过精调策略显著提升模型在安全补丁识别任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前学习基于源代码的安全补丁检测方法无法应用于闭源软件和专有系统，而代码大语言模型在二进制分析任务中的潜力尚未得到充分探索。

Method: 构建了包含19,448个样本的大规模二进制补丁数据集（汇编代码和伪代码两种表示），系统评估19个不同规模的代码LLM，并通过精调策略向模型注入二进制安全补丁领域知识。

Result: 直接使用普通代码LLM无法准确识别安全补丁，而经过精调的LLM在二进制安全补丁检测任务中取得了杰出性能，其中伪代码表示的效果最佳。

Conclusion: 代码LLM通过适当的精调策略可以在二进制安全补丁检测任务中发挥重要作用，为闭源软件安全维护提供了新的解决方案。

Abstract: Security patch detection (SPD) is crucial for maintaining software security,
as unpatched vulnerabilities can lead to severe security risks. In recent
years, numerous learning-based SPD approaches have demonstrated promising
results on source code. However, these approaches typically cannot be applied
to closed-source applications and proprietary systems that constitute a
significant portion of real-world software, as they release patches only with
binary files, and the source code is inaccessible. Given the impressive
performance of code large language models (LLMs) in code intelligence and
binary analysis tasks such as decompilation and compilation optimization, their
potential for detecting binary security patches remains unexplored, exposing a
significant research gap between their demonstrated low-level code
understanding capabilities and this critical security task. To address this
gap, we construct a large-scale binary patch dataset containing \textbf{19,448}
samples, with two levels of representation: assembly code and pseudo-code, and
systematically evaluate \textbf{19} code LLMs of varying scales to investigate
their capability in binary SPD tasks. Our initial exploration demonstrates that
directly prompting vanilla code LLMs struggles to accurately identify security
patches from binary patches, and even state-of-the-art prompting techniques
fail to mitigate the lack of domain knowledge in binary SPD within vanilla
models. Drawing on the initial findings, we further investigate the fine-tuning
strategy for injecting binary SPD domain knowledge into code LLMs through two
levels of representation. Experimental results demonstrate that fine-tuned LLMs
achieve outstanding performance, with the best results obtained on the
pseudo-code representation.

</details>


### [14] [Software Dependencies 2.0: An Empirical Study of Reuse and Integration of Pre-Trained Models in Open-Source Projects](https://arxiv.org/abs/2509.06085)
*Jerin Yasmin,Wenxin Jiang,James C. Davis,Yuan Tian*

Main category: cs.SE

TL;DR: 预训练模型(PTMs)作为新型软件依赖项(Software Dependencies 2.0)在开源项目中的使用情况分析，研究其管理方式、集成模式和管道阶段组织


<details>
  <summary>Details</summary>
Motivation: 预训练模型作为软件依赖项的广泛采用引入了新的维护性和可靠性挑战，需要了解开发者如何在实际项目中管理和集成这些模型

Method: 混合方法分析，从PeaTMOSS数据集的28,575个GitHub仓库中随机抽样401个仓库，定量识别PTM重用模式，定性调查开发者实践

Result: 研究了开源项目如何构建和记录PTM依赖项、PTM重用管道中的阶段和组织模式、以及不同管道阶段中PTM与其他学习组件的交互

Conclusion: 该研究揭示了预训练模型作为软件依赖项2.0在开源项目中的实际集成情况，为理解和管理这种新型依赖关系提供了重要见解

Abstract: Pre-trained models (PTMs) are machine learning models that have been trained
in advance, often on large-scale data, and can be reused for new tasks, thereby
reducing the need for costly training from scratch. Their widespread adoption
introduces a new class of software dependency, which we term Software
Dependencies 2.0, extending beyond conventional libraries to learned behaviors
embodied in trained models and their associated artifacts. The integration of
PTMs as software dependencies in real projects remains unclear, potentially
threatening maintainability and reliability of modern software systems that
increasingly rely on them. Objective: In this study, we investigate Software
Dependencies 2.0 in open-source software (OSS) projects by examining the reuse
of PTMs, with a focus on how developers manage and integrate these models.
Specifically, we seek to understand: (1) how OSS projects structure and
document their PTM dependencies; (2) what stages and organizational patterns
emerge in the reuse pipelines of PTMs within these projects; and (3) the
interactions among PTMs and other learned components across pipeline stages. We
conduct a mixed-methods analysis of a statistically significant random sample
of 401 GitHub repositories from the PeaTMOSS dataset (28,575 repositories
reusing PTMs from Hugging Face and PyTorch Hub). We quantitatively examine PTM
reuse by identifying patterns and qualitatively investigate how developers
integrate and manage these models in practice.

</details>


### [15] [Agentic Software Engineering: Foundational Pillars and a Research Roadmap](https://arxiv.org/abs/2509.06216)
*Ahmed E. Hassan,Hao Li,Dayi Lin,Bram Adams,Tse-Hsun Chen,Yutaro Kashiwa,Dong Qiu*

Main category: cs.SE

TL;DR: 本文提出了结构化智能体软件工程(SASE)愿景，将软件工程重新构想为人类导向和智能体导向的双重模式，并设计了ACE和AEE两个工作台来支持双向人机协作。


<details>
  <summary>Details</summary>
Motivation: 随着智能体软件工程(SE 3.0)时代的到来，需要确保智能体在完成复杂目标导向任务时的可信度，同时重新思考软件工程的基础支柱以适应人机协作的新范式。

Method: 提出SE for Humans和SE for Agents双重模式，设计Agent Command Environment(ACE)作为人类指挥中心，Agent Execution Environment(AEE)作为智能体工作空间，支持双向人机交互和结构化工程活动。

Result: 建立了结构化智能体软件工程(SASE)框架，定义了新的工程流程和输出格式(MRPs和CRPs)，为人机协作提供了概念支架和结构化词汇。

Conclusion: SASE愿景为软件工程社区提供了向规模化、可信赖的智能体未来转型的路线图，需要重新思考教育体系和基础原则以适应人机协作的新时代。

Abstract: Agentic Software Engineering (SE 3.0) represents a new era where intelligent
agents are tasked not with simple code generation, but with achieving complex,
goal-oriented SE objectives. To harness these new capabilities while ensuring
trustworthiness, we must recognize a fundamental duality within the SE field in
the Agentic SE era, comprising two symbiotic modalities: SE for Humans and SE
for Agents. This duality demands a radical reimagining of the foundational
pillars of SE (actors, processes, tools, and artifacts) which manifest
differently across each modality. We propose two purpose-built workbenches to
support this vision. The Agent Command Environment (ACE) serves as a command
center where humans orchestrate and mentor agent teams, handling outputs such
as Merge-Readiness Packs (MRPs) and Consultation Request Packs (CRPs). The
Agent Execution Environment (AEE) is a digital workspace where agents perform
tasks while invoking human expertise when facing ambiguity or complex
trade-offs. This bi-directional partnership, which supports agent-initiated
human callbacks and handovers, gives rise to new, structured engineering
activities (i.e., processes) that redefine human-AI collaboration, elevating
the practice from agentic coding to true agentic software engineering. This
paper presents the Structured Agentic Software Engineering (SASE) vision,
outlining several of the foundational pillars for the future of SE. The paper
culminates in a research roadmap that identifies a few key challenges and
opportunities while briefly discussing the resulting impact of this future on
SE education. Our goal is not to offer a definitive solution, but to provide a
conceptual scaffold with structured vocabulary to catalyze a community-wide
dialogue, pushing the SE community to think beyond its classic, human-centric
tenets toward a disciplined, scalable, and trustworthy agentic future.

</details>


### [16] [Learning From Software Failures: A Case Study at a National Space Research Center](https://arxiv.org/abs/2509.06301)
*Dharun Anandayuvaraj,Zain Hammadeh,Andreas Lund,Alexandra Holloway,James C. Davis*

Main category: cs.SE

TL;DR: 这篇论文通过10个深度访谈研究了高可靠性组织中软件工程师从失败中学习的实践和挑战，发现失败学习通常是非正式、随机的，缺乏结构化过程，导致重复失败持续发生。


<details>
  <summary>Details</summary>
Motivation: 软件失败可能造成严重后果，但对失败进行评估和学习的实践在行业中应用不一。高可靠性组织(HROs)在建设高风险软件系统时必须持续学习，但缺乏对实践和挑战的深度行业视角。

Method: 通过在国家太空研究中心进行10个深度访谈，并包括来自其他高可靠性组织的5个访谈数据以评估可转移性。研究者调查了工程师如何收集、文档化、分享和应用失败教训。

Result: 研究发现：(1)失败学习通常是非正式、随机的，并不一致地集成到软件开发生命周期中；(2)缺乏结构化过程导致重复失败持续发生；(3)时间约束、人员替换造成的知识丢失、文档分散和过程执行弱化等关键挑战娱制了系统性学习。

Conclusion: 这些发现深化了对软件工程师如何从失败中学习的理解，并为改善失败管理实践提供了指导。研究结果显示需要强化结构化过程和工具支持，以确保更有效地从软件失败中学习和防止重复发生。

Abstract: Software failures can have significant consequences, making learning from
failures a critical aspect of software engineering. While software
organizations are recommended to conduct postmortems, the effectiveness and
adoption of these practices vary widely. Understanding how engineers gather,
document, share, and apply lessons from failures is essential for improving
reliability and preventing recurrence. High-reliability organizations (HROs)
often develop software systems where failures carry catastrophic risks,
requiring continuous learning to ensure reliability. These organizations
provide a valuable setting to examine practices and challenges for learning
from software failures. Such insight could help develop processes and tools to
improve reliability and prevent recurrence. However, we lack in-depth industry
perspectives on the practices and challenges of learning from failures.
  To address this gap, we conducted a case study through 10 in-depth interviews
with research software engineers at a national space research center. We
examine how they learn from failures: how they gather, document, share, and
apply lessons. To assess transferability, we include data from 5 additional
interviews at other HROs. Our findings provide insight into how engineers learn
from failures in practice. To summarize: (1) failure learning is informal, ad
hoc, and inconsistently integrated into SDLC; (2) recurring failures persist
due to absence of structured processes; and (3) key challenges, including time
constraints, knowledge loss from turnover and fragmented documentation, and
weak process enforcement, undermine systematic learning. Our findings deepen
understanding of how software engineers learn from failures and offer guidance
for improving failure management practices.

</details>


### [17] [A Generic and Efficient Python Runtime Verification System and its Large-scale Evaluation](https://arxiv.org/abs/2509.06324)
*Zhuohang Shen,Mohammed Yaseen,Denini Silva,Kevin Guan,Junho Lee,Marcelo d'Amorim,Owolabi Legunsen*

Main category: cs.SE

TL;DR: PyMOP是一个通用、可扩展且高效的Python运行时验证系统，支持多种逻辑和监控算法，在大量测试中表现出色，比现有系统快上千倍，并帮助发现了121个bug。


<details>
  <summary>Details</summary>
Motivation: Python生态系统缺乏像Java那样可扩展的运行时验证系统，现有的Python RV系统要么局限于特定领域或规范逻辑，要么速度缓慢，无法满足大规模应用需求。

Method: 开发了PyMOP系统，支持五种逻辑，实现了五种现有监控算法，包含73个API规范，支持三种插桩策略，并且用户可以轻松扩展更多功能。

Result: 在1,463个GitHub项目的290,133个单元测试中，PyMOP比两个最近的动态分析系统快达1,168.3倍，帮助发现了121个bug，其中44个已被开发者修复。

Conclusion: PyMOP的通用性和高效性使其成为Python运行时验证领域的优秀平台，为未来的研究进展奠定了良好基础。

Abstract: Runtime verification (RV) now scales for testing thousands of open-source
Java projects, helping find hundreds of bugs. The popular Python ecosystem
could use such benefits. But, today's Python RV systems are limited to a domain
or specification logic, or slow. We propose PyMOP, a generic, extensible, and
efficient RV system for Python. PyMOP supports five logics, implements five
existing monitoring algorithms, ships with 73 API specs of Python and
widely-used libraries, supports three instrumentation strategies, and users can
easily add more of these. On 290,133 unit tests in 1,463 GitHub projects, we
find mainly that (i) the default monitoring algorithm for Java is often not the
fastest for Python; (ii) PyMOP is up to 1,168.3x faster than two recent dynamic
analysis systems; and (iii) 44 of 121 bugs that PyMOP helped find so far were
fixed by developers. PyMOP's generality and efficiency position it well as an
excellent platform for the next advances on RV for Python.

</details>


### [18] [Analyzing the Instability of Large Language Models in Automated Bug Injection and Correction](https://arxiv.org/abs/2509.06429)
*Mehmet Bilal Er,Nagehan İlhan,Umut Kuran*

Main category: cs.SE

TL;DR: 研究评估ChatGPT在代码修复任务中的稳定性，发现随着温度参数的增加，模型输出的结构性和功能性不稳定性显著增加


<details>
  <summary>Details</summary>
Motivation: 大语言模型在软件工程任务中应用日益增多，但存在输出不稳定问题，需要系统性评估代码修复任务中的一致性

Method: 使用不同温度设置(0, 0.5, 1)让ChatGPT产生代码修复建议，针对20个问题每个温度产生3个建议，共9个输出进行比较，采用语法相似度和输出等效率指标评估结构性和功能性一致性

Result: 温度越高输出越不稳定，高温度下功能失败率特别高，低温度下语法结构相似度较高，高温度下结构差异显著

Conclusion: 研究提供了重要方法论见解，对LLM基于错误编码系统在软件开发中的可靠性提出疑问，为更一致地应用这些系统提供指导

Abstract: The use of Large Language Models (LLMs) in software engineering tasks is
growing, especially in the areas of bug fixing and code generation.
Nevertheless, these models often yield unstable results; when executed at
different times with the same input, they can generate radically different
code. The consistency of LLMs in bug-fixing tasks has not yet been thoroughly
assessed, despite the fact that this instability has typically been discussed
in the literature in relation to code generation. The purpose of this study is
to look into how unstable an LLM like ChatGPT is when it comes to fixing code
bugs. We examine the structural, syntactic, and functional variations among
several fix recommendations made in response to the same prompt using code
samples with various error types. Additionally, we assess how instability is
affected by the temperature settings (0, 0.5, and 1) used for the model's
deterministic operation. For a total of 20 problems in the experimental
analysis, the model produced three fix suggestions at each temperature value,
comparing nine distinct outputs for each problem. The Syntax Similarity and
Output Equivalence Rate (OER) metrics were used to assess the outputs'
structural and functional consistency. The results demonstrate that the model's
outputs become much more unstable and variable as the temperature rises, with
high temperatures showing especially high rates of functional failure.
According to syntax similarity analyses, the suggested fixes show notable
structural differences at high temperatures but are fairly similar at low
temperatures. The purpose of this study is to provide important methodological
insights into how LLM-based error correction systems can be applied more
consistently in software development processes while also casting doubt on
their dependability.

</details>


### [19] [Modeling in the Design Multiverse](https://arxiv.org/abs/2509.06530)
*Sylvain Guérin,Salvador Martinez,Ciprian Teodorov*

Main category: cs.SE

TL;DR: 提出了Design Multiverse概念，旨在在建模空间中集成设计修订和变体，以管理设计路径的演化和分歧，支持多团队并行设计和系统变体管理。


<details>
  <summary>Details</summary>
Motivation: 现实设计过程中存在设计路径的演化和分歧（分支、修订、合并等），特别是多团队并行操作和探索复杂异构系统的不同替代方案时，当前建模空间无法直接管理这种时空变异性。

Method: 引入Design Multiverse概念，在建模空间中集成设计修订和变体的选择，表示由多个工件组成的设计状态快照；利用模型联邦范式实现。

Result: 提出了Design Multiverse的概念定义，讨论了模型产品线和模型/元模型协同演化等使用场景。

Conclusion: Design Multiverse能够使利益相关者无缝追踪、分析和管理设计决策、系统变体及其相互依赖关系，解决了当前建模工具无法直接管理设计过程时空变异性的问题。

Abstract: Real-world design processes often involve the evolution and divergence of
design paths (by branching, revising, merging, etc.), especially when multiple
stakeholders or teams operate concurrently and/or explore different
alternatives for complex and heterogeneous systems. Unfortunately, this
variability in time and space can not be directly managed in current modeling
spaces but requires resorting to external tools and methodologies.
  In order to tackle this problem, we introduce the Design Multiverse. The
Design Multiverse aims to integrate in the modeling space a selection of
revisions and variants, representing snapshots of a design state composed of
multiple artifacts. This enables stakeholders to seamlessly trace, analyze, and
manage design decisions, system variants, and their interdependencies.
Concretely, in this paper we present a conceptual definition of the Design
Multiverse, discuss usage scenarios such as model product lines and
model/metamodel co-evolution, and propose an implementation leveraging the
model federation paradigm.

</details>


### [20] [Design and Implementation of a Domain-specific Language for Modelling Evacuation Scenarios Using Eclipse EMG/GMF Tool](https://arxiv.org/abs/2509.06688)
*Heerok Banerjee*

Main category: cs.SE

TL;DR: Bmod是一个基于Eclipse EMF/GMF构建的领域特定语言，用于建模疏散场景，并与其他建模工具进行了比较分析


<details>
  <summary>Details</summary>
Motivation: 解决企业内部依赖关系，提升业务管理流程，缩短新手用户的使用差距，需要开发支持图形界面、层次结构和易实现的建模框架

Method: 使用Eclipse建模框架(EMF)和Eclipse图形建模框架(GMF)构建Bmod DSL，并与AToMPM、metaDepth、Sirius等工具在表达能力、学习曲线和性能方面进行比较

Result: 成功开发了Bmod DSL用于疏散场景建模，并通过比较分析展示了不同建模工具的特点

Conclusion: 基于Eclipse EMF/GMF的Bmod DSL为疏散场景建模提供了有效的解决方案，建模工具的选择需要综合考虑表达能力、易用性和性能等因素

Abstract: Domain-specific languages (DSLs) play a crucial role in resolving internal
dependencies across enterprises and boosts their upfront business management
processes. Yet, a lot of development is needed to build modelling frameworks
which support graphical interfaces (canvas, pallettes etc.), hierarchical
structures and easy implementation to shorten the gap for novice users. In this
paper, a DSL namely, Bmod is introduced, which can be used to model evacuation
scenarios. The language is built using Eclipse Modelling Framework (EMF) and
Eclipse Graphical Modelling Framework (GMF). Furthermore, a comparison is also
shown between Eclipse EMF/GMF and other modelling tools such as AToMPM,
metaDepth, Sirius etc with respect to expressiveness, learning curve and
performance.

</details>


### [21] [Efficiently Ranking Software Variants with Minimal Benchmarks](https://arxiv.org/abs/2509.06716)
*Théo Matricon,Mathieu Acher,Helge Spieker,Arnaud Gotlieb*

Main category: cs.SE

TL;DR: 提出BISS方法，通过测试套件优化技术减少基准测试的计算成本，同时保持变体排名的稳定性。


<details>
  <summary>Details</summary>
Motivation: 基准测试在软件工程中用于评估软件变体的质量和性能，但执行完整基准测试的计算资源和时间成本极高。

Method: BISection Sampling (BISS)方法，策略性地保留最关键测试，并应用新颖的分治方法在相关剩余测试中进行高效采样。

Result: 在LLM排行榜、SAT竞赛和可配置系统性能建模等数据集上，BISS方法平均将基准测试计算成本降低至44%，超过一半的基准测试可减少高达99%的成本，且排名稳定性无损失。

Conclusion: BISS方法能显著减少基准测试的计算开销，同时保持变体排名的准确性，为软件性能评估提供了高效的解决方案。

Abstract: Benchmarking is a common practice in software engineering to assess the
qualities and performance of software variants, coming from multiple competing
systems or from configurations of the same system. Benchmarks are used notably
to compare and understand variant performance, fine-tune software, detect
regressions, or design new software systems. The execution of benchmarks to get
a complete picture of software variants is highly costly in terms of
computational resources and time. In this paper, we propose a novel approach
for reducing benchmarks while maintaining stable rankings, using test suite
optimization techniques. That is, we remove instances from the benchmarks while
trying to keep the same rankings of the variants on all tests. Our method,
BISection Sampling, BISS, strategically retains the most critical tests and
applies a novel divide-and-conquer approach to efficiently sample among
relevant remaining tests. We experiment with datasets and use cases from LLM
leaderboards, SAT competitions, and configurable systems for performance
modeling. Our results show that our method outperforms baselines even when
operating on a subset of variants. Using BISS, we reduce the computational cost
of the benchmarks on average to 44% and on more than half the benchmarks by up
to 99% without loss in ranking stability.

</details>


### [22] [OpenCoderRank: AI-Driven Technical Assessments Made Easy](https://arxiv.org/abs/2509.06774)
*Hridoy Sankar Dutta,Sana Ansari,Swati Kumari,Shounak Ravi Bhalerao*

Main category: cs.SE

TL;DR: OpenCoderRank是一个易于使用的技术评估平台，在LLM时代连接问题设置者和解决者，帮助解决者准备时间限制和陌生问题，同时允许设置者自托管评估。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型时代，虽然LLM可以帮助问题设置者生成多样化的问题，但也会破坏评估的完整性，因为解决者可以轻易获得解决方案。需要一种平衡的评估平台。

Method: 开发OpenCoderRank平台，模拟技术评估环境，支持问题设置者自托管评估，为问题解决者提供时间限制和陌生问题的准备训练。

Result: 创建了一个无成本、可定制的技术评估解决方案，特别适合资源受限的环境，既保护评估完整性又提供有效的准备工具。

Conclusion: OpenCoderRank成功搭建了问题设置者与解决者之间的桥梁，在LLM时代为技术评估提供了有效的平衡解决方案。

Abstract: Organizations and educational institutions use time-bound assessment tasks to
evaluate coding and problem-solving skills. These assessments measure not only
the correctness of the solutions, but also their efficiency. Problem setters
(educator/interviewer) are responsible for crafting these challenges, carefully
balancing difficulty and relevance to create meaningful evaluation experiences.
Conversely, problem solvers (student/interviewee) apply coding efficiency and
logical thinking to arrive at correct solutions. In the era of Large Language
Models (LLMs), LLMs assist problem setters in generating diverse and
challenging questions, but they can undermine assessment integrity for problem
solvers by providing easy access to solutions. This paper introduces
OpenCoderRank, an easy-to-use platform designed to simulate technical
assessments. It acts as a bridge between problem setters and problem solvers,
helping solvers prepare for time constraints and unfamiliar problems while
allowing setters to self-host assessments, offering a no-cost and customizable
solution for technical assessments in resource-constrained environments.

</details>


### [23] [Hypergraph-Guided Regex Filter Synthesis for Event-Based Anomaly Detection](https://arxiv.org/abs/2509.06911)
*Margarida Ferreira,Victor Nicolet,Luan Pham,Joey Dodds,Daniel Kroening,Ines Lynce,Ruben Martins*

Main category: cs.SE

TL;DR: HyGLAD是一种新颖的算法，通过构建可解释的模式来检测事件数据中的异常，在性能和效率上都优于现有的深度学习方法


<details>
  <summary>Details</summary>
Motivation: 在静态系统中检测基于事件的异常，任何偏离过去行为的情况都可能表明恶意活动，需要可解释的异常检测方法

Method: 算法推断具有相似行为的实体等价类，然后构建捕获这些实体值的正则表达式，生成直接可解释的模式

Result: 在5个真实系统数据集上评估，平均性能优于所有7种DeepOD的无监督方法，训练和推理效率提高一个数量级，精确度提高1.2倍，召回率提高1.3倍

Conclusion: HyGLAD提供了一种可解释且高效的异常检测方法，在保持高性能的同时显著优于深度学习基准

Abstract: We propose HyGLAD, a novel algorithm that automatically builds a set of
interpretable patterns that model event data. These patterns can then be used
to detect event-based anomalies in a stationary system, where any deviation
from past behavior may indicate malicious activity. The algorithm infers
equivalence classes of entities with similar behavior observed from the events,
and then builds regular expressions that capture the values of those entities.
As opposed to deep-learning approaches, the regular expressions are directly
interpretable, which also translates to interpretable anomalies. We evaluate
HyGLAD against all 7 unsupervised anomaly detection methods from DeepOD on five
datasets from real-world systems. The experimental results show that on average
HyGLAD outperforms existing deep-learning methods while being an order of
magnitude more efficient in training and inference (single CPU vs GPU).
Precision improved by 1.2x and recall by 1.3x compared to the second-best
baseline.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [24] [Compositional Inductive Invariant Inference via Assume-Guarantee Reasoning](https://arxiv.org/abs/2509.06250)
*Ian Dardik,Eunsuk Kang*

Main category: cs.LO

TL;DR: 提出了一种基于组件分解的归纳不变式推理方法，通过为每个组件分配假设-保证契约并推断局部归纳不变式，从而更高效地验证复杂系统的安全性。


<details>
  <summary>Details</summary>
Motivation: 传统的归纳不变式方法需要处理整个系统的复杂转移关系，导致不变式推断困难且复杂。本文观察到这种复杂性主要源于需要闭包整个系统的转移关系。

Method: 将系统分解为组件，为每个组件分配假设-保证契约，推断局部归纳不变式（只需闭包组件自身的转移关系），最后通过所有局部不变式的合取构成整个系统的归纳不变式。

Result: 在两个案例研究中证明，该方法比全局技术更高效地推断不变式，并且局部归纳不变式提供了全局不变式无法提供的模块化洞察。

Conclusion: 组合式归纳不变式推理框架通过分解系统复杂度，提供了更高效和模块化的安全性验证方法，为复杂系统验证提供了新思路。

Abstract: A common technique for verifying the safety of complex systems is the
inductive invariant method. Inductive invariants are inductive formulas that
overapproximate the reachable states of a system and imply a desired safety
property. However, inductive invariants are notoriously complex, which makes
inductive invariant inference a challenging problem. In this work, we observe
that inductive invariant formulas are complex primarily because they must be
closed over the transition relation of an entire system. Therefore, we propose
a new approach in which we decompose a system into components, assign an
assume-guarantee contract to each component, and prove that each component
fulfills its contract by inferring a local inductive invariant. The key
advantage of local inductive invariant inference is that the local invariant
need only be closed under the transition relation for the component, which is
simpler than the transition relation for the entire system. Once local
invariant inference is complete, system-wide safety follows by construction
because the conjunction of all local invariants becomes an inductive invariant
for the entire system. We apply our compositional inductive invariant inference
technique to two case studies, in which we provide evidence that our framework
can infer invariants more efficiently than the global technique. Our case
studies also show that local inductive invariants provide modular insights
about a specification that are not offered by global invariants.

</details>


### [25] [Verifying Sampling Algorithms via Distributional Invariants](https://arxiv.org/abs/2509.06410)
*Kevin Batz,Joost-Pieter Katoen,Tobias Winkler,Daniel Zilken*

Main category: cs.LO

TL;DR: 这篇论文开发了一个验证框架，通过将概率程序视为分布变换器，引入分布循环不变量来验证离散采样算法的正确性。


<details>
  <summary>Details</summary>
Motivation: 为了确保离散采样算法的正确性，需要一个形式化的验证框架。受到验证Markov模型的分布验证方法的启发，将概率程序视为分布变换器来构建验证框架。

Method: 引入分布循环不变量的概念，将其嵌入到类似Hoare的验证框架中，包括完全正确性和部分正确性的证明规则。

Result: 成功构建了一个概率程序验证框架，并用于证明两个离散采样算法（Fast Dice Roller和Fast Loaded Dice Roller）的正确性。

Conclusion: 该框架通过形式化的分布验证方法，有效地验证了离散采样算法的正确性，为概率程序的可靠性验证提供了新的方法。

Abstract: This paper develops a verification framework aimed at establishing the
correctness of discrete sampling algorithms. We do so by considering
probabilistic programs as distribution transformers. Inspired by recent work on
distributional verification of Markov models, we introduce the notion of
(inductive) distributional loop invariants for discrete probabilistic programs.
These invariants are embedded in a Hoare-like verification framework that
includes proof rules for total and partial correctness. To illustrate the
applicability of our framework, we prove the correctness of two discrete
sampling algorithms: the Fast Dice Roller and the Fast Loaded Dice Roller.

</details>


### [26] [Tabular intermediate logics comparison](https://arxiv.org/abs/2509.06841)
*Paweł Rzążewski,Michał Stronkowski*

Main category: cs.LO

TL;DR: 研究表格中间逻辑的包含关系决策问题复杂度，通过图论到偏序集的构造转换，证明多个受限版本的NP完全性，并提出树结构偏序集的多项式时间算法


<details>
  <summary>Details</summary>
Motivation: 研究表格中间逻辑的包含关系决策问题LogContain的复杂度特性，该问题与存在满射p-态射问题SPMorph相关，两者都属于NP类但具体复杂度未知

Method: 1) 构造从图到偏序集的映射，将图的局部满射同态问题转换为偏序集的p-态射问题；2) 利用图论的NP完全性结果证明相关问题的NP完全性；3) 针对树结构偏序集设计多项式时间算法

Result: 1) 证明了LogContain和SPMorph的多个受限版本都是NP完全的；2) 找到了一个18元素的偏序集使得对应的逻辑包含问题是NP完全的；3) 对于树结构的偏序集，给出了多项式时间算法

Conclusion: 表格中间逻辑的包含关系决策问题在一般情况下是困难的（NP完全），但对于特殊结构（如树）存在高效算法，这为理解中间逻辑的复杂性提供了重要见解

Abstract: Tabular intermediate logics are intermediate logics characterized by finite
posets treated as Kripke frames. For a poset $\mathbb{P}$, let $L(\mathbb{P})$
denote the corresponding tabular intermediate logic. We investigate the
complexity of the following decision problem $\mathsf{LogContain}$: given two
finite posets $\mathbb P$ and $\mathbb Q$, decide whether $L(\mathbb P)
\subseteq L(\mathbb Q)$.
  By Jankov's and de Jongh's theorem, the problem $\mathsf{LogContain}$ is
related to the problem $\mathsf{SPMorph}$: given two finite posets $\mathbb P$
and $\mathbb Q$, decide whether there exists a surjective $p$-morphism from
$\mathbb P$ onto $\mathbb Q$. Both problems belong to the complexity class NP.
  We present two contributions. First, we describe a construction which,
starting with a graph $\mathbb{G}$, gives a poset $\mathsf{Pos}(\mathbb{G})$
such that there is a surjective locally surjective homomorphism (the
graph-theoretic analog of a $p$-morphism) from $\mathbb{G}$ onto $\mathbb{H}$
if and only if there is a surjective $p$-morphism from
$\mathsf{Pos}(\mathbb{G})$ onto $\mathsf{Pos}(\mathbb{H})$. This allows us to
translate some hardness results from graph theory and obtain that several
restricted versions of the problems $\mathsf{LogContain}$ and
$\mathsf{SPMorph}$ are NP-complete. Among other results, we present a
18-element poset $\mathbb{Q}$ such that the problem to decide, for a given
poset $\mathbb{P}$, whether $L(\mathbb{P})\subseteq L(\mathbb{Q})$ is
NP-complete.
  Second, we describe a polynomial-time algorithm that decides
$\mathsf{LogContain}$ and $\mathsf{SPMorph}$ for posets $\mathbb{T}$ and
$\mathbb{Q}$, when $\mathbb{T}$ is a tree.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [27] [Scalable Learning of One-Counter Automata via State-Merging Algorithms](https://arxiv.org/abs/2509.05762)
*Shibashis Guha,Anirban Majumdar,Prince Mathew,A. V. Sreejith*

Main category: cs.FL

TL;DR: OPNI是一种被动学习算法，用于学习确定性实时单计数器自动机(DROCA)，结合主动学习后可扩展性优于现有算法


<details>
  <summary>Details</summary>
Motivation: 受到正则语言RPNI算法的启发，需要为DROCA开发有效的学习算法，现有算法在可扩展性方面存在不足

Method: 提出OPNI被动学习算法，能够从有效样本集构建一致的DROCA，并将其与DROCA的主动学习方法相结合

Result: 实验结果表明该方法比现有最先进算法具有更好的可扩展性，并在学习可见单计数器自动机方面表现良好

Conclusion: OPNI算法为DROCA学习提供了有效的解决方案，结合主动学习方法显著提升了学习效率和可扩展性

Abstract: We propose One-counter Positive Negative Inference (OPNI), a passive learning
algorithm for deterministic real-time one-counter automata (DROCA). Inspired by
the RPNI algorithm for regular languages, OPNI constructs a DROCA consistent
with any given valid sample set.
  We further present a method for combining OPNI with active learning of DROCA,
and provide an implementation of the approach. Our experimental results
demonstrate that this approach scales more effectively than existing
state-of-the-art algorithms. We also evaluate the performance of the proposed
approach for learning visibly one-counter automata.

</details>


### [28] [On Synthesis of Timed Regular Expressions](https://arxiv.org/abs/2509.06262)
*Ziran Wang,Jie An,Naijun Zhan,Miaomiao Zhang,Zhenya Zhang*

Main category: cs.FL

TL;DR: 本文研究了定时正则表达式的合成问题，提出了一种从正负行为示例中生成最小长度定时正则表达式的方法，并通过SMT求解器验证一致性。


<details>
  <summary>Details</summary>
Motivation: 定时正则表达式是描述信息物理系统实时行为的形式化方法，需要从给定的系统行为示例中自动合成符合要求的表达式，以提高系统规范制定的效率。

Method: 采用两阶段方法：首先枚举并剪枝候选参数化定时正则表达式，然后通过SMT公式编码一致性要求并求解时间约束参数。

Result: 证明了合成问题的可判定性，并在基准测试（包括随机生成的行为和目标定时模型案例研究）上验证了方法的有效性。

Conclusion: 提出的方法能够有效合成与给定正负行为示例一致的定时正则表达式，为解决信息物理系统行为规范合成问题提供了可行方案。

Abstract: Timed regular expressions serve as a formalism for specifying real-time
behaviors of Cyber-Physical Systems. In this paper, we consider the synthesis
of timed regular expressions, focusing on generating a timed regular expression
consistent with a given set of system behaviors including positive and negative
examples, i.e., accepting all positive examples and rejecting all negative
examples. We first prove the decidability of the synthesis problem through an
exploration of simple timed regular expressions. Subsequently, we propose our
method of generating a consistent timed regular expression with minimal length,
which unfolds in two steps. The first step is to enumerate and prune candidate
parametric timed regular expressions. In the second step, we encode the
requirement that a candidate generated by the first step is consistent with the
given set into a Satisfiability Modulo Theories (SMT) formula, which is
consequently solved to determine a solution to parametric time constraints.
Finally, we evaluate our approach on benchmarks, including randomly generated
behaviors from target timed models and a case study.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [29] [Comparing Methods for the Cross-Level Verification of SystemC Peripherals with Symbolic Execution](https://arxiv.org/abs/2509.05504)
*Karl Aaron Rudkowski,Sallar Ahmadi-Pour,Rolf Drechsler*

Main category: cs.PL

TL;DR: 提出了CrosSym和SEFOS两种虚拟原型符号执行方法，分别通过修改SystemC内核和符号执行引擎来实现外设的跨层级验证，相比现有TLM方法具有更好的灵活性。


<details>
  <summary>Details</summary>
Motivation: 现代硬件设计中虚拟原型(VP)在SystemC中实现，用于复杂设计的早期分析。现有符号执行方法需要修改SystemC内核或忽略跨层级场景，无法专注于外设等特定子系统的特殊挑战。

Method: CrosSym修改SystemC内核，SEFOS修改现代符号执行引擎，两种方法都支持外设的多抽象层级符号执行验证。

Result: 在多种外设和不同抽象层级上进行了广泛评估，展示了两种工具的功能：(1)支持不同验证场景；(2)识别了300多个变异体。SEFOS保持未修改的SystemC内核和外设，CrosSym在运行时间和内存使用上稍好。

Conclusion: 与仅限于事务级建模(TLM)的最先进方法相比，我们的工具提供了相当的运行时间，同时实现了符号执行的跨层级验证能力。

Abstract: Virtual Prototypes (VPs) are important tools in modern hardware development.
At high abstractions, they are often implemented in SystemC and offer early
analysis of increasingly complex designs. These complex designs often combine
one or more processors, interconnects, and peripherals to perform tasks in
hardware or interact with the environment. Verifying these subsystems is a
well-suited task for VPs, as they allow reasoning across different abstraction
levels. While modern verification techniques like symbolic execution can be
seamlessly integrated into VP-based workflows, they require modifications in
the SystemC kernel. Hence, existing approaches therefore modify and replace the
SystemC kernel, or ignore the opportunity of cross-level scenarios completely,
and would not allow focusing on special challenges of particular subsystems
like peripherals. We propose CrosSym and SEFOS, two opposing approaches for a
versatile symbolic execution of peripherals. CrosSym modifies the SystemC
kernel, while SEFOS instead modifies a modern symbolic execution engine. Our
extensive evaluation applies our tools to various peripherals on different
levels of abstractions. Both tools extensive sets of features are demonstrated
for (1) different verification scenarios, and (2) identifying 300+ mutants. In
comparison with each other, SEFOS convinces with the unmodified SystemC kernel
and peripheral, while CrosSym offers slightly better runtime and memory usage.
In comparison to the state-of-the-art, that is limited to Transaction Level
Modelling (TLM), our tools offered comparable runtime, while enabling
cross-level verification with symbolic execution.

</details>


### [30] [Fixed Parameter Tractable Linearizability Monitoring for Stack, Queue and Anagram Agnostic Data Types](https://arxiv.org/abs/2509.05586)
*Lee Zheng Han,Umang Mathur*

Main category: cs.PL

TL;DR: 针对并发数据结构线性化验证的NP难问题，提出了基于最大并发度参数的固定参数可追踪算法，适用于栈、队列和变位词无关数据类型(AADTs)，通过前沿图和分区状态等技术实现搜索空间的有界控制。


<details>
  <summary>Details</summary>
Motivation: 并发数据结构的线性化验证即使在简单类型下也是NP难问题，需要开发在有限并发条件下高效的验证算法。

Method: 使用前沿图和分区状态来限制搜索空间；对AADTs利用线性化等价性实现对数线性时间监控；对栈采用基于文法的方法并归约到矩阵乘法；对队列使用分割序列转移系统支持高效动态规划。

Result: 开发了针对栈、队列和AADTs的固定参数可追踪算法，在最大并发度参数化下实现了高效的线性化验证。

Conclusion: 这些结果统一了在有界并发条件下对顺序敏感和变位词无关数据类型的可追踪性保证，为并发数据结构验证提供了新的理论框架。

Abstract: Verifying linearizability of concurrent data structures is NP-hard, even for
simple types. We present fixed-parameter tractable algorithms for monitoring
stacks, queues, and anagram-agnostic data types (AADTs), parameterized by the
maximum concurrency. Our approach leverages frontier graphs and partition
states to bound the search space. For AADTs, equivalence of linearizations
enables monitoring in log-linear time. For stacks, we introduce a grammar-based
method with a sub-cubic reduction to matrix multiplication, and for queues, a
split-sequence transition system supporting efficient dynamic programming.
These results unify tractability guarantees for both order-sensitive and
anagram-agnostic data types under bounded concurrency.

</details>


### [31] [Pacing Types: Safe Monitoring of Asynchronous Streams](https://arxiv.org/abs/2509.06724)
*Florian Kohn,Arthur Correnson,Jan Baumeister,Bernd Finkbeiner*

Main category: cs.PL

TL;DR: 提出了pacing types类型系统来确保异步流监控器在运行时行为良好，解决了RTLola监控框架中数据同步策略可能导致的运行时错误问题。


<details>
  <summary>Details</summary>
Motivation: 流式监控是无人机等复杂信息物理系统的实时安全保障机制，但异步数据流的处理容易导致监控器运行时错误，需要确保监控器的可靠性。

Method: 开发了pacing types类型系统，在RTLola框架中实现，通过形式化核心片段并建立新的逻辑关系来证明类型系统的正确性。

Result: 提出了一个能够确保异步流监控器运行时行为良好的类型系统，并提供了形式化证明。

Conclusion: pacing types类型系统有效解决了异步流监控中的运行时错误问题，提高了监控器的可靠性。

Abstract: Stream-based monitoring is a real-time safety assurance mechanism for complex
cyber-physical systems such as unmanned aerial vehicles. In this context, a
monitor aggregates streams of input data from sensors and other sources to give
real-time statistics and assessments of the system's health. Since monitors are
safety-critical components, it is crucial to ensure that they are free of
potential runtime errors. One of the central challenges in designing reliable
stream-based monitors is to deal with the asynchronous nature of data streams:
in concrete applications, the different sensors being monitored produce values
at different speeds, and it is the monitor's responsibility to correctly react
to the asynchronous arrival of different streams of values. To ease this
process, modern frameworks for stream-based monitoring such as RTLola feature
an expressive specification language that allows to finely specify data
synchronization policies. While this feature dramatically simplifies the design
of monitors, it can also lead to subtle runtime errors. To mitigate this issue,
this paper presents pacing types, a novel type system implemented in RTLola to
ensure that monitors for asynchronous streams are well-behaved at runtime. We
formalize the essence of pacing types for a core fragment of RTLola, and
present a soundness proof of the pacing type system using a new logical
relation.

</details>


### [32] [Termination Analysis of Linear-Constraint Programs](https://arxiv.org/abs/2509.06752)
*Amir M. Ben-Amram,Samir Genaim,Joël Ouaknine,James Worrell*

Main category: cs.PL

TL;DR: 这篇调查性论文系统总结了线性约束程序终止性分析技术，包括排序函数、连接良基迁移不变量等方法，分析了这些方法在表达力与计算复杂性之间的承赞关系。


<details>
  <summary>Details</summary>
Motivation: 程序终止性分析在数值变量和线性约束迁移的情况下存在不可判定问题，需要系统性的方法来应对这种本质困难。

Method: 调查了基础性的可判定性结果、排序函数的使用、以及连接良基迁移不变量等技术，同时也讨论了用于证明程序不会停止的非终止性证据。

Result: 对这些方法的算法和复杂性方面进行了分析，展示了不同方法在表达力与计算复杂性之间的承赞关系。

Conclusion: 该调查主要关注线性约束程序的终止性分析，未涵盖实际编程语言的分析方法以及非线性算术、概率选择或项重写系统等更复杂的抽象模型。

Abstract: This Survey provides an overview of techniques in termination analysis for
programs with numerical variables and transitions defined by linear
constraints. This subarea of program analysis is challenging due to the
existence of undecidable problems, and this Survey systematically explores
approaches that mitigate this inherent difficulty. These include foundational
decidability results, the use of ranking functions, and disjunctive
well-founded transition invariants. The Survey also discusses non-termination
witnesses, used to prove that a program will not halt. We examine the
algorithmic and complexity aspects of these methods, showing how different
approaches offer a trade-off between expressive power and computational
complexity. The Survey does not discuss how termination analysis is performed
on real-world programming languages, nor does it consider more expressive
abstract models that include non-linear arithmetic, probabilistic choice, or
term rewriting systems.

</details>


### [33] [Dato: A Task-Based Programming Model for Dataflow Accelerators](https://arxiv.org/abs/2509.06794)
*Shihan Fang,Hongzheng Chen,Niansong Zhang,Jiajie Li,Han Meng,Adrian Liu,Zhiru Zhang*

Main category: cs.PL

TL;DR: Dato是一个用于数据流加速器的Python嵌入式任务编程模型，通过将数据通信和分片作为一等类型构造，显著简化了优化代码编写，在NPU和FPGA上均实现了高性能。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习工作负载的计算需求超过了内存系统的承载能力，许多内核因数据移动而非计算而停滞。现有编程模型难以有效利用数据流加速器的片上流处理能力，低级接口开发开销大，高级语言又抽象掉了通信细节。

Method: 提出Dato编程模型，开发者通过显式流类型连接任务图，使用布局类型指定分片输入。任务首先虚拟映射到加速器的空间结构上，然后编译器生成符合硬件约束的物理映射。

Result: 在AMD Ryzen AI NPU上，Dato实现了高达84%的硬件利用率，注意力内核相比最先进的商业框架提速2.81倍。在FPGA上，生成自定义脉动阵列时性能超越领先框架，达到理论峰值性能的98%。

Conclusion: Dato通过将数据通信和分片提升为一等类型构造，在保持高性能的同时显著降低了编写优化代码的负担，为数据流加速器提供了有效的编程解决方案。

Abstract: Recent deep learning workloads increasingly push computational demand beyond
what current memory systems can sustain, with many kernels stalling on data
movement rather than computation. While modern dataflow accelerators
incorporate on-chip streaming to mitigate off-chip bandwidth limitations,
existing programming models struggle to harness these capabilities effectively.
Low-level interfaces provide fine-grained control but impose significant
development overhead, whereas high-level tile-based languages abstract away
communication details, restricting optimization and forcing compilers to
reconstruct the intended dataflow. We present Dato, a Python-embedded,
task-based programming model for dataflow accelerators that elevates data
communication and sharding to first-class type constructs. Developers write
programs as a graph of tasks connected via explicit stream types, with sharded
inputs specified using layout types. These tasks are first mapped virtually
onto the accelerator's spatial fabric, and the compiler then generates a
physical mapping that respects hardware constraints. Experimental results on
both AMD Ryzen AI NPU and Alveo FPGA devices demonstrate that Dato achieves
high performance while significantly reducing the burden of writing optimized
code. On the NPU, Dato attains up to 84% hardware utilization for GEMM and
delivers a 2.81x speedup on attention kernels compared to a state-of-the-art
commercial framework. On the FPGA, Dato surpasses leading frameworks in
performance when generating custom systolic arrays, achieving 98% of the
theoretical peak performance.

</details>


### [34] [MIO: Multiverse Debugging in the Face of Input/Output -- Extended Version with Additional Appendices](https://arxiv.org/abs/2509.06845)
*Tom Lauwaerts,Maarten Steevens,Christophe Scholliers*

Main category: cs.PL

TL;DR: 提出了一种新的多宇宙调试方法MIO，能够处理广泛的I/O操作，确保只探索常规执行可达的程序状态，避免传统多宇宙调试器中的不可达状态问题。


<details>
  <summary>Details</summary>
Motivation: 现有多宇宙调试器在处理涉及输入/输出操作的程序时，会暴露出常规执行中不可达的程序状态，这会严重阻碍调试过程，甚至导致程序员误认为代码存在bug。

Method: 开发了MIO原型，基于WARDuino WebAssembly虚拟机，提供了方法的语义并证明了调试器的正确性，确保只探索常规执行可达的状态。

Result: 通过乐高Mindstorms电机和颜色传感器构建的颜色拨盘示例，展示了该方法在STM32微控制器上实现多宇宙调试的可行性和效率。

Conclusion: 该方法能够支持广泛的I/O操作，同时保证只探索可达的程序状态，有效解决了传统多宇宙调试器在处理I/O操作时的问题。

Abstract: Debugging non-deterministic programs on microcontrollers is notoriously
challenging, especially when bugs manifest in unpredictable, input-dependent
execution paths. A recent approach, called multiverse debugging, makes it
easier to debug non-deterministic programs by allowing programmers to explore
all potential execution paths. Current multiverse debuggers enable both forward
and backward traversal of program paths, and some facilitate jumping to any
previously visited states, potentially branching into alternative execution
paths within the state space.
  Unfortunately, debugging programs that involve input/output operations using
existing multiverse debuggers can reveal inaccessible program states, i.e.
states which are not encountered during regular execution. This can
significantly hinder the debugging process, as the programmer may spend
substantial time exploring and examining inaccessible program states, or worse,
may mistakenly assume a bug is present in the code, when in fact, the issue is
caused by the debugger.
  This paper presents a novel approach to multiverse debugging, which can
accommodate a broad spectrum of input/output operations. We provide the
semantics of our approach and prove the correctness of our debugger, ensuring
that despite having support for a wide range of input/output operations the
debugger will only explore those program states which can be reached during
regular execution.
  We have developed a prototype, called MIO, leveraging the WARDuino
WebAssembly virtual machine to demonstrate the feasibility and efficiency of
our techniques. As a demonstration of the approach we highlight a color dial
built with a Lego Mindstorms motor, and color sensor, providing a tangible
example of how our approach enables multiverse debugging for programs running
on an STM32 microcontroller.

</details>


### [35] [Mechanized Metatheory of Forward Reasoning for End-to-End Linearizability Proofs](https://arxiv.org/abs/2509.06872)
*Zachary Kent,Ugur Y. Yavuz,Siddhartha Jayanti,Stephanie Balzer,Guy Blelloch*

Main category: cs.PL

TL;DR: 这篇论文形式化了Jayanti等人提出的前向推理线性化证明技术，并在Rocq中机械证明其安全性和完备性，通过并发寄存器案例进行验证。


<details>
  <summary>Details</summary>
Motivation: 虽然Jayanti等人的前向推理线性化证明技术被证明为安全和完备的，但其重要的元理论结果尚未得到机械化验证，无法生成验证的端到端线性化证明。

Method: 在Rocq中形式化前向推理线性化技术，机械证明其安全性和完备性，并使用简单并发寄存器作为案例研究进行验证。

Result: 实现了该技术的形式化机械证明，生成了第一个验证的端到端线性化证明，减小了可信计算基础的规模。

Conclusion: 该工作通过形式化机械证明前向推理线性化技术，为生成验证的并发数据结构线性化证明提供了可靠的基础。

Abstract: In the past decade, many techniques have been developed to prove
linearizability, the gold standard of correctness for concurrent data
structures. Intuitively, linearizability requires that every operation on a
concurrent data structure appears to take place instantaneously, even when
interleaved with other operations. Most recently, Jayanti et al. presented the
first sound and complete "forward reasoning" technique for proving
linearizability that relates the behavior of a concurrent data structure to a
reference atomic data structure as time moves forward. This technique can be
used to produce machine-checked proofs of linearizability in TLA+. However,
while Jayanti et al.'s approach is shown to be sound and complete, a
mechanization of this important metatheoretic result is still outstanding. As a
result, it is not possible to produce verified end-to-end proofs of
linearizability. To reduce the size of this trusted computing base, we
formalize this forward reasoning technique and mechanize proofs of its
soundness and completeness in Rocq. As a case study, we use the approach to
produce a verified end-to-end proof of linearizability for a simple concurrent
register.

</details>
