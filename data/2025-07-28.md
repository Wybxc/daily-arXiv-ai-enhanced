<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.LO](#cs.LO) [Total: 11]
- [cs.SE](#cs.SE) [Total: 15]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Decompiling Rust: An Empirical Study of Compiler Optimizations and Reverse Engineering Challenges](https://arxiv.org/abs/2507.18792)
*Zixu Zhou*

Main category: cs.PL

TL;DR: 评估Rust二进制文件反编译质量的基准测试，发现泛型、特质方法和错误处理结构显著降低质量，尤其是在发布构建中。


<details>
  <summary>Details</summary>
Motivation: 由于Rust的丰富类型系统、编译器优化和高层抽象，反编译其二进制文件具有挑战性。

Method: 通过自动化评分框架和典型案例研究，分析核心Rust功能和编译器构建模式对反编译质量的影响。

Result: 泛型、特质方法和错误处理结构显著降低反编译质量，尤其在发布构建中。

Conclusion: 研究为工具开发者提供了实用建议，并强调需要Rust感知的反编译策略。

Abstract: Decompiling Rust binaries is challenging due to the language's rich type
system, aggressive compiler optimizations, and widespread use of high-level
abstractions. In this work, we conduct a benchmark-driven evaluation of
decompilation quality across core Rust features and compiler build modes. Our
automated scoring framework shows that generic types, trait methods, and error
handling constructs significantly reduce decompilation quality, especially in
release builds. Through representative case studies, we analyze how specific
language constructs affect control flow, variable naming, and type information
recovery. Our findings provide actionable insights for tool developers and
highlight the need for Rust-aware decompilation strategies.

</details>


### [2] [IsaMini: Redesigned Isabelle Proof Lanugage for Machine Learning](https://arxiv.org/abs/2507.18885)
*Qiyuan Xu,Renxi Wang,Haonan Li,David Sanan,Conrad Watt*

Main category: cs.PL

TL;DR: 论文提出了一种改进的证明语言MiniLang，用于提升神经定理证明（NTP）在Isabelle/HOL中的表现，实验显示其显著提高了LLMs在PISA基准上的成功率。


<details>
  <summary>Details</summary>
Motivation: 通过优化证明语言设计，提升大型语言模型（LLMs）在自动定理证明中的表现，以减少证明工程中的高成本。

Method: 引入MiniLang，一种为Isabelle/HOL设计的改进证明语言，结合增强版Sledgehammer工具，并在PISA基准上测试两种微调LLMs的性能。

Result: MiniLang使LLMs在PISA基准上的成功率最高提升29%，pass@1达到69.1%，超越之前Baldur的pass@64（65.7%）；pass@8达到79.2%，超越当前最佳Magnushammer（71.0%）。

Conclusion: MiniLang显著提升了NTP的性能，证明了优化证明语言对LLMs在自动定理证明中的重要性。

Abstract: Neural Theorem Proving (NTP) employs deep learning methods, particularly
Large Language Models (LLMs), to automate formal proofs in proof assistants.
This approach holds promise for reducing the dramatic labor costs or
computation costs required in proof engineering, which is fundamental to formal
verification and other software engineering methods. The paper explores the
potential of improving NTP by redesigning the proof language, given that LLMs'
capabilities depend highly on representations. We introduce \emph{MiniLang}, a
redesigned proof language for Isabelle/HOL incorporating an improved version of
Sledgehammer. Experiments show MiniLang benefits two fine-tuned LLMs by
improving the success rate on the PISA benchmark by up to 29\% in comparison to
generation of Isar proof script. The success rate under one attempt (so-called
\emph{pass@1}) reaches 69.1\%, exceeding the previous Baldur's pass@64
(65.7\%); The pass@8 reaches 79.2\%, exceeding the state-of-the-art on PISA
(71.0\%) achieved by Magnushammer.

</details>


### [3] [An Enumerative Embedding of the Python Type System in ACL2s](https://arxiv.org/abs/2507.19015)
*Samuel Xifaras,Panagiotis Manolios,Andrew T. Walter,William Robertson*

Main category: cs.PL

TL;DR: 该论文提出了一种在ACL2s中嵌入Python类型系统子集的方法，用于生成输入以模糊测试Python程序，并评估了其效果。


<details>
  <summary>Details</summary>
Motivation: Python作为行业标准语言，其代码验证工具仍有不足，尤其是类型检查器未能检测到的错误。

Method: 在ACL2s中嵌入Python类型系统子集，生成类型实例作为模糊测试输入，并评估代码覆盖率。

Result: 在四个开源仓库中测试，代码覆盖率为68%至80%，并识别了影响覆盖率的代码模式。

Conclusion: 该方法有效，未来工作可进一步优化覆盖率并扩展支持的类型范围。

Abstract: Python is a high-level interpreted language that has become an industry
standard in a wide variety of applications. In this paper, we take a first step
towards using ACL2s to reason about Python code by developing an embedding of a
subset of the Python type system in ACL2s. The subset of Python types we
support includes many of the most commonly used type annotations as well as
user-defined types comprised of supported types. We provide ACL2s definitions
of these types, as well as defdata enumerators that are customized to provide
code coverage and identify errors in Python programs. Using the ACL2s
embedding, we can generate instances of types that can then be used as inputs
to fuzz Python programs, which allows us to identify bugs in Python code that
are not detected by state-of-the-art Python type checkers. We evaluate our work
against four open-source repositories, extracting their type information and
generating inputs for fuzzing functions with type signatures that are in the
supported subset of Python types. Note that we only use the type signatures of
functions to generate inputs and treat the bodies of functions as black boxes.
We measure code coverage, which ranges from about 68% to more than 80%, and
identify code patterns that hinder coverage such as complex branch conditions
and external file system dependencies. We conclude with a discussion of the
results and recommendations for future work.

</details>


### [4] [A Programming Language for Feasible Solutions](https://arxiv.org/abs/2507.19176)
*Weijun Chen,Yuxi Fu,Huan Long*

Main category: cs.PL

TL;DR: 本文提出了一种新型命令式编程语言，其静态类型系统确保所有可定义程序均在多项式时间内运行，且所有多项式时间可解问题均可由该语言实现。


<details>
  <summary>Details</summary>
Motivation: 为解决程序验证中的运行时效率和终止性问题，避免临时性处理，设计一个保证这些性质的稳健框架。

Method: 基于静态类型系统设计新语言，确保程序运行时间多项式化，并通过等价性定理证明其完备性。

Result: 理论证明了等价性定理，实践上实现了语言解释器，验证了方法的可行性。

Conclusion: 该语言为多项式时间计算提供了高效验证和编程框架，理论和实践均具贡献。

Abstract: Runtime efficiency and termination are crucial properties in the studies of
program verification. Instead of dealing with these issues in an ad hoc manner,
it would be useful to develop a robust framework in which such properties are
guaranteed by design. This paper introduces a new imperative programming
language whose design is grounded in a static type system that ensures the
following equivalence property: All definable programs are guaranteed to run in
polynomial time; Conversely, all problems solvable in polynomial time can be
solved by some programs of the language. The contribution of this work is
twofold. On the theoretical side, the foundational equivalence property is
established, and the proof of the equivalence theorem is non-trivial. On the
practical side, a programming approach is proposed that can streamline program
analysis and verification for feasible computations. An interpreter for the
language has been implemented, demonstrating the feasibility of the approach in
practice.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [5] [Who Wins the Multi-Structural Game?](https://arxiv.org/abs/2507.18718)
*Ronald Fagin,Neil Immerman,Phokion Kolaitis,Jonathan Lenchner,Rik Sengupta*

Main category: cs.LO

TL;DR: 本文研究了多结构（MS）游戏的决策问题，证明其为PSPACE难但包含于NEXPTIME中，并解决了Pezzoli提出的关于EF游戏难度与模式基数依赖的开放问题。


<details>
  <summary>Details</summary>
Motivation: 研究多结构（MS）游戏的决策问题复杂性，以扩展对形式逻辑语言游戏的理解，并解决EF游戏的相关开放问题。

Method: 结合Pezzoli的构造方法、优化问题的不可近似性理论以及MS游戏的并行玩法技术。

Result: 证明MS游戏的决策问题为PSPACE难但包含于NEXPTIME中，同时解决了EF游戏模式基数依赖的开放问题。

Conclusion: MS游戏的决策问题复杂性介于PSPACE难和NEXPTIME之间，为形式逻辑语言游戏的研究提供了新视角。

Abstract: Combinatorial games played between two players, called Spoiler and
Duplicator, have often been used to capture syntactic properties of formal
logical languages. For instance, the widely used Ehrenfeucht-Fra\"iss\'e (EF)
game captures the syntactic measure of quantifier rank of first-order formulas.
For every such game, there is an associated natural decision problem: "given an
instance of the game, does Spoiler win the game on that instance?" For EF
games, this problem was shown to be PSPACE-complete by Pezzoli in 1998. In this
present paper, we show that the same problem for the *multi-structural* (MS)
games of recent interest is PSPACE-hard, but contained in NEXPTIME. In the
process, we also resolve an open problem posed by Pezzoli about the dependence
of the hardness results for EF games on the arity of the schema under
consideration. Our techniques combine adaptations of Pezzoli's constructions
together with insights from the theory of inapproximability of optimization
problems, as well as the recently developed technique of parallel play for MS
games.

</details>


### [6] [Higher-order Kripke models for intuitionistic and non-classical modal logics](https://arxiv.org/abs/2507.18798)
*Victor Barroso-Nascimento*

Main category: cs.LO

TL;DR: 论文提出了高阶克里普克模型，推广了标准克里普克模型，通过层级化定义模态语义，适用于非经典逻辑。


<details>
  <summary>Details</summary>
Motivation: 通过层级化模型，为非经典逻辑（如直觉主义模态逻辑）提供更直观的语义解释。

Method: 将标准克里普克模型视为0阶模型，高阶模型则由低阶模型构成，并引入可访问性关系定义模态。

Result: 展示了1阶模型与双关系模型（IK）或新逻辑（MK）的等价性，并提供了直观的时间线和替代时间线解释。

Conclusion: 高阶克里普克模型为多种非经典逻辑提供了统一的语义框架，并具有直观的解释性。

Abstract: This paper introduces higher-order Kripke models, a generalization of
standard Kripke models that is remarkably close to Kripke's original idea -
both mathematically and conceptually. Standard Kripke models are now considered
$0$-ary models, whereas an $n$-ary model for $n > 0$ is a model whose set of
objects (''possible worlds'') contains only $(n-1)$-ary Kripke models. Models
with infinitely many layers are also considered. This framework is obtained by
promoting a radical change of perspective in how modal semantics for
non-classical logics are defined: just like classical modalities are obtained
through use of an accessibility relation between classical propositional
models, non-classical modalities are now obtained through use of an
accessibility relation between non-classical propositional models (even when
they are Kripke models already). The paper introduces the new models after
dealing specifically with the case of intuitionistic modal logic. It is shown
that, depending on which intuitionistic $0$-ary propositional models are
allowed, we may obtain $1$-ary models equivalent to either birelational models
for $IK$ or for a new logic called $MK$. Those $1$-ary models have an intuitive
reading that adds to the interpretation of intuitionistic models in terms of
''timelines'' the concept of ''alternative timelines''. More generally, the
$1$-ary models can be read as defining a concept of ''alternative'' for any
substantive interpretation of the $0$-ary models. The semantic clauses for
necessity and possibility of $MK$ are also modular and can be used to obtain
similar modal semantics for every non-classical logic, each of which can be
provided with a similar intuitive reading. After intuitionistic modal logic is
dealt with, the general structure of High-order Kripke Models and some of its
variants are defined, and a series of conjectures about their properties are
stated.

</details>


### [7] [A Formalization of the Yul Language and Some Verified Yul Code Transformations](https://arxiv.org/abs/2507.19012)
*Alessandro Coglio,Eric McCarthy*

Main category: cs.LO

TL;DR: 论文介绍了Yul作为Solidity编译中间语言的形式化验证，使用ACL2定理证明器确保代码转换的正确性。


<details>
  <summary>Details</summary>
Motivation: 确保Yul代码转换及其序列的正确性，避免智能合约中的潜在错误。

Method: 使用ACL2定理证明器对Yul的语法和语义进行形式化，并验证静态与动态语义的关系及代码转换的正确性。

Result: 开发了Yul的形式化模型，证明了代码转换的正确性。

Conclusion: 通过形式化验证，提高了Yul代码转换的可靠性，为智能合约的安全性提供了保障。

Abstract: Yul is an intermediate language used in the compilation of the Solidity
programming language for Ethereum smart contracts. The compiler applies
customizable sequences of transformations to Yul code. To help ensure the
correctness of these transformations and their sequencing, we used the ACL2
theorem prover to develop a formalization of the syntax and semantics of Yul,
proofs relating static and dynamic semantics, a formalization of some Yul code
transformations, and correctness proofs for these transformations.

</details>


### [8] [A Proof of the Schröder-Bernstein Theorem in ACL2](https://arxiv.org/abs/2507.19008)
*Grant Jurgensen*

Main category: cs.LO

TL;DR: 在ACL2中验证Schröder-Bernstein定理，通过链理论定义非可计算见证。


<details>
  <summary>Details</summary>
Motivation: 验证经典集合论中的Schröder-Bernstein定理在形式化逻辑中的适用性。

Method: 采用链理论定义非可计算见证，遵循已知证明路径。

Result: 成功在ACL2中验证了Schröder-Bernstein定理。

Conclusion: 证明了该定理在形式化逻辑中的有效性，为相关理论提供了支持。

Abstract: The Schr\"oder-Bernstein theorem states that, for any two sets P and Q, if
there exists an injection from P to Q and an injection from Q to P, then there
must exist a bijection between the two sets. Classically, it follows that the
ordering of the cardinal numbers is antisymmetric. We describe a formulation
and verification of the Schr\"oder-Bernstein theorem in ACL2 following a
well-known proof, introducing a theory of chains to define a non-computable
witness.

</details>


### [9] [RV32I in ACL2](https://arxiv.org/abs/2507.19009)
*Carl Kwan*

Main category: cs.LO

TL;DR: 本文介绍了一个基于ACL2的RISC-V 32位基础指令集架构模拟器，采用操作语义风格编写，验证了状态对象的多项定理，并分离了解码函数与语义函数。


<details>
  <summary>Details</summary>
Motivation: 为RISC-V 32位指令集提供一个简单且可验证的模拟器，强调解码与语义的分离以实现自动验证。

Method: 使用ACL2编写操作语义风格的模拟器，验证状态对象的读写定理，并分离指令解码与语义功能。

Result: 成功验证了RV32I指令的编码/解码函数，所有证明均为自动完成。

Conclusion: 该模拟器为RISC-V指令集提供了一种简单且可验证的建模方法，解码与语义的分离提高了验证效率。

Abstract: We present a simple ACL2 simulator for the RISC-V 32-bit base instruction set
architecture, written in the operational semantics style. Like many other ISA
models, our RISC-V state object is a single-threaded object and we prove
read-over-write, write-over-write, writing-the-read, and state well-formedness
theorems. Unlike some other models, we separate the instruction decoding
functions from their semantic counterparts. Accordingly, we verify encoding /
decoding functions for each RV32I instruction, the proofs for which are
entirely automatic.

</details>


### [10] [On Automating Proofs of Multiplier Adder Trees using the RTL Books](https://arxiv.org/abs/2507.19010)
*Mayank Manjrekar*

Main category: cs.LO

TL;DR: 介绍了一种实验性、已验证的条款处理器ctv-cp，用于Arm的算术硬件设计形式验证框架，自动化了整数乘法模块的ACL2证明开发。


<details>
  <summary>Details</summary>
Motivation: 提高整数乘法模块在硬件设计（如浮点除法和矩阵乘法）中形式验证的效率。

Method: 开发并验证了ctv-cp条款处理器，集成到Arm的形式验证框架中。

Result: ctv-cp显著自动化了ACL2证明开发过程。

Conclusion: ctv-cp为硬件设计的形式验证提供了高效、自动化的解决方案。

Abstract: We present an experimental, verified clause processor ctv-cp that fits into
the framework used at Arm for formal verification of arithmetic hardware
designs. This largely automates the ACL2 proof development effort for integer
multiplier modules that exist in designs ranging from floating-point division
to matrix multiplication.

</details>


### [11] [A Formalization of the Correctness of the Floodsub Protocol](https://arxiv.org/abs/2507.19013)
*Ankit Kumar,Panagiotis Manolios*

Main category: cs.LO

TL;DR: Floodsub是一个简单、健壮的P2P发布/订阅协议，本文通过Well-Founded Simulation证明其正确性。


<details>
  <summary>Details</summary>
Motivation: 验证Floodsub协议的正确性，展示其与抽象规范Broadcastsub的关系。

Method: 使用Well-Founded Simulation（WFS）进行局部状态和后续状态的推理，机械化证明Floodsub是Broadcastsub的模拟细化。

Result: 成功机械化证明了Floodsub是Broadcastsub的模拟细化，首次实现真实世界发布/订阅协议的机械化验证。

Conclusion: 本文通过WFS机械化验证了Floodsub的正确性，为类似协议提供了验证方法。

Abstract: Floodsub is a simple, robust and popular peer-to-peer publish/subscribe
(pubsub) protocol, where nodes can arbitrarily leave or join the network,
subscribe to or unsubscribe from topics and forward newly received messages to
all of their neighbors, except the sender or the originating peer. To show the
correctness of Floodsub, we propose its specification: Broadcastsub, in which
implementation details like network connections and neighbor subscriptions are
elided. To show that Floodsub does really implement Broadcastsub, one would
have to show that the two systems have related infinite computations. We prove
this by reasoning locally about states and their successors using Well-Founded
Simulation (WFS). In this paper, we focus on the mechanization of a proof which
shows that Floodsub is a simulation refinement of Broadcastsub using WFS. To
the best of our knowledge, ours is the first mechanized refinement-based
verification of a real world pubsub protocol.

</details>


### [12] [An ACL2s Interface to Z3](https://arxiv.org/abs/2507.19014)
*Andrew T. Walter,Panagiotis Manolios*

Main category: cs.LO

TL;DR: Lisp-Z3是一个扩展ACL2s系统的框架，支持使用Z3 SMT求解器，结合Common Lisp、ACL2/s和Z3的功能，适用于多种应用场景。


<details>
  <summary>Details</summary>
Motivation: 结合SMT求解器和交互式定理证明的优势，提供更强大的工具开发能力。

Method: 通过Lisp-Z3框架，将Z3与ACL2/s集成，支持Common Lisp开发工具。

Result: 成功应用于数独求解器、字符串求解器SeqSolve和硬件在环模糊测试，表现优异。

Conclusion: Lisp-Z3展示了在多种应用中的潜力，未来计划进一步扩展其在依赖类型中的应用。

Abstract: We present Lisp-Z3, an extension to the ACL2s systems programming framework
(ASPF) that supports the use of the Z3 satisfiability modulo theories (SMT)
solver. Lisp-Z3 allows one to develop tools written using the full feature set
of Common Lisp that can use both ACL2/s (either ACL2 or ACL2s) and Z3 as
services, combining the power of SMT and interactive theorem proving. Lisp-Z3
is usable by anyone who would like to interact with Z3 from Common Lisp, as it
does not depend on the availability of ACL2/s. We discuss the use of Lisp-Z3 in
three applications. The first is a Sudoku solver. The second is SeqSolve, a
string solver which solved a larger number of benchmark problems more quickly
than any other existing solver at the time of its publishing. Finally, Lisp-Z3
was also used in the context of hardware-in-the-loop fuzzing of wireless
routers, where low latency was an important goal. The latter two applications
leveraged the ability of Lisp-Z3 to integrate Z3 with ACL2s code. We have
further plans to use Lisp-Z3 inside of ACL2s to provide more powerful automated
support for dependent types, and in particular more efficient generation of
counterexamples to properties involving dependent types. This paper describes
the usage and implementation of Lisp-Z3, as well as an evaluation of its use in
the aforementioned applications.

</details>


### [13] [A CASP-based Solution for Traffic Signal Optimisation](https://arxiv.org/abs/2507.19061)
*Alice Tarzariol,Marco Maratea,Mauro Vallati*

Main category: cs.LO

TL;DR: 本文提出了一种基于约束答案集编程（CASP）的交通信号优化方法，替代了传统的PDDL+语言，实验表明该方法在解决质量和优化能力上优于PDDL+。


<details>
  <summary>Details</summary>
Motivation: 传统PDDL+语言在优化交通信号时存在局限性，无法有效指定优化目标和计算最优方案。

Method: 使用CASP语言进行编码，并通过clingcon 3系统求解，实验基于英国哈德斯菲尔德镇的历史数据。

Result: 实验结果表明，CASP方法在解决交通信号优化问题上优于PDDL+，提高了解决方案的质量。

Conclusion: CASP方法在交通信号优化中具有潜力，能够克服PDDL+的局限性并提升性能。

Abstract: In the context of urban traffic control, traffic signal optimisation is the
problem of determining the optimal green length for each signal in a set of
traffic signals. The literature has effectively tackled such a problem, mostly
with automated planning techniques leveraging the PDDL+ language and solvers.
However, such language has limitations when it comes to specifying optimisation
statements and computing optimal plans. In this paper, we provide an
alternative solution to the traffic signal optimisation problem based on
Constraint Answer Set Programming (CASP). We devise an encoding in a CASP
language, which is then solved by means of clingcon 3, a system extending the
well-known ASP solver clingo. We performed experiments on real historical data
from the town of Huddersfield in the UK, comparing our approach to the PDDL+
model that obtained the best results for the considered benchmark. The results
showed the potential of our approach for tackling the traffic signal
optimisation problem and improving the solution quality of the PDDL+ plans.

</details>


### [14] [Transfinite Fixed Points in Alpay Algebra as Ordinal Game Equilibria in Dependent Type Theory](https://arxiv.org/abs/2507.19245)
*Faruk Alpay,Bugra Kilictas,Taylan Alpay*

Main category: cs.LO

TL;DR: 论文通过迭代变换证明自引用过程的稳定结果与无限修订对话的唯一均衡相同，并将结果嵌入依赖类型理论，提供机器验证的收敛性证明。


<details>
  <summary>Details</summary>
Motivation: 为Alpay代数提供理论基础，验证语义收敛的哲学主张，并为无限自引用系统的推理提供形式化工具。

Method: 结合经典不动点定理、良基归纳和序论连续性，扩展到超限域，并嵌入依赖类型理论进行形式化验证。

Result: 证明了迭代对话必然稳定且极限唯一，为无限自引用系统的收敛提供了严格基础。

Conclusion: 通过统一不动点理论、游戏语义、序数分析和类型理论，为无限自引用系统的推理和计算验证提供了实用工具。

Abstract: This paper contributes to the Alpay Algebra by demonstrating that the stable
outcome of a self referential process, obtained by iterating a transformation
through all ordinal stages, is identical to the unique equilibrium of an
unbounded revision dialogue between a system and its environment. The analysis
initially elucidates how classical fixed point theorems guarantee such
convergence in finite settings and subsequently extends the argument to the
transfinite domain, relying upon well founded induction and principles of order
theoretic continuity.
  Furthermore, the resulting transordinal fixed point operator is embedded into
dependent type theory, a formalization which permits every step of the
transfinite iteration and its limit to be verified within a modern proof
assistant. This procedure yields a machine checked proof that the iterative
dialogue necessarily stabilizes and that its limit is unique. The result
provides a foundation for Alpay's philosophical claim of semantic convergence
within the framework of constructive logic. By unifying concepts from fixed
point theory, game semantics, ordinal analysis, and type theory, this research
establishes a broadly accessible yet formally rigorous foundation for reasoning
about infinite self referential systems and offers practical tools for
certifying their convergence within computational environments.

</details>


### [15] [Order in Partial Markov Categories](https://arxiv.org/abs/2507.19424)
*Elena Di Lavore,Mario Román,Paweł Sobociński,Márk Széles*

Main category: cs.LO

TL;DR: 本文探讨了部分马尔可夫范畴中的两种序关系，证明了其具有预序集和单调映射的范畴富集性，并提出了柯西-施瓦茨不等式的合成版本。


<details>
  <summary>Details</summary>
Motivation: 研究部分马尔可夫范畴中的序关系，以支持抽象概率计算的不等式推理。

Method: 通过构造预序集和单调映射的范畴富集性，分析比较器（codiagonal maps）与序性质的关系。

Result: 证明了部分马尔可夫范畴的序富集性，并提出了柯西-施瓦茨不等式的合成版本。

Conclusion: 序富集性和柯西-施瓦茨不等式为部分马尔可夫范畴中的不等式推理提供了新工具。

Abstract: Partial Markov categories are a recent framework for categorical probability
theory, providing an abstract account of partial probabilistic computation. In
this article, we discuss two order relations on the morphisms of a partial
Markov category. In particular, we prove that every partial Markov category is
canonically enriched over the category of preordered sets and monotone maps. We
show that our construction recovers several well-known order enrichments. We
also demonstrate that the existence of codiagonal maps (comparators) is closely
related to order properties of partial Markov categories. We propose a
synthetic version of the Cauchy-Schwarz inequality to facilitate inequational
reasoning in partial Markov categories. We apply this new axiom to prove that
updating a prior distribution with an evidence predicate increases the
likelihood of the evidence in the posterior.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [16] [Exploring the Landscape of Fairness Interventions in Software Engineering](https://arxiv.org/abs/2507.18726)
*Sadia Afrin Mim*

Main category: cs.SE

TL;DR: 论文综述了AI公平性问题及其解决方案。


<details>
  <summary>Details</summary>
Motivation: AI在实际应用中存在数据偏见等风险，需解决公平性问题。

Method: 总结了多种公平性干预措施的研究和方法。

Result: 提供了解决AI公平性问题的综合视角。

Conclusion: 公平性干预是AI应用中的重要研究方向。

Abstract: Current developments in AI made it broadly significant for reducing human
labor and expenses across several essential domains, including healthcare and
finance. However, the application of AI in the actual world poses multiple
risks and disadvantages due to potential risk factors in data (e.g., biased
dataset). Practitioners developed a number of fairness interventions for
addressing these kinds of problems. The paper acts as a survey, summarizing the
various studies and approaches that have been developed to address fairness
issues

</details>


### [17] [Agentic Program Repair from Test Failures at Scale: A Neuro-symbolic approach with static analysis and test execution feedback](https://arxiv.org/abs/2507.18755)
*Chandra Maddila,Adam Tait,Claire Chang,Daniel Cheng,Nauman Ahmad,Vijayaraghavan Murali,Marshall Roch,Arnaud Avondet,Aaron Meltzer,Victor Montalvao,Michael Hopko,Chris Waterson,Parth Thakkar,Renuka Fernandez,Kristian Kristensen,Sivan Barzily,Sherry Chen,Rui Abreu,Nachiappan Nagappan,Payam Shodjai,Killian Murphy,James Everingham,Aparna Ramani,Peter C. Rigby*

Main category: cs.SE

TL;DR: 开发了一个基于LLM的工程代理，用于大规模修复代码中的测试失败问题，结合ReAct框架和LLM-as-a-Judge，在离线评估和生产环境中表现良好。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的发展，大型组织可以利用其能力进行大规模的代码修复，提高效率。

Method: 以Llama为基础，使用ReAct框架开发代理，通过静态分析和测试反馈优化解决方案，并利用LLM-as-a-Judge确保修复质量。

Result: 离线评估中，70B模型表现接近405B模型，生产环境中31.5%的修复被采纳。

Conclusion: 工程代理在代码修复中表现出潜力，但仍有改进空间，尤其是在部分正确解决方案的处理上。

Abstract: Aim: With the advent of LLMs, sophisticated agentic program repair has become
viable at large organizations with large codebases. In this work, we develop an
Engineering Agent that fixes the source code based on test failures at scale
across diverse software offerings internally.
  Method: Using Llama as the base, we employ the ReAct harness to develop an
agent. We start with a test failure that was triaged by a rule-based test
failure bot. We then set up an agentic harness and allow the agent to reason
and run a set of 15 actions from reading a file to generating a patch. We
provide feedback to the agent through static analysis and test failures so it
can refine its solution. We leverage an LLM-as-a-Judge to ensure that the patch
conforms to the standards followed by a human review to land fixes.
  Benchmark Findings: We curated offline benchmarks for our patch generator,
the Engineering Agent loop, and the LLM-as-a-Judge. In offline evaluations we
found that a specialized 70B model is highly competitive with the much larger
but vanilla Llama-405B. In an ablation study, we found that the ReAct harness
(neural model) benefited from the symbolic information from static analysis
tools and test execution traces. A model that strikes a balance between the
solve rate and error rate vs the cost and latency has a benchmark solve rate of
42.3% using an average 11.8 feedback iterations.
  Production Findings: In a three month period, 80% of the generated fixes were
reviewed, of which 31.5% were landed (25.5% of the total number of generated
fixes).
  Feedback from Engineers: We used open coding to extract qualitative themes
from engineers' feedback. We saw positive feedback in the form of quick
approvals, gratitude, and surprise. We also found mixed feedback when the
Engineering Agent's solution was partially correct and it served as a good
starting point.

</details>


### [18] [MemoCoder: Automated Function Synthesis using LLM-Supported Agents](https://arxiv.org/abs/2507.18812)
*Yiping Jia,Zhen Ming Jiang,Shayan Noei,Ying Zou*

Main category: cs.SE

TL;DR: MemoCoder是一个多代理框架，通过协作解决问题和从过去的修复中学习，提升代码生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如微调或自我修复策略）成本高或缺乏知识积累机制，无法有效解决迭代调试和多样化问题结构的挑战。

Method: 提出MemoCoder框架，包含修复知识集和导师代理，支持协作修复和知识复用。

Result: 在多个基准测试中，MemoCoder表现优于零样本提示和自我修复策略，提升显著。

Conclusion: MemoCoder在迭代优化和知识引导的代码生成中表现出色，为AI辅助编程提供新思路。

Abstract: With the widespread adoption of Large Language Models (LLMs) such as GitHub
Copilot and ChatGPT, developers increasingly rely on AI-assisted tools to
support code generation. While LLMs can generate syntactically correct
solutions for well-structured programming tasks, they often struggle with
challenges that require iterative debugging, error handling, or adaptation to
diverse problem structures. Existing approaches such as fine-tuning or
self-repair strategies either require costly retraining or lack mechanisms to
accumulate and reuse knowledge from previous attempts.
  To address these limitations, we propose MemoCoder, a multi-agent framework
that enables collaborative problem solving and persistent learning from past
fixes. At the core of MemoCoder is a Fixing Knowledge Set, which stores
successful repairs and supports retrieval for future tasks. A central Mentor
Agent supervises the repair process by identifying recurring error patterns and
refining high-level fixing strategies, providing a novel supervisory role that
guides the self-repair loop. We evaluate MemoCoder across three public
benchmarks -- MBPP, HumanEval, and LiveCodeBench -- spanning a range of problem
complexities. Experimental results show that MemoCoder consistently outperforms
both zero-shot prompting and a Self-Repair strategy, with improvements ranging
from 3.1% to 12.1% in Pass@10 and from 1.4% to 14.5% in Pass@50, demonstrating
its effectiveness in iterative refinement and knowledge-guided code generation.

</details>


### [19] [Fine-Tuning Multilingual Language Models for Code Review: An Empirical Study on Industrial C# Projects](https://arxiv.org/abs/2507.19271)
*Igli Begolli,Meltem Aksoy,Daniel Neider*

Main category: cs.SE

TL;DR: 论文评估了单语言微调对开源语言模型在代码审查任务中的表现，发现其优于多语言基线，但人类审查员在复杂任务中仍更优。


<details>
  <summary>Details</summary>
Motivation: 代码审查对软件质量至关重要，但耗时且认知负担重，语言模型的进步为自动化审查任务提供了新途径。

Method: 对CodeReviewer、CodeLlama-7B和DeepSeek-R1-Distill三种模型在C#数据集上进行单语言微调，评估其在代码变更质量估计、审查评论生成和代码优化任务中的表现。

Result: 单语言微调提高了模型的准确性和相关性，但人类审查员在语义复杂或上下文敏感的变更中表现更优。

Conclusion: 语言对齐和任务特定适配对优化语言模型在自动化代码审查中的表现至关重要。

Abstract: Code review is essential for maintaining software quality but often
time-consuming and cognitively demanding, especially in industrial
environments. Recent advancements in language models (LMs) have opened new
avenues for automating core review tasks. This study presents the empirical
evaluation of monolingual fine-tuning on the performance of open-source LMs
across three key automated code review tasks: Code Change Quality Estimation,
Review Comment Generation, and Code Refinement. We fine-tuned three distinct
models, CodeReviewer, CodeLlama-7B, and DeepSeek-R1-Distill, on a C\# specific
dataset combining public benchmarks with industrial repositories. Our study
investigates how different configurations of programming languages and natural
languages in the training data affect LM performance, particularly in comment
generation. Additionally, we benchmark the fine-tuned models against an
automated software analysis tool (ASAT) and human reviewers to evaluate their
practical utility in real-world settings. Our results show that monolingual
fine-tuning improves model accuracy and relevance compared to multilingual
baselines. While LMs can effectively support code review workflows, especially
for routine or repetitive tasks, human reviewers remain superior in handling
semantically complex or context-sensitive changes. Our findings highlight the
importance of language alignment and task-specific adaptation in optimizing LMs
for automated code review.

</details>


### [20] [Exploring the Jupyter Ecosystem: An Empirical Study of Bugs and Vulnerabilities](https://arxiv.org/abs/2507.18833)
*Wenyuan Jiang,Diany Pressato,Harsh Darji,Thibaud Lutellier*

Main category: cs.SE

TL;DR: 该论文对Jupyter Notebook中的错误和漏洞进行了大规模实证研究，揭示了配置问题和API使用错误是常见问题，并探讨了部署框架的风险。


<details>
  <summary>Details</summary>
Motivation: 由于Jupyter Notebook的独特性，传统软件工程工具和方法无法有效分析其行为，因此需要专门研究其错误和漏洞。

Method: 通过收集和分析两大平台的Notebook数据，结合定量分析和定性研究（扎根理论），建立了错误分类法，并评估了部署框架的安全风险。

Result: 研究发现配置问题和API使用错误是Notebook中最常见的错误，同时部署框架存在安全风险。

Conclusion: Notebook的支持和维护不如传统软件，导致代码复杂、配置错误和维护不善。

Abstract: Background. Jupyter notebooks are one of the main tools used by data
scientists. Notebooks include features (configuration scripts, markdown,
images, etc.) that make them challenging to analyze compared to traditional
software. As a result, existing software engineering models, tools, and studies
do not capture the uniqueness of Notebook's behavior. Aims. This paper aims to
provide a large-scale empirical study of bugs and vulnerabilities in the
Notebook ecosystem. Method. We collected and analyzed a large dataset of
Notebooks from two major platforms. Our methodology involved quantitative
analyses of notebook characteristics (such as complexity metrics, contributor
activity, and documentation) to identify factors correlated with bugs.
Additionally, we conducted a qualitative study using grounded theory to
categorize notebook bugs, resulting in a comprehensive bug taxonomy. Finally,
we analyzed security-related commits and vulnerability reports to assess risks
associated with Notebook deployment frameworks. Results. Our findings highlight
that configuration issues are among the most common bugs in notebook documents,
followed by incorrect API usage. Finally, we explore common vulnerabilities
associated with popular deployment frameworks to better understand risks
associated with Notebook development. Conclusions. This work highlights that
notebooks are less well-supported than traditional software, resulting in more
complex code, misconfiguration, and poor maintenance.

</details>


### [21] [SLICEMATE: Accurate and Scalable Static Program Slicing via LLM-Powered Agents](https://arxiv.org/abs/2507.18957)
*Jianming Chang,Jieke Shi,Yunbo Lyu,Xin Zhou,Lulu Wang,Zhou Yang,Bixin Li,David Lo*

Main category: cs.SE

TL;DR: SliceMate是一种基于大型语言模型（LLM）的静态程序切片解决方案，通过三个专门代理（合成、验证和优化）实现高精度切片，无需显式依赖图构建。


<details>
  <summary>Details</summary>
Motivation: 传统切片工具依赖计算成本高的依赖图分析，难以扩展到大型程序或处理语法不完整代码；现有学习方法在性能上仍不及传统方法。

Method: SliceMate集成三个代理：合成代理生成候选切片，验证代理检查切片质量，优化代理修复切片。控制模块协调代理工作。

Result: 实验结果表明，SliceMate在2,200个手动标注的Java和Python程序上显著优于传统和学习方法。

Conclusion: SliceMate通过LLM代理实现了高效、高精度的静态程序切片，解决了传统方法的扩展性和鲁棒性问题。

Abstract: Static program slicing, which extracts the executable portions of a program
that affect the values at a specific location, supports many software analysis
tasks such as debugging and security auditing. However, traditional slicing
tools rely on computationally expensive reachability analysis over dependency
graphs, which struggle to scale to large programs and often fail to handle code
with incomplete syntax. Recently emerged learning-based methods, while more
robust to such cases, still fall short of achieving comparable performance to
traditional methods on well-formed code.
  In this work, we propose SliceMate, a novel static program slicing solution
powered by Large Language Model (LLM) agents. It bypasses the need for explicit
dependency graph construction and achieving superior slicing accuracy.
Concretely, SliceMate integrates three specialized agents: (1) a synthesis
agent that produces candidate slices by incrementally expanding the scan scope
across functions and files guided by LLM-inferred dependencies; (2) a
verification agent that performs conciseness and completeness checks of the
candidate slices, detecting missing or irrelevant statements; and (3) a
refinement agent that repairs the slices with minimal edits in accordance with
the verification results. These agents are orchestrated by a control module
that ensures timely convergence and outputs high-quality slices without manual
intervention. For rigorous evaluation, we construct a new and high-quality
benchmark, SliceBench, comprising 2,200 manually annotated Java and Python
programs, with program lengths ranging from 5 to 8,577 lines, significantly
larger than those in existing slicing benchmarks. Experimental results show
that SliceMate greatly outperforms both traditional and learning-based slicing
tools.

</details>


### [22] [Classifying Issues in Open-source GitHub Repositories](https://arxiv.org/abs/2507.18982)
*Amir Hossain Raaj,Fairuz Nawer Meem,Sadia Afrin Mim*

Main category: cs.SE

TL;DR: 该论文旨在利用机器学习和深度神经网络模型对GitHub开源社区中的问题进行分类，以解决标签缺失的问题，从而加快开发流程。


<details>
  <summary>Details</summary>
Motivation: GitHub上大多数开源仓库未对问题进行标签化，导致问题解决效率低下。通过自动分类问题，可以提升开发效率。

Method: 分析了GitHub上的开源仓库，使用ML和DNN模型对问题进行常见标签分类（如API、文档、Bug等）。

Result: 研究表明，DNN模型在问题分类任务中表现优于其他方法。

Conclusion: 自动分类GitHub问题标签能显著提升开发效率，DNN模型是实现这一目标的有效工具。

Abstract: GitHub is the most widely used platform for software maintenance in the
open-source community. Developers report issues on GitHub from time to time
while facing difficulties. Having labels on those issues can help developers
easily address those issues with prior knowledge of labels. However, most of
the GitHub repositories do not maintain regular labeling for the issues. The
goal of this work is to classify issues in the open-source community using ML
\& DNN models. There are thousands of open-source repositories on GitHub. Some
of the repositories label their issues properly whereas some of them do not.
When issues are pre-labeled, the problem-solving process and the immediate
assignment of corresponding personnel are facilitated for the team, thereby
expediting the development process. In this work, we conducted an analysis of
prominent GitHub open-source repositories. We classified the issues in some
common labels which are: API, Documentation, Enhancement, Question, Easy,
Help-wanted, Dependency, CI, Waiting for OP's response, Test, Bug, etc. Our
study shows that DNN models outperf

</details>


### [23] [SESR-Eval: Dataset for Evaluating LLMs in the Title-Abstract Screening of Systematic Reviews](https://arxiv.org/abs/2507.19027)
*Aleksi Huotala,Miikka Kuutila,Mika Mäntylä*

Main category: cs.SE

TL;DR: 论文创建了一个评估LLMs在系统综述标题-摘要筛选中的性能的数据集，发现LLMs表现相似，但准确性差异较大，目前不建议自动化使用。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在软件工程系统综述标题-摘要筛选中的性能，并提供是否建议使用的证据。

Method: 从169个SR研究中选择24个构建数据集，用9个LLMs进行基准测试。

Result: 构建了包含34,528个标注研究的SESR-Eval数据集，LLMs表现相似，但准确性差异大，成本较低。

Conclusion: 目前不建议自动化使用LLMs，未来将研究影响性能的因素。

Abstract: Background: The use of large language models (LLMs) in the title-abstract
screening process of systematic reviews (SRs) has shown promising results, but
suffers from limited performance evaluation. Aims: Create a benchmark dataset
to evaluate the performance of LLMs in the title-abstract screening process of
SRs. Provide evidence whether using LLMs in title-abstract screening in
software engineering is advisable. Method: We start with 169 SR research
artifacts and find 24 of those to be suitable for inclusion in the dataset.
Using the dataset we benchmark title-abstract screening using 9 LLMs. Results:
We present the SESR-Eval (Software Engineering Systematic Review Evaluation)
dataset containing 34,528 labeled primary studies, sourced from 24 secondary
studies published in software engineering (SE) journals. Most LLMs performed
similarly and the differences in screening accuracy between secondary studies
are greater than differences between LLMs. The cost of using an LLM is
relatively low - less than $40 per secondary study even for the most expensive
model. Conclusions: Our benchmark enables monitoring AI performance in the
screening task of SRs in software engineering. At present, LLMs are not yet
recommended for automating the title-abstract screening process, since accuracy
varies widely across secondary studies, and no LLM managed a high recall with
reasonable precision. In future, we plan to investigate factors that influence
LLM screening performance between studies.

</details>


### [24] [Exploring the Use of LLMs for Requirements Specification in an IT Consulting Company](https://arxiv.org/abs/2507.19113)
*Liliana Pasquale,Azzurra Ragone,Emanuele Piemontese,Armin Amiri Darban*

Main category: cs.SE

TL;DR: 论文探讨了利用大型语言模型（LLM）自动化需求规范生成的过程，结果显示LLM能减少时间和人力，但需人工修订。


<details>
  <summary>Details</summary>
Motivation: 需求规范的生成过程繁琐且耗时，知识分散于多种来源，希望通过LLM自动化这一过程。

Method: 使用LLM基于需求文档和模板生成Epic FDS和用户故事，并与人工生成结果对比。

Result: LLM能自动化并标准化需求规范，但质量依赖输入且需人工修订。

Conclusion: 建议采用LLM与人工协同的方式，LLM作为起草工具，人工提供上下文和技术监督。

Abstract: In practice, requirements specification remains a critical challenge. The
knowledge necessary to generate a specification can often be fragmented across
diverse sources (e.g., meeting minutes, emails, and high-level product
descriptions), making the process cumbersome and time-consuming. In this paper,
we report our experience using large language models (LLMs) in an IT consulting
company to automate the requirements specification process. In this company,
requirements are specified using a Functional Design Specification (FDS), a
document that outlines the functional requirements and features of a system,
application, or process. We provide LLMs with a summary of the requirements
elicitation documents and FDS templates, prompting them to generate Epic FDS
(including high-level product descriptions) and user stories, which are
subsequently compiled into a complete FDS document. We compared the correctness
and quality of the FDS generated by three state-of-the-art LLMs against those
produced by human analysts. Our results show that LLMs can help automate and
standardize the requirements specification, reducing time and human effort.
However, the quality of LLM-generated FDS highly depends on inputs and often
requires human revision. Thus, we advocate for a synergistic approach in which
an LLM serves as an effective drafting tool while human analysts provide the
critical contextual and technical oversight necessary for high-quality
requirements engineering (RE) documentation.

</details>


### [25] [Automated Code Review Using Large Language Models at Ericsson: An Experience Report](https://arxiv.org/abs/2507.19115)
*Shweta Ramesh,Joy Bose,Hamender Singh,A K Raghavan,Sujoy Roychowdhury,Giriprasad Sridhara,Nishrith Saini,Ricardo Britto*

Main category: cs.SE

TL;DR: 论文探讨了利用大型语言模型（LLM）和静态程序分析自动化代码审查的经验，旨在减轻开发者的认知负担。


<details>
  <summary>Details</summary>
Motivation: 代码审查是保证软件质量的重要手段，但依赖经验丰富的开发者且耗时。自动化代码审查可以缓解开发者的负担，使其专注于编写代码。

Method: 开发了一个轻量级工具，结合LLM和静态程序分析，用于自动化代码审查。

Result: 初步实验结果显示，该工具在经验丰富的开发者中获得了积极的评价。

Conclusion: 自动化代码审查工具具有潜力，未来可进一步优化和扩展。

Abstract: Code review is one of the primary means of assuring the quality of released
software along with testing and static analysis. However, code review requires
experienced developers who may not always have the time to perform an in-depth
review of code. Thus, automating code review can help alleviate the cognitive
burden on experienced software developers allowing them to focus on their
primary activities of writing code to add new features and fix bugs. In this
paper, we describe our experience in using Large Language Models towards
automating the code review process in Ericsson. We describe the development of
a lightweight tool using LLMs and static program analysis. We then describe our
preliminary experiments with experienced developers in evaluating our code
review tool and the encouraging results.

</details>


### [26] [Mut4All: Fuzzing Compilers via LLM-Synthesized Mutators Learned from Bug Reports](https://arxiv.org/abs/2507.19275)
*Bo Wang,Pengyang Wang,Chong Chen,Qi Sun,Jieke Shi,Chengran Yang,Ming Deng,Youfang Lin,Zhou Yang,David Lo*

Main category: cs.SE

TL;DR: Mut4All是一个自动化、语言无关的框架，利用LLM和编译器知识生成高质量变异器，显著提升模糊测试效果。


<details>
  <summary>Details</summary>
Motivation: 现代语言结构复杂（如模板、宏），现有变异器设计依赖人工，难以扩展和跨语言通用。

Method: Mut4All通过三个代理（发明、实现合成、优化）自动生成变异器，结合LLM和编译器知识。

Result: 处理1000个错误报告，生成722个变异器，成本低；发现62个Rust和34个C++编译器错误。

Conclusion: Mut4All在独特崩溃检测和覆盖率上优于现有方法，验证了其高效性和通用性。

Abstract: Mutation-based fuzzing is effective for uncovering compiler bugs, but
designing high-quality mutators for modern languages with complex constructs
(e.g., templates, macros) remains challenging. Existing methods rely heavily on
manual design or human-in-the-loop correction, limiting scalability and
cross-language generalizability.
  We present Mut4All, a fully automated, language-agnostic framework that
synthesizes mutators using Large Language Models (LLMs) and compiler-specific
knowledge from bug reports. It consists of three agents: (1) a mutator
invention agent that identifies mutation targets and generates mutator metadata
using compiler-related insights; (2) a mutator implementation synthesis agent,
fine-tuned to produce initial implementations; and (3) a mutator refinement
agent that verifies and corrects the mutators via unit-test feedback.
  Mut4All processes 1000 bug reports (500 Rust, 500 C++), yielding 319 Rust and
403 C++ mutators at ~$0.08 each via GPT-4o. Our customized fuzzer, using these
mutators, finds 62 bugs in Rust compilers (38 new, 7 fixed) and 34 bugs in C++
compilers (16 new, 1 fixed). Mut4All outperforms existing methods in both
unique crash detection and coverage, ranking first on Rust and second on C++.

</details>


### [27] [ReCatcher: Towards LLMs Regression Testing for Code Generation](https://arxiv.org/abs/2507.19390)
*Altaf Allah Abbassi,Leuson Da Silva,Amin Nikanjam,Foutse Khomh*

Main category: cs.SE

TL;DR: ReCatcher是一个用于Python代码生成的回归测试框架，通过比较逻辑正确性、代码质量和执行性能来评估LLM更新中的回归问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的快速更新（如微调、合并或新模型发布），可能引入回归问题，影响代码的正确性、质量和性能。

Method: ReCatcher系统比较两个LLM（当前模型和候选更新）在逻辑正确性、静态代码质量和执行性能三个维度上的表现。

Result: 评估显示，微调、合并和新模型发布均可能导致不同程度的回归问题，如语法错误增加、正确性下降或性能退化。

Conclusion: ReCatcher在逻辑和性能方面表现优于基线方法，强调了系统回归评估的重要性，帮助用户做出更明智的更新决策。

Abstract: Large Language Models (LLMs) for code generation evolve rapidly through
fine-tuning, merging, or new model releases. However, such updates can
introduce regressions, not only in correctness but also in code quality and
performance. To address this, we present ReCatcher, a regression testing
framework for Python code generation. ReCatcher systematically compares two
LLMs, typically a current model and a candidate update, across three
dimensions: logical correctness, static code quality, and execution
performance. We apply ReCatcher to assess regressions across three update
scenarios, fine-tuning, merging, and model release, using CodeLlama,
DeepSeek-Coder, and GPT-4o. Our evaluation shows that fine-tuning with
cross-language datasets increases syntax errors by up to 12%. Merging with
general-purpose models like Llama2 leads to regressions in correctness by up to
18%. GPT-4o introduces regressions of up to 50% in handling missing imports
compared to GPT-3.5-turbo, while GPT-4o-mini suffers up to 80% performance
degradation in execution time versus GPT-4o. Overall, logical correctness,
performance, and error handling (e.g., syntax errors and missing imports) are
the most regression-prone areas. Comparing ReCatcher with baseline solutions,
it presents better and consistent accuracy across logical and performance
aspects. ReCatcher highlights the importance of systematic regression
evaluation before adopting new models, while assisting researchers and
practitioners in making more informed update decisions.

</details>


### [28] [SDVDiag: A Modular Platform for the Diagnosis of Connected Vehicle Functions](https://arxiv.org/abs/2507.19403)
*Matthias Weiß,Falk Dettinger,Michael Weyrich*

Main category: cs.SE

TL;DR: SDVDiag是一个可扩展平台，用于自动诊断联网车辆功能，通过动态依赖图和异常监控快速定位故障根源。


<details>
  <summary>Details</summary>
Motivation: 联网车辆的高可靠性和可用性要求需要快速解决故障，但复杂的云/边缘架构和依赖关系使手动分析不可行。

Method: SDVDiag平台支持从数据收集到根因追踪的自动化管道，动态更新依赖图并监控异常，通过图遍历生成最可能原因的排名。

Result: 在5G测试车队环境中，SDVDiag可靠检测注入的故障，有望减少停机时间并提前发现问题。

Conclusion: SDVDiag为联网车辆故障诊断提供了高效自动化解决方案，显著提升系统可靠性和维护效率。

Abstract: Connected and software-defined vehicles promise to offer a broad range of
services and advanced functions to customers, aiming to increase passenger
comfort and support autonomous driving capabilities. Due to the high
reliability and availability requirements of connected vehicles, it is crucial
to resolve any occurring failures quickly. To achieve this however, a complex
cloud/edge architecture with a mesh of dependencies must be navigated to
diagnose the responsible root cause. As such, manual analyses become unfeasible
since they would significantly delay the troubleshooting.
  To address this challenge, this paper presents SDVDiag, an extensible
platform for the automated diagnosis of connected vehicle functions. The
platform enables the creation of pipelines that cover all steps from initial
data collection to the tracing of potential root causes. In addition, SDVDiag
supports self-adaptive behavior by the ability to exchange modules at runtime.
Dependencies between functions are detected and continuously updated, resulting
in a dynamic graph view of the system. In addition, vital system metrics are
monitored for anomalies. Whenever an incident is investigated, a snapshot of
the graph is taken and augmented by relevant anomalies. Finally, the analysis
is performed by traversing the graph and creating a ranking of the most likely
causes.
  To evaluate the platform, it is deployed inside an 5G test fleet environment
for connected vehicle functions. The results show that injected faults can be
detected reliably. As such, the platform offers the potential to gain new
insights and reduce downtime by identifying problems and their causes at an
early stage.

</details>


### [29] [Resolving Build Conflicts via Example-Based and Rule-Based Program Transformations](https://arxiv.org/abs/2507.19432)
*Sheikh Shadab Towqir,Fei He,Todd Mytkowicz,Na Meng*

Main category: cs.SE

TL;DR: BUCOR是一种新的构建冲突解决工具，结合了基于示例和基于规则的策略，有效解决了合并冲突问题。


<details>
  <summary>Details</summary>
Motivation: 合并冲突（包括文本冲突和构建/测试冲突）会降低软件质量和开发效率，现有工具对此支持不足。

Method: BUCOR通过比较基础版本、左分支和右分支检测冲突，并采用基于示例（BUCOR-E）和基于规则（BUCOR-R）的两种策略解决冲突。

Result: 在88个真实构建冲突中，BUCOR为65个案例生成至少一个解决方案，并正确解决了43个冲突。

Conclusion: 混合方法（上下文感知的示例学习和结构化规则）能有效解决冲突，为未来更智能的合并工具提供了方向。

Abstract: Merge conflicts often arise when developers integrate changes from different
software branches. The conflicts can result from overlapping edits in programs
(i.e., textual conflicts) or cause build and test errors (i.e., build and test
conflicts). They degrade software quality and hinder programmer productivity.
While several tools detect build conflicts, few offer meaningful support for
resolving cases like those caused by method removal. To overcome limitations of
existing tools, we introduce BUCOR (Build Conflict Resolver), a new conflict
resolver. BUCOR first detects conflicts by comparing three versions related to
a merging scenario: base b, left l, and right r. To resolve conflicts, it
employs two complementary strategies: example-based transformation (BUCOR-E)
and rule-based transformation (BUCOR-R). BUCOR-R applies predefined rules to
handle common, well-understood conflicts. BUCOR-E mines branch versions (l and
r) for exemplar edits applied to fix related build errors. From these examples,
it infers and generalizes program transformation patterns to resolve more
complex conflicts.
  We evaluated BUCOR on 88 real-world build conflicts spanning 21 distinct
conflict types. BUCOR generated at least one solution for 65 cases and
correctly resolved 43 conflicts. We observed that this hybrid
approach--combining context-aware, example-based learning with structured,
rule-based resolution--can effectively help resolve conflicts. Our research
sheds light on future directions for more intelligent and automated merge
tools.

</details>


### [30] [An OpenSource CI/CD Pipeline for Variant-Rich Software-Defined Vehicles](https://arxiv.org/abs/2507.19446)
*Matthias Weiß,Anish Navalgund,Johannes Stümpfle,Falk Dettinger,Michael Weyrich*

Main category: cs.SE

TL;DR: 本文提出了一种针对软件定义车辆（SDV）的开源CI/CD流水线，支持自动化构建、测试和部署，并通过定制OTA中间件管理软件更新和回滚，验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: SDV的软件版本和变体因车辆、云/边缘环境和利益相关者的多样性而不断增加，缺乏统一的集成环境，需要动态编排功能以确保异构系统的可靠运行。

Method: 采用容器化开源工具构建标准化、可移植和可扩展的CI/CD流水线，支持AI模型的持续开发和部署，并通过定制OTA中间件分发更新和回滚。

Result: 在自动代客泊车（AVP）场景中验证了流水线的有效性，实现了无缝OTA更新、正确的变体选择和跨目标编排。

Conclusion: 该流水线为SDV的软件变体和OTA更新提供了可扩展且高效的解决方案，推动了未来移动技术的发展。

Abstract: Software-defined vehicles (SDVs) offer a wide range of connected
functionalities, including enhanced driving behavior and fleet management.
These features are continuously updated via over-the-air (OTA) mechanisms,
resulting in a growing number of software versions and variants due to the
diversity of vehicles, cloud/edge environments, and stakeholders involved. The
lack of a unified integration environment further complicates development, as
connected mobility solutions are often built in isolation. To ensure reliable
operations across heterogeneous systems, a dynamic orchestration of functions
that considers hardware and software variability is essential. This paper
presents an open-source CI/CD pipeline tailored for SDVs. It automates the
build, test, and deployment phases using a combination of containerized
open-source tools, creating a standardized, portable, and scalable ecosystem
accessible to all stakeholders. Additionally, a custom OTA middleware
distributes software updates and supports rollbacks across vehicles and backend
services. Update variants are derived based on deployment target dependencies
and hardware configurations. The pipeline also supports continuous development
and deployment of AI models for autonomous driving features. Its effectiveness
is evaluated using an automated valet parking (AVP) scenario involving
TurtleBots and a coordinating backend server. Two object detection variants are
developed and deployed to match hardware-specific requirements. Results
demonstrate seamless OTA updates, correct variant selection, and successful
orchestration across all targets. Overall, the proposed pipeline provides a
scalable and efficient solution for managing software variants and OTA updates
in SDVs, contributing to the advancement of future mobility technologies.

</details>
