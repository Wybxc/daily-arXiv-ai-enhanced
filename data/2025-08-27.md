<div id=toc></div>

# Table of Contents

- [cs.LO](#cs.LO) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 12]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [1] [Decidability of Extensions of Presburger Arithmetic by Hardy Field Functions](https://arxiv.org/abs/2508.19206)
*Hera Brown,Jakub Konieczny*

Main category: cs.LO

TL;DR: 该论文研究了Presburger算术通过亚多项式Hardy场函数的扩展，证明了大多数此类扩展是不可判定的。


<details>
  <summary>Details</summary>
Motivation: 研究Presburger算术在添加Hardy场函数和最近整数算子后的可判定性，探索数学逻辑中理论可判定性的边界。

Method: 通过分析不同增长率的Hardy场函数（多项式增长和亚线性但多项式速度增长），结合最近整数算子，研究相应理论的可判定性。

Result: 当f以多项式速度增长时，理论Th(ℤ; <, +, ⌊f⌉)是不可判定的；当f亚线性但以多项式速度增长时，该理论也是不可判定的。

Conclusion: 大多数包含Hardy场函数和最近整数算子的Presburger算术扩展都是不可判定的，这为理解此类数学理论的可判定性界限提供了重要结果。

Abstract: We study the extension of Presburger arithmetic by the class of
sub-polynomial Hardy field functions, and show the majority of these extensions
to be undecidable. More precisely, we show that the theory
$\mathrm{Th}(\mathbb{Z}; <, +, \lfloor f \rceil)$, where $f$ is a Hardy field
function and $\lfloor \cdot \rceil$ the nearest integer operator, is
undecidable when $f$ grows polynomially faster than $x$. Further, we show that
when $f$ grows sub-linearly quickly, but still as fast as some polynomial, the
theory $\mathrm{Th}(\mathbb{Z}; <, +, \lfloor f \rceil)$ is undecidable.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [2] [CASP: An evaluation dataset for formal verification of C code](https://arxiv.org/abs/2508.18798)
*Niclas Hertzberg,Merlijn Sevenhuijsen,Liv Kåreborn,Anna Lokrantz*

Main category: cs.FL

TL;DR: 创建了一个经过严格验证的C代码-ACSL形式规范对数据集，用于基于LLM的代码生成验证研究的基准测试


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在代码生成方面显示出潜力，但生成的程序缺乏严格的正确性保证，而形式验证又需要专业知识且耗时耗力

Method: 通过多阶段筛选过程从The Stack 1和The Stack 2中提取506对C代码和ACSL形式规范对，包括识别带形式语言注释的C文件、确保验证通过、使用LLM改进未通过验证的文件、使用Frama-C进行形式验证，以及手动审查确认每个对的正确性

Result: 创建了包含506对C代码-ACSL规范对的CASP数据集，每个规范-实现对都经过了形式验证和手动审查

Conclusion: 该数据集为基于LLM的自动化代码生成与验证正确性的研究提供了系统性的基准测试基础

Abstract: Recent developments in Large Language Models (LLMs) have shown promise in
automating code generation, yet the generated programs lack rigorous
correctness guarantees. Formal verification can address this shortcoming, but
requires expertise and is time-consuming to apply. Currently, there is no
dataset of verified C code paired with formal specifications that enables
systematic benchmarking in this space. To fill this gap, we present a curated
evaluation dataset of C code paired with formal specifications written in
ANSI/ISO C Specification Language (ACSL). We develop a multi-stage filtering
process to carefully extract 506 pairs of C code and formal specifications from
The Stack 1 and The Stack 2. We first identify C files annotated with formal
languages. Then, we ensure that the annotated C files formally verify, and
employ LLMs to improve non-verifying files. Furthermore, we post-process the
remaining files into pairs of C code and ACSL specifications, where each
specification-implementation pair is formally verified using Frama-C. To ensure
the quality of the pairs, a manual inspection is conducted to confirm the
correctness of every pair. The resulting dataset of C-ACSL specification pairs
(CASP) provides a foundation for benchmarking and further research on
integrating automated code generation with verified correctness.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [Training Language Model Agents to Find Vulnerabilities with CTF-Dojo](https://arxiv.org/abs/2508.18370)
*Terry Yue Zhuo,Dingmin Wang,Hantian Ding,Varun Kumar,Zijian Wang*

Main category: cs.SE

TL;DR: 提出了CTF-Dojo，首个大规模可执行运行时环境，用于训练LLM并获取可验证反馈，包含658个CTF挑战。通过自动化流水线CTF-Forge快速生成环境，仅用486条高质量轨迹训练就实现了11.6%的性能提升，32B模型达到31.9% Pass@1的新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有可执行运行时环境稀缺，限制了ML代理的训练进展。需要可扩展、通用化的执行环境来训练更强大的ML代理。

Method: 开发CTF-Dojo大规模可执行运行时环境，包含658个Docker容器化的CTF挑战；创建CTF-Forge自动化流水线，将公开资源快速转换为可用执行环境；使用执行验证的高质量轨迹训练LLM代理。

Result: 仅用486条高质量轨迹训练就实现了最高11.6%的绝对性能提升；32B模型达到31.9% Pass@1，创造了新的开源权重SOTA，媲美DeepSeek-V3-0324和Gemini-2.5-Flash等前沿模型。

Conclusion: CTF风格任务可作为可执行代理学习的基准，执行基础训练信号不仅有效而且对推进高性能ML代理至关重要，无需依赖昂贵的专有系统。

Abstract: Large language models (LLMs) have demonstrated exceptional capabilities when
trained within executable runtime environments, notably excelling at software
engineering tasks through verified feedback loops. Yet, scalable and
generalizable execution-grounded environments remain scarce, limiting progress
in training more capable ML agents. We introduce CTF-Dojo, the first
large-scale executable runtime tailored for training LLMs with verifiable
feedback, featuring 658 fully functional Capture-The-Flag (CTF)-style
challenges containerized in Docker with guaranteed reproducibility. To enable
rapid scaling without manual intervention, we develop CTF-Forge, an automated
pipeline that transforms publicly available artifacts into ready-to-use
execution environments in minutes, eliminating weeks of expert configuration
traditionally required. We trained LLM-based agents on just 486 high-quality,
execution-verified trajectories from CTF-Dojo, achieving up to 11.6% absolute
gains over strong baselines across three competitive benchmarks: InterCode-CTF,
NYU CTF Bench, and Cybench. Our best-performing 32B model reaches 31.9% Pass@1,
establishing a new open-weight state-of-the-art that rivals frontier models
like DeepSeek-V3-0324 and Gemini-2.5-Flash. By framing CTF-style tasks as a
benchmark for executable-agent learning, CTF-Dojo demonstrates that
execution-grounded training signals are not only effective but pivotal in
advancing high-performance ML agents without dependence on costly proprietary
systems.

</details>


### [4] [DTInsight: A Tool for Explicit, Interactive, and Continuous Digital Twin Reporting](https://arxiv.org/abs/2508.18431)
*Kérian Fiter,Louis Malassigné-Onfroy,Bentley Oakes*

Main category: cs.SE

TL;DR: DTInsight是一个自动化工具，用于生成数字孪生(DT)的持续报告，提供架构可视化、特性摘要和CI/CD集成功能


<details>
  <summary>Details</summary>
Motivation: 随着数字孪生系统的构建和演进，利益相关者需要工具来随时了解系统的当前特性和概念架构

Method: 基于DT描述框架(DTDF)的建模描述，提供交互式架构可视化、基于本体数据的特性摘要生成，以及CI/CD流水线集成

Result: 能够生成最新且详细的报告，增强利益相关者对数字孪生系统的理解

Conclusion: DTInsight为数字孪生系统提供了系统化和自动化的持续报告解决方案

Abstract: With Digital Twin (DT) construction and evolution occurring over time,
stakeholders require tools to understand the current characteristics and
conceptual architecture of the system at any time. We introduce DTInsight, a
systematic and automated tool and methodology for producing continuous
reporting for DTs. DTInsight offers three key features: (a) an interactive
conceptual architecture visualization of DTs; (b) generation of summaries of DT
characteristics based on ontological data; and (c) integration of these outputs
into a reporting page within a continuous integration and continuous deployment
(CI/CD) pipeline. Given a modeled description of the DT aligning to our DT
Description Framework (DTDF), DTInsight enables up-to-date and detailed reports
for enhanced stakeholder understanding.

</details>


### [5] [Engineering a Digital Twin for the Monitoring and Control of Beer Fermentation Sampling](https://arxiv.org/abs/2508.18452)
*Pierre-Emmanuel Goffi,Raphaël Tremblay,Bentley Oakes*

Main category: cs.SE

TL;DR: 本文提供了一个安全关键性酒粉酵过程数字双胞体开发的实践经验，通过三阶段工程方法实现了从被动监控到双向控到的转变，减少了91%的手动采样时间。


<details>
  <summary>Details</summary>
Motivation: 工业数字双胞体开发复杂，特别是需要实现超越被动监控的服务功能时。本文针对安全关键性应用场景，提供可执行的双向控到数字双胞体开发指南。

Method: 采用三阶段工程方法，将被动监控系统转换为具有实时控到能力的Type 2数字双胞体。包含多层安全协议、Arduino控到器与Unity可视化的硬件-软件集成策略、实时同步解决方案，以及使用星座报告框架促进跨领域协作。

Result: 实现了在7射压压力系统上运行的实时控到能力，将手动采样时间减少91%，并提供了持续的采样监测服务。

Conclusion: 安全优先设计、模拟驱动开发和渐进式实施策略是关键成功因素。本研究为实践者开发安全关键性应用中的双向控到数字双胞体提供了可操作的指南。

Abstract: Successfully engineering interactive industrial DTs is a complex task,
especially when implementing services beyond passive monitoring. We present
here an experience report on engineering a safety-critical digital twin (DT)
for beer fermentation monitoring, which provides continual sampling and reduces
manual sampling time by 91%. We document our systematic methodology and
practical solutions for implementing bidirectional DTs in industrial
environments. This includes our three-phase engineering approach that
transforms a passive monitoring system into an interactive Type 2 DT with
real-time control capabilities for pressurized systems operating at seven bar.
We contribute details of multi-layered safety protocols, hardware-software
integration strategies across Arduino controllers and Unity visualization, and
real-time synchronization solutions. We document specific engineering
challenges and solutions spanning interdisciplinary integration, demonstrating
how our use of the constellation reporting framework facilitates cross-domain
collaboration. Key findings include the critical importance of safety-first
design, simulation-driven development, and progressive implementation
strategies. Our work thus provides actionable guidance for practitioners
developing DTs requiring bidirectional control in safety-critical applications.

</details>


### [6] [How do Humans and LLMs Process Confusing Code?](https://arxiv.org/abs/2508.18547)
*Youssef Abdelsalam,Norman Peitek,Anna-Maria Maurer,Mariya Toneva,Sven Apel*

Main category: cs.SE

TL;DR: 研究发现LLM困惑度峰值与人类脑电生理反应在位置和幅度上相关，表明LLM和人类对代码的困惑模式相似，基于此开发了识别代码困惑区域的方法


<details>
  <summary>Details</summary>
Motivation: 研究LLM与人类程序员在代码理解上的对齐程度，避免因理解差异导致的误解、低效和代码质量问题

Method: 通过比较LLM困惑度和人类脑电生理反应（EEG-based fixation-related potentials），分析清洁代码和困惑代码的理解差异

Result: 发现LLM困惑度峰值与人类神经生理反应在位置和幅度上存在相关性，表明两者对代码的困惑模式相似

Conclusion: LLM和人类对代码的困惑具有相似性，基于此可以开发数据驱动的LLM方法来识别导致人类程序员困惑的代码区域

Abstract: Already today, humans and programming assistants based on large language
models (LLMs) collaborate in everyday programming tasks. Clearly, a
misalignment between how LLMs and programmers comprehend code can lead to
misunderstandings, inefficiencies, low code quality, and bugs.
  A key question in this space is whether humans and LLMs are confused by the
same kind of code. This would not only guide our choices of integrating LLMs in
software engineering workflows, but also inform about possible improvements of
LLMs.
  To this end, we conducted an empirical study comparing an LLM to human
programmers comprehending clean and confusing code. We operationalized
comprehension for the LLM by using LLM perplexity, and for human programmers
using neurophysiological responses (in particular, EEG-based fixation-related
potentials).
  We found that LLM perplexity spikes correlate both in terms of location and
amplitude with human neurophysiological responses that indicate confusion. This
result suggests that LLMs and humans are similarly confused about the code.
Based on these findings, we devised a data-driven, LLM-based approach to
identify regions of confusion in code that elicit confusion in human
programmers.

</details>


### [7] [LaQual: A Novel Framework for Automated Evaluation of LLM App Quality](https://arxiv.org/abs/2508.18636)
*Yan Wang,Xinyi Hou,Yanjie Zhao,Weiguo Lin,Haoyu Wang,Junjun Si*

Main category: cs.SE

TL;DR: LaQual是一个自动化评估LLM应用质量的框架，通过分层分类、静态指标筛选和动态场景自适应评估三阶段方法，能有效识别高质量应用，实验显示与人工评估高度一致且显著提升用户体验。


<details>
  <summary>Details</summary>
Motivation: 当前LLM应用商店主要依赖用户活跃度和收藏等静态指标进行排名推荐，难以帮助用户高效找到高质量应用，需要更有效的质量评估方法。

Method: 三阶段框架：1)分层分类LLM应用以匹配不同场景；2)使用时序加权用户参与度和功能能力指标等静态指标筛选低质量应用；3)进行动态场景自适应评估，由LLM生成场景特定的评估指标、评分规则和任务。

Result: 在流行LLM应用商店上的实验显示：自动化评分与人工判断高度一致（法律咨询Spearman's rho=0.62，旅行规划rho=0.60）；能减少66.7%-81.3%的候选应用；用户研究表明在决策信心、比较效率（5.45 vs 3.30）和评估报告价值感知（4.75 vs 2.25）方面显著优于基线系统。

Conclusion: LaQual提供了一个可扩展、客观且以用户为中心的解决方案，能够有效发现和推荐现实使用场景中的高质量LLM应用。

Abstract: LLM app stores are quickly emerging as platforms that gather a wide range of
intelligent applications based on LLMs, giving users many choices for content
creation, coding support, education, and more. However, the current methods for
ranking and recommending apps in these stores mostly rely on static metrics
like user activity and favorites, which makes it hard for users to efficiently
find high-quality apps. To address these challenges, we propose LaQual, an
automated framework for evaluating the quality of LLM apps. LaQual consists of
three main stages: first, it labels and classifies LLM apps in a hierarchical
way to accurately match them to different scenarios; second, it uses static
indicators, such as time-weighted user engagement and functional capability
metrics, to filter out low-quality apps; and third, it conducts a dynamic,
scenario-adaptive evaluation, where the LLM itself generates scenario-specific
evaluation metrics, scoring rules, and tasks for a thorough quality assessment.
Experiments on a popular LLM app store show that LaQual is effective. Its
automated scores are highly consistent with human judgments (with Spearman's
rho of 0.62 and p=0.006 in legal consulting, and rho of 0.60 and p=0.009 in
travel planning). By effectively screening, LaQual can reduce the pool of
candidate LLM apps by 66.7% to 81.3%. User studies further confirm that LaQual
significantly outperforms baseline systems in decision confidence, comparison
efficiency (with average scores of 5.45 compared to 3.30), and the perceived
value of its evaluation reports (4.75 versus 2.25). Overall, these results
demonstrate that LaQual offers a scalable, objective, and user-centered
solution for finding and recommending high-quality LLM apps in real-world use
cases.

</details>


### [8] [Requirements Development and Formalization for Reliable Code Generation: A Multi-Agent Vision](https://arxiv.org/abs/2508.18675)
*Xu Lu,Weisong Sun,Yiran Zhang,Ming Hu,Cong Tian,Zhi Jin,Yang Liu*

Main category: cs.SE

TL;DR: 提出了ReDeFo多智能体框架，通过需求开发和形式化方法实现可靠的代码生成，使用形式化规范连接自然语言需求和精确代码


<details>
  <summary>Details</summary>
Motivation: 现有仅依赖大语言模型的代码生成方法在代码质量上不足，缺乏系统性的需求开发和建模策略，无法保证满足实际需求

Method: 基于需求开发和形式化的多智能体框架，包含三个增强了形式化方法知识和技术的智能体，使用形式化规范作为自然语言需求与可执行代码之间的桥梁

Result: 框架能够进行严格的正确性推理，发现隐藏错误，并在整个开发过程中强制执行关键属性

Conclusion: ReDeFo框架朝着实现可靠、自动生成软件的长期愿景迈出了有希望的一步

Abstract: Automated code generation has long been considered the holy grail of software
engineering. The emergence of Large Language Models (LLMs) has catalyzed a
revolutionary breakthrough in this area. However, existing methods that only
rely on LLMs remain inadequate in the quality of generated code, offering no
guarantees of satisfying practical requirements. They lack a systematic
strategy for requirements development and modeling. Recently, LLM-based agents
typically possess powerful abilities and play an essential role in facilitating
the alignment of LLM outputs with user requirements. In this paper, we envision
the first multi-agent framework for reliable code generation based on
\textsc{re}quirements \textsc{de}velopment and \textsc{fo}rmalization, named
\textsc{ReDeFo}. This framework incorporates three agents, highlighting their
augmentation with knowledge and techniques of formal methods, into the
requirements-to-code generation pipeline to strengthen quality assurance. The
core of \textsc{ReDeFo} is the use of formal specifications to bridge the gap
between potentially ambiguous natural language requirements and precise
executable code. \textsc{ReDeFo} enables rigorous reasoning about correctness,
uncovering hidden bugs, and enforcing critical properties throughout the
development process. In general, our framework aims to take a promising step
toward realizing the long-standing vision of reliable, auto-generated software.

</details>


### [9] [LLM as an Execution Estimator: Recovering Missing Dependency for Practical Time-travelling Debugging](https://arxiv.org/abs/2508.18721)
*Yunrui Pei,Hongshu Wang,Wenjie Zhang,Yun Lin,Weiyu Kong,Jin song Dong*

Main category: cs.SE

TL;DR: RecovSlicing是一种使用部分插桩和LLM推理来在单次运行中计算动态数据依赖的新方法，解决了传统方法的高成本和不可行性问题


<details>
  <summary>Details</summary>
Motivation: 传统动态数据依赖分析方法需要完全插桩或程序重运行，成本高昂且对非确定性程序不可行，特别是在库函数调用和非确定性场景下

Method: 利用LLM从部分记录的执行轨迹和代码上下文中推断程序行为，恢复缺失的执行信息来估计运行时变量定义，支持显式和隐式变量

Result: 在三个切片基准测试的8300个数据依赖上，准确率达到80.3%、91.1%和98.3%，显著优于最佳基线方法（39.0%、82.0%、59.9%）

Conclusion: RecovSlicing在单次运行中有效解决了动态数据依赖分析问题，性能显著优于现有方法，在回归错误定位中可多发现16%的回归问题

Abstract: Dynamic data dependency, answering "why a variable has this value?", is
critical for debugging. Given a program step `s` reading a variable `v`,
finding the dynamic definition of `v` is challenging. Traditional methods
require either (1) exhaustive instrumentation of all possible definitions of
`v` in one run or (2) replicating the run to re-examine reads/writes - both
costly. If `v` is defined in a library, instrumentation becomes expensive; for
non-deterministic programs, replication is infeasible.
  We propose RecovSlicing, which computes dynamic data dependency in a single
run with partial instrumentation. We leverage LLMs to infer program behavior
from a partially recorded trace and code context. Given a trace and a slicing
criterion (step `s` and variable `v`), RecovSlicing estimates the runtime
definition of `v` by recovering the missing execution.It also supports implicit
variables, such as those in `list.get(i)`. Technically, RecovSlicing tackles:
(1) recovering runtime values and structures, and (2) aligning recovered
variables with recorded memory to analyze definitions.
  We evaluate RecovSlicing on 8,300 data dependencies across three slicing
benchmarks, comparing it with Slicer4J, ND-Slicer, LLM Slicer, and re-execution
Slicer. RecovSlicing achieves accuracy of 80.3%, 91.1%, and 98.3%,
outperforming the best baseline (39.0%, 82.0%, 59.9%), and also leads in recall
(91.1%, 91.1%, 98.3% vs. 53.4%, 79.1%, 87.1%). Integrated into a regression bug
localizer, it enables finding 16% more regressions.

</details>


### [10] [Does AI Code Review Lead to Code Changes? A Case Study of GitHub Actions](https://arxiv.org/abs/2508.18771)
*Kexin Sun,Hongyu Kuang,Sebastian Baltes,Xin Zhou,He Zhang,Xiaoxing Ma,Guoping Rong,Dong Shao,Christoph Treude*

Main category: cs.SE

TL;DR: 研究分析16款AI代码审查工具在GitHub中的影响，发现简洁、包含代码片段、手动触发的审查评论更容易被采纳


<details>
  <summary>Details</summary>
Motivation: 虽然AI基础的代码审查工具逐渐普及，但它们的实际效果和影响因素仍不明确

Method: 对178个仓库中22,000多条审查评论进行大规模实证研究，使用两阶段LLM辅助框架分析评论是否被处理，采用可解释的机器学习识别影响因素

Result: 采用率在增长但效果差异较大，简洁、包含代码片段、手动触发的审查评论更容易导致代码变更，尤其是hunk-level审查工具

Conclusion: 细心的工具设计对于AI代码审查系统的效果至关重要，研究结果为改进AI基础代码审查系统提供了方向

Abstract: AI-based code review tools automatically review and comment on pull requests
to improve code quality. Despite their growing presence, little is known about
their actual impact. We present a large-scale empirical study of 16 popular
AI-based code review actions for GitHub workflows, analyzing more than 22,000
review comments in 178 repositories. We investigate (1) how these tools are
adopted and configured, (2) whether their comments lead to code changes, and
(3) which factors influence their effectiveness. We develop a two-stage
LLM-assisted framework to determine whether review comments are addressed, and
use interpretable machine learning to identify influencing factors. Our
findings show that, while adoption is growing, effectiveness varies widely.
Comments that are concise, contain code snippets, and are manually triggered,
particularly those from hunk-level review tools, are more likely to result in
code changes. These results highlight the importance of careful tool design and
suggest directions for improving AI-based code review systems.

</details>


### [11] [Dealing with SonarQube Cloud: Initial Results from a Mining Software Repository Study](https://arxiv.org/abs/2508.18816)
*Sabato Nocera,Davide Fucci,Giuseppe Scanniello*

Main category: cs.SE

TL;DR: 研究通过挖掘GitHub项目了解SonarQube Cloud静态代码分析工具的使用和自定义情况，发现许项目依赖预定义配置，但也有许多项目根据特定质量目标进行自定义


<details>
  <summary>Details</summary>
Motivation: 静态代码分析工具广泛使用但很少知道开源项目如何使用和自定义这些工具，特别是SonarQube Cloud这种流行工具

Method: 通过挖掘GitHub项目进行研究，分析321个使用SonarQube Cloud的GitHub项目，分析它们如何连接和自定义质量门配置

Result: 81%项目正确连接SonarQube Cloud，75%使用组织默认质量门，55%使用内置质量门，45%自定义质量门条件。最常见的质量条件包括安全、可维护性、可靠性、覆盖率和代码重复

Conclusion: 许项目依赖预定义配置，但也有许多项目根据特定质量目标进行自定义。未来研究可以将质量门配置与实际软件效果相关联，为各种场景下配置SCA工具提供基于证据的建议

Abstract: Background: Static Code Analysis (SCA) tools are widely adopted to enforce
code quality standards. However, little is known about how open-source projects
use and customize these tools. Aims: This paper investigates how GitHub
projects use and customize a popular SCA tool, namely SonarQube Cloud. Method:
We conducted a mining study of GitHub projects that are linked through GitHub
Actions to SonarQube Cloud projects. Results: Among 321 GitHub projects using
SonarQube Cloud, 81% of them are correctly connected to SonarQube Cloud
projects, while others exhibit misconfigurations or restricted access. Among
265 accessible SonarQube Cloud projects, 75% use the organization's default
quality gate, i.e., a set of conditions that deployed source code must meet to
pass automated checks. While 55% of the projects use the built-in quality gate
provided by SonarQube Cloud, 45% of them customize their quality gate with
different conditions. Overall, the most common quality conditions align with
SonarQube Cloud's "Clean as You Code" principle and enforce security,
maintainability, reliability, coverage, and a few duplicates on newly added or
modified source code. Conclusions: Many projects rely on predefined
configurations, yet a significant portion customize their configurations to
meet specific quality goals. Building on our initial results, we envision a
future research agenda linking quality gate configurations to actual software
outcomes (e.g., improvement of software security). This would enable
evidence-based recommendations for configuring SCA tools like SonarQube Cloud
in various contexts.

</details>


### [12] [Interleaving Large Language Models for Compiler Testing](https://arxiv.org/abs/2508.18955)
*Yunbo Ni,Shaohua Li*

Main category: cs.SE

TL;DR: 提出LegoFuzz框架，通过离线生成代码片段和在线组合的方式，高效测试C编译器，发现GCC和LLVM中66个bug


<details>
  <summary>Details</summary>
Motivation: 当前基于AI的编译器测试方法存在两个问题：生成的测试程序过于简单，且LLM测试计算成本高昂

Method: 将测试过程解耦为两个阶段：离线阶段使用LLM生成小型但功能丰富的代码片段；在线阶段策略性地组合这些片段构建高质量测试程序

Result: 在GCC和LLVM中发现66个bug，其中近一半是现有LLM工具无法发现的严重错误编译bug

Conclusion: 这种高效设计为在软件测试中超越C编译器使用AI模型开辟了新可能性

Abstract: Testing compilers with AI models, especially large language models (LLMs),
has shown great promise. However, current approaches struggle with two key
problems: The generated programs for testing compilers are often too simple,
and extensive testing with the LLMs is computationally expensive. In this
paper, we propose a novel compiler testing framework that decouples the testing
process into two distinct phases: an offline phase and an online phase. In the
offline phase, we use LLMs to generate a collection of small but feature-rich
code pieces. In the online phase, we reuse these code pieces by strategically
combining them to build high-quality and valid test programs, which are then
used to test compilers.
  We implement this idea in a tool, LegoFuzz, for testing C compilers. The
results are striking: we found 66 bugs in GCC and LLVM, the most widely used C
compilers. Almost half of the bugs are miscompilation bugs, which are serious
and hard-to-find bugs that none of the existing LLM-based tools could find. We
believe this efficient design opens up new possibilities for using AI models in
software testing beyond just C compilers.

</details>


### [13] [GitTaskBench: A Benchmark for Code Agents Solving Real-World Tasks Through Code Repository Leveraging](https://arxiv.org/abs/2508.18993)
*Ziyi Ni,Huacan Wang,Shuo Zhang,Shuo Lu,Ziyang He,Wang You,Zhenheng Tang,Yuntao Du,Bill Sun,Hongzhang Liu,Sen Hu,Ronghao Chen,Bo Li,Xin Li,Chen Hu,Binxing Jiao,Daxin Jiang,Pin Lyu*

Main category: cs.SE

TL;DR: GitTaskBench是一个新的基准测试，用于评估代码代理在真实GitHub仓库工作流中的任务解决能力，包含54个跨7个领域和模式的任务，并提出了经济价值指标来衡量代理性能。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试很少评估代码代理在真实工作流驱动的GitHub仓库场景中的能力，需要填补这一空白。

Method: 设计了54个现实任务，每个任务配对相关仓库和自动化评估工具，提出alpha-value经济价值指标，整合任务成功率、代币成本和开发者薪资。

Result: 实验显示即使在最先进的代理框架和LLM下，任务解决率仅为48.15%，超过一半的失败源于环境设置和依赖解析等基础步骤。

Conclusion: 需要更强大的工作流管理和超时准备，GitTaskBench的发布旨在推动仓库感知的代码推理、执行和部署能力的发展。

Abstract: Beyond scratch coding, exploiting large-scale code repositories (e.g.,
GitHub) for practical tasks is vital in real-world software development, yet
current benchmarks rarely evaluate code agents in such authentic,
workflow-driven scenarios. To bridge this gap, we introduce GitTaskBench, a
benchmark designed to systematically assess this capability via 54 realistic
tasks across 7 modalities and 7 domains. Each task pairs a relevant repository
with an automated, human-curated evaluation harness specifying practical
success criteria. Beyond measuring execution and task success, we also propose
the alpha-value metric to quantify the economic benefit of agent performance,
which integrates task success rates, token cost, and average developer
salaries. Experiments across three state-of-the-art agent frameworks with
multiple advanced LLMs show that leveraging code repositories for complex task
solving remains challenging: even the best-performing system, OpenHands+Claude
3.7, solves only 48.15% of tasks. Error analysis attributes over half of
failures to seemingly mundane yet critical steps like environment setup and
dependency resolution, highlighting the need for more robust workflow
management and increased timeout preparedness. By releasing GitTaskBench, we
aim to drive progress and attention toward repository-aware code reasoning,
execution, and deployment -- moving agents closer to solving complex,
end-to-end real-world tasks. The benchmark and code are open-sourced at
https://github.com/QuantaAlpha/GitTaskBench.

</details>


### [14] [A Slice-Based Change Impact Analysis for Regression Test Case Prioritization of Object-Oriented Programs](https://arxiv.org/abs/2508.19056)
*S. Panda,D. Munjal,D. P. Mohapatra*

Main category: cs.SE

TL;DR: 本文提出了一种基于受影响组件耦合度(ACC)的静态测试用例优先级排序方法，通过构建受影响切片图(ASG)来识别面向对象程序中易出错的部分，并为覆盖高ACC值节点的测试用例分配更高优先级。


<details>
  <summary>Details</summary>
Motivation: 测试用例优先级排序旨在通过优化测试用例执行顺序来尽早发现故障，从而节省回归测试的时间和成本。传统方法可能无法准确识别程序中真正易出错的部分。

Method: 构建受影响切片图(ASG)来表示受影响的程序部分，计算各节点的受影响组件耦合度(ACC)值来确定故障倾向性，为覆盖高ACC值节点的测试用例分配更高执行优先级。

Result: 通过7个案例研究和变异故障分析表明，执行易出错程序部分的测试用例有更高概率早期发现故障，相比现有技术具有可接受的性能表现。

Conclusion: 基于ACC的静态优先级排序方法是可行的，能够有效识别易出错程序部分并优化测试用例执行顺序，提高故障检测效率。

Abstract: Test case prioritization focuses on finding a suitable order of execution of
the test cases in a test suite to meet some performance goals like detecting
faults early. It is likely that some test cases execute the program parts that
are more prone to errors and will detect more errors if executed early during
the testing process. Finding an optimal order of execution for the selected
regression test cases saves time and cost of retesting. This paper presents a
static approach to prioritizing the test cases by computing the affected
component coupling (ACC) of the affected parts of object-oriented programs. We
construct a graph named affected slice graph (ASG) to represent these affected
program parts.We determine the fault-proneness of the nodes of ASG by computing
their respective ACC values. We assign higher priority to those test cases that
cover the nodes with higher ACC values. Our analysis with mutation faults shows
that the test cases executing the fault-prone program parts have a higher
chance to reveal faults earlier than other test cases in the test suite. The
result obtained from seven case studies justifies that our approach is feasible
and gives acceptable performance in comparison to some existing techniques.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [15] [A Case Study on the Effectiveness of LLMs in Verification with Proof Assistants](https://arxiv.org/abs/2508.18587)
*Barış Bayazıt,Yao Li,Xujie Si*

Main category: cs.PL

TL;DR: LLMs在证明助手验证任务中的有效性研究：通过hs-to-coq和Verdi项目案例发现，外部依赖和上下文对证明生成有显著帮助，LLMs在小证明上表现优异但也能生成大证明，在不同验证项目中表现各异，能生成简洁智能证明但也会犯奇怪错误


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在证明助手验证任务中的实际效果，了解LLMs在自动化证明生成方面的能力和局限性

Method: 基于两个成熟的Rocq项目（hs-to-coq工具和Verdi）进行案例研究，通过定量和定性分析评估LLMs生成证明的有效性

Result: 研究发现：(1)外部依赖和同源文件上下文显著帮助证明生成；(2)LLMs在小证明上表现优异但也能生成大证明；(3)在不同验证项目中表现不同；(4)能生成简洁智能证明，能将经典技术应用于新定义，但也会犯奇怪错误

Conclusion: LLMs在证明助手验证方面具有潜力，但效果受项目类型、上下文依赖等因素影响，需要进一步研究其在不同场景下的适用性和可靠性

Abstract: Large language models (LLMs) can potentially help with verification using
proof assistants by automating proofs. However, it is unclear how effective
LLMs are in this task. In this paper, we perform a case study based on two
mature Rocq projects: the hs-to-coq tool and Verdi. We evaluate the
effectiveness of LLMs in generating proofs by both quantitative and qualitative
analysis. Our study finds that: (1) external dependencies and context in the
same source file can significantly help proof generation; (2) LLMs perform
great on small proofs but can also generate large proofs; (3) LLMs perform
differently on different verification projects; and (4) LLMs can generate
concise and smart proofs, apply classical techniques to new definitions, but
can also make odd mistakes.

</details>
