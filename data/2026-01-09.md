<div id=toc></div>

# Table of Contents

- [cs.LO](#cs.LO) [Total: 5]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 9]
- [cs.PL](#cs.PL) [Total: 3]


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [1] [Limited Math: Aligning Mathematical Semantics with Finite Computation](https://arxiv.org/abs/2601.04634)
*Lian Wen*

Main category: cs.LO

TL;DR: LM框架将经典数学语义与有限计算对齐，通过显式的数值幅度、精度和结构复杂度约束，为有限计算环境提供有界语义基础。


<details>
  <summary>Details</summary>
Motivation: 编程语言语义学中的经典数学模型依赖无限精度实数、无界集合等理想化抽象，而实际计算是有限的、有界的。这种差异使得在有限执行环境下对数值行为、代数性质和终止性的语义推理变得复杂。

Method: 引入有限数学(LM)框架，通过参数化边界M定义有限数值域，使用确定性值映射算子强制量化和显式边界行为。函数和操作符保持经典数学解释，仅在语义边界处映射到有界域，分离语义含义和有界评估。

Result: 在可表示边界内，LM与经典算术一致；超出边界时，偏差是显式、确定性和可分析的。通过限制集合基数，LM防止隐式无限行为通过结构构造重新进入。由此产生的计算诱导有限状态语义模型。

Conclusion: LM为有限计算环境中的算术、结构和执行推理提供了原则性基础，将有界约束显式化并作为语义基础，使数学推理与实际有限计算对齐。

Abstract: Classical mathematical models used in the semantics of programming languages and computation rely on idealized abstractions such as infinite-precision real numbers, unbounded sets, and unrestricted computation. In contrast, concrete computation is inherently finite, operating under bounded precision, bounded memory, and explicit resource constraints. This discrepancy complicates semantic reasoning about numerical behavior, algebraic properties, and termination under finite execution.
  This paper introduces Limited Math (LM), a bounded semantic framework that aligns mathematical reasoning with finite computation. Limited Math makes constraints on numeric magnitude, numeric precision, and structural complexity explicit and foundational. A finite numeric domain parameterized by a single bound \(M\) is equipped with a deterministic value-mapping operator that enforces quantization and explicit boundary behavior. Functions and operators retain their classical mathematical interpretation and are mapped into the bounded domain only at a semantic boundary, separating meaning from bounded evaluation.
  Within representable bounds, LM coincides with classical arithmetic; when bounds are exceeded, deviations are explicit, deterministic, and analyzable. By additionally bounding set cardinality, LM prevents implicit infinitary behavior from re-entering through structural constructions. As a consequence, computations realized under LM induce finite-state semantic models, providing a principled foundation for reasoning about arithmetic, structure, and execution in finite computational settings.

</details>


### [2] [Generalised Quantifiers Based on Rabin-Mostowski Index](https://arxiv.org/abs/2601.04739)
*Denis Kuperberg,Damian Niwiński,Paweł Parys,Michał Skrzypczak*

Main category: cs.LO

TL;DR: 本文引入新的广义量词表达自动机的Rabin-Mostowski指数，研究其在ω-字和无限树上的MSO逻辑扩展的表达能力和可判定性，发现两者情况截然不同。


<details>
  <summary>Details</summary>
Motivation: 研究如何用逻辑量词表达自动机的Rabin-Mostowski指数，探索MSO逻辑扩展后的表达能力和可判定性，特别是在ω-字和无限树这两种不同结构上的差异。

Method: 引入新的广义量词表达自动机指数；研究MSO逻辑扩展后的表达能力和可判定性；在ω-字和无限树两种情况下分别分析；使用游戏量词扩展MSO实现指数量词消除；提供特定的量词消除过程；构建实现ω-正则游戏中策略的转换器。

Result: 在ω-字情况下，新量词可以在纯MSO逻辑中有效表达；在无限树情况下，添加这些量词会导致不可判定的形式系统；实现了游戏量词的量词消除过程；构建了实现ω-正则游戏策略的转换器。

Conclusion: 新引入的指数量词在ω-字和无限树上的表现完全不同：在ω-字中可有效表达于纯MSO，而在无限树中会导致不可判定性，揭示了这两种结构在逻辑表达能力上的根本差异。

Abstract: In this work we introduce new generalised quantifiers which allow us to express the Rabin-Mostowski index of automata. Our main results study expressive power and decidability of the monadic second-order (MSO) logic extended with these quantifiers. We study these problems in the realm of both $ω$-words and infinite trees. As it turns out, the pictures in these two cases are very different. In the case of $ω$-words the new quantifiers can be effectively expressed in pure MSO logic. In contrast, in the case of infinite trees, addition of these quantifiers leads to an undecidable formalism.
  To realise index-quantifier elimination, we consider the extension of MSO by game quantifiers. As a tool, we provide a specific quantifier-elimination procedure for them. Moreover, we introduce a novel construction of transducers realising strategies in $ω$-regular games with monadic parameters.

</details>


### [3] [The Rezk Completion for Elementary Topoi](https://arxiv.org/abs/2601.04814)
*Kobe Wullaert,Niels van der Weide*

Main category: cs.LO

TL;DR: 提出了一个模块化框架，将Rezk完备化从范畴提升到带结构的范畴，并以提升到初等topos为例展示了其模块性。


<details>
  <summary>Details</summary>
Motivation: 在单值基础中，范畴通常假设为单值的，但Kleisli范畴、Karoubi包络等构造不一定产生单值范畴。虽然Rezk完备化可以将范畴补全为单值范畴，但对于带结构的范畴，需要证明Rezk完备化能继承原范畴的结构。

Method: 提出了一个模块化框架，通过可管理的步骤将Rezk完备化从范畴提升到带结构的范畴。框架设计为模块化，允许逐步处理复杂的结构继承问题。

Result: 成功展示了该框架的模块性，通过可管理的步骤将Rezk完备化从范畴提升到初等topos，证明了框架在处理复杂结构继承问题时的有效性。

Conclusion: 提出的模块化框架为解决Rezk完备化在带结构范畴中的继承问题提供了系统方法，其模块性设计使得处理复杂结构成为可能，为单值基础中范畴理论的发展提供了重要工具。

Abstract: The development of category theory in univalent foundations and the formalization thereof is an active field of research. Categories in that setting are often assumed to be univalent which means that identities and isomorphisms of objects coincide. One consequence hereof is that equivalences and identities coincide for univalent categories and that structure on univalent categories transfers along equivalences. However, constructions such as the Kleisli category, the Karoubi envelope, and the tripos-to-topos construction, do not necessarily give univalent categories. To deal with that problem, one uses the Rezk completion, which completes a category into a univalent one. However, to use the Rezk completion when considering categories with structure, one also needs to show that the Rezk completion inherits the structure from the original category. In this work, we present a modular framework for lifting the Rezk completion from categories to categories with structure. We demonstrate the modularity of our framework by lifting the Rezk completion from categories to elementary topoi in manageable steps.

</details>


### [4] [Approximation theory for distant Bang calculus](https://arxiv.org/abs/2601.05199)
*Kostia Chardonnet,Jules Chouquet,Axel Kerinec*

Main category: cs.LO

TL;DR: 在dBang（带显式替换和远距离约减的Bang演算）中开发近似语义，定义Böhm树和Taylor展开，统一捕捉CbN和CbV的无穷性和资源敏感语义。


<details>
  <summary>Details</summary>
Motivation: 传统上CbN和CbV需要分开的理论，需要统一的近似语义框架。Bang演算通过线性逻辑翻译包含CbN和CbV，但尚未开发完整的近似语义。

Method: 在dBang（带显式替换和远距离约减的Bang演算）中定义Böhm树和Taylor展开，建立基本性质，通过翻译包含并推广CbN和CbV。

Result: 建立了dBang的近似语义理论，定义了Böhm树和Taylor展开，证明了基本性质，统一了CbN和CbV的近似语义。

Conclusion: Bang演算为近似语义提供了统一框架，通过dBang中的Böhm树和Taylor展开，统一捕捉了不同求值策略的无穷性和资源敏感语义。

Abstract: Approximation semantics capture the observable behaviour of λ-terms, with Böhm Trees and Taylor Expansion standing as two central paradigms. Although conceptually different, these notions are related via the Commutation Theorem, which links the Taylor expansion of a term to that of its Böhm tree. These notions are well understood in Call-by-Name λ-calculus and have been more recently introduced in Call-by-Value settings. Since these two evaluation strategies traditionally require separate theories, a natural next step is to seek a unified setting for approximation semantics. The Bang-calculus offers exactly such a framework, subsuming both CbN and CbV through linear-logic translations while providing robust rewriting properties. However, its approximation semantics is yet to be fully developed. In this work, we develop the approximation semantics for dBang, the Bang-calculus with explicit substitutions and distant reductions. We define Böhm trees and Taylor expansion within dBang and establish their fundamental properties. Our results subsume and generalize Call-By-Name and Call-By-Value through their translations into Bang, offering a single framework that uniformly captures infinitary and resource-sensitive semantics across evaluation strategies.

</details>


### [5] [Random Models and Guarded Logic](https://arxiv.org/abs/2601.05247)
*Oskar Fiuk*

Main category: cs.LO

TL;DR: 本文为护卫片段（Guarded Fragment）提供了新的概率证明方法，证明了有限模型性质，并获得了最优的双指数上界。


<details>
  <summary>Details</summary>
Motivation: 基于Gurevich和Shelah在哥德尔类中的思想，为护卫片段构建更简洁的概率证明方法，同时获得最优的模型大小上界。

Method: 采用概率方法证明护卫片段的有限模型性质，通过精确分析获得模型大小的常数因子界限，并将方法扩展到三护卫片段，最后通过确定性哈希函数实现去随机化。

Result: 获得了护卫片段有限模型性质的最优双指数上界，精确分析了常数因子，构造了匹配大小的句子，并将方法成功扩展到三护卫片段和双变量片段。

Conclusion: 概率方法为护卫片段提供了简洁的有限模型性质证明，获得了最优界限，并能扩展到更广泛的逻辑片段，最后通过确定性构造实现了去随机化。

Abstract: Building on ideas of Gurevich and Shelah for the Gödel Class, we present a new probabilistic proof of the finite model property for the Guarded Fragment of First-Order Logic. Our proof is conceptually simple and yields the optimal doubly-exponential upper bound on the size of minimal models. We precisely analyse the obtained bound, up to constant factors in the exponents, and construct sentences that enforce models of tightly matching size. The probabilistic approach adapts naturally to the Triguarded Fragment, an extension of the Guarded Fragment that also subsumes the Two-Variable Fragment. Finally, we derandomise the probabilistic proof by providing an explicit model construction which replaces randomness with deterministic hash functions.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [6] [One-clock synthesis problems](https://arxiv.org/abs/2601.04902)
*Sławomir Lasota,Mathieu Lehaut,Julie Parreaux,Radosław Piórkowski*

Main category: cs.FL

TL;DR: 本文研究了定时Büchi-Landweber游戏的推广，证明了在定时游戏中几乎所有变体的策略和控制综合问题都是不可判定的，即使对于单时钟自动机也是如此。


<details>
  <summary>Details</summary>
Motivation: 将经典的Büchi-Landweber游戏推广到定时设置，研究定时游戏中的综合问题，包括不同玩家获胜条件的指定和策略/控制器的寻找。

Method: 系统研究定时游戏的所有变体，分析哪些玩家的获胜条件被指定，以及寻求哪些玩家的策略（或控制器，即有限记忆策略）。

Result: 证明了在所有变体中，无论是策略综合还是控制器综合，都普遍存在不可判定性，即使对于单时钟自动机指定的获胜条件也是如此。这加强并推广了先前已知的不可判定性结果。

Conclusion: 定时游戏中的综合问题普遍不可判定，但完全刻画了有限记忆足够获胜的情况（即存在策略意味着存在控制器）。类似结果也适用于数据设置中的单寄存器自动机。

Abstract: We study a generalisation of Büchi-Landweber games to the timed setting. The winning condition is specified by a non-deterministic timed automaton, and one of the players can elapse time. We perform a systematic study of synthesis problems in all variants of timed games, depending on which player's winning condition is specified, and which player's strategy (or controller, a finite-memory strategy) is sought. As our main result we prove ubiquitous undecidability in all the variants, both for strategy and controller synthesis, already for winning conditions specified by one-clock automata. This strengthens and generalises previously known undecidability results. We also fully characterise those cases where finite memory is sufficient to win, namely existence of a strategy implies existence of a controller. All our results are stated in the timed setting, while analogous results hold in the data setting where one-clock automata are replaced by one-register ones.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [7] [Sphinx: Benchmarking and Modeling for LLM-Driven Pull Request Review](https://arxiv.org/abs/2601.04252)
*Daoan Zhang,Shuo Zhang,Zijian Jin,Jiebo Luo,Shengyu Fu,Elsie Nallipogu*

Main category: cs.SE

TL;DR: SPHINX是一个基于LLM的PR代码审查统一框架，通过结构化数据生成、清单式评估基准和清单奖励策略优化三个组件，解决了现有自动化代码审查中的噪声监督、上下文理解不足和评估指标不完善等问题。


<details>
  <summary>Details</summary>
Motivation: 当前自动化PR代码审查面临三大挑战：1）噪声监督导致模型学习不准确；2）模型缺乏对代码变更上下文的深入理解；3）现有评估指标（如BLEU）无法准确衡量审查质量。需要开发一个能生成上下文丰富、语义准确的审查意见，并提供结构化评估的框架。

Method: SPHINX包含三个核心组件：1）结构化数据生成管道，通过比较伪修改代码和合并代码来生成上下文丰富、语义基础的审查意见；2）清单式评估基准，基于可操作的验证点结构化覆盖来评估审查质量；3）清单奖励策略优化（CRPO），使用基于规则、可解释的奖励来对齐模型行为与实际审查实践。

Result: 实验表明，使用SPHINX训练的模型在审查完整性和精确性方面达到最先进水平，在清单覆盖率上比专有和开源基线模型高出40%。模型不仅生成流畅的审查意见，而且具有上下文感知能力、技术精确性，并能实际部署到开发工作流中。

Conclusion: SPHINX框架成功解决了自动化PR代码审查的关键挑战，通过结构化数据生成、清单式评估和奖励优化，开发出既流畅又具有上下文感知和技术精确性的审查模型，能够实际应用于真实开发工作流程。数据将在评审后发布。

Abstract: Pull request (PR) review is essential for ensuring software quality, yet automating this task remains challenging due to noisy supervision, limited contextual understanding, and inadequate evaluation metrics. We present Sphinx, a unified framework for LLM-based PR review that addresses these limitations through three key components: (1) a structured data generation pipeline that produces context-rich, semantically grounded review comments by comparing pseudo-modified and merged code; (2) a checklist-based evaluation benchmark that assesses review quality based on structured coverage of actionable verification points, moving beyond surface-level metrics like BLEU; and (3) Checklist Reward Policy Optimization (CRPO), a novel training paradigm that uses rule-based, interpretable rewards to align model behavior with real-world review practices. Extensive experiments show that models trained with Sphinx achieve state-of-the-art performance on review completeness and precision, outperforming both proprietary and open-source baselines by up to 40\% in checklist coverage. Together, Sphinx enables the development of PR review models that are not only fluent but also context-aware, technically precise, and practically deployable in real-world development workflows. The data will be released after review.

</details>


### [8] [A Systematic Mapping Study on the Debugging of Autonomous Driving Systems](https://arxiv.org/abs/2601.04293)
*Nathan Shaw,Sanjeetha Pennada,Robert M Hierons,Donghwan Shin*

Main category: cs.SE

TL;DR: 这篇论文是关于自动驾驶系统调试的系统性映射研究，旨在概述当前研究现状、识别研究空白，并提出未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶系统向商业化部署发展，确保其安全性和可靠性变得日益重要。虽然已有大量研究关注ADS测试方法，但调试方面却很少受到关注。调试是测试失败后定位和修复系统故障的关键过程，对维护系统安全可靠性至关重要。

Method: 采用系统性映射研究（SMS）方法，对自动驾驶系统调试领域进行全面调研和分析，总结现有方法并识别研究空白。

Result: 研究发现存在多种ADS调试方法，揭示了当前研究领域虽然分散但前景广阔。研究还提出了问题定义和术语标准化的建议。

Conclusion: 该研究为ADS调试领域提供了全面概述，识别了研究空白，并提出了未来研究方向，有助于推动该领域的发展和标准化。

Abstract: As Autonomous Driving Systems (ADS) progress towards commercial deployment, there is an increasing focus on ensuring their safety and reliability. While considerable research has been conducted on testing methods for detecting faults in ADS, very little attention has been paid to debugging in ADS. Debugging is an essential process that follows test failures to localise and repair the faults in the systems to maintain their safety and reliability. This Systematic Mapping Study (SMS) aims to provide a detailed overview of the current landscape of ADS debugging, highlighting existing approaches and identifying gaps in research. The study also proposes directions for future work and standards for problem definition and terminology in the field. Our findings reveal various methods for ADS debugging and highlight the current fragmented yet promising landscape.

</details>


### [9] [Advancing Language Models for Code-related Tasks](https://arxiv.org/abs/2601.04526)
*Zhao Tian*

Main category: cs.SE

TL;DR: 该研究通过三个互补方向提升语言模型在复杂编程场景中的能力：改进代码数据质量、增强模型架构、提升模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在复杂编程场景中仍面临挑战，主要受限于数据质量、模型架构和推理能力。需要系统性地解决这些限制，以促进语言模型在软件开发中的实际应用。

Method: 采用三个互补方向：1) 通过代码差异引导的对抗增强技术(CODA)和代码去噪技术(CodeDenoise)改进代码数据质量；2) 通过语法引导的代码语言模型(LEAM和LEAM++)增强模型架构；3) 通过提示技术(muFiX)和基于代理的技术(Specine)提升模型推理能力。

Result: 开发了一系列技术方法，包括CODA、CodeDenoise、LEAM、LEAM++、muFiX和Specine，这些技术共同提升了语言模型处理复杂编程任务的能力。

Conclusion: 通过系统性解决数据质量、模型架构和推理能力三个关键挑战，该研究促进了语言模型在软件开发中的实际应用，并推动了智能软件工程的进一步发展。

Abstract: Recent advances in language models (LMs) have driven significant progress in various software engineering tasks. However, existing LMs still struggle with complex programming scenarios due to limitations in data quality, model architecture, and reasoning capability. This research systematically addresses these challenges through three complementary directions: (1) improving code data quality with a code difference-guided adversarial augmentation technique (CODA) and a code denoising technique (CodeDenoise); (2) enhancing model architecture via syntax-guided code LMs (LEAM and LEAM++); and (3) advancing model reasoning with a prompting technique (muFiX) and an agent-based technique (Specine). These techniques aim to promote the practical adoption of LMs in software development and further advance intelligent software engineering.

</details>


### [10] [AdaptEval: A Benchmark for Evaluating Large Language Models on Code Snippet Adaptation](https://arxiv.org/abs/2601.04540)
*Tanghaoran Zhang,Xinjun Mao,Shangwen Wang,Yuxin Zhao,Yao Lu,Jin Zhang,Zhang Zhang,Kang Yang,Yue Yu*

Main category: cs.SE

TL;DR: AdaptEval是一个专门评估大语言模型在代码片段适应任务上性能的新基准，填补了现有基准在代码重用适应能力评估方面的空白。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在软件工程任务中应用广泛，但缺乏专门评估其在代码适应（代码重用关键活动）方面能力的基准，导致其在该领域的实际效用不明确。

Method: AdaptEval基准具有三个独特特征：1) 基于Stack Overflow和GitHub开发者实践的实际上下文；2) 任务级和适应级的多粒度标注；3) 适应级和函数级测试的两层细粒度评估框架。

Result: 实验评估了6个指令调优LLM和3个推理LLM，结果显示AdaptEval能从多角度评估模型适应能力，并揭示了当前LLM的局限性，特别是在遵循明确指令方面的困难。

Conclusion: AdaptEval填补了代码适应评估基准的空白，为评估和提升LLM在代码片段适应方面的能力提供了工具，支持其在真实世界应用中的发展。

Abstract: Recent advancements in large language models (LLMs) have automated various software engineering tasks, with benchmarks emerging to evaluate their capabilities. However, for adaptation, a critical activity during code reuse, there is no benchmark to assess LLMs' performance, leaving their practical utility in this area unclear. To fill this gap, we propose AdaptEval, a benchmark designed to evaluate LLMs on code snippet adaptation. Unlike existing benchmarks, AdaptEval incorporates the following three distinctive features: First, Practical Context. Tasks in AdaptEval are derived from developers' practices, preserving rich contextual information from Stack Overflow and GitHub communities. Second, Multi-granularity Annotation. Each task is annotated with requirements at both task and adaptation levels, supporting the evaluation of LLMs across diverse adaptation scenarios. Third, Fine-grained Evaluation. AdaptEval includes a two-tier testing framework combining adaptation-level and function-level tests, which enables evaluating LLMs' performance across various individual adaptations. Based on AdaptEval, we conduct the first empirical study to evaluate six instruction-tuned LLMs and especially three reasoning LLMs on code snippet adaptation. Experimental results demonstrate that AdaptEval enables the assessment of LLMs' adaptation capabilities from various perspectives. It also provides critical insights into their current limitations, particularly their struggle to follow explicit instructions. We hope AdaptEval can facilitate further investigation and enhancement of LLMs' capabilities in code snippet adaptation, supporting their real-world applications.

</details>


### [11] [4D-ARE: Bridging the Attribution Gap in LLM Agent Requirements Engineering](https://arxiv.org/abs/2601.04556)
*Bo Yu,Lei Zhao*

Main category: cs.SE

TL;DR: 提出4D-ARE方法，用于在LLM代理设计阶段系统化地指定需要推理的内容，弥补当前运行时推理框架（如ReAct）只关注"如何推理"而忽视"推理什么"的缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理虽然具备ReAct等运行时推理能力，但在实际部署中发现，即使代理能完美执行任务，当被问及"为什么完成率是80%"时，它只会返回指标数据而非因果解释。这表明代理知道如何推理，但设计者没有明确指定需要推理什么内容。设计阶段的规范制定（确定代理需要哪些领域知识）仍然未被充分探索。

Method: 提出4D-ARE（四维归因驱动代理需求工程）方法，核心洞察是决策者寻求的是归因而非答案。归因问题组织成四个维度（结果→过程→支持→长期），受Pearl因果层次理论启发。框架通过五个层次操作化，产生可直接编译为系统提示的工件。在金融服务领域进行了工业试点部署验证。

Result: 通过工业试点部署展示了4D-ARE方法的可行性。该方法解决了代理应该推理什么内容的问题，补充了运行时框架只关注如何推理的不足。作者假设系统化的规范制定能够放大这些基础性进展的威力。

Conclusion: 4D-ARE是一个初步的方法论提案，已通过工业验证，但严格的实证评估计划在未来工作中进行。该方法填补了LLM代理设计中的关键空白，使代理不仅能执行任务，还能提供有意义的因果解释和归因分析。

Abstract: We deployed an LLM agent with ReAct reasoning and full data access. It executed flawlessly, yet when asked "Why is completion rate 80%?", it returned metrics instead of causal explanation. The agent knew how to reason but we had not specified what to reason about. This reflects a gap: runtime reasoning frameworks (ReAct, Chain-of-Thought) have transformed LLM agents, but design-time specification--determining what domain knowledge agents need--remains under-explored. We propose 4D-ARE (4-Dimensional Attribution-Driven Agent Requirements Engineering), a preliminary methodology for specifying attribution-driven agents. The core insight: decision-makers seek attribution, not answers. Attribution concerns organize into four dimensions (Results -> Process -> Support -> Long-term), motivated by Pearl's causal hierarchy. The framework operationalizes through five layers producing artifacts that compile directly to system prompts. We demonstrate the methodology through an industrial pilot deployment in financial services. 4D-ARE addresses what agents should reason about, complementing runtime frameworks that address how. We hypothesize systematic specification amplifies the power of these foundational advances. This paper presents a methodological proposal with preliminary industrial validation; rigorous empirical evaluation is planned for future work.

</details>


### [12] [Extending Delta Debugging Minimization for Spectrum-Based Fault Localization](https://arxiv.org/abs/2601.04689)
*Charaka Geethal Kapugama*

Main category: cs.SE

TL;DR: DDMIN-LOC结合Delta调试最小化与频谱故障定位技术，仅需单个失败输入即可定位字符串输入程序中的故障语句。


<details>
  <summary>Details</summary>
Motivation: DDMIN算法能找出最小失败输入但无法定位故障语句，需要结合故障定位技术来识别具体故障代码位置。

Method: 在DDMIN过程中收集通过和失败输入，使用SBFL算法（Tarantula、Ochiai等）计算语句和谓词的可疑度分数，综合排名语句故障可能性。

Result: 在QuixBugs和Codeflaws基准的136个程序上评估，Jaccard算法表现最佳：多数情况下只需检查少于20%的可执行行，故障语句通常排名前3。

Conclusion: DDMIN-LOC能有效结合输入最小化和故障定位，仅需单个失败输入即可准确定位字符串输入程序中的故障语句。

Abstract: This paper introduces DDMIN-LOC, a technique that combines Delta Debugging Minimization (DDMIN) with Spectrum-Based Fault Localization (SBFL). It can be applied to programs taking string inputs, even when only a single failure-inducing input is available. DDMIN is an algorithm that systematically explores the minimal failure-inducing input that exposes a bug, given an initial failing input. However, it does not provide information about the faulty statements responsible for the failure. DDMIN-LOC addresses this limitation by collecting the passing and failing inputs generated during the DDMIN process and computing suspiciousness scores for program statements and predicates using SBFL algorithms. These scores are then combined to rank statements according to their likelihood of being faulty. DDMIN-LOC requires only one failing input of the buggy program, although it can be applied only to programs that take string inputs. DDMIN-LOC was evaluated on 136 programs selected from the QuixBugs and Codeflaws benchmarks using the SBFL algorithms Tarantula, Ochiai, GenProg, Jaccard and DStar. Experimental results show that DDMIN-LOC performs best with Jaccard: in most subjects, fewer than 20% executable lines need to be examined to locate the faulty statements. Moreover, in most subjects, faulty statements are ranked within the top 3 positions in all the generated test suites derived from different failing inputs.

</details>


### [13] [A Longitudinal Analysis of Gamification in Untappd: Ethical Reflections on a Social Drinking Application](https://arxiv.org/abs/2601.04841)
*Jefferson Seide Molléri,Sami Hyrynsalmi,Antti Hakkala,Kai K. Kimppa,Jouni Smed*

Main category: cs.SE

TL;DR: 对社交饮酒应用Untappd的纵向伦理分析显示，尽管平台进行了微小调整，但通过徽章、连续记录等游戏化设计鼓励饮酒的伦理问题依然存在，需要将伦理反思嵌入软件生命周期。


<details>
  <summary>Details</summary>
Motivation: 研究Untappd这类社交饮酒应用的伦理影响，特别是其游戏化设计（徽章、连续记录、社交分享）如何可能鼓励饮酒行为，以及平台是否通过时间推移改进了伦理框架。

Method: 采用纵向研究方法，在2020年探索性研究基础上，于2025年重新审视Untappd平台；结合传统伦理理论和软件工程实践框架，分析五类徽章及其对用户自主权和福祉的影响。

Result: 研究发现，尽管平台进行了微小调整和表面免责声明，但原始伦理问题大多依然存在；游戏化设计可能通过设计使风险行为正常化。

Conclusion: 需要将持续伦理反思嵌入软件生命周期，防止通过设计使风险行为正常化；软件开发者应主动考虑设计决策的伦理影响。

Abstract: This paper presents a longitudinal ethical analysis of Untappd, a social drinking application that gamifies beer consumption through badges, streaks, and social sharing. Building on an exploratory study conducted in 2020, we revisit the platform in 2025 to examine how its gamification features and ethical framings have evolved. Drawing on traditional ethical theory and practical frameworks for Software Engineering, we analyze five categories of badges and their implications for user autonomy and well-being. Our findings show that, despite small adjustments and superficial disclaimers, many of the original ethical issues remain. We argue for continuous ethical reflection built embedded into software lifecycles to prevent the normalization of risky behaviors through design.

</details>


### [14] [Analyzing Message-Code Inconsistency in AI Coding Agent-Authored Pull Requests](https://arxiv.org/abs/2601.04886)
*Jingzhi Gong,Giovanni Pinna,Yixin Bian,Jie M. Zhang*

Main category: cs.SE

TL;DR: 研究分析了AI生成的PR描述与实际代码变更之间的不一致性，发现1.7%的PR存在高不一致性，导致接受率降低51.7%，合并时间延长3.5倍。


<details>
  <summary>Details</summary>
Motivation: AI编码代理生成的PR描述是向人类评审者传达代码变更的主要渠道，但这些描述与实际变更之间的一致性尚未被探索，引发了人们对AI代理可信度的担忧。

Method: 使用PR消息-代码不一致性(PR-MCI)方法分析了五个AI代理的23,247个PR，贡献了974个手动标注的PR，识别了八种PR-MCI类型。

Result: 发现406个PR(1.7%)表现出高PR-MCI，其中声称未实现变更的描述是最常见问题(45.4%)。高MCI的PR接受率降低51.7%(28.3% vs 80.0%)，合并时间延长3.5倍(55.8 vs 16.0小时)。

Conclusion: 不可靠的PR描述会削弱对AI代理的信任，需要建立PR-MCI验证机制和改进PR生成方法，以实现可信的人机协作。

Abstract: Pull request (PR) descriptions generated by AI coding agents are the primary channel for communicating code changes to human reviewers. However, the alignment between these messages and the actual changes remains unexplored, raising concerns about the trustworthiness of AI agents. To fill this gap, we analyzed 23,247 agentic PRs across five agents using PR message-code inconsistency (PR-MCI). We contributed 974 manually annotated PRs, found 406 PRs (1.7%) exhibited high PR-MCI, and identified eight PR-MCI types, revealing that descriptions claiming unimplemented changes was the most common issue (45.4%). Statistical tests confirmed that high-MCI PRs had 51.7% lower acceptance rates (28.3% vs. 80.0%) and took 3.5x longer to merge (55.8 vs. 16.0 hours). Our findings suggest that unreliable PR descriptions undermine trust in AI agents, highlighting the need for PR-MCI verification mechanisms and improved PR generation to enable trustworthy human-AI collaboration.

</details>


### [15] [AVX / NEON Intrinsic Functions: When Should They Be Used?](https://arxiv.org/abs/2601.04922)
*Théo Boivin,Joeffrey Legaux*

Main category: cs.SE

TL;DR: 提出跨配置基准测试，评估AVX/NEON内在函数在通用开发项目中的能力与限制，指导开发者根据OS、架构和编译器选择是否使用内在函数进行向量化优化。


<details>
  <summary>Details</summary>
Motivation: 在需要向量化策略优化代码的通用开发项目中，探索AVX/NEON内在函数的能力与限制，帮助开发者根据操作系统、架构和可用编译器做出合适的选择。

Method: 提出跨配置基准测试方法，在不同操作系统、架构和编译器环境下，对比分析使用内在函数与普通代码的性能表现。

Result: 内在函数在条件分支处理中表现出色，执行时间仅为普通代码的5%左右；但在许多情况下，编译器已能很好地自动向量化代码，使得内在函数变得不必要。

Conclusion: 内在函数在特定场景（如条件分支）下非常高效，但开发者应谨慎评估是否真正需要，因为现代编译器已具备良好的自动向量化能力。

Abstract: A cross-configuration benchmark is proposed to explore the capacities and limitations of AVX / NEON intrinsic functions in a generic context of development project, when a vectorisation strategy is required to optimise the code. The main aim is to guide developers to choose when using intrinsic functions, depending on the OS, architecture and/or available compiler. Intrinsic functions were observed highly efficient in conditional branching, with intrinsic version execution time reaching around 5% of plain code execution time. However, intrinsic functions were observed as unnecessary in many cases, as the compilers already well auto-vectorise the code.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [16] [Scalable Floating-Point Satisfiability via Staged Optimization](https://arxiv.org/abs/2601.04492)
*Yuanzhuo Zhang,Zhoulai Fu,Binoy Ravindran*

Main category: cs.PL

TL;DR: StageSAT：一种新的浮点可满足性求解方法，通过三阶段优化将SMT求解与数值优化结合，无需比特级推理，实现高精度求解


<details>
  <summary>Details</summary>
Motivation: 传统浮点SMT求解器依赖复杂的比特级推理和专门抽象，计算成本高且可扩展性有限。需要一种更高效、更可靠的方法来处理浮点公式的可满足性问题。

Method: 采用三阶段优化方法：1）快速投影辅助下降引导搜索到可行区域；2）比特级精度的ULP²优化；3）n-ULP网格细化。通过正交投影实现线性约束的部分单调下降特性，将复杂算术视为黑盒，仅使用运行时评估。

Result: 在SMT-COMP'25测试集和先前工作的困难案例上，StageSAT比现有优化方法更具可扩展性和准确性。在相同时间预算下解决了更多公式，在可满足案例中达到99.4%召回率和0%假SAT，速度比传统比特精确SMT和数值求解器快5-10倍。

Conclusion: 分阶段优化显著提高了浮点可满足性求解的性能和正确性，提供了一种无需比特级推理的高效可靠解决方案。

Abstract: This work introduces StageSAT, a new approach to solving floating-point satisfiability that bridges SMT solving with numerical optimization. StageSAT reframes a floating-point formula as a series of optimization problems in three stages of increasing precision. It begins with a fast, projection-aided descent objective to guide the search toward a feasible region, proceeding to bit-level accuracy with ULP$^2$ optimization and a final $n$-ULP lattice refinement.
  By construction, the final stage uses a representing function that is zero if and only if a candidate satisfies all constraints. Thus, when optimization drives the objective to zero, the resulting assignment is a valid solution, providing a built-in guarantee of soundness.
  To improve search, StageSAT introduces a partial monotone descent property on linear constraints via orthogonal projection, preventing the optimizer from stalling on flat or misleading landscapes. Critically, this solver requires no heavy bit-level reasoning or specialized abstractions; it treats complex arithmetic as a black-box, using runtime evaluations to navigate the input space.
  We implement StageSAT and evaluate it on extensive benchmarks, including SMT-COMP'25 suites and difficult cases from prior work. StageSAT proved more scalable and accurate than state-of-the-art optimization-based alternatives. It solved strictly more formulas than any competing solver under the same time budget, finding most satisfiable instances without producing spurious models. This amounts to 99.4% recall on satisfiable cases with 0% false SAT, exceeding the reliability of prior optimization-based solvers. StageSAT also delivered significant speedups (often 5--10$\times$) over traditional bit-precise SMT and numeric solvers. These results demonstrate that staged optimization significantly improves performance and correctness of floating-point satisfiability solving.

</details>


### [17] [Lenses for Partially-Specified States (Extended Version)](https://arxiv.org/abs/2601.04573)
*Kazutaka Matsuda,Minh Nguyen,Meng Wang*

Main category: cs.PL

TL;DR: 提出部分状态透镜，通过部分指定源和视图状态来精确表示用户更新意图，支持多视图更新合并，确保更新保持性


<details>
  <summary>Details</summary>
Motivation: 双向变换在多视图共享源时面临挑战：一个视图的更新会影响其他视图，难以同时保持对应关系和用户更新，特别是在多视图同时更改时。在组合框架中确保这些属性更具挑战性。

Method: 提出部分状态透镜，允许源和视图状态部分指定以精确表示用户更新意图。这些意图是部分有序的，为合并来自多视图的更新意图提供了清晰的语义，并提供了与这种合并兼容的更新保持性精确定义。形式化了部分状态透镜及其部分指定感知的良好行为性，支持组合推理并确保更新保持性。

Result: 通过示例展示了所提出系统的实用性，提供了处理多视图双向变换中更新冲突和意图合并的解决方案。

Conclusion: 部分状态透镜为多视图双向变换提供了精确的更新意图表示和合并机制，解决了多视图更新时的保持性问题，支持组合推理，具有实际应用价值。

Abstract: A bidirectional transformation is a pair of transformations satisfying certain well-behavedness properties: one maps source data into view data, and the other translates changes on the view back to the source. However, when multiple views share a source, an update on one view may affect the others, making it hard to maintain correspondence while preserving the user's update, especially when multiple views are changed at once. Ensuring these properties within a compositional framework is even more challenging. In this paper, we propose partial-state lenses, which allow source and view states to be partially specified to precisely represent the user's update intentions. These intentions are partially ordered, providing clear semantics for merging intentions of updates coming from multiple views and a refined notion of update preservation compatible with this merging. We formalize partial-state lenses, together with partial-specifiedness-aware well-behavedness that supports compositional reasoning and ensures update preservation. In addition, we demonstrate the utility of the proposed system through examples.

</details>


### [18] [The Squirrel Parser: A Linear-Time PEG Packrat Parser Capable of Left Recursion and Optimal Error Recovery](https://arxiv.org/abs/2601.05012)
*Luke A. D. Hutchison*

Main category: cs.PL

TL;DR: 提出松鼠解析器，一种PEG packrat解析器，能直接处理所有形式的左递归并具有最优错误恢复能力，同时保持线性时间复杂度


<details>
  <summary>Details</summary>
Motivation: 传统方法处理左递归需要语法重写或复杂算法扩展，现有方法在错误恢复方面存在不足，需要一种能同时处理左递归和最优错误恢复的解析器

Method: 基于第一原理推导最小算法：通过每位置状态跟踪进行循环检测，后代到祖先递归帧的O(1)通信，迭代扩展的固定点搜索；错误恢复基于四公理十二约束，使用约束满足机制搜索所有可能性

Result: 实现了能直接处理所有形式左递归的解析器，具有最优错误恢复能力，即使在任意数量错误存在下也能保持输入长度的线性时间复杂度

Conclusion: 松鼠解析器提供了一种理论上最优且鲁棒的解决方案，同时解决了左递归处理和错误恢复两个关键问题，性能得到保证

Abstract: We present the squirrel parser, a PEG packrat parser that directly handles all forms of left recursion with optimal error recovery, while maintaining linear time complexity in the length of the input even in the presence of an arbitrary number of errors. Traditional approaches to handling left recursion in a recursive descent parser require grammar rewriting or complex algorithmic extensions. We derive a minimal algorithm from first principles: cycle detection via per-position state tracking and $O(1)$-per-LR-cycle communication from descendant to ancestor recursion frames, and fixed-point search via iterative expansion. For error recovery, we derived a set of four axioms and twelve constraints that must be imposed upon an optimal error recovery design to ensure completeness, correctness, optimality of performance, and intuitiveness of behavior. We utilized a constraint satisfaction mechanism to search the space of all possibilities, arriving at a provably optimal and robust error recovery strategy that maintains perfect performance linearity.

</details>
