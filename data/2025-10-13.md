<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 25]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.PL](#cs.PL) [Total: 5]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Comparative Analysis of Large Language Models for the Machine-Assisted Resolution of User Intentions](https://arxiv.org/abs/2510.08576)
*Justus Flerlage,Alexander Acker,Odej Kao*

Main category: cs.SE

TL;DR: 本研究评估开源大语言模型作为本地部署意图解析系统的可行性，与GPT-4进行性能对比，探索下一代操作系统中本地AI组件的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前基于云端专有模型的意图解析系统存在隐私、自主性和可扩展性限制，需要验证本地部署的开源LLMs作为未来意图操作系统的可行性。

Method: 对多个开源和开放访问模型进行能力评估，与OpenAI的GPT-4系统进行对比分析，测试其在为用户意图生成工作流方面的性能。

Result: 提供了关于开源LLMs作为本地可操作组件的实际可行性、性能权衡和潜力的实证见解。

Conclusion: 研究结果表明本地嵌入式智能可以实现更无缝、自适应和隐私保护的用户设备交互，推动AI基础设施的去中心化和民主化。

Abstract: Large Language Models (LLMs) have emerged as transformative tools for natural
language understanding and user intent resolution, enabling tasks such as
translation, summarization, and, increasingly, the orchestration of complex
workflows. This development signifies a paradigm shift from conventional,
GUI-driven user interfaces toward intuitive, language-first interaction
paradigms. Rather than manually navigating applications, users can articulate
their objectives in natural language, enabling LLMs to orchestrate actions
across multiple applications in a dynamic and contextual manner. However,
extant implementations frequently rely on cloud-based proprietary models, which
introduce limitations in terms of privacy, autonomy, and scalability. For
language-first interaction to become a truly robust and trusted interface
paradigm, local deployment is not merely a convenience; it is an imperative.
This limitation underscores the importance of evaluating the feasibility of
locally deployable, open-source, and open-access LLMs as foundational
components for future intent-based operating systems. In this study, we examine
the capabilities of several open-source and open-access models in facilitating
user intention resolution through machine assistance. A comparative analysis is
conducted against OpenAI's proprietary GPT-4-based systems to assess
performance in generating workflows for various user intentions. The present
study offers empirical insights into the practical viability, performance
trade-offs, and potential of open LLMs as autonomous, locally operable
components in next-generation operating systems. The results of this study
inform the broader discussion on the decentralization and democratization of AI
infrastructure and point toward a future where user-device interaction becomes
more seamless, adaptive, and privacy-conscious through locally embedded
intelligence.

</details>


### [2] [Which Is Better For Reducing Outdated and Vulnerable Dependencies: Pinning or Floating?](https://arxiv.org/abs/2510.08609)
*Imranur Rahman,Jill Marley,William Enck,Laurie Williams*

Main category: cs.SE

TL;DR: 该研究评估了不同版本约束类型对依赖项过时或存在漏洞可能性的影响，发现在npm、PyPI和Cargo生态系统中，浮动次要版本约束最可能导致过时或漏洞依赖，而固定版本约束次之。浮动主版本约束最不容易过时，浮动次要版本约束最不容易出现漏洞。


<details>
  <summary>Details</summary>
Motivation: 开发者使用版本约束来指定依赖项的可接受版本，固定依赖可以减少破坏性变更的风险，但需要手动管理过时和漏洞依赖的替换；浮动依赖可以自动获取修复，但存在破坏性变更的风险。安全从业者建议固定依赖以防止软件供应链攻击，但固定约束最容易导致依赖过时。目前尚不清楚不同版本约束类型对依赖过时或漏洞可能性的具体影响。

Method: 首先识别npm、PyPI和Cargo生态系统中依赖版本约束使用的趋势和开发者更改版本约束类型的模式，然后使用生存分析建模依赖状态转换，估计使用固定版本约束与其他版本约束类型相比，依赖项变得过时或存在漏洞的可能性变化。

Result: 在过时和存在漏洞的依赖项中，最常用的版本约束类型是浮动次要版本约束，固定版本约束次之。浮动主版本约束最不容易导致依赖过时，浮动次要版本约束最不容易导致依赖存在漏洞。

Conclusion: 研究结果可以帮助开发者在选择依赖版本约束时做出更明智的决策，理解不同约束类型对依赖项状态的影响，平衡安全性和维护成本。

Abstract: Developers consistently use version constraints to specify acceptable
versions of the dependencies for their project. \emph{Pinning} dependencies can
reduce the likelihood of breaking changes, but comes with a cost of manually
managing the replacement of outdated and vulnerable dependencies. On the other
hand, \emph{floating} can be used to automatically get bug fixes and security
fixes, but comes with the risk of breaking changes. Security practitioners
advocate \emph{pinning} dependencies to prevent against software supply chain
attacks, e.g., malicious package updates. However, since \emph{pinning} is the
tightest version constraint, \emph{pinning} is the most likely to result in
outdated dependencies. Nevertheless, how the likelihood of becoming outdated or
vulnerable dependencies changes across version constraint types is unknown. The
goal of this study is to aid developers in making an informed dependency
version constraint choice by empirically evaluating the likelihood of
dependencies becoming outdated or vulnerable across version constraint types at
scale. In this study, we first identify the trends in dependency version
constraint usage and the patterns of version constraint type changes made by
developers in the npm, PyPI, and Cargo ecosystems. We then modeled the
dependency state transitions using survival analysis and estimated how the
likelihood of becoming outdated or vulnerable changes when using \emph{pinning}
as opposed to the rest of the version constraint types. We observe that among
outdated and vulnerable dependencies, the most commonly used version constraint
type is \emph{floating-minor}, with \emph{pinning} being the next most common.
We also find that \emph{floating-major} is the least likely to result in
outdated and \emph{floating-minor} is the least likely to result in vulnerable
dependencies.

</details>


### [3] [Relative Positioning Based Code Chunking Method For Rich Context Retrieval In Repository Level Code Completion Task With Code Language Model](https://arxiv.org/abs/2510.08610)
*Imranur Rahman,Md Rayhanur Rahman*

Main category: cs.SE

TL;DR: 提出了一种有效的上下文收集策略，通过代码分块和相对位置排序来提升LLM在代码补全任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有IDE中的代码补全功能缺乏对如何基于IDE可用信息构建良好上下文的研究，特别是针对大语言模型优化代码补全性能的研究不足。

Method: 将代码仓库预处理为更小的代码块，然后使用基于语法和语义相似性的代码块检索方法，并结合相对位置排序来构建最终上下文。

Result: 研究发现代码分块和代码块在最终上下文中的相对位置排序能够提升代码补全任务的性能。

Conclusion: 提出的上下文收集策略通过代码分块和相对位置排序有效改善了LLM在代码补全任务中的表现。

Abstract: Code completion can help developers improve efficiency and ease the
development lifecycle. Although code completion is available in modern
integrated development environments (IDEs), research lacks in determining what
makes a good context for code completion based on the information available to
the IDEs for the large language models (LLMs) to perform better. In this paper,
we describe an effective context collection strategy to assist the LLMs in
performing better at code completion tasks. The key idea of our strategy is to
preprocess the repository into smaller code chunks and later use syntactic and
semantic similarity-based code chunk retrieval with relative positioning. We
found that code chunking and relative positioning of the chunks in the final
context improve the performance of code completion tasks.

</details>


### [4] [Literate Tracing](https://arxiv.org/abs/2510.09073)
*Matthew Sotoudeh*

Main category: cs.SE

TL;DR: 本文介绍了一种称为"literate tracing"的程序文档范式，通过带注释的具体执行轨迹来解释软件系统，并开发了TReX工具来创建交互式、可视化且保证语义准确的可读轨迹。


<details>
  <summary>Details</summary>
Motivation: 随着计算机系统日益庞大复杂，系统专家需要向新手解释程序工作原理。现有代码注释缺乏全局上下文，而设计文档又缺乏与代码的具体连接。

Method: 提出literate tracing范式，使用带注释的具体执行轨迹来解释软件系统。开发TReX工具，创建交互式、可视化的可读轨迹，并通过构造保证其与程序语义的一致性。

Result: 使用TReX工具为大型系统软件（包括Linux内核、Git源代码控制系统和GCC编译器）的组件编写了可读轨迹。

Conclusion: Literate tracing通过具体执行轨迹补充了代码注释和设计文档的不足，为程序理解提供了新的有效方法。

Abstract: As computer systems grow ever larger and more complex, a crucial task in
software development is for one person (the system expert) to communicate to
another (the system novice) how a certain program works. This paper reports on
the author's experiences with a paradigm for program documentation that we call
literate tracing. A literate trace explains a software system using annotated,
concrete execution traces of the system. Literate traces complement both
in-code comments (which often lack global context) and out-of-band design docs
(which often lack a concrete connection to the code). We also describe TReX,
our tool for making literate traces that are interactive, visual, and
guaranteed by construction to be faithful to the program semantics. We have
used TReX to write literate traces explaining components of large systems
software including the Linux kernel, Git source control system, and GCC
compiler.

</details>


### [5] [Impact of LLMs on Team Collaboration in Software Development](https://arxiv.org/abs/2510.08612)
*Devang Dhanuka*

Main category: cs.SE

TL;DR: 本文研究了LLMs在软件开发生命周期中对团队协作的影响，发现LLMs能显著提升效率、改善沟通，但也带来模型局限性和隐私等新挑战。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs越来越多地集成到软件开发过程中，需要了解它们如何影响团队协作流程和生产力，特别是在SDLC的各个阶段。

Method: 通过文献综述、行业案例、团队调查和两个案例研究，评估LLM辅助工具对协作软件开发实践的影响。

Result: LLMs能够显著提高效率（通过自动化重复任务和文档）、增强沟通清晰度、促进跨职能协作，但也引入了模型局限性和隐私问题等新挑战。

Conclusion: LLMs在软件开发团队协作中具有显著潜力，但需要解决模型定制、工具集成以及信任安全策略等未来研究方向。

Abstract: Large Language Models (LLMs) are increasingly being integrated into software
development processes, with the potential to transform team workflows and
productivity. This paper investigates how LLMs affect team collaboration
throughout the Software Development Life Cycle (SDLC). We reframe and update a
prior study with recent developments as of 2025, incorporating new literature
and case studies. We outline the problem of collaboration hurdles in SDLC and
explore how LLMs can enhance productivity, communication, and decision-making
in a team context. Through literature review, industry examples, a team survey,
and two case studies, we assess the impact of LLM-assisted tools (such as code
generation assistants and AI-powered project management agents) on
collaborative software engineering practices. Our findings indicate that LLMs
can significantly improve efficiency (by automating repetitive tasks and
documentation), enhance communication clarity, and aid cross-functional
collaboration, while also introducing new challenges like model limitations and
privacy concerns. We discuss these benefits and challenges, present research
questions guiding the investigation, evaluate threats to validity, and suggest
future research directions including domain-specific model customization,
improved integration into development tools, and robust strategies for ensuring
trust and security.

</details>


### [6] [Automating Android Build Repair: Bridging the Reasoning-Execution Gap in LLM Agents with Domain-Specific Tools](https://arxiv.org/abs/2510.08640)
*Ha Min Son,Huan Ren,Xin Liu,Zhe Zhao*

Main category: cs.SE

TL;DR: 提出了GradleFixer，一个专门用于修复Android构建错误的LLM代理，通过领域特定工具显著提升了构建错误的解决率。


<details>
  <summary>Details</summary>
Motivation: Android是最大的移动平台，但自动构建应用仍面临挑战。虽然LLM在代码修复方面有潜力，但在修复Android构建错误方面的应用仍未被充分探索。

Method: 首先创建了AndroidBuildBench基准测试集，包含1,019个构建失败案例；然后提出了GradleFixer，这是一个配备领域特定工具的LLM代理，用于检查和操作Gradle构建环境。

Result: GradleFixer实现了81.4%的解决率（pass@1），显著优于依赖通用shell的最先进编码代理。

Conclusion: LLM具备解决这些失败的高级知识，但难以使用通用shell将这些知识转化为有效的低级操作。工具桥接策略通过提供API格式的工具和约束操作空间，弥合了模型高级推理与有效低级执行之间的差距。

Abstract: Android is the largest mobile platform, yet automatically building
applications remains a practical challenge. While Large Language Models (LLMs)
show promise for code repair, their use for fixing Android build errors remains
underexplored. To address this gap, we first introduce AndroidBuildBench, a
benchmark of 1,019 build failures curated from the commit histories of 43
open-source Android projects. Each problem is paired with a verified solution
from a subsequent commit, ensuring that fixes are feasible. Second, we propose
GradleFixer, an LLM agent with domain-specific tools for inspecting and
manipulating the Gradle build environment. GradleFixer achieves a resolve rate
of 81.4% (pass@1), significantly outperforming a state-of-the-art coding agent
that relies on a general-purpose shell. GradleFixer's success suggests that
while LLMs possess the high-level knowledge to solve these failures, they
struggle to translate this knowledge into effective low-level actions using a
general-purpose shell. We demonstrate the effectiveness of a strategy we term
Tool Bridging, which replaces general-purpose shell commands with domain-aware
abstractions. We hypothesize this approach works through two mechanisms: 1) it
provides tools in an API-like format that LLMs use more reliably, and 2) it
constrains the action space to relevant operations. This approach bridges the
gap between the model's high-level reasoning and effective low-level execution.

</details>


### [7] [Faver: Boosting LLM-based RTL Generation with Function Abstracted Verifiable Middleware](https://arxiv.org/abs/2510.08664)
*Jianan Mu,Mingyu Shi,Yining Wang,Tianmeng Yang,Bin Sun,Xing Hu,Jing Ye,Huawei Li*

Main category: cs.SE

TL;DR: 提出了一种名为Faver的函数抽象可验证中间件，通过将LLM友好的代码结构与基于规则的模板相结合，简化了基于LLM的RTL生成工作流中的验证过程，将生成准确率提升了最高14%。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在RTL生成中存在显著语义鸿沟，由于高层次规范与RTL之间的语义差异大且训练数据有限，现有模型生成准确率不高。受人类设计经验启发，设计结合验证可提高准确率，但RTL测试平台数据更稀缺，不利于LLM。

Method: 开发了函数抽象可验证中间件(Faver)，通过混合LLM友好的代码结构和基于规则的模板，解耦电路验证细节，让LLM专注于功能本身。

Result: 在SFT模型和开源模型上的实验表明，Faver将模型生成准确率提升了最高14%。

Conclusion: Faver中间件有效简化了RTL验证流程，显著提升了LLM在RTL生成任务中的准确率，为解决语义鸿沟问题提供了可行方案。

Abstract: LLM-based RTL generation is an interesting research direction, as it holds
the potential to liberate the least automated stage in the current chip design.
However, due to the substantial semantic gap between high-level specifications
and RTL, coupled with limited training data, existing models struggle with
generation accuracy. Drawing on human experience, design with verification
helps improving accuracy. However, as the RTL testbench data are even more
scarce, it is not friendly for LLMs. Although LLMs excel at higher-level
languages like Python/C, they have a huge semantic gap from RTL. When
implementing the same functionality, Python/C code and hardware code differ
significantly in the spatiotemporal granularity, requiring the LLM not only to
consider high-level functional semantics but also to ensure the low-level
details align with the circuit code. It is not an easy task. In this paper, we
propose a function abstracted verifiable middleware (Faver) that streamlines
RTL verification in LLM-based workflows. By mixing LLM-friendly code structures
with a rule-based template, Faver decouples the details of circuit
verification, allowing the LLM to focus on the functionality itself. In our
experiments on the SFT model and open-source models, Faver improved the model's
generation accuracy by up to 14%.

</details>


### [8] [RA-Gen: A Controllable Code Generation Framework Using ReAct for Multi-Agent Task Execution](https://arxiv.org/abs/2510.08665)
*Aofan Liu,Haoxuan Li,Bin Wang,Ao Yang,Hui Li*

Main category: cs.SE

TL;DR: 提出基于ReAct范式的可控代码生成框架，采用多智能体系统实现高效、精确和可解释的代码生成，通过四个专业智能体的协作提升安全性和用户控制。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成模型在安全性、准确性和可控性方面存在不足，特别是在复杂任务中缺乏动态工具集成、透明推理和用户安全控制。

Method: 采用多智能体架构，包含规划器、基于ReAct的搜索器、代码生成器和数据提取器四个专业智能体，通过推理追踪与动作执行的交替实现内部知识与外部工具的集成。

Result: 在多种编程语言上表现有效，在SVEN数据集上使用CodeQL达到94.8%的安全率，优于现有方法。

Conclusion: 该框架通过透明推理过程增强用户信任并提高可控性，为代码生成提供了更安全、准确和可解释的解决方案。

Abstract: Code generation models based on large language models (LLMs) have gained wide
adoption, but challenges remain in ensuring safety, accuracy, and
controllability, especially for complex tasks. Existing methods often lack
dynamic integration of external tools, transparent reasoning, and user control
over safety. To address these issues, we propose a controllable code generation
framework utilizing the ReAct paradigm for multi-agent task execution. This
framework is a multi-agent system designed to enable efficient, precise, and
interpretable code generation through dynamic interactions between LLMs and
external resources. The framework adopts a collaborative architecture
comprising four specialized agents: a Planner for task decomposition, a
Searcher that leverages the ReAct framework for reasoning and tool integration,
a CodeGen agent for accurate code generation, and an Extractor for structured
data retrieval. The ReAct-based Searcher alternates between generating
reasoning traces and executing actions, facilitating seamless integration of
internal knowledge with external tools (such as search engines) to enhance
accuracy and user control. Experimental results show the framework's
effectiveness across multiple languages, achieving a 94.8% security rate on the
SVEN dataset with CodeQL, outperforming existing approaches. Its transparent
reasoning process fosters user trust and improves controllability.

</details>


### [9] [RAG4Tickets: AI-Powered Ticket Resolution via Retrieval-Augmented Generation on JIRA and GitHub Data](https://arxiv.org/abs/2510.08667)
*Mohammad Baqar*

Main category: cs.SE

TL;DR: 提出基于检索增强生成(RAG)的框架，整合JIRA工单和GitHub数据，通过语义嵌入和向量搜索为软件团队提供上下文感知的工单解决建议。


<details>
  <summary>Details</summary>
Motivation: 解决软件团队因知识分散在JIRA工单、开发者讨论和GitHub PR中而导致的重复或相关问题解决延迟问题。

Method: 使用Sentence-Transformers生成语义嵌入，FAISS进行向量搜索，检索历史相似案例，再由大语言模型合成基于证据的解决方案建议。

Result: 实验评估显示，该系统在解决准确性、修复质量和知识重用方面显著提升，减少了解决时间并提高了开发者接受度。

Conclusion: 该RAG框架为现代DevOps环境提供了有效的知识整合和工单解决推荐系统，改善了团队协作效率。

Abstract: Modern software teams frequently encounter delays in resolving recurring or
related issues due to fragmented knowledge scattered across JIRA tickets,
developer discussions, and GitHub pull requests (PRs). To address this
challenge, we propose a Retrieval-Augmented Generation (RAG) framework that
integrates Sentence-Transformers for semantic embeddings with FAISS-based
vector search to deliver context-aware ticket resolution recommendations. The
approach embeds historical JIRA tickets, user comments, and linked PR metadata
to retrieve semantically similar past cases, which are then synthesized by a
Large Language Model (LLM) into grounded and explainable resolution
suggestions. The framework contributes a unified pipeline linking JIRA and
GitHub data, an embedding and FAISS indexing strategy for heterogeneous
software artifacts, and a resolution generation module guided by retrieved
evidence. Experimental evaluation using precision, recall, resolution time
reduction, and developer acceptance metrics shows that the proposed system
significantly improves resolution accuracy, fix quality, and knowledge reuse in
modern DevOps environments.

</details>


### [10] [BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution](https://arxiv.org/abs/2510.08697)
*Terry Yue Zhuo,Xiaolong Jin,Hange Liu,Juyong Jiang,Tianyang Liu,Chen Gong,Bhupesh Bishnoi,Vaisakhi Mishra,Marek Suppa,Noah Ziems,Saiteja Utpala,Ming Xu,Guangyu Song,Kaixin Li,Yuhan Cao,Bo Liu,Zheng Liu,Sabina Abdurakhmanova,Wenhao Yu,Mengzhao Jia,Jihan Yao,Kenneth Hamilton,Kumar Shridhar,Minh Chien Vu,Dingmin Wang,Jiawei Liu,Zijian Wang,Qian Liu,Binyuan Hui,Meg Risdal,Ahsen Khaliq,Atin Sood,Zhenchang Xing,Wasi Uddin Ahmad,John Grundy,David Lo,Banghua Zhu,Xiaoning Du,Torsten Scholak,Leandro von Werra*

Main category: cs.SE

TL;DR: BigCodeArena是一个用于代码生成评估的开放人类评估平台，集成了实时执行环境，收集了超过14,000个代码对话会话，并开发了两个基准测试来系统评估LLM的代码能力。


<details>
  <summary>Details</summary>
Motivation: 在代码领域，手动评估LLM生成内容的质量极具挑战性，需要理解长段原始代码并模拟代码执行过程。

Method: 基于Chatbot Arena构建BigCodeArena平台，提供全面的实时执行环境，允许人类与代码执行过程和结果交互。收集了10个LLM的14,000+代码对话会话，涵盖10种语言和8种执行环境。

Result: 识别了4,700多个带有人类偏好的多轮对话样本，发现LLM在细粒度领域（任务、语言、框架）存在未被充分探索的偏好。大多数LLM在有执行结果时能更好地判断编码偏好。

Conclusion: 专有LLM如GPT-5、Claude-Sonnet-4和Claude-Opus-4在代码生成性能方面仍领先于新兴模型。

Abstract: Crowdsourced model evaluation platforms, such as Chatbot Arena, enable
real-time evaluation from human perspectives to assess the quality of model
responses. In the coding domain, manually examining the quality of
LLM-generated content is extremely challenging, as it requires understanding
long chunks of raw code and deliberately simulating code execution. To this
end, we introduce BigCodeArena, an open human evaluation platform for code
generation backed by a comprehensive and on-the-fly execution environment.
Built on top of Chatbot Arena, BigCodeArena enables the execution of
LLM-generated code and allows humans to interact with the execution process and
outcomes. We collected over 14,000 raw code-centric conversation sessions
across 10 widely used LLMs, spanning 10 languages and 8 types of execution
environments. Among these conversations, we identified more than 4,700
multi-turn samples with pairwise human preferences. Further analysis uncovers
underexplored preferences of LLMs in fine-grained domains characterized by
tasks, languages, and frameworks. To systematically examine code understanding
and generation capabilities of frontier LLMs, we curated two benchmarks based
on the collected data, namely BigCodeReward and AutoCodeArena. For
BigCodeReward, we post-processed the 4,700 conversations and evaluated the
consistency between reward models and human preferences. The evaluation shows
that most LLMs have superior performance in judging coding preferences when the
execution results are available. Inspired by these findings, we propose
AutoCodeArena, an automatic Elo rating benchmark designed to assess the coding
quality of LLMs without human involvement. We find that proprietary LLMs like
GPT-5, Claude-Sonnet-4, and Claude-Opus-4 still lead in code generation
performance among recent emerging models.

</details>


### [11] [Search-based Hyperparameter Tuning for Python Unit Test Generation](https://arxiv.org/abs/2510.08716)
*Stephan Lukasczyk,Gordon Fraser*

Main category: cs.SE

TL;DR: 使用差分进化算法优化DynaMOSA和MIO多目标搜索算法的超参数，显著提高了测试套件的覆盖率，且比基本网格搜索更高效。


<details>
  <summary>Details</summary>
Motivation: 搜索式测试生成算法有众多配置选项，用户通常使用默认值，但这些默认值可能无法达到最佳效果。超参数调优可以找到更好的参数值，但通常需要大量资源。

Method: 探索使用差分进化算法来调优Pynguin框架中DynaMOSA和MIO多目标搜索算法的超参数。

Result: 结果显示，经过调优的DynaMOSA算法显著提高了测试套件的覆盖率，且差分进化比基本网格搜索更高效。

Conclusion: 差分进化算法是优化搜索式测试生成算法超参数的有效方法，能够显著提升测试覆盖率，同时比传统网格搜索方法更高效。

Abstract: Search-based test-generation algorithms have countless configuration options.
Users rarely adjust these options and usually stick to the default values,
which may not lead to the best possible results. Tuning an algorithm's
hyperparameters is a method to find better hyperparameter values, but it
typically comes with a high demand of resources. Meta-heuristic search
algorithms -- that effectively solve the test-generation problem -- have been
proposed as a solution to also efficiently tune parameters. In this work we
explore the use of differential evolution as a means for tuning the
hyperparameters of the DynaMOSA and MIO many-objective search algorithms as
implemented in the Pynguin framework. Our results show that significant
improvement of the resulting test suite's coverage is possible with the tuned
DynaMOSA algorithm and that differential evolution is more efficient than basic
grid search.

</details>


### [12] [PyMigTool: a tool for end-to-end Python library migration](https://arxiv.org/abs/2510.08810)
*Mohayeminul Islam,Ajay Kumar Jha,May Mahmoud,Sarah Nadi*

Main category: cs.SE

TL;DR: 开发了PyMigTool，一个结合LLM、静态分析和动态分析的端到端Python库迁移工具，能够自动迁移任意功能相似的Python库对


<details>
  <summary>Details</summary>
Motivation: 手动库迁移耗时且容易出错，现有自动化技术大多停留在API映射阶段或支持有限的库和代码转换

Method: 使用LLM作为主要迁移引擎，结合静态分析和动态分析进行后处理，开发PyMigTool命令行应用

Result: 在717个真实Python应用上评估，PyMigTool能完全正确迁移32%的案例，剩余案例中超过一半项目只有14%的迁移相关更改需要开发者修复

Conclusion: LLM能有效执行库迁移，结合后处理步骤可进一步提高性能，PyMigTool为Python库迁移提供了实用的端到端解决方案

Abstract: Library migration is the process of replacing a library with a similar one in
a software project. Manual library migration is time consuming and error prone,
as it requires developers to understand the Application Programming Interfaces
(API) of both libraries, map equivalent APIs, and perform the necessary code
transformations. Due to the difficulty of the library migration process, most
of the existing automated techniques and tooling stop at the API mapping stage
or support a limited set of libraries and code transformations. In this paper,
we develop an end-to-end solution that can automatically migrate code between
any arbitrary pair of Python libraries that provide similar functionality. Due
to the promising capabilities of Large Language Models (LLMs) in code
generation and transformation, we use LLMs as the primary engine for migration.
Before building the tool, we first study the capabilities of LLMs for library
migration on a benchmark of 321 real-world library migrations. We find that
LLMs can effectively perform library migration, but some post-processing steps
can further improve the performance. Based on this, we develop PyMigTool, a
command line application that combines the power of LLMs, static analysis, and
dynamic analysis to provide accurate library migration. We evaluate PyMigTool
on 717 real-world Python applications that are not from our benchmark. We find
that PyMigTool can migrate 32% of the migrations with complete correctness. Of
the remaining migrations, only 14% of the migration-related changes are left
for developers to fix for more than half of the projects.

</details>


### [13] [McMining: Automated Discovery of Misconceptions in Student Code](https://arxiv.org/abs/2510.08827)
*Erfan Al-Hossami,Razvan Bunescu*

Main category: cs.SE

TL;DR: McMining任务旨在从学生代码样本中挖掘编程错误概念，研究构建了一个可扩展的基准数据集，并证明LLM模型能有效发现学生代码中的错误概念。


<details>
  <summary>Details</summary>
Motivation: 学生在学习编程时经常形成各种错误概念，这不仅会导致bug和低效代码，还会阻碍相关概念的学习。

Method: 开发了可扩展的基准数据集，包含错误概念和大量体现这些错误概念的代码样本；引入了两种基于LLM的McMiner方法。

Result: 通过广泛评估表明，Gemini、Claude和GPT系列的模型在发现学生代码中的错误概念方面非常有效。

Conclusion: LLM模型能够有效识别学生编程中的错误概念，为编程教育提供了有价值的工具。

Abstract: When learning to code, students often develop misconceptions about various
programming language concepts. These can not only lead to bugs or inefficient
code, but also slow down the learning of related concepts. In this paper, we
introduce McMining, the task of mining programming misconceptions from samples
of code from a student. To enable the training and evaluation of McMining
systems, we develop an extensible benchmark dataset of misconceptions together
with a large set of code samples where these misconceptions are manifested. We
then introduce two LLM-based McMiner approaches and through extensive
evaluations show that models from the Gemini, Claude, and GPT families are
effective at discovering misconceptions in student code.

</details>


### [14] [Identifying Video Game Debugging Bottlenecks: An Industry Perspective](https://arxiv.org/abs/2510.08834)
*Carlos Pinto Gomez,Fabio Petrillo*

Main category: cs.SE

TL;DR: 游戏开发者在调试过程中主要时间花费在检查游戏构件（36.6%）和本地复现bug（35.1%）上，并识别了影响bug解决效率的关键调试活动。


<details>
  <summary>Details</summary>
Motivation: 传统软件调试技术在游戏开发中应用有限，需要针对游戏特点开发专门的调试技术，如屏幕控制台、调试绘制、调试相机等。

Method: 通过记录20名资深游戏开发者在处理崩溃、对象行为和对象持久性等关键bug时的调试会话，进行主题分析。

Result: 识别了瓶颈调试活动，确定了使用的调试工具，展示了不同专业角色在调试中的协作方式，技术角色在调试中处于核心地位。

Conclusion: 游戏调试需要专门的工具和技术，开发者大部分时间用于检查游戏构件和复现bug，技术角色在调试过程中发挥关键作用。

Abstract: Conventional debugging techniques used in traditional software are similarly
used when debugging video games. However, the reality of video games require
its own set of unique debugging techniques such as On-Screen Console, Debug
Draws, Debug Camera, Cheats and In-Game Menus, and Data Scrubbing. In this
article, we provide insights from a video game studio on how 20 seasoned
industry game developers debug during the production of a game. Our experiments
rely on the recordings of debugging sessions for the most critical bugs
categorized as Crashes, Object Behaviors, and Object Persistence. In this
paper, we focus on identifying the debugging activities that bottleneck bug
resolution. We also identify the debugging tools used to perform debugging
techniques. Lastly, we present how different disciplines collaborate during
debugging and how technical roles are at the core of debugging. Our thematic
analysis has identified game developers spend 36.6\% of their time inspecting
game artifacts and 35.1\% of their time reproducing the bug locally.

</details>


### [15] [Repository-Aware File Path Retrieval via Fine-Tuned LLMs](https://arxiv.org/abs/2510.08850)
*Vasudha Yanuganti,Ishaan Puri,Swapnil Chhatre,Mantinder Singh,Ashok Jallepalli,Hritvik Shrivastava,Pradeep Kumar Sharma*

Main category: cs.SE

TL;DR: 提出了一种基于LLM的文件路径检索方法，通过QLoRA和Unsloth优化微调Qwen3-8B模型，直接从自然语言查询预测相关文件路径。


<details>
  <summary>Details</summary>
Motivation: 传统代码搜索缺乏语义上下文和跨文件链接，而LLM虽然理解自然语言但缺乏仓库特定细节，需要结合两者优势。

Method: 使用六种基于AST结构和仓库内容的代码感知策略生成训练数据，通过QLoRA和Unsloth优化微调Qwen3-8B模型进行文件路径检索。

Result: 在Python项目中达到91%精确匹配和93%召回率，在PyTorch等大型代码库中达到59%召回率，明显优于单策略训练。

Conclusion: 多级代码信号帮助LLM推理跨文件上下文，展示了检索与基于LLM的代码智能集成的潜力，但大型仓库的上下文长度仍是限制因素。

Abstract: Modern codebases make it hard for developers and AI coding assistants to find
the right source files when answering questions like "How does this feature
work?" or "Where was the bug introduced?" Traditional code search (keyword or
IR based) often misses semantic context and cross file links, while large
language models (LLMs) understand natural language but lack repository specific
detail. We present a method for file path retrieval that fine tunes a strong
LLM (Qwen3-8B) with QLoRA and Unsloth optimizations to predict relevant file
paths directly from a natural language query. To build training data, we
introduce six code aware strategies that use abstract syntax tree (AST)
structure and repository content to generate realistic question-answer pairs,
where answers are sets of file paths. The strategies range from single file
prompts to hierarchical repository summaries, providing broad coverage. We fine
tune on Python projects including Flask, Click, Jinja, FastAPI, and PyTorch,
and obtain high retrieval accuracy: up to 91\% exact match and 93\% recall on
held out queries, clearly beating single strategy training. On a large codebase
like PyTorch (about 4,000 Python files), the model reaches 59\% recall, showing
scalability. We analyze how multi level code signals help the LLM reason over
cross file context and discuss dataset design, limits (for example, context
length in very large repos), and future integration of retrieval with LLM based
code intelligence.

</details>


### [16] [Vector Graph-Based Repository Understanding for Issue-Driven File Retrieval](https://arxiv.org/abs/2510.08876)
*Kostiantyn Bevziuk,Andrii Fatula,Svetozar Lashin Yaroslav Opanasenko,Anna Tukhtarova,Ashok Jallepalli Pradeepkumar Sharma,Hritvik Shrivastava*

Main category: cs.SE

TL;DR: 提出一个将大型软件仓库转换为向量化知识图谱的系统，通过捕捉语义关系和项目结构，实现仓库开发的自动化。


<details>
  <summary>Details</summary>
Motivation: 解决大型软件仓库中代码理解和开发自动化的挑战，通过结构化表示来增强语义关系的捕捉。

Method: 使用知识图谱编码语法关系（包含、实现、引用、调用、继承），并用LLM生成摘要和向量嵌入，结合语义检索和图感知扩展的混合检索管道。

Result: 系统能够有效表示仓库结构，支持自动化开发任务，通过LLM助手生成可读的解释和受限的图查询。

Conclusion: 该方法通过向量化知识图谱和混合检索，显著提升了软件仓库的理解和自动化开发能力。

Abstract: We present a repository decomposition system that converts large software
repositories into a vectorized knowledge graph which mirrors project
architectural and semantic structure, capturing semantic relationships and
allowing a significant level of automatization of further repository
development. The graph encodes syntactic relations such as containment,
implementation, references, calls, and inheritance, and augments nodes with
LLM-derived summaries and vector embeddings. A hybrid retrieval pipeline
combines semantic retrieval with graph-aware expansion, and an LLM-based
assistant formulates constrained, read-only graph requests and produces
human-oriented explanations.

</details>


### [17] [SEER: Sustainability Enhanced Engineering of Software Requirements](https://arxiv.org/abs/2510.08981)
*Mandira Roy,Novarun Deb,Nabendu Chaki,Agostino Cortesi*

Main category: cs.SE

TL;DR: 提出了SEER框架，在软件开发早期阶段处理可持续性问题，通过识别、评估和优化可持续性需求，使用LLM推理能力和代理RAG方法实现。


<details>
  <summary>Details</summary>
Motivation: 现有方法多为高层次指导，实施耗时且依赖团队适应性，而可持续性评估应从需求工程阶段开始。

Method: SEER框架三阶段：从通用分类中识别相关可持续性需求；基于识别的SR评估系统需求可持续性；优化不满足任何SR的系统需求。使用LLM推理和代理RAG方法实现。

Result: 在四个不同领域软件项目上实验，使用Gemini 2.5推理模型，结果表明该方法能准确识别跨领域的广泛可持续性问题。

Conclusion: SEER框架有效解决了软件开发早期阶段的可持续性关注，能够准确识别不同领域的可持续性问题。

Abstract: The rapid expansion of software development has significant environmental,
technical, social, and economic impacts. Achieving the United Nations
Sustainable Development Goals by 2030 compels developers to adopt sustainable
practices. Existing methods mostly offer high-level guidelines, which are
time-consuming to implement and rely on team adaptability. Moreover, they focus
on design or implementation, while sustainability assessment should start at
the requirements engineering phase. In this paper, we introduce SEER, a
framework which addresses sustainability concerns in the early software
development phase. The framework operates in three stages: (i) it identifies
sustainability requirements (SRs) relevant to a specific software product from
a general taxonomy; (ii) it evaluates how sustainable system requirements are
based on the identified SRs; and (iii) it optimizes system requirements that
fail to satisfy any SR. The framework is implemented using the reasoning
capabilities of large language models and the agentic RAG (Retrieval Augmented
Generation) approach. SEER has been experimented on four software projects from
different domains. Results generated using Gemini 2.5 reasoning model
demonstrate the effectiveness of the proposed approach in accurately
identifying a broad range of sustainability concerns across diverse domains.

</details>


### [18] [Towards a Taxonomy of Sustainability Requirements for Software Design](https://arxiv.org/abs/2510.08990)
*Mandira Roy,Novarun Deb,Nabendu Chaki,Agostino Cortesi*

Main category: cs.SE

TL;DR: 该研究通过系统文献综述构建了一个全面的可持续性需求分类法，涵盖环境、技术、社会和经济四个维度，并为每个类别提供定义、指标和度量方法，同时展示了跨维度类别间的相关性矩阵。


<details>
  <summary>Details</summary>
Motivation: 现有可持续性需求研究存在碎片化、局限于特定维度或应用领域的问题，缺乏统一的综合分类法，无法系统指导软件工程实践。

Method: 采用系统文献综述方法，从最新研究中提取和组织可持续性需求，构建跨四个维度的分类法。

Result: 开发了一个包含四个维度可持续性需求的综合分类法，包含清晰定义、相关指标和度量方法，以及展示类别间正负影响的相关性矩阵。

Conclusion: 该分类法为软件开发者和研究人员提供了系统化参考，有助于在可持续软件开发中有效制定、管理和协调需求权衡。

Abstract: Software systems are a significant contributor to global sustainability
concerns, demanding that environmental, social, technical, and economic factors
be systematically addressed from the initial requirements engineering phase.
Although existing research provides various sustainability requirements (SRs),
these contributions are often fragmented, specific to certain dimensions, or
limited to particular application domains, resulting in a critical lack of a
unified, comprehensive taxonomy for the software engineering community. To
address this gap, this research conducts a Systematic Literature Review (SLR)
to extract and organize sustainability requirements from the state-of-the-art.
The primary contribution is a comprehensive taxonomy of SRs across the four
dimensions of sustainability (environmental, technical, social, and economic).
For each identified category, we provide clear definitions, associated metrics,
and measures. Furthermore, we depict a correlation matrix that projects the
positive and negative influences (synergies and conflicts) among categories
across different dimensions. This systematized reference assists both software
developers and researchers in effectively formulating, managing, and
reconciling trade-offs within sustainable software development.

</details>


### [19] [Saving SWE-Bench: A Benchmark Mutation Approach for Realistic Agent Evaluation](https://arxiv.org/abs/2510.08996)
*Spandan Garg,Ben Steenhoek,Yufan Huang*

Main category: cs.SE

TL;DR: 提出了一个新的基准测试框架，将现有的正式基准转化为现实用户查询，发现现有基准显著高估了聊天式编程助手在真实场景中的能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于GitHub问题的基准测试无法准确反映开发者在IDE中与聊天式编程助手的真实交互方式，导致对代理能力的高估。

Method: 通过分析开发者与聊天式代理的交互模式，将正式基准转化为现实用户查询的基准测试框架，应用于多个基准数据集。

Result: 现有基准显著高估了代理能力，在公共基准上高估超过50%，在内部基准上高估约10-16%。

Conclusion: 这项工作通过基准变异技术建立了评估交互式聊天软件工程代理的新范式。

Abstract: Current benchmarks for evaluating software engineering agents, such as
SWE-Bench Verified, are predominantly derived from GitHub issues and fail to
accurately reflect how developers interact with chat-based coding assistants in
integrated development environments (IDEs). We posit that this mismatch leads
to a systematic overestimation of agent's capabilities in real-world scenarios,
especially bug fixing. We introduce a novel benchmarking framework that
transforms existing formal benchmarks into realistic user queries through
systematic analysis of developer interaction patterns with chat-based agents.
Our methodology is flexible and can be easily extended to existing benchmarks.
In this paper, we apply our testing framework to SWE-Bench Verified, the
TypeScript subset of Multi-SWE-Bench and a private benchmark, SWE-Bench C# and
transform formal GitHub issue descriptions into realistic user-style queries
based on telemetry analysis of a popular chat-based agent interactions. Our
findings reveal that existing benchmarks significantly overestimate agent
capabilities for some models by >50% over baseline performance for public
benchmarks and ~10-16% for our internal benchmark. This work establishes a new
paradigm for evaluating interactive chat-based software engineering agents
through benchmark mutation techniques.

</details>


### [20] [Cost-Efficient Long Code Translation using LLMs while Leveraging Identifier Replacements](https://arxiv.org/abs/2510.09045)
*Manojit Chakraborty,Madhusudan Ghosh,Rishabh Gupta*

Main category: cs.SE

TL;DR: 提出一种零样本代码翻译方法，通过将长标识符替换为通用占位符来减少token数量，提高长代码翻译的效率和准确性


<details>
  <summary>Details</summary>
Motivation: LLM在翻译长源代码时经常因超出上下文窗口而产生不准确的翻译结果

Method: 使用标识符替换技术，将用户给定的长标识符替换为通用占位符，使LLM专注于代码逻辑结构

Result: 实验结果表明该方法保留了语法和层次结构信息，并产生了token数量减少的翻译结果

Conclusion: 该方法通过减少token数量和内存使用，提高了长代码翻译的效率和成本效益

Abstract: In the domain of software development, LLMs have been utilized to automate
tasks such as code translation, where source code from one programming language
is translated to another while preserving its functionality. However, LLMs
often struggle with long source codes that don't fit into the context window,
which produces inaccurate translations. To address this, we propose a novel
zero-shot code translation method that incorporates identifier replacement. By
substituting user-given long identifiers with generalized placeholders during
translation, our method allows the LLM to focus on the logical structure of the
code, by reducing token count and memory usage, which improves the efficiency
and cost-effectiveness of long code translation. Our empirical results
demonstrate that our approach preserves syntactical and hierarchical
information and produces translation results with reduced tokens.

</details>


### [21] [Model-Assisted and Human-Guided: Perceptions and Practices of Software Professionals Using LLMs for Coding](https://arxiv.org/abs/2510.09058)
*Italo Santos,Cleyton Magalhaes,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 对131名软件从业者的调查显示，LLM在软件开发中被广泛用于编码任务，能提高生产力但存在输出不准确、上下文理解有限等风险，开发者主要将其视为辅助工具而非独立解决方案。


<details>
  <summary>Details</summary>
Motivation: 了解LLM在实际软件开发中的使用情况以及专业人士对其优势和局限性的看法，填补对LLM实际应用认知的空白。

Method: 通过对全球131名软件从业者进行问卷调查，收集关于LLM在软件开发中应用的第一手数据。

Result: LLM被用于各种编码任务，带来生产力提升、认知负荷减少和学习加速等好处，但也存在输出不准确、上下文意识有限和伦理风险等问题。

Conclusion: 开发者普遍将LLM视为辅助工具而非独立解决方案，体现了谨慎而实用的集成态度，为未来研究和LLM在软件工程中的负责任使用提供了重要参考。

Abstract: Large Language Models have quickly become a central component of modern
software development workflows, and software practitioners are increasingly
integrating LLMs into various stages of the software development lifecycle.
Despite the growing presence of LLMs, there is still a limited understanding of
how these tools are actually used in practice and how professionals perceive
their benefits and limitations. This paper presents preliminary findings from a
global survey of 131 software practitioners. Our results reveal how LLMs are
utilized for various coding-specific tasks. Software professionals report
benefits such as increased productivity, reduced cognitive load, and faster
learning, but also raise concerns about LLMs' inaccurate outputs, limited
context awareness, and associated ethical risks. Most developers treat LLMs as
assistive tools rather than standalone solutions, reflecting a cautious yet
practical approach to their integration. Our findings provide an early,
practitioner-focused perspective on LLM adoption, highlighting key
considerations for future research and responsible use in software engineering.

</details>


### [22] [Constraint-Guided Unit Test Generation for Machine Learning Libraries](https://arxiv.org/abs/2510.09108)
*Lukas Krodinger,Altin Hajdari,Stephan Lukasczyk,Gordon Fraser*

Main category: cs.SE

TL;DR: PynguinML通过从官方API文档中提取约束条件，改进Pynguin测试生成器，为机器学习API生成合规输入，显著提高代码覆盖率。


<details>
  <summary>Details</summary>
Motivation: 机器学习库如PyTorch和TensorFlow对输入有严格约束，现有自动化测试工具如Pynguin不了解这些约束，导致测试失败和代码覆盖率有限。

Method: 从官方API文档中提取约束条件，改进Pynguin测试生成器以利用这些约束生成合规的ML API输入。

Result: 在PyTorch和TensorFlow的165个模块上评估，PynguinML相比Pynguin显著提高测试效果，代码覆盖率最高提升63.9%。

Conclusion: 利用API文档中的约束条件可以有效改进测试生成器，为机器学习库生成更有效的测试用例。

Abstract: Machine learning (ML) libraries such as PyTorch and TensorFlow are essential
for a wide range of modern applications. Ensuring the correctness of ML
libraries through testing is crucial. However, ML APIs often impose strict
input constraints involving complex data structures such as tensors. Automated
test generation tools such as Pynguin are not aware of these constraints and
often create non-compliant inputs. This leads to early test failures and
limited code coverage. Prior work has investigated extracting constraints from
official API documentation. In this paper, we present PynguinML, an approach
that improves the Pynguin test generator to leverage these constraints to
generate compliant inputs for ML APIs, enabling more thorough testing and
higher code coverage. Our evaluation is based on 165 modules from PyTorch and
TensorFlow, comparing PynguinML against Pynguin. The results show that
PynguinML significantly improves test effectiveness, achieving up to 63.9 %
higher code coverage.

</details>


### [23] [A Semantic Framework for Patient Digital Twins in Chronic Care](https://arxiv.org/abs/2510.09134)
*Amal Elgammal,Bernd J. Krämer,Michael P. Papazoglou,Mira Raheem*

Main category: cs.SE

TL;DR: 提出患者医疗数字孪生(PMDT)框架，通过本体驱动方法整合多模态健康数据，实现个性化慢性病管理的语义互操作和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 当前数字孪生应用多为器官特异性或孤立数据类型，缺乏统一且隐私保护的框架来支持个性化慢性病管理。

Method: 基于OWL 2.0构建本体驱动的患者数字孪生框架，采用模块化蓝图结构，通过专家研讨会、问卷和真实世界免疫治疗患者试点研究进行迭代验证。

Result: 验证了本体覆盖度、推理正确性、可用性和GDPR合规性，能够统一异构数据并支持描述性、预测性和规范性分析。

Conclusion: PMDT为下一代数字健康生态系统提供了经过验证的基础，通过弥合数据碎片化和语义标准化差距，推动慢性病管理向主动、持续优化和公平的方向发展。

Abstract: Personalized chronic care requires the integration of multimodal health data
to enable precise, adaptive, and preventive decision-making. Yet most current
digital twin (DT) applications remain organ-specific or tied to isolated data
types, lacking a unified and privacy-preserving foundation. This paper
introduces the Patient Medical Digital Twin (PMDT), an ontology-driven in
silico patient framework that integrates physiological, psychosocial,
behavioral, and genomic information into a coherent, extensible model.
Implemented in OWL 2.0, the PMDT ensures semantic interoperability, supports
automated reasoning, and enables reuse across diverse clinical contexts. Its
ontology is structured around modular Blueprints (patient, disease and
diagnosis, treatment and follow-up, trajectories, safety, pathways, and adverse
events), formalized through dedicated conceptual views. These were iteratively
refined and validated through expert workshops, questionnaires, and a pilot
study in the EU H2020 QUALITOP project with real-world immunotherapy patients.
Evaluation confirmed ontology coverage, reasoning correctness, usability, and
GDPR compliance. Results demonstrate the PMDT's ability to unify heterogeneous
data, operationalize competency questions, and support descriptive, predictive,
and prescriptive analytics in a federated, privacy-preserving manner. By
bridging gaps in data fragmentation and semantic standardization, the PMDT
provides a validated foundation for next-generation digital health ecosystems,
transforming chronic care toward proactive, continuously optimized, and
equitable management.

</details>


### [24] [A Model-Driven Engineering Approach to AI-Powered Healthcare Platforms](https://arxiv.org/abs/2510.09308)
*Mira Raheem,Amal Elgammal,Michael Papazoglou,Bernd Krämer,Neamat El-Tazi*

Main category: cs.SE

TL;DR: 提出基于模型驱动工程的医疗AI框架，使用图形化领域特定语言MILA和联邦学习架构，实现多中心医疗数据协作而无需共享原始患者数据。


<details>
  <summary>Details</summary>
Motivation: 解决医疗AI应用中数据碎片化、隐私保护和系统复杂性等挑战，促进医疗AI的实际应用。

Method: 采用模型驱动工程框架，包含形式化元模型、领域特定语言和自动化转换，核心是图形化医疗互操作性语言MILA，结合联邦学习架构。

Result: 在多中心癌症免疫治疗研究中，生成的分析管道在关键任务中达到98.5%和98.3%的准确率，同时显著减少手动编码工作量。

Conclusion: 模型驱动工程原则为构建可互操作、可复现和可信赖的数字健康平台提供了实用路径。

Abstract: Artificial intelligence (AI) has the potential to transform healthcare by
supporting more accurate diagnoses and personalized treatments. However, its
adoption in practice remains constrained by fragmented data sources, strict
privacy rules, and the technical complexity of building reliable clinical
systems. To address these challenges, we introduce a model driven engineering
(MDE) framework designed specifically for healthcare AI. The framework relies
on formal metamodels, domain-specific languages (DSLs), and automated
transformations to move from high level specifications to running software. At
its core is the Medical Interoperability Language (MILA), a graphical DSL that
enables clinicians and data scientists to define queries and machine learning
pipelines using shared ontologies. When combined with a federated learning
architecture, MILA allows institutions to collaborate without exchanging raw
patient data, ensuring semantic consistency across sites while preserving
privacy. We evaluate this approach in a multi center cancer immunotherapy
study. The generated pipelines delivered strong predictive performance, with
support vector machines achieving up to 98.5 percent and 98.3 percent accuracy
in key tasks, while substantially reducing manual coding effort. These findings
suggest that MDE principles metamodeling, semantic integration, and automated
code generation can provide a practical path toward interoperable,
reproducible, and trustworthy digital health platforms.

</details>


### [25] [TIT: A Tree-Structured Instruction Tuning Approach for LLM-Based Code Translation](https://arxiv.org/abs/2510.09400)
*He Jiang,Yufu Wang,Hao Lin,Peiyu Zou,Zhide Zhou,Ang Jia,Xiaochen Li,Zhilei Ren*

Main category: cs.SE

TL;DR: TIT是一种树结构指令调优范式，通过集成语言无关的句法特征和细粒度并行数据增强，显著提升了LLM在代码翻译中的表现，减少了句法混淆。


<details>
  <summary>Details</summary>
Motivation: 主流LLM代码翻译方法存在两个关键限制：对语言特定特征敏感导致句法混淆，以及缺乏细粒度语义对齐导致语义偏差。

Method: TIT包含三个模块：句法信息表示模块集成语言无关句法特征，细粒度并行数据集增强模块通过语句级分割和对齐匹配生成高质量数据，双阶段树指令调优模块减轻LLM的上下文处理负担。

Result: 实验结果表明，该方法在多个LLM中显著优于现有方法，代码翻译成功率提高1.22-1.75倍，同时显著减少句法混淆。

Conclusion: TIT通过树结构指令调优有效解决了LLM代码翻译中的句法混淆和语义对齐问题，为代码翻译任务提供了新的解决方案。

Abstract: Large Language Models (LLMs) have shown strong performance in automated
source-to-target code translation through pretraining on extensive code
corpora. However, mainstream LLM-based code translation methods suffer from two
critical limitations. First, they are highly sensitive to language-specific
features, which often introduce source-language syntax or lexicon into the
output, leading to syntactic confusion. Second, they lack fine-grained semantic
alignment due to an over-reliance on function-level parallel datasets,
resulting in semantic misalignment between the translated code and the original
source. To overcome these limitations, we propose TIT, a Tree-structured
Instruction Tuning paradigm for LLM-based code translation. Specifically, TIT
consists of three modules. First, to mitigate syntactic confusion, the
syntactic information representation module integrates language-agnostic
syntactic features via structured parsing. Then, to generate high-quality
fine-grained parallel data, the fine-grained parallel dataset augmentation
module aligns nodes with code segments through statement-level segmentation and
contrastive matching. Finally, we leverage the dual-stage tree instruction
tuning module to alleviate the contextual processing burden on the LLM caused
by the introduction of syntactic information. The first stage employs
syntax-aware fine-tuning to enable the LLM to autonomously comprehend
structured syntactic information, while the second stage utilizes code
generation fine-tuning to guide the model in generating accurate target code
based on function-level syntactic dependencies. The experimental results
demonstrate that the proposed method significantly outperforms existing
approaches in multiple LLMs, achieving a success rate 1.22x-1.75x higher in
code translation while markedly reducing syntactic confusion.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [26] [U-Turn: Enhancing Incorrectness Analysis by Reversing Direction](https://arxiv.org/abs/2510.09292)
*Flavio Ascari,Roberto Bruni,Roberta Gori,Azalea Raad*

Main category: cs.LO

TL;DR: 本文提出了一种结合不正确性逻辑(IL)和充分不正确性逻辑(SIL)的分析方法，通过重用第一个分析的启发式选择来指导第二个分析，从而获得比单独使用任一逻辑更有效和更具信息性的程序错误检测。


<details>
  <summary>Details</summary>
Motivation: 随着各种不正确性逻辑的出现，关键问题是如何结合这些技术来提升精度、表达力、自动化和可扩展性。本文采用互补策略，将识别可达错误状态的IL与找到可能导致这些错误的输入状态的SIL相结合。

Method: 通过将第二个逻辑的证明规则与第一个逻辑的推导结果进行工具化，以归纳方式指导规则选择和应用。这是首个支持这种跨分析工具化的规则格式。

Result: 结合分析比单独使用任一逻辑更有效，能够同时揭示可达错误及其原因，有助于调试和测试。

Conclusion: 这种组合分析为将不正确性洞察嵌入到可扩展、表达力强、自动化的代码契约中开辟了新途径。

Abstract: O'Hearn's Incorrectness Logic (IL) has sparked renewed interest in static
analyses that aim to detect program errors rather than prove their absence,
thereby avoiding false alarms -- a critical factor for practical adoption in
industrial settings. As new incorrectness logics emerge to capture diverse
error-related properties, a key question arises: can the combination of
(in)correctness techniques enhance precision, expressiveness, automation, or
scalability? Notable frameworks, such as outcome logic, UNTer, local
completeness logic, and exact separation logic, unify multiple analyses within
a single proof system. In this work, we adopt a complementary strategy. Rather
than designing a unified logic, we combine IL, which identifies reachable error
states, with Sufficient Incorrectness Logic (SIL), which finds input states
potentially leading to those errors. As a result, we get a more informative and
effective analysis than either logic in isolation. Rather than naively
sequencing them, our key innovation is reusing heuristic choices from the first
analysis to steer the second. In fact, both IL and SIL rely on
under-approximation and thus their automation legitimates heuristics that avoid
exhaustive path enumeration (e.g., selective disjunct pruning, loop unrolling).
Concretely, we instrument the second logic's proof rules with derivations
coming from the first to inductively guide rule selection and application. To
our knowledge, this is the first rule format enabling such inter-analysis
instrumentation. This combined analysis aids debugging and testing by revealing
both reachable errors and their causes, and opens new avenues for embedding
incorrectness insights into (a new kind of) scalable, expressive, automated
code contracts.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [27] [Neptune: Advanced ML Operator Fusion for Locality and Parallelism on GPUs](https://arxiv.org/abs/2510.08726)
*Yifan Zhao,Egan Johnson,Prasanth Chatarasi,Vikram Adve,Sasa Misailovic*

Main category: cs.PL

TL;DR: Neptune是一个张量编译器，通过打破循环依赖并构建代数校正表达式来实现复杂归约计算的高级算子融合，在注意力机制等基准测试中显著优于现有编译器。


<details>
  <summary>Details</summary>
Motivation: 现有张量编译器难以融合涉及循环依赖的复杂归约计算（如注意力机制），限制了算子融合优化的效果。

Method: 提出Neptune编译器，通过有意打破某些现有依赖关系，并构建代数校正表达式来补偿，从而实现高级算子融合。

Result: 在10个基于注意力的基准测试中，Neptune在NVIDIA和AMD的4种GPU架构上平均比次优替代方案快1.35倍，优于Triton、TVM和FlexAttention等现有编译器。

Conclusion: Neptune通过创新的依赖关系处理和校正机制，有效解决了复杂归约计算的融合问题，显著提升了深度学习工作负载的性能。

Abstract: Operator fusion has become a key optimization for deep learning, which
combines multiple deep learning operators to improve data reuse and reduce
global memory transfers. However, existing tensor compilers struggle to fuse
complex reduction computations involving loop-carried dependencies, such as
attention mechanisms.
  The paper introduces Neptune, a tensor compiler for advanced operator fusion
for sequences of reduction operators. Neptune presents a new approach for
advanced operator fusion, which intentionally breaks some existing dependencies
and compensates by constructing algebraic correction expressions that allow the
kernel to produce the correct result.
  On ten attention-based benchmarks, Neptune, starting from simple attention
code and a high-level scheduling template, outperforms existing compilers like
Triton, TVM, and FlexAttention, including Triton-based implementations of
FlashAttention. Across four different GPU architectures from NVIDIA and AMD,
Neptune-generated kernels have average speedup of $1.35\times$ over the next
best alternative, demonstrating its effectiveness for deep learning workloads.

</details>


### [28] [Typestate via Revocable Capabilities](https://arxiv.org/abs/2510.08889)
*Songlin Jia,Craig Liu,Siyuan He,Haotian Deng,Yuyan Bao,Tiark Rompf*

Main category: cs.PL

TL;DR: 提出了一种统一基于作用域和基于流敏感状态管理的方法，通过将流不敏感的能力机制扩展到流敏感的类型状态跟踪，在Scala 3中实现表达性强且安全的状态管理。


<details>
  <summary>Details</summary>
Motivation: 解决状态资源管理中的安全性和表达性挑战，传统基于作用域的方法限制了表达性和并行性，而流敏感方法需要复杂的类型状态分析和显式状态跟踪。

Method: 扩展流不敏感能力机制为流敏感类型状态跟踪，解耦能力生命周期与词法作用域，允许函数以流敏感方式提供、撤销和返回能力，在Scala 3编译器中利用路径依赖类型和隐式解析实现。

Result: 原型系统通用支持多种状态模式，包括文件操作、高级锁定协议、DOM构建和会话类型，实现了简洁、静态安全且表达性强的类型状态编程。

Conclusion: 证明通过最小扩展现有基于能力的语言，可以实现表达性强且安全的状态管理，为更健壮和符合人体工程学的状态编程铺平道路。

Abstract: Managing stateful resources safely and expressively is a longstanding
challenge in programming languages, especially in the presence of aliasing.
While scope-based constructs such as Java's synchronized blocks offer ease of
reasoning, they restrict expressiveness and parallelism. Conversely,
imperative, flow-sensitive management enables fine-grained control but demands
sophisticated typestate analyses and often burdens programmers with explicit
state tracking.
  In this work, we present a novel approach that unifies the strengths of both
paradigms by extending flow-insensitive capability mechanisms into
flow-sensitive typestate tracking. Our system decouples capability lifetimes
from lexical scopes, allowing functions to provide, revoke, and return
capabilities in a flow-sensitive manner, based on the existing mechanisms
explored for the safety and ergonomics of scoped capability programming.
  We implement our approach as an extension to the Scala 3 compiler, leveraging
path-dependent types and implicit resolution to enable concise, statically
safe, and expressive typestate programming. Our prototype generically supports
a wide range of stateful patterns, including file operations, advanced locking
protocols, DOM construction, and session types. This work demonstrates that
expressive and safe typestate management can be achieved with minimal
extensions to existing capability-based languages, paving the way for more
robust and ergonomic stateful programming.

</details>


### [29] [Free to Move: Reachability Types with Flow-Sensitive Effects for Safe Deallocation and Ownership Transfer](https://arxiv.org/abs/2510.08939)
*Haotian Deng,Siyuan He,Songlin Jia,Yuyan Bao,Tiark Rompf*

Main category: cs.PL

TL;DR: 提出了一种基于可达性类型的流敏感效应系统，支持高阶不纯函数式语言中的显式内存管理，包括Rust风格的移动语义。


<details>
  <summary>Details</summary>
Motivation: 将基于可达性的推理与显式资源控制相结合，推进高阶函数式语言中安全手动内存管理的技术水平。

Method: 使用多态的use和kill效应来记录引用如何被读取、写入、转移和释放，通过限定符跟踪每个资源上的操作。

Result: 系统能够表达所有权转移、上下文新鲜度和破坏性更新，无需区域或线性性，并验证了释放后使用安全性。

Conclusion: 该研究成功地将可达性推理与显式资源控制集成，为高阶函数式语言提供了安全的手动内存管理方案。

Abstract: We present a flow-sensitive effect system for reachability types that
supports explicit memory management, including Rust-style move semantics, in
higher-order impure functional languages. Our system refines the existing
reachability qualifier with polymorphic \emph{use} and \emph{kill} effects that
record how references are read, written, transferred, and deallocated. The
effect discipline tracks operations performed on each resource using
qualifiers, enabling the type system to express ownership transfer, contextual
freshness, and destructive updates without regions or linearity. We formalize
the calculus, its typing and effect rules, and a compositional operational
semantics that validates use-after-free safety. All metatheoretic results,
including preservation, progress, and effect soundness, are mechanized. The
system models idioms such as reference deallocation, move semantics, reference
swapping, while exposing precise safety guarantee. Together, these
contributions integrate reachability-based reasoning with explicit resource
control, advancing the state of the art in safe manual memory management for
higher-order functional languages.

</details>


### [30] [Concept-Based Generic Programming in C++](https://arxiv.org/abs/2510.08969)
*Bjarne Stroustrup*

Main category: cs.PL

TL;DR: 介绍C++概念编程技术，展示如何通过概念约束泛型代码，提供类型系统避免窄化转换和范围检查，强调概念在泛型编程中的实用性和基本原理。


<details>
  <summary>Details</summary>
Motivation: 展示C++概念编程的设施和原理，说明概念如何表达泛型代码约束，提供用户自定义类型系统扩展，消除窄化转换和范围检查的开销。

Method: 使用概念编程技术，通过简单类型系统实现约束，利用C++26静态反射、lambda表达式、可变参数模板等特性支持泛型编程。

Result: 成功展示了概念编程的实用性，提供了无额外符号或运行时开销的类型安全机制，概念成为C++泛型编程的有机组成部分。

Conclusion: 概念是C++泛型编程的关键设施，能够有效约束代码并提供类型安全，同时支持面向对象编程模式，是C++语言设计的重要演进。

Abstract: We present programming techniques to illustrate the facilities and principles
of C++ generic programming using concepts. Concepts are C++'s way to express
constraints on generic code. As an initial example, we provide a simple type
system that eliminates narrowing conversions and provides range checking
without unnecessary notational or run-time overheads. Concepts are used
throughout to provide user-defined extensions to the type system. The aim is to
show their utility and the fundamental ideas behind them, rather than to
provide a detailed or complete explanation of C++'s language support for
generic programming or the extensive support provided by the standard library.
Generic programming is an integral part of C++, rather than an isolated
sub-language. In particular, key facilities support general programming as well
as generic programming (e.g., uniform notation for types, lambdas, variadic
templates, and C++26 static reflection). Finally, we give design rationales and
origins for key parts of the concept design, including use patterns, the
relationship to Object-Oriented Programming, value arguments, notation, concept
type-matching, and definition checking.

</details>


### [31] [A Multilingual Python Programming Language](https://arxiv.org/abs/2510.09591)
*Saad Ahmed Bazaz,Mirza Omer Beg*

Main category: cs.PL

TL;DR: 提出了一个名为UniversalPython的语言转译器，允许用户使用自己的母语编写Python代码，解决了编程语言对英语知识的依赖问题。


<details>
  <summary>Details</summary>
Motivation: 现有编程语言都要求英语知识，这对没有时间或资源学习英语的新手构成了主要障碍。研究表明人们用母语学习效果更好。

Method: 构建了一个基于Python的语言转译器，能够将用不同人类语言编写的代码转换为标准Python代码，并以乌尔都语Python为例进行了演示。

Result: 成功创建了"乌尔都语Python"，证明了该转译器的可行性。源代码已开源。

Conclusion: 该转译器有潜力扩展到更多人类语言，从而增加编程的可及性。

Abstract: All widely used and useful programming languages have a common problem. They
restrict entry on the basis of knowledge of the English language. The lack of
knowledge of English poses a major hurdle to many newcomers who do not have the
resources, in terms of time and money, to learn the English language. Studies
show that people learn better in their own language. Therefore, we propose a
language transpiler built on top of the Python programming language, called
UniversalPython, which allows one to write Python in their own human language.
We demonstrate the ability to create an "Urdu Python" with this transpiler. In
the future, we aim to scale the language to encapsulate more human languages to
increase the availability of programming. The source code for this transpiler
is open-source, and available at
https://github.com/universalpython/universalpython

</details>
