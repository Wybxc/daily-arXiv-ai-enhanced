<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 6]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.SE](#cs.SE) [Total: 16]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Prompt Decorators: A Declarative and Composable Syntax for Reasoning, Formatting, and Control in LLMs](https://arxiv.org/abs/2510.19850)
*Mostapha Kalami Heris*

Main category: cs.PL

TL;DR: 提出了Prompt Decorators框架，通过声明式、可组合的控制令牌来规范LLM行为，实现推理风格、表达方式等行为维度的可控调节，而无需改变任务内容。


<details>
  <summary>Details</summary>
Motivation: 传统提示工程依赖冗长的自然语言指令，限制了可重现性、模块化和可解释性。用户缺乏对LLM推理和输出表达方式的一致性控制。

Method: 定义了20个核心装饰器，分为认知与生成、表达与系统两大功能族，每个装饰器通过紧凑的控制令牌（如+++Reasoning、+++Tone）修改特定行为维度。提供统一语法、作用域模型和确定性处理流水线。

Result: 演示用例显示提高了推理透明度、减少了提示复杂性，并在不同领域实现了标准化的模型行为。

Conclusion: 通过将任务意图与执行行为解耦，Prompt Decorators创建了可重用和可解释的提示设计接口，对互操作性、行为一致性以及可扩展AI系统的声明式接口开发具有重要意义。

Abstract: Large Language Models (LLMs) are central to reasoning, writing, and
decision-support workflows, yet users lack consistent control over how they
reason and express outputs. Conventional prompt engineering relies on verbose
natural-language instructions, limiting reproducibility, modularity, and
interpretability. This paper introduces Prompt Decorators, a declarative,
composable syntax that governs LLM behavior through compact control tokens such
as +++Reasoning, +++Tone(style=formal), and +++Import(topic="Systems
Thinking"). Each decorator modifies a behavioral dimension, such as reasoning
style, structure, or tone, without changing task content. The framework
formalizes twenty core decorators organized into two functional families
(Cognitive & Generative and Expressive & Systemic), each further decomposed
into subcategories that govern reasoning, interaction, expression, and
session-control. It defines a unified syntax, scoping model, and deterministic
processing pipeline enabling predictable and auditable behavior composition. By
decoupling task intent from execution behavior, Prompt Decorators create a
reusable and interpretable interface for prompt design. Illustrative use cases
demonstrate improved reasoning transparency, reduced prompt complexity, and
standardized model behavior across domains. The paper concludes with
implications for interoperability, behavioral consistency, and the development
of declarative interfaces for scalable AI systems.

</details>


### [2] [A Specification's Realm: Characterizing the Knowledge Required for Executing a Given Algorithm Specification](https://arxiv.org/abs/2510.19853)
*Assaf Marron,David Harel*

Main category: cs.PL

TL;DR: 本文提出了算法规范的"领域"概念，即执行算法所需的前提知识集合，包括语法语义、领域知识、实体关系、因果规则和操作指令等，这些知识可整理成实用文档并部分自动化生成。


<details>
  <summary>Details</summary>
Motivation: 为了使自然语言或伪代码的算法规范能够被机械执行，需要明确执行代理所需的前提知识，这些知识应该独立于具体系统实现。

Method: 提出算法规范"领域"的概念，将其定义为执行算法所需的前提知识集合，包括语法语义、领域知识、实体关系、因果规则和操作指令等，并探讨如何系统化分析和生成这些知识文档。

Result: 建立了算法规范领域的初始特征描述，表明这些前提知识可以整理成实用大小的文档，且部分生成过程可通过大语言模型和现有文档重用实现自动化。

Conclusion: 算法规范领域的概念有助于算法在不同系统中的方法化实现和形式化验证，同时为评估执行忠实度（而非正确性）提供了理论基础。

Abstract: An algorithm specification in natural language or pseudocode is expected to
be clear and explicit enough to enable mechanical execution. In this position
paper we contribute an initial characterization of the knowledge that an
executing agent, human or machine, should possess in order to be able to carry
out the instructions of a given algorithm specification as a stand-alone
entity, independent of any system implementation. We argue that, for that
algorithm specification, such prerequisite knowledge, whether unique or shared
with other specifications, can be summarized in a document of practical size.
We term this document the realm of the algorithm specification. The generation
of such a realm is itself a systematic analytical process, significant parts of
which can be automated with the help of large language models and the reuse of
existing documents. The algorithm-specification's realm would consist of
specification language syntax and semantics, domain knowledge restricted to the
referenced entities, inter-entity relationships, relevant underlying
cause-and-effect rules, and detailed instructions and means for carrying out
certain operations. Such characterization of the realm can contribute to
methodological implementation of the algorithm specification in diverse systems
and to its formalization for mechanical verification. The paper also touches
upon the question of assessing execution faithfulness, which is distinct from
correctness: in the absence of a reference interpretation of natural language
or pseudocode specification with a given vocabulary, how can we determine if an
observed agent's execution indeed complies with the input specification.

</details>


### [3] [Deconstructed Proto-Quipper: A Rational Reconstruction](https://arxiv.org/abs/2510.20018)
*Ryan Kavanagh,Chuta Sano,Brigitte Pientka*

Main category: cs.PL

TL;DR: Proto-Quipper-A是对Proto-Quipper量子编程语言家族的重构，使用线性λ演算和伴随逻辑基础来简化量子电路的静态生成和推理。


<details>
  <summary>Details</summary>
Motivation: Proto-Quipper语言具有复杂的操作语义，依赖于集合论操作和新鲜名称生成来操纵量子电路，这使得使用标准编程语言技术进行推理和机械化变得困难。

Method: 使用线性λ演算描述量子电路，其范式与盒线电路图紧密对应。通过伴随逻辑基础将电路语言与线性/非线性函数语言集成，重构Proto-Quipper的电路编程抽象。

Result: Proto-Quipper-A具有简单的按值调用归约语义，并且被证明是规范化的。使用标准逻辑关系证明了线性和子结构系统的规范化。

Conclusion: Proto-Quipper-A为Proto-Quipper语言提供了一个更易处理的基础，避免了现有线性逻辑关系的固有复杂性。

Abstract: The Proto-Quipper family of programming languages aims to provide a formal
foundation for the Quipper quantum programming language. Unfortunately,
Proto-Quipper languages have complex operational semantics: they are inherently
effectful, and they rely on set-theoretic operations and fresh name generation
to manipulate quantum circuits. This makes them difficult to reason about using
standard programming language techniques and, ultimately, to mechanize. We
introduce Proto-Quipper-A, a rational reconstruction of Proto-Quipper languages
for static circuit generation. It uses a linear $\lambda$-calculus to describe
quantum circuits with normal forms that closely correspond to box-and-wire
circuit diagrams. Adjoint-logical foundations integrate this circuit language
with a linear/non-linear functional language and let us reconstruct
Proto-Quipper's circuit programming abstractions using more primitive
adjoint-logical operations. Proto-Quipper-A enjoys a simple call-by-value
reduction semantics, and to illustrate its tractability as a foundation for
Proto-Quipper languages, we show that it is normalizing. We show how to use
standard logical relations to prove normalization of linear and substructural
systems, thereby avoiding the inherent complexity of existing linear logical
relations.

</details>


### [4] [Deciding not to Decide: Sound and Complete Effect Inference in the Presence of Higher-Rank Polymorphism](https://arxiv.org/abs/2510.20532)
*Patrycja Balik,Szymon Jędras,Piotr Polesiuk*

Main category: cs.PL

TL;DR: 提出了一种用于类型和效应系统的效应推断算法，该系统包含子类型、高阶多态性和直观的集合式效应语义。通过将效应约束转换为命题逻辑公式来处理高阶多态性的作用域问题。


<details>
  <summary>Details</summary>
Motivation: 类型和效应系统尚未广泛采用，因为现有推断算法在表达能力、直观性和可判定性之间做出妥协。需要一种更复杂但实用的推断算法。

Method: 使用命题逻辑公式延迟求解效应约束，以处理高阶多态性的作用域问题。算法支持子类型、高阶多态性和集合式效应语义。

Result: 证明了算法相对于声明式类型和效应系统的健全性和完备性。已在Rocq证明助手中形式化，并在实际编程语言中成功实现。

Conclusion: 提出的效应推断算法在保持表达能力的同时解决了高阶多态性的作用域问题，为类型和效应系统的实际应用提供了可行方案。

Abstract: Type-and-effect systems help the programmer to organize data and
computational effects in a program. While for traditional type systems
expressive variants with sophisticated inference algorithms have been developed
and widely used in programming languages, type-and-effect systems did not yet
gain widespread adoption. One reason for this is that type-and-effect systems
are more complex and the existing inference algorithms make compromises between
expressiveness, intuitiveness, and decidability. In this work, we present an
effect inference algorithm for a type-and-effect system with subtyping,
expressive higher-rank polymorphism, and intuitive set-like semantics of
effects. In order to deal with scoping issues of higher-rank polymorphism, we
delay solving of effect constraints by transforming them into formulae of
propositional logic. We prove soundness and completeness of our algorithm with
respect to a declarative type-and-effect system. All the presented results have
been formalized in the Rocq proof assistant, and the algorithm has been
successfully implemented in a realistic programming language.

</details>


### [5] [Compiling the Mimosa programming language to RTOS tasks](https://arxiv.org/abs/2510.20547)
*Nikolaus Huber,Susanne Graf,Philipp Rümmer,Wang Yi*

Main category: cs.PL

TL;DR: 提出了Mimosa编程语言的编译方案，基于MIMOS计算模型，将嵌入式系统软件描述为通过FIFO队列通信的时间触发进程集合


<details>
  <summary>Details</summary>
Motivation: 为Mimosa语言开发正式的编译方案，将协调层映射到实时操作系统原语

Method: 采用Lustre编译方案的适配版本，针对Mimosa语义进行形式化描述

Result: 成功展示了如何将协调层映射到实时操作系统原语

Conclusion: 该编译方案为Mimosa语言提供了从高级描述到实时系统实现的完整路径

Abstract: This paper introduces a compilation scheme for programs written in the Mimosa
programming language, which builds upon the MIMOS model of computation. Mimosa
describes embedded systems software as a collection of time-triggered processes
which communicate through FIFO queues. We formally describe an adaptation of
the Lustre compilation scheme to the semantics of Mimosa and show how the
coordination layer can be mapped to real-time operating system primitives.

</details>


### [6] [SafeFFI: Efficient Sanitization at the Boundary Between Safe and Unsafe Code in Rust and Mixed-Language Applications](https://arxiv.org/abs/2510.20688)
*Oliver Braunsdorf,Tim Lange,Konrad Hohentanner,Julian Horsch,Johannes Kinder*

Main category: cs.PL

TL;DR: SafeFFI是一个优化Rust二进制文件中内存安全检测的系统，通过在unsafe和safe代码边界处进行检查，将内存安全执行从sanitizer转移到Rust类型系统，显著减少检查次数并提升性能。


<details>
  <summary>Details</summary>
Motivation: Unsafe Rust代码在与C/C++库互操作和实现底层数据结构时是必要的，但可能导致内存安全违规。现有sanitizer会引入许多不必要的检查，即使是对Rust类型系统保证安全的内存访问。

Method: SafeFFI系统优化内存安全检测，在unsafe和safe代码边界进行检查，将内存安全执行从sanitizer转移到Rust类型系统，避免昂贵的全程序分析。

Result: 与现有方法相比，SafeFFI编译时开销更小（2.64倍对比8.83倍以上），在流行Rust crate和已知易受攻击代码上减少sanitizer检查达98%，同时保持正确性并标记所有空间和时间内存安全违规。

Conclusion: SafeFFI在保持内存安全的同时显著提升了性能，为Rust程序的内存安全检测提供了更高效的解决方案。

Abstract: Unsafe Rust code is necessary for interoperability with C/C++ libraries and
implementing low-level data structures, but it can cause memory safety
violations in otherwise memory-safe Rust programs. Sanitizers can catch such
memory errors at runtime, but introduce many unnecessary checks even for memory
accesses guaranteed safe by the Rust type system. We introduce SafeFFI, a
system for optimizing memory safety instrumentation in Rust binaries such that
checks occur at the boundary between unsafe and safe code, handing over the
enforcement of memory safety from the sanitizer to the Rust type system. Unlike
previous approaches, our design avoids expensive whole-program analysis and
adds much less compile-time overhead (2.64x compared to over 8.83x). On a
collection of popular Rust crates and known vulnerable Rust code, SafeFFI
achieves superior performance compared to state-of-the-art systems, reducing
sanitizer checks by up to 98%, while maintaining correctness and flagging all
spatial and temporal memory safety violations.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [7] [Resource-Aware Hybrid Quantum Programming with General Recursion and Quantum Control](https://arxiv.org/abs/2510.20452)
*Kostia Chardonnet,Emmanuel Hainry,Romain Péchoux,Thomas Vinet*

Main category: cs.LO

TL;DR: 本文介绍了具有通用递归的混合量子语言Hyrql，旨在进行资源分析。该语言无需指定初始量子门集合，适合通用成本分析。通过将量子程序编译为简单类型项重写系统，可重用现有技术分析复杂度。


<details>
  <summary>Details</summary>
Motivation: 不同量子门集合的语言会导致量子电路表示复杂度不同，需要一种不依赖特定量子门集合的通用资源分析方法。

Method: 设计Hyrql混合量子语言，提供语义保持的编译算法将量子程序转换为简单类型项重写系统。

Result: 证明了该方法的多功能性，通过多个示例展示了其有效性。

Conclusion: 提出的方法能够对量子程序进行通用的资源分析，不依赖于特定的量子门集合。

Abstract: This paper introduces the hybrid quantum language with general recursion
$\mathtt{Hyrql}$, driven towards resource-analysis. By design, $\mathtt{Hyrql}$
does not require the specification of an initial set of quantum gates and,
hence, is well amenable towards a generic cost analysis. Indeed, languages
using different sets of quantum gates lead to representations of quantum
circuits whose complexity varies. Towards resource-analysis, a
semantics-preserving compilation algorithm to simply-typed term rewrite systems
is described; allowing a generic reuse of all known techniques for analyzing
the complexity of term rewrite systems. We prove the versatility of this
approach through many examples.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [8] [E-Test: E'er-Improving Test Suites](https://arxiv.org/abs/2510.19860)
*Ketai Qiu,Luca Di Grazia,Leonardo Mariani,Mauro Pezzè*

Main category: cs.SE

TL;DR: E-Test是一种利用大语言模型从生产环境中识别未测试执行场景并生成相应测试用例的方法，显著提升测试套件的覆盖率和质量。


<details>
  <summary>Details</summary>
Motivation: 测试套件天然不完美，需要不断补充新测试用例来提高软件可靠性。但发现现有测试套件未覆盖的执行场景非常困难且耗时，特别是在管理大型测试套件时。

Method: E-Test利用大语言模型从大规模生产场景中识别当前测试套件未充分覆盖的执行场景，并生成新的测试用例来增强测试套件。

Result: 在1,975个生产场景数据集上的评估显示，E-Test的F1分数达到0.55，显著优于现有回归测试方法(0.34)和普通大语言模型(0.39)。

Conclusion: E-Test通过有效定位未测试执行场景，显著提升了测试套件的增强效果，减少了维护测试套件所需的手动工作量。

Abstract: Test suites are inherently imperfect, and testers can always enrich a suite
with new test cases that improve its quality and, consequently, the reliability
of the target software system. However, finding test cases that explore
execution scenarios beyond the scope of an existing suite can be extremely
challenging and labor-intensive, particularly when managing large test suites
over extended periods.
  In this paper, we propose E-Test, an approach that reduces the gap between
the execution space explored with a test suite and the executions experienced
after testing by augmenting the test suite with test cases that explore
execution scenarios that emerge in production. E-Test (i) identifies executions
that have not yet been tested from large sets of scenarios, such as those
monitored during intensive production usage, and (ii) generates new test cases
that enhance the test suite. E-Test leverages Large Language Models (LLMs) to
pinpoint scenarios that the current test suite does not adequately cover, and
augments the suite with test cases that execute these scenarios.
  Our evaluation on a dataset of 1,975 scenarios, collected from highly-starred
open-source Java projects already in production and Defects4J, demonstrates
that E-Test retrieves not-yet-tested execution scenarios significantly better
than state-of-the-art approaches. While existing regression testing and field
testing approaches for this task achieve a maximum F1-score of 0.34, and
vanilla LLMs achieve a maximum F1-score of 0.39, E-Test reaches 0.55. These
results highlight the impact of E-Test in enhancing test suites by effectively
targeting not-yet-tested execution scenarios and reducing manual effort
required for maintaining test suites.

</details>


### [9] [Exploring Large Language Models for Access Control Policy Synthesis and Summarization](https://arxiv.org/abs/2510.20692)
*Adarsh Vatsa,Bethel Hall,William Eiers*

Main category: cs.SE

TL;DR: 本文探索了大型语言模型在访问控制策略生成和总结中的应用效果，发现LLMs在策略生成方面存在权限问题，但在与符号方法结合分析现有策略时表现出潜力。


<details>
  <summary>Details</summary>
Motivation: 云计算的普及使得访问控制策略日益复杂，手动编写容易出错且难以精确分析。LLMs在代码合成和总结方面的成功表明它们可能用于自动生成访问控制策略或帮助理解现有策略。

Method: 首先研究不同LLMs进行访问控制策略合成，然后引入基于语义的请求总结方法，利用LLMs生成策略允许请求的精确特征描述。

Result: LLMs能有效生成语法正确的策略，但存在权限问题：非推理LLMs生成符合规格的策略占45.8%，推理LLMs占93.7%。在策略分析方面，LLMs与符号方法结合显示出良好效果。

Conclusion: 虽然利用LLMs进行自动策略生成存在显著障碍，但LLMs与符号方法结合在分析现有策略方面显示出有前景的结果。

Abstract: Cloud computing is ubiquitous, with a growing number of services being hosted
on the cloud every day. Typical cloud compute systems allow administrators to
write policies implementing access control rules which specify how access to
private data is governed. These policies must be manually written, and due to
their complexity can often be error prone. Moreover, existing policies often
implement complex access control specifications and thus can be difficult to
precisely analyze in determining their behavior works exactly as intended.
Recently, Large Language Models (LLMs) have shown great success in automated
code synthesis and summarization. Given this success, they could potentially be
used for automatically generating access control policies or aid in
understanding existing policies. In this paper, we explore the effectiveness of
LLMs for access control policy synthesis and summarization. Specifically, we
first investigate diverse LLMs for access control policy synthesis, finding
that: although LLMs can effectively generate syntactically correct policies,
they have permissiveness issues, generating policies equivalent to the given
specification 45.8% of the time for non-reasoning LLMs, and 93.7% of the time
for reasoning LLMs. We then investigate how LLMs can be used to analyze
policies by introducing a novel semantic-based request summarization approach
which leverages LLMs to generate a precise characterization of the requests
allowed by a policy. Our results show that while there are significant hurdles
in leveraging LLMs for automated policy generation, LLMs show promising results
when combined with symbolic approaches in analyzing existing policies.

</details>


### [10] [SODBench: A Large Language Model Approach to Documenting Spreadsheet Operations](https://arxiv.org/abs/2510.19864)
*Amila Indika,Igor Molybog*

Main category: cs.SE

TL;DR: 提出了电子表格操作文档化(SOD)任务，通过AI将电子表格操作代码转换为自然语言解释，并构建了包含111个代码片段的基准数据集，评估了5个LLM模型的性能。


<details>
  <summary>Details</summary>
Motivation: 电子表格在商业、会计和金融领域广泛使用，但缺乏系统化文档方法，阻碍了自动化、协作和知识传递，导致重要机构知识流失的风险。

Method: 构建包含111个电子表格操作代码片段及其对应自然语言摘要的基准数据集，使用BLEU、GLEU、ROUGE-L和METEOR指标评估GPT-4o、GPT-4o-mini、LLaMA-3.3-70B、Mixtral-8x7B和Gemma2-9B五个LLM模型。

Result: 研究发现LLM能够生成准确的电子表格文档，使SOD成为增强电子表格可重现性、可维护性和协作工作流程的可行前提步骤。

Conclusion: LLM可以有效地生成电子表格操作文档，但仍有需要解决的挑战，SOD为实现更好的电子表格工作流程提供了可行路径。

Abstract: Numerous knowledge workers utilize spreadsheets in business, accounting, and
finance. However, a lack of systematic documentation methods for spreadsheets
hinders automation, collaboration, and knowledge transfer, which risks the loss
of crucial institutional knowledge. This paper introduces Spreadsheet
Operations Documentation (SOD), an AI task that involves generating
human-readable explanations from spreadsheet operations. Many previous studies
have utilized Large Language Models (LLMs) for generating spreadsheet
manipulation code; however, translating that code into natural language for SOD
is a less-explored area. To address this, we present a benchmark of 111
spreadsheet manipulation code snippets, each paired with a corresponding
natural language summary. We evaluate five LLMs, GPT-4o, GPT-4o-mini,
LLaMA-3.3-70B, Mixtral-8x7B, and Gemma2-9B, using BLEU, GLEU, ROUGE-L, and
METEOR metrics. Our findings suggest that LLMs can generate accurate
spreadsheet documentation, making SOD a feasible prerequisite step toward
enhancing reproducibility, maintainability, and collaborative workflows in
spreadsheets, although there are challenges that need to be addressed.

</details>


### [11] [Knowledge-Guided Multi-Agent Framework for Application-Level Software Code Generation](https://arxiv.org/abs/2510.19868)
*Qian Xiong,Bo Yang,Weisong Sun,Yiran Zhang,Tianlin Li,Yang Liu,Zhi Jin*

Main category: cs.SE

TL;DR: 提出了KGACG框架，通过多智能体协作将软件需求规范和架构设计文档转换为可执行代码，解决复杂应用级软件代码生成的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法在大型应用级软件代码生成中表现不足，难以确保项目代码的合理组织结构，也难以维护代码生成过程。

Method: 采用知识引导的多智能体框架，包含代码组织与规划智能体(COPA)、编码智能体(CA)和测试智能体(TA)，通过反馈机制形成协作闭环。

Result: 通过Java坦克大战游戏案例展示了KGACG框架中智能体的协作过程，同时识别了面临的挑战。

Conclusion: KGACG致力于推进应用级软件开发自动化进程。

Abstract: Automated code generation driven by Large Lan- guage Models (LLMs) has
enhanced development efficiency, yet generating complex application-level
software code remains challenging. Multi-agent frameworks show potential, but
existing methods perform inadequately in large-scale application-level software
code generation, failing to ensure reasonable orga- nizational structures of
project code and making it difficult to maintain the code generation process.
To address this, this paper envisions a Knowledge-Guided Application-Level Code
Generation framework named KGACG, which aims to trans- form software
requirements specification and architectural design document into executable
code through a collaborative closed- loop of the Code Organization & Planning
Agent (COPA), Coding Agent (CA), and Testing Agent (TA), combined with a
feedback mechanism. We demonstrate the collaborative process of the agents in
KGACG in a Java Tank Battle game case study while facing challenges. KGACG is
dedicated to advancing the automation of application-level software
development.

</details>


### [12] [BugPilot: Complex Bug Generation for Efficient Learning of SWE Skills](https://arxiv.org/abs/2510.19898)
*Atharv Sonwane,Isadora White,Hyunji Lee,Matheus Pereira,Lucas Caccia,Minseon Kim,Zhengyan Shi,Chinmay Singh,Alessandro Sordoni,Marc-Alexandre Côté,Xingdi Yuan*

Main category: cs.SE

TL;DR: 提出一种通过让软件工程代理在代码库中添加功能时无意中破坏测试来生成高质量、多样化bug的新方法，相比故意生成bug的方法更接近真实开发过程。


<details>
  <summary>Details</summary>
Motivation: 高质量bug对于训练下一代基于语言模型的软件工程代理至关重要，现有方法通过故意生成bug会产生分布外效应，不能反映真实的开发过程。

Method: 指导软件工程代理在代码库中引入新功能，在此过程中可能无意间破坏测试，从而产生bug。这种方法更接近人类开发者的编辑模式。

Result: 该方法生成的bug数据在监督微调中表现更高效，仅用1200个bug就比其他数据集的3000个bug性能提升2%。训练出的FrogBoss模型在SWE-bench Verified上达到54.6%的pass@1，FrogMini达到45.3%。

Conclusion: 通过模拟真实开发过程生成bug的方法能产生更高质量的训练数据，显著提升软件工程代理的性能，为下一代语言模型代理的训练提供了有效途径。

Abstract: High quality bugs are key to training the next generation of language model
based software engineering (SWE) agents. We introduce a novel method for
synthetic generation of difficult and diverse bugs. Our method instructs SWE
Agents to introduce a feature into the codebase whereby they may
unintentionally break tests, resulting in bugs. Prior approaches often induce
an out-of-distribution effect by generating bugs intentionally (e.g. by
introducing local perturbation to existing code), which does not reflect
realistic development processes. We perform qualitative analysis to demonstrate
that our approach for generating bugs more closely reflects the patterns found
in human-authored edits. Through extensive experiments, we demonstrate that our
bugs provide more efficient training data for supervised fine-tuning,
outperforming other bug datasets by 2% with half the training data (1.2k vs. 3k
bugs). We train on our newly generated bugs in addition to existing bug
datasets to get FrogBoss a state-of-the-art 32B parameter model on SWE-bench
Verified with a pass@1 of 54.6% and FrogMini a state-of-the-art 14B model on
SWE-bench Verified with a pass@1 of 45.3% on SWE-bench Verified averaged over
three seeds.

</details>


### [13] [On Interaction Effects in Greybox Fuzzing](https://arxiv.org/abs/2510.19984)
*Konstantinos Kitsios,Marcel Böhme,Alberto Bacchelli*

Main category: cs.SE

TL;DR: MuoFuzz是一个灰盒模糊测试工具，通过学习和选择最有前景的变异器序列来提高测试效率。相比固定概率选择变异器的AFL++和单独优化变异器概率的MOPT，MuoFuzz在代码覆盖率和漏洞发现方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究发现变异器应用于种子输入的顺序对灰盒模糊测试器的有效性有重要影响。实验证实了变异器对之间存在交互效应，这为通过选择更有前景的变异器序列来提高模糊测试效率提供了可能。

Method: MuoFuzz学习下一个变异器产生有趣输入的条件概率（基于之前选择的变异器），然后使用随机游走从学习到的概率中采样生成变异器序列。

Result: 在FuzzBench和MAGMA基准测试中，MuoFuzz实现了最高的代码覆盖率，并发现了AFL++遗漏的4个漏洞以及AFL++和MOPT都遗漏的1个漏洞。

Conclusion: 通过学习变异器序列的条件概率，MuoFuzz能够更有效地选择变异器顺序，从而在模糊测试中取得更好的覆盖率和漏洞发现能力。

Abstract: A greybox fuzzer is an automated software testing tool that generates new
test inputs by applying randomly chosen mutators (e.g., flipping a bit or
deleting a block of bytes) to a seed input in random order and adds all
coverage-increasing inputs to the corpus of seeds. We hypothesize that the
order in which mutators are applied to a seed input has an impact on the
effectiveness of greybox fuzzers. In our experiments, we fit a linear model to
a dataset that contains the effectiveness of all possible mutator pairs and
indeed observe the conjectured interaction effect. This points us to more
efficient fuzzing by choosing the most promising mutator sequence with a higher
likelihood. We propose MuoFuzz, a greybox fuzzer that learns and chooses the
most promising mutator sequences. MuoFuzz learns the conditional probability
that the next mutator will yield an interesting input, given the previously
selected mutator. Then, it samples from the learned probability using a random
walk to generate mutator sequences. We compare the performance of MuoFuzz to
AFL++, which uses a fixed selection probability, and MOPT, which optimizes the
selection probability of each mutator in isolation. Experimental results on the
FuzzBench and MAGMA benchmarks show that MuoFuzz achieves the highest code
coverage and finds four bugs missed by AFL++ and one missed by both AFL++ and
MOPT.

</details>


### [14] [A Framework for the Adoption and Integration of Generative AI in Midsize Organizations and Enterprises (FAIGMOE)](https://arxiv.org/abs/2510.19997)
*Abraham Itzhak Weinberg*

Main category: cs.SE

TL;DR: 本文提出了FAIGMOE框架，专门针对中型组织和大型企业的生成式AI采用挑战，整合了技术采纳理论、组织变革管理和创新扩散视角。


<details>
  <summary>Details</summary>
Motivation: 现有技术采纳框架（如TAM、TOE、DOI）缺乏针对生成式AI在不同规模组织中实施的具体性，无法解决中型组织的资源限制和大型企业的组织复杂性等独特挑战。

Method: 开发FAIGMOE概念框架，包含四个相互关联的阶段：战略评估、规划与用例开发、实施与集成、运营与优化，每个阶段提供可扩展的指导。

Result: FAIGMOE是首个全面解决中型和大型企业生成式AI采用的概念框架，提供了可操作的实施方案、评估工具和治理模板。

Conclusion: 该框架填补了生成式AI采纳文献的关键空白，但需要通过未来研究进行实证验证。

Abstract: Generative Artificial Intelligence (GenAI) presents transformative
opportunities for organizations, yet both midsize organizations and larger
enterprises face distinctive adoption challenges. Midsize organizations
encounter resource constraints and limited AI expertise, while enterprises
struggle with organizational complexity and coordination challenges. Existing
technology adoption frameworks, including TAM (Technology Acceptance Model),
TOE (Technology Organization Environment), and DOI (Diffusion of Innovations)
theory, lack the specificity required for GenAI implementation across these
diverse contexts, creating a critical gap in adoption literature. This paper
introduces FAIGMOE (Framework for the Adoption and Integration of Generative AI
in Midsize Organizations and Enterprises), a conceptual framework addressing
the unique needs of both organizational types. FAIGMOE synthesizes technology
adoption theory, organizational change management, and innovation diffusion
perspectives into four interconnected phases: Strategic Assessment, Planning
and Use Case Development, Implementation and Integration, and
Operationalization and Optimization. Each phase provides scalable guidance on
readiness assessment, strategic alignment, risk governance, technical
architecture, and change management adaptable to organizational scale and
complexity. The framework incorporates GenAI specific considerations including
prompt engineering, model orchestration, and hallucination management that
distinguish it from generic technology adoption frameworks. As a perspective
contribution, FAIGMOE provides the first comprehensive conceptual framework
explicitly addressing GenAI adoption across midsize and enterprise
organizations, offering actionable implementation protocols, assessment
instruments, and governance templates requiring empirical validation through
future research.

</details>


### [15] [The Cost of Downgrading Build Systems: A Case Study of Kubernetes](https://arxiv.org/abs/2510.20041)
*Gareema Ranjan,Mahmoud Alfadel,Gengyi Sun,Shane McIntosh*

Main category: cs.SE

TL;DR: Kubernetes项目从Bazel降级到Go Build的案例研究显示，虽然Bazel构建速度更快，但内存占用更大，在并行构建时CPU负载更高。降级可能导致CI资源成本增加76%。


<details>
  <summary>Details</summary>
Motivation: 研究构建系统降级的影响，特别是从基于工件的构建工具（如Bazel）降级到语言特定解决方案的性能影响，这在之前的研究中很少被探索。

Method: 对Kubernetes项目进行案例研究，重现和分析降级期间变更集的完整和增量构建，并复制研究到其他四个从Bazel降级的项目。

Result: Bazel构建速度比Go Build快23.06-38.66%，但内存占用高81.42-351.07%，在并行度高于8时CPU负载更大。降级可能增加CI资源成本76%。

Conclusion: 放弃基于工件的构建工具虽然可能提高可维护性，但对大型项目会带来显著的性能成本，需要权衡利弊。

Abstract: Since developers invoke the build system frequently, its performance can
impact productivity. Modern artifact-based build tools accelerate builds, yet
prior work shows that teams may abandon them for alternatives that are easier
to maintain. While prior work shows why downgrades are performed, the
implications of downgrades remain largely unexplored. In this paper, we
describe a case study of the Kubernetes project, focusing on its downgrade from
an artifact-based build tool (Bazel) to a language-specific solution (Go
Build). We reproduce and analyze the full and incremental builds of change sets
during the downgrade period. On the one hand, we find that Bazel builds are
faster than Go Build, completing full builds in 23.06-38.66 up to 75.19 impose
a larger memory footprint than Go Build of 81.42-351.07 respectively. Bazel
builds also impose a greater CPU load at parallelism settings above eight for
full builds and above one for incremental builds. We estimate that downgrading
from Bazel can increase CI resource costs by up to 76 explore whether our
observations generalize by replicating our Kubernetes study on four other
projects that also downgraded from Bazel to older build tools. We observe that
while build time penalties decrease, Bazel consistently consumes more memory.
We conclude that abandoning artifact-based build tools, despite perceived
maintainability benefits, tends to incur considerable performance costs for
large projects. Our observations may help stakeholders to balance trade-offs in
build tool adoption

</details>


### [16] [Developing a Model-Driven Reengineering Approach for Migrating PL/SQL Triggers to Java: A Practical Experience](https://arxiv.org/abs/2510.20121)
*Carlos J. Fernandez-Candel,Jesus Garcia-Molina,Francisco Javier Bermudez Ruiz,Jose Ramon Hoyos Barcelo,Diego Sevilla Ruiz,Benito Jose Cuesta Viera*

Main category: cs.SE

TL;DR: 提出了一种基于模型驱动工程的软件重构过程，用于将PL/SQL代码迁移到Java代码，采用类似TDD的方法开发模型转换，并包含三种代码验证机制。


<details>
  <summary>Details</summary>
Motivation: 现代软件技术的发展促使企业需要迁移RAD平台（如Oracle Forms）的遗留应用，特别是将PL/SQL单体代码转换为分层的Java代码。

Method: 使用KDM模型表示遗留代码，采用模型驱动的重构方法，集成类似TDD的增量式模型转换开发，包含三种生成代码验证机制。

Result: 开发了一个迁移工具，成功将PL/SQL代码转换为分层的Java代码，详细描述了重构方法的实现和验证过程。

Conclusion: 模型驱动工程方法在软件重构和迁移场景中具有实际应用价值，能够有效处理遗留系统的现代化改造。

Abstract: Model-driven software engineering (MDE) techniques are not only useful in
forward engineering scenarios, but can also be successfully applied to evolve
existing systems. RAD (Rapid Application Development) platforms emerged in the
nineties, but the success of modern software technologies motivated that a
large number of enterprises tackled the migration of their RAD applications,
such as Oracle Forms. Our research group has collaborated with a software
company in developing a solution to migrate PL/SQL monolithic code on Forms
triggers and program units to Java code separated in several tiers.
  Our research focused on the model-driven reengineering process applied to
develop the migration tool for the conversion of PL/SQL code to Java. Legacy
code is represented in form of KDM (Knowledge-Discovery Metamodel) models. In
this paper, we propose a software process to implement a model-driven
re-engineering. This process integrates a TDD-like approach to incrementally
develop model transformations with three kinds of validations for the generated
code. The implementation and validation of the re-engineering approach are
explained in detail, as well as the evaluation of some issues related with the
application of MDE.

</details>


### [17] [Automated Cloud Infrastructure-as-Code Reconciliation with AI Agents](https://arxiv.org/abs/2510.20211)
*Zhenning Yang,Hui Guan,Victor Nicolet,Brandon Paulsen,Joey Dodds,Daniel Kroening,Ang Chen*

Main category: cs.SE

TL;DR: NSync是一个自动化系统，通过分析云API调用序列来检测基础设施漂移，并使用LLM推断意图并更新IaC配置，实现基础设施与代码的自动同步。


<details>
  <summary>Details</summary>
Motivation: 当基础设施即代码(IaC)与云控制台、CLI或SDK同时使用时，IaC会失去对外部变化的可见性，导致基础设施漂移问题，使得配置过时，后续IaC操作可能撤销有效更新或引发错误。

Method: NSync采用基于代理的架构，从API跟踪中获取洞察，检测漂移（非IaC变更），并利用LLM从嘈杂的API序列推断高层意图，使用专用工具合成针对性的IaC更新，通过自演进知识库持续改进。

Result: 在5个真实Terraform项目和372个漂移场景的实验表明，NSync在准确率（从0.71提升到0.97 pass@3）和token效率（1.47倍改进）方面均优于基线。

Conclusion: NSync能够有效解决IaC与外部工具混合使用时的基础设施漂移问题，通过自动化API分析实现高效的IaC配置同步。

Abstract: Cloud infrastructure is managed through a mix of interfaces -- traditionally,
cloud consoles, command-line interfaces (CLI), and SDKs are the tools of
choice. Recently, Infrastructure-as-Code/IaC frameworks (e.g., Terraform) have
quickly gained popularity. Unlike conventional tools, IaC~frameworks encode the
infrastructure in a "source-of-truth" configuration. They are capable of
automatically carrying out modifications to the cloud -- deploying, updating,
or destroying resources -- to bring the actual infrastructure into alignment
with the IaC configuration. However, when IaC is used alongside consoles, CLIs,
or SDKs, it loses visibility into external changes, causing infrastructure
drift, where the configuration becomes outdated, and later IaC operations may
undo valid updates or trigger errors.
  We present NSync, an automated system for IaC reconciliation that propagates
out-of-band changes back into the IaC program. Our key insight is that
infrastructure changes eventually all occur via cloud API invocations -- the
lowest layer for cloud management operations. NSync gleans insights from API
traces to detect drift (i.e., non-IaC changes) and reconcile it (i.e., update
the IaC configuration to capture the changes). It employs an agentic
architecture that leverages LLMs to infer high-level intents from noisy API
sequences, synthesize targeted IaC updates using specialized tools, and
continually improve through a self-evolving knowledge base of past
reconciliations. We further introduce a novel evaluation pipeline for injecting
realistic drifts into cloud infrastructure and assessing reconciliation
performance. Experiments across five real-world Terraform projects and 372
drift scenarios show that NSync outperforms the baseline both in terms of
accuracy (from 0.71 to 0.97 pass@3) and token efficiency (1.47$\times$
improvement).

</details>


### [18] [Classport: Designing Runtime Dependency Introspection for Java](https://arxiv.org/abs/2510.20340)
*Serena Cofano,Daniel Williams,Aman Sharma,Martin Monperrus*

Main category: cs.SE

TL;DR: Classport系统通过在Java类文件中嵌入依赖信息，实现了运行时依赖自省功能，解决了Java缺乏运行时依赖观察能力的问题。


<details>
  <summary>Details</summary>
Motivation: Java缺乏对运行时依赖关系的自省能力，这对于软件供应链安全至关重要。需要能够在程序执行期间观察当前使用的依赖项。

Method: 开发Classport系统，将依赖信息嵌入到Java类文件中，从而可以在运行时检索依赖信息。

Result: 在六个真实项目上评估Classport，证明了在运行时识别依赖项的可行性。

Conclusion: Classport的运行时依赖自省功能为运行时完整性检查开辟了重要途径。

Abstract: Runtime introspection of dependencies, i.e., the ability to observe which
dependencies are currently used during program execution, is fundamental for
Software Supply Chain security. Yet, Java has no support for it. We solve this
problem with Classport, a system that embeds dependency information into Java
class files, enabling the retrieval of dependency information at runtime. We
evaluate Classport on six real-world projects, demonstrating the feasibility in
identifying dependencies at runtime. Runtime dependency introspection with
Classport opens important avenues for runtime integrity checking.

</details>


### [19] [Symmetry in Software Platforms as an Architectural Principle](https://arxiv.org/abs/2510.20389)
*Bjorn Remseth*

Main category: cs.SE

TL;DR: 软件平台作为结构保持系统，通过强制执行结构规律性来产生架构鲁棒性


<details>
  <summary>Details</summary>
Motivation: 探索软件平台如何通过保持结构对称性来实现架构稳健性

Method: 将软件平台视为结构保持系统，分析其提供的稳定接口和行为在特定变换下的对称性

Result: 发现架构鲁棒性源于强制执行结构规律性

Conclusion: 软件平台的稳健性可以通过维护结构对称性来实现

Abstract: Software platforms often act as structure preserving systems. They provide
consistent interfaces and behaviors that remain stable under specific
transformations that we denote as symmetries. This paper explores the idea that
architectural robustness emerges from enforcing such structural regularities

</details>


### [20] [FMI-Based Distributed Co-Simulation with Enhanced Security and Intellectual Property Safeguards](https://arxiv.org/abs/2510.20403)
*Santiago Gil,Ecem E. Baş,Christian D. Jensen,Sebastian Engelsgaard,Giuseppe Abbiati,Cláudio Gomes*

Main category: cs.SE

TL;DR: 提出了一种基于UniFMU的分布式协同仿真方法，通过增强网络安全和知识产权保护机制，确保连接由客户端发起且模型和二进制文件位于可信平台上。


<details>
  <summary>Details</summary>
Motivation: 分布式协同仿真在促进不同利益相关者协作建模和仿真的同时保护知识产权，但目前缺乏针对连续时间或混合系统的分布式协同仿真指南，无法防范潜在的黑客攻击。

Method: 在UniFMU基础上构建分布式协同仿真框架，采用增强的网络安全和IP保护机制，确保客户端发起连接，模型和二进制文件驻留在可信平台上。

Result: 通过四个不同网络设置下的两个协同仿真演示验证了该方法的功能性，并分析了IP保护分布与性能效率之间的权衡关系。

Conclusion: 该方法成功实现了分布式协同仿真的网络安全和知识产权保护，为连续时间或混合系统的安全协同仿真提供了可行解决方案。

Abstract: Distributed co-simulation plays a key role in enabling collaborative modeling
and simulation by different stakeholders while protecting their Intellectual
Property (IP). Although IP protection is provided implicitly by co-simulation,
there is no consensus in the guidelines to conduct distributed co-simulation of
continuous-time or hybrid systems with no exposure to potential hacking
attacks. We propose an approach for distributed co-simulation on top of UniFMU
with enhanced cybersecurity and IP protection mechanisms, ensuring that the
connection is initiated by the client and the models and binaries live on
trusted platforms. We showcase the functionality of this approach using two
co-simulation demos in four different network settings and analyze the
trade-off between IP-protected distribution and performance efficiency in these
settings.

</details>


### [21] [Toward Practical Deductive Verification: Insights from a Qualitative Survey in Industry and Academia](https://arxiv.org/abs/2510.20514)
*Lea Salome Brugger,Xavier Denis,Peter Müller*

Main category: cs.SE

TL;DR: 该研究通过访谈30位验证从业者，分析了演绎验证成功应用的因素和阻碍广泛采用的问题，揭示了证明维护、自动化控制不足和可用性等未充分探索的障碍，并提出了具体建议。


<details>
  <summary>Details</summary>
Motivation: 尽管演绎验证在特定项目中已被证明有效可行，但尚未成为主流技术。研究旨在探索促进演绎验证成功应用的因素和阻碍其广泛采用的根本问题。

Method: 采用半结构化访谈方法，访谈了来自工业界和学术界的30位验证从业者，并运用主题分析方法系统分析收集的数据。

Result: 除了确认已知挑战（如进行形式化证明需要高水平专业知识）外，数据还揭示了多个未充分探索的障碍，包括证明维护、对自动化的控制不足以及可用性问题。

Conclusion: 研究结果可用于提取演绎验证的促进因素和障碍，并为从业者、工具构建者和研究人员制定具体建议，包括可用性、自动化和与现有工作流集成的原则。

Abstract: Deductive verification is an effective method to ensure that a given system
exposes the intended behavior. In spite of its proven usefulness and
feasibility in selected projects, deductive verification is still not a
mainstream technique. To pave the way to widespread use, we present a study
investigating the factors enabling successful applications of deductive
verification and the underlying issues preventing broader adoption. We
conducted semi-structured interviews with 30 practitioners of verification from
both industry and academia and systematically analyzed the collected data
employing a thematic analysis approach. Beside empirically confirming familiar
challenges, e.g., the high level of expertise needed for conducting formal
proofs, our data reveal several underexplored obstacles, such as proof
maintenance, insufficient control over automation, and usability concerns. We
further use the results from our data analysis to extract enablers and barriers
for deductive verification and formulate concrete recommendations for
practitioners, tool builders, and researchers, including principles for
usability, automation, and integration with existing workflows.

</details>


### [22] [Large Language Models for Fault Localization: An Empirical Study](https://arxiv.org/abs/2510.20521)
*YingJian Xiao,RongQun Hu,WeiWei Gong,HongWei Li,AnQuan Jie*

Main category: cs.SE

TL;DR: 本文系统评估了大型语言模型在代码故障定位任务中的表现，比较了开源和闭源模型在不同提示策略下的性能，发现错误报告上下文能显著提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对LLM在代码故障定位任务中的全面评估，而修复效果高度依赖于上游故障定位的性能。

Method: 在HumanEval-Java和Defects4J数据集上评估代表性开源和闭源模型，研究标准提示、少样本示例和思维链等不同提示策略对模型性能的影响。

Result: 包含错误报告上下文显著提升模型性能；少样本学习有改进潜力但存在边际收益递减；思维链推理效果高度依赖模型固有推理能力。

Conclusion: 研究揭示了不同模型在故障定位任务中的性能特征和权衡，为改进故障定位效果提供了有价值的见解。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
code-related tasks, particularly in automated program repair. However, the
effectiveness of such repairs is highly dependent on the performance of
upstream fault localization, for which comprehensive evaluations are currently
lacking. This paper presents a systematic empirical study on LLMs in the
statement-level code fault localization task. We evaluate representative
open-source models (Qwen2.5-coder-32b-instruct, DeepSeek-V3) and closed-source
models (GPT-4.1 mini, Gemini-2.5-flash) to assess their fault localization
capabilities on the HumanEval-Java and Defects4J datasets. The study
investigates the impact of different prompting strategies--including standard
prompts, few-shot examples, and chain-of-reasoning--on model performance, with
a focus on analysis across accuracy, time efficiency, and economic cost
dimensions. Our experimental results show that incorporating bug report context
significantly enhances model performance. Few-shot learning shows potential for
improvement but exhibits noticeable diminishing marginal returns, while
chain-of-thought reasoning's effectiveness is highly contingent on the model's
inherent reasoning capabilities. This study not only highlights the performance
characteristics and trade-offs of different models in fault localization tasks,
but also offers valuable insights into the strengths of current LLMs and
strategies for improving fault localization effectiveness.

</details>


### [23] [A Soundness and Precision Benchmark for Java Debloating Tools](https://arxiv.org/abs/2510.20679)
*Jonas Klauke,Tom Ohlmer,Stefan Schott,Serena Elisa Ponta,Wolfram Fischer,Eric Bodden*

Main category: cs.SE

TL;DR: 开发了Deblometer微基准测试套件来评估Java去膨胀工具，发现现有工具（Deptrim、JShrink、ProGuard）都存在移除必需程序构造的问题，特别是动态类加载功能导致所有工具都不健全。


<details>
  <summary>Details</summary>
Motivation: 现代软件开发平均依赖36个库，其中80%是传递依赖，但只有24.9%在运行时真正需要。现有去膨胀工具需要在精确度（移除未使用构造）和健全性（保留必需构造）之间取得平衡，但缺乏系统性评估方法。

Method: 开发Deblometer微基准测试套件，包含59个测试用例，每个用例都有手动标注的基准真值，指定必需和冗余的类、方法和字段。用此基准评估三个流行Java去膨胀工具：Deptrim、JShrink和ProGuard。

Result: 所有工具都移除了必需的程序构造，导致语义改变或执行崩溃。动态类加载功能在所有评估工具中都引入了不健全性。Deptrim保留更多冗余构造，ProGuard移除更多必需构造，JShrink因对注解支持有限而导致去膨胀产物损坏。

Conclusion: 现有去膨胀工具存在严重的健全性问题，需要改进以确保去膨胀软件的稳定性和可靠性。

Abstract: Modern software development reuses code by importing libraries as
dependencies. Software projects typically include an average of 36
dependencies, with 80% being transitive, meaning they are dependencies of
dependencies. Recent research indicates that only 24.9% of these dependencies
are required at runtime, and even within those, many program constructs remain
unused, adding unnecessary code to the project. This has led to the development
of debloating tools that remove unnecessary dependencies and program constructs
while balancing precision by eliminating unused constructs and soundness by
preserving all required constructs. To systematically evaluate this trade-off,
we developed Deblometer, a micro-benchmark consisting of 59 test cases designed
to assess support for various Java language features in debloating tools. Each
test case includes a manually curated ground truth specifying necessary and
bloated classes, methods, and fields, enabling precise measurement of soundness
and precision. Using Deblometer, we evaluated three popular Java debloating
tools: Deptrim, JShrink, and ProGuard. Our evaluation reveals that all tools
remove required program constructs, which results in changed semantics or
execution crashes. In particular, the dynamic class loading feature introduces
unsoundness in all evaluated tools. Our comparison shows that Deptrim retains
more bloated constructs, while ProGuard removes more required constructs.
JShrink's soundness is significantly affected by limited support for
annotations, which leads to corrupted debloated artifacts. These soundness
issues highlight the need to improve debloating tools to ensure stable and
reliable debloated software.

</details>
