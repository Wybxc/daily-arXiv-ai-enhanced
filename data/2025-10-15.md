<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 17]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.PL](#cs.PL) [Total: 5]
- [cs.LO](#cs.LO) [Total: 8]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [eye2vec: Learning Distributed Representations of Eye Movement for Program Comprehension Analysis](https://arxiv.org/abs/2510.11722)
*Haruhiko Yoshioka,Kazumasa Shimari,Hidetake Uwano,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: eye2vec是一个分析软件开发者在阅读源代码时眼动轨迹的基础设施，通过分布式表示将连续注视点表示为语法元素之间的转换，简化眼动分析过程。


<details>
  <summary>Details</summary>
Motivation: 传统眼动追踪研究需要研究人员预先选择分析目标并开发分析方法，过程耗时且结果因分析区域定义不同而异，需要简化这一过程。

Method: 使用分布式表示将连续的两个注视点表示为语法元素之间的转换，便于采用多样化的数据分析方法。

Result: 开发了eye2vec基础设施，能够更有效地分析程序理解过程中的眼动数据。

Conclusion: eye2vec通过分布式表示简化了眼动分析过程，使得眼动追踪研究更加高效和语义丰富。

Abstract: This paper presents eye2vec, an infrastructure for analyzing software
developers' eye movements while reading source code. In common eye-tracking
studies in program comprehension, researchers must preselect analysis targets
such as control flow or syntactic elements, and then develop analysis methods
to extract appropriate metrics from the fixation for source code. Here,
researchers can define various levels of AOIs like words, lines, or code
blocks, and the difference leads to different results. Moreover, the
interpretation of fixation for word/line can vary across the purposes of the
analyses. Hence, the eye-tracking analysis is a difficult task that depends on
the time-consuming manual work of the researchers. eye2vec represents
continuous two fixations as transitions between syntactic elements using
distributed representations. The distributed representation facilitates the
adoption of diverse data analysis methods with rich semantic interpretations.

</details>


### [2] [Task-Aware Reduction for Scalable LLM-Database Systems](https://arxiv.org/abs/2510.11813)
*Marcus Emmanuel Barnes,Taher A. Ghaleb,Safwat Hassan*

Main category: cs.SE

TL;DR: 本文主张将LLM的token预算视为注意力预算，将任务感知的文本缩减作为语言-数据系统的首要设计原则，以解决现实世界中文本密集型数据（如日志、遥测数据）直接输入LLM时面临的成本、可持续性和任务对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的文本密集型数据（如日志、遥测数据、监控流）具有量大、冗长和嘈杂的特点，直接输入LLM成本高昂、环境不可持续，且往往与任务目标不匹配。现有LLM效率优化主要关注模型或架构层面，而减少上游输入冗长性的挑战尚未得到充分探索。

Method: 将输入侧缩减视为注意力分配而非压缩，优先保留与下游任务最相关的信息。提出构建基准测试、设计自适应缩减流程、将token预算感知预处理集成到数据库和检索系统中的研究挑战。

Result: 提出了一个新的设计范式，将任务感知文本缩减作为语言-数据系统的核心原则，为构建可扩展、准确和可持续的LLM-数据集成系统提供了理论框架。

Conclusion: 通过将稀缺的注意力资源引导到嘈杂数据密集型工作流中的有意义信号，可以实现可扩展、准确和可持续的LLM-数据集成，为未来语言-数据系统的设计提供了重要方向。

Abstract: Large Language Models (LLMs) are increasingly applied to data-intensive
workflows, from database querying to developer observability. Yet the
effectiveness of these systems is constrained by the volume, verbosity, and
noise of real-world text-rich data such as logs, telemetry, and monitoring
streams. Feeding such data directly into LLMs is costly, environmentally
unsustainable, and often misaligned with task objectives. Parallel efforts in
LLM efficiency have focused on model- or architecture-level optimizations, but
the challenge of reducing upstream input verbosity remains underexplored. In
this paper, we argue for treating the token budget of an LLM as an attention
budget and elevating task-aware text reduction as a first-class design
principle for language -- data systems. We position input-side reduction not as
compression, but as attention allocation: prioritizing information most
relevant to downstream tasks. We outline open research challenges for building
benchmarks, designing adaptive reduction pipelines, and integrating
token-budget--aware preprocessing into database and retrieval systems. Our
vision is to channel scarce attention resources toward meaningful signals in
noisy, data-intensive workflows, enabling scalable, accurate, and sustainable
LLM--data integration.

</details>


### [3] [Lingxi: Repository-Level Issue Resolution Framework Enhanced by Procedural Knowledge Guided Scaling](https://arxiv.org/abs/2510.11838)
*Xu Yang,Jiayuan Zhou,Michael Pacheco,Wenhan Zhu,Pengfei He,Shaowei Wang,Kui Liu,Ruiqi Pan*

Main category: cs.SE

TL;DR: Lingxi是一个基于程序知识的软件工程问题解决框架，通过从历史问题修复数据中提取程序知识来指导代理解决仓库级问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于代理的方法缺乏程序知识（即问题如何逐步修复及其背后原理），且依赖大量计算能力盲目探索解决方案空间。

Method: Lingxi通过分层抽象机制离线构建程序知识，使代理能够学习修复的步骤和原理；在线应用时采用知识驱动的扩展方法，利用相似问题的程序知识从多个角度智能分析目标问题。

Result: 在SWE-bench Verified基准测试中，Lingxi在Past@1设置下成功解决了74.6%的错误，显著优于五种最先进技术（领先5.4%到14.9%）。消融研究证实其成功直接来源于程序知识的运用。

Conclusion: 程序知识是Lingxi成功的关键因素，其中"设计模式和编码实践"是最关键的知识方面，且不同知识方面在不同阶段（分析、规划和修复）的作用会发生变化。

Abstract: Driven by the advancements of Large Language Models (LLMs), LLM-powered
agents are making significant improvements in software engineering tasks, yet
struggle with complex, repository-level issue resolution. Existing agent-based
methods have two key limitations. First, they lack of procedural knowledge
(i.e., how an issue is fixed step-by-step and rationales behind it) to learn
and leverage for issue resolution. Second, they rely on massive computational
power to blindly explore the solution space. % To address those limitations, we
propose Lingxi, an issue resolution framework that leverages procedural
knowledge extracted from historical issue-fixing data to guide agents in
solving repository-level issues. \ourTool first constructs this knowledge
offline through a hierarchical abstraction mechanism, enabling agents to learn
the how and why behind a fix, not just the final solution. During online
application, it employs a knowledge-driven scaling method that leverages the
procedural knowledge of similar issues to intelligently analyze the target
issue from multiple perspectives, in sharp contrast to undirected, brute-force
exploration. % Lingxi successfully resolves 74.6\% of bugs on the SWE-bench
Verified benchmark in Past@1 setting, outperforming five state-of-the-art
techniques by a significant margin (5.4\% to 14.9\%). Our comprehensive
ablation study confirmed that the success of Lingxi comes directly from its use
of procedural knowledge. Without it, the performance gains from scaling alone
is negligible. Our qualitative study further shows that the ``design patterns
$\&$ coding practices'' is the most critical knowledge aspect, and that the
roles of different knowledge aspects switch across different stages (i.e.,
analysis, planning, and fixing).

</details>


### [4] [DMAS-Forge: A Framework for Transparent Deployment of AI Applications as Distributed Systems](https://arxiv.org/abs/2510.11872)
*Alessandro Cornacchia,Vaastav Anand,Muhammad Bilal,Zafar Qazi,Marco Canini*

Main category: cs.SE

TL;DR: DMAS-Forge是一个框架，旨在简化分布式多智能体AI应用的部署和测试，通过解耦应用逻辑与具体部署选择，自动生成必要的粘合代码和配置。


<details>
  <summary>Details</summary>
Motivation: 随着AI应用越来越依赖具有不同角色、专用工具和内存访问的多智能体系统，部署和测试这些分布式系统变得复杂且劳动密集，需要简化这一过程。

Method: 设计DMAS-Forge框架，解耦应用逻辑与部署选择，透明生成必要的粘合代码和配置，支持在不同部署场景下快速生成分布式多智能体应用。

Result: 提出了DMAS-Forge的愿景、设计原则和原型，展示了该框架如何最小化手动工作来部署分布式多智能体应用。

Conclusion: DMAS-Forge为解决分布式AI智能体部署的复杂性提供了有前景的方法，讨论了该方法的机遇和未来工作方向。

Abstract: Agentic AI applications increasingly rely on multiple agents with distinct
roles, specialized tools, and access to memory layers to solve complex tasks --
closely resembling service-oriented architectures. Yet, in the rapid evolving
landscape of programming frameworks and new protocols, deploying and testing AI
agents as distributed systems remains a daunting and labor-intensive task. We
present DMAS-Forge, a framework designed to close this gap. DMAS-Forge
decouples application logic from specific deployment choices, and aims at
transparently generating the necessary glue code and configurations to spawn
distributed multi-agent applications across diverse deployment scenarios with
minimal manual effort. We present our vision, design principles, and a
prototype of DMAS-Forge. Finally, we discuss the opportunities and future work
for our approach.

</details>


### [5] [TorchCor: High-Performance Cardiac Electrophysiology Simulations with the Finite Element Method on GPUs](https://arxiv.org/abs/2510.12011)
*Bei Zhou,Maximilian Balmus,Cesare Corrado,Ludovica Cicci,Shuang Qian,Steven A. Niederer*

Main category: cs.SE

TL;DR: TorchCor是一个基于PyTorch的高性能Python库，用于在通用GPU上进行心脏电生理学模拟，显著加速3D网格模拟，且免费无限制使用。


<details>
  <summary>Details</summary>
Motivation: 心脏电生理学模拟通常需要高性能计算资源，但许多研究团队和临床医生无法获得这些资源。

Method: 基于PyTorch构建，使用有限元方法在通用GPU上进行心脏电生理学模拟。

Result: TorchCor显著加速了心脏电生理学模拟，特别是对于大型3D网格，其准确性通过制造解析解和N版本基准问题验证。

Conclusion: TorchCor为无法获得高性能计算资源的用户提供了高效的心脏电生理学模拟解决方案，且免费开放使用。

Abstract: Cardiac electrophysiology (CEP) simulations are increasingly used for
understanding cardiac arrhythmias and guiding clinical decisions. However,
these simulations typically require high-performance computing resources with
numerous CPU cores, which are often inaccessible to many research groups and
clinicians. To address this, we present TorchCor, a high-performance Python
library for CEP simulations using the finite element method on general-purpose
GPUs. Built on PyTorch, TorchCor significantly accelerates CEP simulations,
particularly for large 3D meshes. The accuracy of the solver is verified
against manufactured analytical solutions and the $N$-version benchmark
problem. TorchCor is freely available for both academic and commercial use
without restrictions.

</details>


### [6] [Enhancing Neural Code Representation with Additional Context](https://arxiv.org/abs/2510.12082)
*Huy Nguyen,Christoph Treude,Patanamon Thongtanunam*

Main category: cs.SE

TL;DR: 该论文通过实证研究发现，在代码表示中加入上下文信息（如版本历史、调用图）能显著提升深度学习模型在代码克隆检测和代码摘要任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习模型主要依赖源代码本身，忽略了版本历史和结构关系等上下文信息，这限制了模型理解代码演变和运行方式的能力。

Method: 在SeSaMe和CodeSearchNet数据集上，对五种代表性模型（CodeBERT、GraphCodeBERT、CodeT5、PLBART、ASTNN）进行微调，比较仅使用代码和使用上下文增强设置下的性能差异。

Result: 上下文信息普遍提升模型性能：版本历史持续提升克隆检测（如CodeT5 F1提升15.92%）和代码摘要（如GraphCodeBERT METEOR提升5.56%），调用图的效果因模型和任务而异。多上下文组合带来更大提升（最高达21.48% macro-F1）。人工评估显示上下文增强的摘要更受偏好。

Conclusion: 上下文信号有潜力增强代码理解能力，为优化神经软件工程模型中的上下文编码开辟了新方向。

Abstract: Automated program comprehension underpins many software engineering tasks,
from code summarisation to clone detection. Recent deep learning models achieve
strong results but typically rely on source code alone, overlooking contextual
information such as version history or structural relationships. This limits
their ability to capture how code evolves and operates. We conduct an empirical
study on how enriching code representations with such contextual signals
affects neural model performance on key comprehension tasks. Two downstream
tasks, code clone detection and code summarisation, are evaluated using SeSaMe
(1,679 Java methods) and CodeSearchNet (63,259 methods). Five representative
models (CodeBERT, GraphCodeBERT, CodeT5, PLBART, ASTNN) are fine-tuned under
code-only and context-augmented settings. Results show that context generally
improves performance: version history consistently boosts clone detection
(e.g., CodeT5 +15.92% F1) and summarisation (e.g., GraphCodeBERT +5.56%
METEOR), while call-graph effects vary by model and task. Combining multiple
contexts yields further gains (up to +21.48% macro-F1). Human evaluation on 100
Java snippets confirms that context-augmented summaries are significantly
preferred for Accuracy and Content Adequacy (p <= 0.026; |delta| up to 0.55).
These findings highlight the potential of contextual signals to enhance code
comprehension and open new directions for optimising contextual encoding in
neural SE models.

</details>


### [7] [Beyond Postconditions: Can Large Language Models infer Formal Contracts for Automatic Software Verification?](https://arxiv.org/abs/2510.12702)
*Cedric Richter,Heike Wehrheim*

Main category: cs.SE

TL;DR: NL2Contract任务利用LLM将自然语言转换为包含前置条件和后置条件的正式功能合约，相比仅生成后置条件的方法，能显著减少自动验证器产生的误报。


<details>
  <summary>Details</summary>
Motivation: 解决自动软件验证器在实际应用中因缺乏正式规范而难以推广的问题，以及LLM仅生成后置条件时导致验证器产生大量误报的局限性。

Method: 提出NL2Contract任务，使用LLM从代码中的自然语言提示（如函数名、注释）推断完整的功能合约（前置条件+后置条件），并引入基于正确性、错误判别能力和验证可用性的评估指标。

Result: LLM能有效生成对所有可能输入都正确的功能合约；生成的合约具有足够的表达能力来区分错误和正确行为；与仅使用后置条件相比，使用完整功能合约的验证器产生的误报更少。

Conclusion: LLM推断的前置条件与开发者意图高度一致，使得自动软件验证器能够有效捕获真实世界中的错误，为软件验证的实际应用提供了可行方案。

Abstract: Automatic software verifiers have become increasingly effective at the task
of checking software against (formal) specifications. Yet, their adoption in
practice has been hampered by the lack of such specifications in real world
code. Large Language Models (LLMs) have shown promise in inferring formal
postconditions from natural language hints embedded in code such as function
names, comments or documentation. Using the generated postconditions as
specifications in a subsequent verification, however, often leads verifiers to
suggest invalid inputs, hinting at potential issues that ultimately turn out to
be false alarms.
  To address this, we revisit the problem of specification inference from
natural language in the context of automatic software verification. In the
process, we introduce NL2Contract, the task of employing LLMs to translate
informal natural language into formal functional contracts, consisting of
postconditions as well as preconditions. We introduce metrics to validate and
compare different NL2Contract approaches, using soundness, bug discriminative
power of the generated contracts and their usability in the context of
automatic software verification as key metrics. We evaluate NL2Contract with
different LLMs and compare it to the task of postcondition generation
nl2postcond. Our evaluation shows that (1) LLMs are generally effective at
generating functional contracts sound for all possible inputs, (2) the
generated contracts are sufficiently expressive for discriminating buggy from
correct behavior, and (3) verifiers supplied with LLM inferred functional
contracts produce fewer false alarms than when provided with postconditions
alone. Further investigations show that LLM inferred preconditions generally
align well with developers intentions which allows us to use automatic software
verifiers to catch real-world bugs.

</details>


### [8] [Towards Engineering Multi-Agent LLMs: A Protocol-Driven Approach](https://arxiv.org/abs/2510.12120)
*Zhenyu Mao,Jacky Keung,Fengji Zhang,Shuo Liu,Yifei Wang,Jialong Li*

Main category: cs.SE

TL;DR: 本文提出SEMAP协议层方法，通过引入软件工程结构化原则来改进多智能体LLM系统，显著减少了在代码开发和漏洞检测任务中的失败率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体系统在软件工程任务中经常失败，主要由于缺乏明确的软件工程结构化原则，导致规范不足、协调错位和验证不当等问题。

Method: 提出SEMAP协议层方法，基于三个核心SE设计原则：(1)显式行为契约建模；(2)结构化消息传递；(3)生命周期引导的执行与验证，并在Google的A2A基础设施上实现。

Result: 使用MAST框架评估显示，SEMAP显著减少失败：代码开发中函数级开发失败减少69.6%，部署级开发减少56.7%；漏洞检测中Python任务失败减少47.4%，C/C++任务减少28.2%。

Conclusion: SEMAP通过引入软件工程结构化原则，有效解决了多智能体LLM系统在软件工程任务中的核心缺陷，显著提升了系统的可靠性和性能。

Abstract: The increasing demand for software development has driven interest in
automating software engineering (SE) tasks using Large Language Models (LLMs).
Recent efforts extend LLMs into multi-agent systems (MAS) that emulate
collaborative development workflows, but these systems often fail due to three
core deficiencies: under-specification, coordination misalignment, and
inappropriate verification, arising from the absence of foundational SE
structuring principles. This paper introduces Software Engineering Multi-Agent
Protocol (SEMAP), a protocol-layer methodology that instantiates three core SE
design principles for multi-agent LLMs: (1) explicit behavioral contract
modeling, (2) structured messaging, and (3) lifecycle-guided execution with
verification, and is implemented atop Google's Agent-to-Agent (A2A)
infrastructure. Empirical evaluation using the Multi-Agent System Failure
Taxonomy (MAST) framework demonstrates that SEMAP effectively reduces failures
across different SE tasks. In code development, it achieves up to a 69.6%
reduction in total failures for function-level development and 56.7% for
deployment-level development. For vulnerability detection, SEMAP reduces
failure counts by up to 47.4% on Python tasks and 28.2% on C/C++ tasks.

</details>


### [9] [iCodeReviewer: Improving Secure Code Review with Mixture of Prompts](https://arxiv.org/abs/2510.12186)
*Yun Peng,Kisub Kim,Linghan Meng,Kui Liu*

Main category: cs.SE

TL;DR: iCodeReviewer是一个基于大语言模型的自动化安全代码审查方法，采用混合提示架构来提高安全问题的覆盖率，并通过路由算法减少误报。


<details>
  <summary>Details</summary>
Motivation: 当前自动化安全代码审查方法在精度、覆盖率和全面评估方面存在挑战，需要更有效的解决方案来减少人工审查工作量。

Method: 使用大语言模型构建混合提示架构，包含多个提示专家来检查特定安全问题，并通过路由算法基于代码特征激活必要的提示专家。

Result: 在内部数据集上实现63.98%的F1分数，生产环境中生成的审查评论接受率高达84%。

Conclusion: iCodeReviewer在安全问题的识别和定位方面表现出色，能够有效支持代码审查过程。

Abstract: Code review is an essential process to ensure the quality of software that
identifies potential software issues at an early stage of software development.
Among all software issues, security issues are the most important to identify,
as they can easily lead to severe software crashes and service disruptions.
Recent research efforts have been devoted to automated approaches to reduce the
manual efforts required in the secure code review process. Despite the
progress, current automated approaches on secure code review, including static
analysis, deep learning models, and prompting approaches, still face the
challenges of limited precision and coverage, and a lack of comprehensive
evaluation.
  To mitigate these challenges, we propose iCodeReviewer, which is an automated
secure code review approach based on large language models (LLMs).
iCodeReviewer leverages a novel mixture-of-prompts architecture that
incorporates many prompt experts to improve the coverage of security issues.
Each prompt expert is a dynamic prompt pipeline to check the existence of a
specific security issue. iCodeReviewer also implements an effective routing
algorithm to activate only necessary prompt experts based on the code features
in the input program, reducing the false positives induced by LLM
hallucination. Experiment results in our internal dataset demonstrate the
effectiveness of iCodeReviewer in security issue identification and
localization with an F1 of 63.98%. The review comments generated by
iCodeReviewer also achieve a high acceptance rate up to 84% when it is deployed
in production environments.

</details>


### [10] [Show Your Title! A Scoping Review on Verbalization in Software Engineering with LLM-Assisted Screening](https://arxiv.org/abs/2510.12294)
*Gergő Balogh,Dávid Kószó,Homayoun Safarpour Motealegh Mahalegi,László Tóth,Bence Szakács,Áron Búcsú*

Main category: cs.SE

TL;DR: 本文通过LLM辅助筛选9000多篇论文，研究了软件工程与心理学交叉领域中言语数据的使用，发现SE经常借鉴PSY方法但反向很少，主要主题集中在SE技术层面而非人本主题。


<details>
  <summary>Details</summary>
Motivation: 研究软件开发者思维、决策和行为是软件工程的关键挑战，言语化技术提供了一种轻量级方法来研究这些认知方面。

Method: 采用LLM辅助筛选管道，使用GPT基于标题评估9000多篇论文的相关性，验证GPT输出与人工评审的一致性。

Result: GPT与人工评审高度一致，分歧率为13%。主要主题集中在SE技术层面，人本主题代表性不足；SE频繁借鉴PSY方法，但反向很少。

Conclusion: LLM在支持跨学科评审过程中有效，SE与PSY交叉研究存在不平衡，需要更多关注人本主题和双向知识流动。

Abstract: Understanding how software developers think, make decisions, and behave
remains a key challenge in software engineering (SE). Verbalization techniques
(methods that capture spoken or written thought processes) offer a lightweight
and accessible way to study these cognitive aspects. This paper presents a
scoping review of research at the intersection of SE and psychology (PSY),
focusing on the use of verbal data. To make large-scale interdisciplinary
reviews feasible, we employed a large language model (LLM)-assisted screening
pipeline using GPT to assess the relevance of over 9,000 papers based solely on
titles. We addressed two questions: what themes emerge from
verbalization-related work in SE, and how effective are LLMs in supporting
interdisciplinary review processes? We validated GPT's outputs against human
reviewers and found high consistency, with a 13\% disagreement rate. Prominent
themes mainly were tied to the craft of SE, while more human-centered topics
were underrepresented. The data also suggests that SE frequently draws on PSY
methods, whereas the reverse is rare.

</details>


### [11] [(R)evolution of Programming: Vibe Coding as a Post-Coding Paradigm](https://arxiv.org/abs/2510.12364)
*Kevin Krings,Nino S. Bohn,Thomas Ludwig*

Main category: cs.SE

TL;DR: 本文研究了新兴的Vibe Coding范式，这是一种强调开发者与AI系统之间直觉、情感驱动和即兴交互的编程方式，与传统的AI辅助开发工具形成对比。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI特别是大语言模型如何改变软件开发实践，研究Vibe Coding这一新兴范式与传统编程方法的差异。

Method: 通过对10名经验丰富的软件从业者进行5次半结构化访谈，识别出五个主题维度：创造力、可持续性、编程未来、协作和批评。

Result: 将Vibe Coding概念化为"共同漂移"的隐喻，与流行的"共同驾驶"视角形成对比。Vibe Coding重新配置了开发者的角色，模糊了专业开发者与非开发者之间的界限。

Conclusion: Vibe Coding代表了编程文化的有意义转变，虽然支持新颖的表达形式和快速原型设计，但也带来了可重复性、可扩展性和包容性方面的挑战，值得在人机交互和软件工程领域进一步研究。

Abstract: Recent advancements in generative artificial intelligence (GenAI),
particularly large language models, have introduced new possibilities for
software development practices. In our paper we investigate the emerging Vibe
Coding (VC) paradigm that emphasizes intuitive, affect-driven, and
improvisational interactions between developers and AI systems. Building upon
the discourse of End-User Development (EUD), we explore how VC diverges from
conventional programming approaches such as those supported by tools like
GitHub Copilot. Through five semi-structured interview sessions with ten
experienced software practitioners, we identify five thematic dimensions:
creativity, sustainability, the future of programming, collaboration, and
criticism. Our analysis conceptualizes VC within the metaphor of co-drifting,
contrasting it with the prevalent co-piloting perspective of AI-assisted
development. We argue that VC reconfigures the developers role, blurring
boundaries between professional and non-developers. While VC enables novel
forms of expression and rapid prototyping, it also introduces challenges
regarding reproducibility, scalability, and inclusivity. We propose that VC
represents a meaningful shift in programming culture, warranting further
investigation within human-computer interaction (HCI) and software engineering
research.

</details>


### [12] [Should I Run My Cloud Benchmark on Black Friday?](https://arxiv.org/abs/2510.12397)
*Sören Henning,Adriano Vogel,Esteban Perez-Wohlfeil,Otmar Ertl,Rick Rabiser*

Main category: cs.SE

TL;DR: 该研究探讨了云环境中性能基准测试的可变性，特别是全球性事件（如黑色星期五）对基准测试结果的影响。


<details>
  <summary>Details</summary>
Motivation: 云环境中基准测试结果常因性能可变性而受到质疑，研究旨在量化这种可变性对基准测试结果的实际影响。

Method: 通过在不同时间段重复执行流处理应用基准测试，分析性能变化模式，并特别关注全球性事件期间的表现。

Result: 确认了应用级别的性能可变性确实存在，但比通常假设的要小，并识别出了细微的每日和每周性能模式。

Conclusion: 云环境中的性能基准测试虽然存在可变性，但通过大规模研究可以识别出规律性模式，全球性事件可能对基准测试结果产生可观察的影响。

Abstract: Benchmarks and performance experiments are frequently conducted in cloud
environments. However, their results are often treated with caution, as the
presumed high variability of performance in the cloud raises concerns about
reproducibility and credibility. In a recent study, we empirically quantified
the impact of this variability on benchmarking results by repeatedly executing
a stream processing application benchmark at different times of the day over
several months. Our analysis confirms that performance variability is indeed
observable at the application level, although it is less pronounced than often
assumed. The larger scale of our study compared to related work allowed us to
identify subtle daily and weekly performance patterns. We now extend this
investigation by examining whether a major global event, such as Black Friday,
affects the outcomes of performance benchmarks.

</details>


### [13] [DarTwin made precise by SysMLv2 -- An Experiment](https://arxiv.org/abs/2510.12478)
*Øystein Haugen,Stefan Klikovits,Martin Arthur Andersen,Jonathan Beaulieu,Francis Bordeleau,Joachim Denil,Joost Mertens*

Main category: cs.SE

TL;DR: 本文通过SysMLv2开发了DarTwin DSL，用于数字孪生演化建模，评估了SysMLv2在领域特定语言创建方面的能力，并指出了当前工具在图形表示方面的限制。


<details>
  <summary>Details</summary>
Motivation: SysMLv2新增了领域特定概念和语言扩展的内置规范机制，这有望促进领域特定语言(DSL)的创建以及与现有系统描述和技术设计的接口。

Method: 通过具体用例评估SysMLv2能力，开发DarTwin DSL来形式化现有的DarTwin数字孪生演化表示法。

Result: 成功演示了DarTwin DSL，但发现当前SysMLv2工具在图形表示能力方面存在限制。

Conclusion: 这项工作将模型驱动工程与数字孪生相结合，通过SysMLv2将系统化方法与数字孪生演化管理集成到系统工程中。

Abstract: The new SysMLv2 adds mechanisms for the built-in specification of
domain-specific concepts and language extensions. This feature promises to
facilitate the creation of Domain-Specific Languages (DSLs) and interfacing
with existing system descriptions and technical designs. In this paper, we
review these features and evaluate SysMLv2's capabilities using concrete use
cases. We develop DarTwin DSL, a DSL that formalizes the existing DarTwin
notation for Digital Twin (DT) evolution, through SysMLv2, thereby supposedly
enabling the wide application of DarTwin's evolution templates using any
SysMLv2 tool. We demonstrate DarTwin DSL, but also point out limitations in the
currently available tooling of SysMLv2 in terms of graphical notation
capabilities. This work contributes to the growing field of Model-Driven
Engineering (MDE) for DTs and combines it with the release of SysMLv2, thus
integrating a systematic approach with DT evolution management in systems
engineering.

</details>


### [14] [Diff-XYZ: A Benchmark for Evaluating Diff Understanding](https://arxiv.org/abs/2510.12487)
*Evgeniy Glukhov,Michele Conti,Egor Bogomolov,Yaroslav Golubev,Alexander Bezzubov*

Main category: cs.SE

TL;DR: Diff-XYZ是一个用于代码差异理解的紧凑基准测试，包含三个监督任务：应用差异、反向应用差异和差异生成，基于真实提交数据构建。


<details>
  <summary>Details</summary>
Motivation: 可靠处理代码差异对于大规模编辑和重构代码库的智能体至关重要，需要评估和改进LLM处理代码差异的能力。

Method: 从CommitPackFT的真实提交中提取三元组⟨旧代码, 新代码, 差异⟩，使用自动指标和清晰评估协议，比较不同差异表示格式。

Result: 研究发现不同格式应根据使用场景和模型大小选择，例如搜索替换格式适合大模型的差异生成，但不适合差异分析和小模型。

Conclusion: Diff-XYZ基准为评估和改进LLM中的差异处理提供了可重用基础，有助于未来差异格式和代码编辑模型的发展。

Abstract: Reliable handling of code diffs is central to agents that edit and refactor
repositories at scale. We introduce Diff-XYZ, a compact benchmark for code-diff
understanding with three supervised tasks: apply (old code $+$ diff
$\rightarrow$ new code), anti-apply (new code $-$ diff $\rightarrow$ old code),
and diff generation (new code $-$ old code $\rightarrow$ diff). Instances in
the benchmark are triples $\langle \textit{old code}, \textit{new code},
\textit{diff} \rangle$ drawn from real commits in CommitPackFT, paired with
automatic metrics and a clear evaluation protocol. We use the benchmark to do a
focused empirical study of the unified diff format and run a cross-format
comparison of different diff representations. Our findings reveal that
different formats should be used depending on the use case and model size. For
example, representing diffs in search-replace format is good for larger models
in the diff generation scenario, yet not suited well for diff analysis and
smaller models. The Diff-XYZ benchmark is a reusable foundation for assessing
and improving diff handling in LLMs that can aid future development of diff
formats and models editing code. The dataset is published on HuggingFace Hub:
https://huggingface.co/datasets/JetBrains-Research/diff-xyz.

</details>


### [15] [The EmpathiSEr: Development and Validation of Software Engineering Oriented Empathy Scales](https://arxiv.org/abs/2510.12546)
*Hashini Gunatilake,John Grundy,Rashina Hoda,Ingo Mueller*

Main category: cs.SE

TL;DR: 开发了两个针对软件工程领域的共情量表：EmpathiSEr-P（评估从业者之间的共情）和EmpathiSEr-U（评估从业者对用户的共情），填补了软件工程中缺乏经过验证的共情测量工具的空缺。


<details>
  <summary>Details</summary>
Motivation: 软件工程中的共情对协作、沟通和以用户为中心的设计至关重要，但现有的通用共情量表无法适应软件工程独特的社会技术背景，缺乏针对该领域角色特定和领域约束的共情表达的有效测量工具。

Method: 采用严谨的多阶段方法，包括专家评估、认知访谈和两次从业者调查，基于从业者知情概念框架开发量表，涵盖认知共情、情感共情和共情反应三个维度。

Result: 开发出了首个经过心理测量学验证的软件工程专用共情量表，为评估软件团队和用户互动中的共情以及设计共情增强干预提供了工具。

Conclusion: EmpathiSEr量表是软件工程领域首个经过验证的共情测量工具，能够有效评估软件工程特定背景下的共情，为研究和实践提供了重要支持。

Abstract: Empathy plays a critical role in software engineering (SE), influencing
collaboration, communication, and user-centred design. Although SE research has
increasingly recognised empathy as a key human aspect, there remains no
validated instrument specifically designed to measure it within the unique
socio-technical contexts of SE. Existing generic empathy scales, while
well-established in psychology and healthcare, often rely on language,
scenarios, and assumptions that are not meaningful or interpretable for
software practitioners. These scales fail to account for the diverse,
role-specific, and domain-bound expressions of empathy in SE, such as
understanding a non-technical user's frustrations or another practitioner's
technical constraints, which differ substantially from empathy in clinical or
everyday contexts. To address this gap, we developed and validated two
domain-specific empathy scales: EmpathiSEr-P, assessing empathy among
practitioners, and EmpathiSEr-U, capturing practitioner empathy towards users.
Grounded in a practitioner-informed conceptual framework, the scales encompass
three dimensions of empathy: cognitive empathy, affective empathy, and empathic
responses. We followed a rigorous, multi-phase methodology, including expert
evaluation, cognitive interviews, and two practitioner surveys. The resulting
instruments represent the first psychometrically validated empathy scales
tailored to SE, offering researchers and practitioners a tool for assessing
empathy and designing empathy-enhancing interventions in software teams and
user interactions.

</details>


### [16] [Evaluating End-User Device Energy Models in Sustainability Reporting of Browser-Based Web Services](https://arxiv.org/abs/2510.12566)
*Maja H. Kirkeby,Timmie Lagermann*

Main category: cs.SE

TL;DR: 评估简化的能源和碳模型在网站可持续性报告中的准确性，发现基于恒定功率近似的方法与实际能耗存在显著偏差，需要类别感知和设备反射的功率参数。


<details>
  <summary>Details</summary>
Motivation: 虽然丹麦数字政府机构的Digst框架和英国DIMPACT模型等简化能源和碳模型在可持续性报告中广泛采用，但其准确性和精确度仍未得到充分探索。

Method: 通过实证研究，在四种笔记本电脑平台上执行预定义用户流程，测量购物、预订、导航和新闻服务等常见网站类别在实际用户交互中的能耗。

Result: 结果显示，常用的恒定功率近似方法（P * t）与实测能耗存在显著差异，具体取决于网站类别、设备类型和任务特征。模型偏差是系统性的而非随机的。

Conclusion: 研究强调了在可复现的可持续性报告框架中需要采用类别感知和设备反射的功率参数，以提高模型准确性。

Abstract: Sustainability reporting in web-based services increasingly relies on
simplified energy and carbon models such as the Danish Agency of Digital
Government's Digst framework and the United Kingdom-based DIMPACT model.
Although these models are widely adopted, their accuracy and precision remain
underexplored. This paper presents an empirical study evaluating how well such
models reflect actual energy consumption during realistic user interactions
with common website categories. Energy use was measured across shopping,
booking, navigation, and news services using predefined user flows executed on
four laptop platforms. The results show that the commonly applied
constant-power approximation (P * t) can diverge substantially from measured
energy, depending on website category, device type, and task characteristics.
The findings demonstrate that model deviations are systematic rather than
random and highlight the need for category-aware and device-reflective power
parameters in reproducible sustainability reporting frameworks.

</details>


### [17] [Runtime Composition in Dynamic System of Systems: A Systematic Review of Challenges, Solutions, Tools, and Evaluation Methods](https://arxiv.org/abs/2510.12616)
*Muhammad Ashfaq,Ahmed R. Sadik,Teerath Das,Muhammad Waseem,Niko Makitalo,Tommi Mikkonen*

Main category: cs.SE

TL;DR: 本文通过系统文献综述分析了动态系统之系统(SoS)中的运行时组合研究，识别了核心挑战、解决方案、工具和评估方法。


<details>
  <summary>Details</summary>
Motivation: 现代SoS在动态环境中运行，需要运行时组合来实现适应性，但文献缺乏对动态SoS中运行时组合的综合性分析。

Method: 采用系统文献综述方法，筛选了2019-2024年间的1774篇研究，最终选取80篇主要研究进行主题分析。

Result: 识别了四类挑战：建模与分析、弹性操作、系统编排、CS异质性；七类解决方案：协同仿真与数字孪生、语义本体、集成框架、自适应架构、中间件、形式化方法、AI驱动弹性；服务导向框架主导工具，仿真平台支持评估。

Conclusion: 揭示了自治与协调、建模-现实差距、社会技术集成等紧张关系，呼吁标准化评估指标、可扩展去中心化架构和跨领域框架。

Abstract: Context: Modern Systems of Systems (SoSs) increasingly operate in dynamic
environments (e.g., smart cities, autonomous vehicles) where runtime
composition -- the on-the-fly discovery, integration, and coordination of
constituent systems (CSs)--is crucial for adaptability. Despite growing
interest, the literature lacks a cohesive synthesis of runtime composition in
dynamic SoSs. Objective: This study synthesizes research on runtime composition
in dynamic SoSs and identifies core challenges, solution strategies, supporting
tools, and evaluation methods. Methods: We conducted a Systematic Literature
Review (SLR), screening 1,774 studies published between 2019 and 2024 and
selecting 80 primary studies for thematic analysis (TA). Results: Challenges
fall into four categories: modeling and analysis, resilient operations, system
orchestration, and heterogeneity of CSs. Solutions span seven areas:
co-simulation and digital twins, semantic ontologies, integration frameworks,
adaptive architectures, middleware, formal methods, and AI-driven resilience.
Service-oriented frameworks for composition and integration dominate tooling,
while simulation platforms support evaluation. Interoperability across tools,
limited cross-toolchain workflows, and the absence of standardized benchmarks
remain key gaps. Evaluation approaches include simulation-based,
implementation-driven, and human-centered studies, which have been applied in
domains such as smart cities, healthcare, defense, and industrial automation.
Conclusions: The synthesis reveals tensions, including autonomy versus
coordination, the modeling-reality gap, and socio-technical integration. It
calls for standardized evaluation metrics, scalable decentralized
architectures, and cross-domain frameworks. The analysis aims to guide
researchers and practitioners in developing and implementing dynamically
composable SoSs.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [18] [Bringing Algebraic Hierarchical Decompositions to Concatenative Functional Languages](https://arxiv.org/abs/2510.12481)
*Attila Egri-Nagy*

Main category: cs.FL

TL;DR: 该论文研究如何将代数分解（Krohn-Rhodes理论）应用于编程语言，特别是连接性函数式编程语言，通过将代数理论推广到范畴层面，设计具有显式半群表示的程序语言家族。


<details>
  <summary>Details</summary>
Motivation: 编程语言与纯数学之间存在差距，许多理论结果尚未实现其应用潜力。代数分解为理解和控制计算过程提供了方法，但迄今应用仅限于理论研究。

Method: 使用将代数理论从半群推广到半群范畴的最新成果，针对连接性函数式编程语言这一特殊类别，应用半群分解方法。

Result: 作为半群分解的首次应用，开始设计具有显式半群表示的程序语言家族。

Conclusion: 代数分解可以成功应用于编程语言设计，为连接性函数式编程语言提供了新的理论基础和设计方法。

Abstract: Programming languages tend to evolve over time to use more and more concepts
from theoretical computer science. Still, there is a gap between programming
and pure mathematics. Not all theoretical results have realized their promising
applications. The algebraic decomposition of finite state automata
(Krohn-Rhodes Theory) constructs an emulating hierarchical structure from
simpler components for any computing device. These decompositions provide ways
to understand and control computational processes, but so far the applications
were limited to theoretical investigations. Here, we study how to apply
algebraic decompositions to programming languages. We use recent results on
generalizing the algebraic theory to the categorical level (from semigroups to
semigroupoids) and work with the special class of concatenative functional
programming languages. As a first application of semigroupoid decompositions,
we start to design a family of programming languages with an explicit
semigroupoid representation.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [19] [Verifying Correctness of Shared Channels in a Cooperatively Scheduled Process-Oriented Language](https://arxiv.org/abs/2510.11751)
*Jan Pedersen,Kevin Chalmers*

Main category: cs.PL

TL;DR: 分析在协作调度运行时中共享通信通道的行为，使用FDR工具验证ProcessJ语言中通道实现的正确性，发现正确行为依赖于充足的执行资源。


<details>
  <summary>Details</summary>
Motivation: 理解并发组件在特定条件下的行为很重要，需要分析协作调度运行时中共享通信通道的行为特性。

Method: 使用FDR精化检查和建模工具，开发共享通道的行为规范和在ProcessJ语言中的实现模型。

Result: 虽然能够实现正确的通道行为，但结果依赖于是否有足够资源来执行所有相关进程。

Conclusion: 建模并发组件的运行时环境对于确保组件在现实世界中按规范行为是必要的。

Abstract: Correct concurrent behaviour is important in understanding how components
will act within certain conditions. In this work. we analyse the behaviour of
shared communicating channels within a coorporatively scheduled runtime. We use
the refinement checking and modelling tool FDR to develop both specifications
of how such shared channels should behave and models of the implementations of
these channels in the cooperatively scheduled language ProcessJ. Our results
demonstrate that although we can certainly implement the correct behaviour of
such channels, the outcome is dependant on having adequate resources available
to execute all processes involved. We conclude that modelling the runtime
environment of concurrent components is necessary to ensure components behave
as specified in the real world.

</details>


### [20] [AwareCompiler: Agentic Context-Aware Compiler Optimization via a Synergistic Knowledge-Data Driven Framework](https://arxiv.org/abs/2510.11759)
*Hongyu Lin,Haolin Pan,Haoran Luo,Yuchen Li,Kaichun Yao,Libo Zhang,Mingjie Xing,Yanjun Wu*

Main category: cs.PL

TL;DR: AwareCompiler是一个基于LLM的编译器优化代理框架，通过结构化知识集成、知识驱动的自适应pass生成和数据驱动的混合训练管道，解决了语义对齐、交互效率和奖励稀疏性等挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的基于LLM的编译器优化方法面临三个主要挑战：(1)抽象程序表示与具体优化pass之间的语义不对齐；(2)代理与编译器环境之间的低效交互机制；(3)在大型优化空间中决策过程的奖励稀疏性。

Method: AwareCompiler框架包含三个关键创新：结构化知识集成和数据集构建、知识驱动的自适应pass生成、数据驱动的混合训练管道。

Result: 在标准基准测试中，AwareCompiler在性能和效率方面显著优于现有基线方法。

Conclusion: 该研究表明，通过协同的知识-数据驱动方法，可以有效解决编译器优化中的关键挑战，提升优化效果。

Abstract: Compiler optimization is crucial for enhancing program performance by
transforming the sequence of optimization passes while maintaining correctness.
Despite the promising potential of large language models (LLMs)-based agent for
software optimization, automating compiler optimization remains challenging due
to: (1) semantic misalignment between abstract program representations and
concrete optimization passes, (2) inefficient interaction mechanisms between
agents and compiler environments, and (3) reward sparsity from the extensive
decision-making process within large optimization spaces. This paper introduces
\textbf{AwareCompiler}, an agentic framework for compiler optimization that
addresses these challenges through three key innovations: structured knowledge
integration and dataset construction, knowledge-driven adaptive pass
generation, and data-driven hybrid training pipeline. Experimental results on
standard benchmarks demonstrate that AwareCompiler significantly outperforms
existing baselines in both performance and efficiency, highlighting the
effectiveness of our synergistic knowledge-data-driven approach. Our code is
publicly available at https://github.com/LHY-24/AwareCompiler.

</details>


### [21] [Functional Reasoning for Distributed Systems with Failures](https://arxiv.org/abs/2510.12131)
*Haobin Ni,Robbert van Renesse,Greg Morrisett*

Main category: cs.PL

TL;DR: 本文提出Sync和Async双语言方法，将分布式系统的异步推理形式化为同步推理，并通过编译保证安全属性在异步系统中的保持。


<details>
  <summary>Details</summary>
Motivation: 分布式系统理论中常用的非形式化Hoare式推理方法存在缺陷，需要建立与形式化证明的直接对应关系。

Method: 设计Sync和Async双语言：Sync作为同步数据并行程序，具有函数式指称语义；Async作为异步交互式程序，具有基于轨迹的操作语义。Sync可编译为Async并生成可执行代码。

Result: 证明了Sync程序在其指称语义中证明的任何安全属性，在其编译后的Async程序的操作语义中都能保持。在Rocq中实现了该方法，并验证了BOSCO和SeqPaxos两个容错共识协议的安全属性。

Conclusion: 该方法为分布式系统提供了一种组合式形式推理框架，能够处理包括拜占庭故障在内的复杂情况，将异步系统简化为同步系统进行更直接的推理。

Abstract: Distributed system theory literature often argues for correctness using an
informal, Hoare-like style of reasoning. While these arguments are intuitive,
they have not all been foolproof, and whether they directly correspond to
formal proofs is in question. We formally ground this kind of reasoning and
connect it to standard formal approaches through language design and
meta-analysis, which leads to a functional style of compositional formal
reasoning for a class of distributed systems, including cases involving
Byzantine faults. The core of our approach is twin languages: Sync and Async,
which formalize the insight from distributed system theory that an asynchronous
system can be reduced to a synchronous system for more straightforward
reasoning under certain conditions. Sync describes a distributed system as a
single, synchronous, data-parallel program. It restricts programs syntactically
and has a functional denotational semantics suitable for Hoare-style formal
reasoning. Async models a distributed system as a collection of interacting
monadic programs, one for each non-faulty node in the system. It has a standard
trace-based operational semantics, modeling asynchrony with interleaving. Sync
compiles to Async and can then be extracted to yield executable code. We prove
that any safety property proven for a Sync program in its denotational
semantics is preserved in the operational semantics of its compiled Async
programs. We implement the twin languages in Rocq and verify the safety
properties of two fault-tolerant consensus protocols: BOSCO and SeqPaxos.

</details>


### [22] [Operational methods in semantics](https://arxiv.org/abs/2510.12295)
*Roberto M. Amadio*

Main category: cs.PL

TL;DR: 这些讲义笔记关注编程语言操作语义的抽象模型、基本思想和结果，从程序的抽象计算步骤描述出发，构建语义等价性、规范语言和静态分析。


<details>
  <summary>Details</summary>
Motivation: 操作语义方法需要适度的数学复杂度，能够很好地扩展到各种编程特性，是构建可移植语言实现、指定和测试程序属性的合适框架。

Method: 从程序的抽象计算步骤描述开始，然后在其基础上构建语义等价性、规范语言和静态分析。

Result: 操作语义被证明是构建可移植语言实现、指定和测试程序属性的有效框架，并常规用于更复杂的任务如证明编译器或静态分析器的正确性。

Conclusion: 操作语义方法在编程语言语义中特别有效，它需要适度的数学复杂度，能够很好地扩展到各种编程特性，是实用的语义框架。

Abstract: The focus of these lecture notes is on abstract models and basic ideas and
results that relate to the operational semantics of programming languages
largely conceived. The approach is to start with an abstract description of the
computation steps of programs and then to build on top semantic equivalences,
specification languages, and static analyses. While other approaches to the
semantics of programming languages are possible, it appears that the
operational one is particularly effective in that it requires a moderate level
of mathematical sophistication and scales reasonably well to a large variety of
programming features. In practice, operational semantics is a suitable
framework to build portable language implementations and to specify and test
program properties. It is also used routinely to tackle more ambitious tasks
such as proving the correctness of a compiler or a static analyzer.

</details>


### [23] [GUPPY: Pythonic Quantum-Classical Programming](https://arxiv.org/abs/2510.12582)
*Mark Koch,Alan Lawrence,Kartik Singhal,Seyon Sivarajah,Ross Duncan*

Main category: cs.PL

TL;DR: Guppy是一个嵌入Python的领域特定语言，用于编写具有复杂控制流的高层次混合量子程序，目标是能在真实量子硬件上运行。


<details>
  <summary>Details</summary>
Motivation: 开发一种能在Python语法中编写复杂控制流量子程序的语言，使其能在实际量子硬件上执行。

Method: 创建嵌入Python的领域特定语言Guppy，支持Pythonic语法编写混合量子程序。

Result: 提出了Guppy语言的设计和实现，能够表达复杂的量子控制流。

Conclusion: Guppy为编写高层次混合量子程序提供了有效的Python嵌入式解决方案。

Abstract: We present ongoing work on Guppy, a domain-specific language embedded in
Python that allows users to write high-level hybrid quantum programs with
complex control flow in Pythonic syntax, aiming to run them on actual quantum
hardware.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [24] [Ground Stratification for a Logic of Definitions with Induction](https://arxiv.org/abs/2510.12297)
*Nathan Guermond,Gopalan Nadathur*

Main category: cs.LO

TL;DR: 本文扩展了Abella证明助手中的逻辑定义机制，放宽了严格分层限制，引入了地面分层概念，并将其扩展到包含归纳定义的情况，但发现对归纳定义使用地面分层会导致不一致性。


<details>
  <summary>Details</summary>
Motivation: Abella证明助手的原始逻辑包含严格分层条件，这对某些应用（如基于逻辑关系的语义等价）过于严格。需要找到更灵活的定义机制。

Method: 使用地面分层概念来放宽严格分层限制，并研究其在包含归纳定义的逻辑中的适用性。

Result: 成功将地面分层扩展到包含归纳定义的逻辑，但发现对归纳定义使用地面分层会导致不一致性。

Conclusion: 虽然地面分层可以用于任意固定点定义，但对归纳定义使用地面分层会导致不一致性。所描述的推广方式与实践中逻辑关系的使用方式一致，是构建更灵活定义机制的重要步骤。

Abstract: The logic underlying the Abella proof assistant includes mechanisms for
interpreting atomic predicates through fixed point definitions that can
additionally be treated inductively or co-inductively. However, the original
formulation of the logic includes a strict stratification condition on
definitions that is too restrictive for some applications such as those that
use a logical relations based approach to semantic equivalence. Tiu has shown
how this restriction can be eased by utilizing a weaker notion referred to as
ground stratification. Tiu's results were limited to a version of the logic
that does not treat inductive definitions. We show here that they can be
extended to cover such definitions. While our results are obtained by using
techniques that have been previously deployed in related ways in this context,
their use is sensitive to the particular way in which we generalize the logic.
In particular, although ground stratification may be used with arbitrary
fixed-point definitions, we show that weakening stratification to this form for
inductive definitions leads to inconsistency. The particular generalization we
describe accords well with the way logical relations are used in practice. Our
results are also a intermediate step to building a more flexible form for
definitions into the full logic underlying Abella, which additionally includes
co-induction, generic quantification, and a mechanism referred to as nominal
abstraction for analyzing occurrences of objects in terms that are governed by
generic quantifiers.

</details>


### [25] [Flavors of Quantifiers in Hyperlogics](https://arxiv.org/abs/2510.12298)
*Marek Chalupa,Thomas A. Henzinger,Ana Oliveira da Costa*

Main category: cs.LO

TL;DR: 扩展了hypertrace逻辑，引入了无约束迹量化器，研究不同量化模式对可满足性问题可判定性的影响


<details>
  <summary>Details</summary>
Motivation: 现有超逻辑如HyperLTL只有约束迹量化器，需要研究无约束迹量化器的影响

Method: 在hypertrace逻辑中引入两种迹变量：约束迹变量（范围固定）和无约束迹变量（可赋值为任意迹）

Result: 证明无约束迹量化器的hypertrace逻辑等价于S1S；迹前缀片段等价于HyperQPTL；某些量化模式保持可判定性

Conclusion: 该框架可用于研究时间前缀超逻辑，提供了新的可判定性和不可判定性结果

Abstract: Hypertrace logic is a sorted first-order logic with separate sorts for time
and execution traces. Its formulas specify hyperproperties, which are
properties relating multiple traces. In this work, we extend hypertrace logic
by introducing trace quantifiers that range over the set of all possible
traces. In this extended logic, formulas can quantify over two kinds of trace
variables: constrained trace variables, which range over a fixed set of traces
defined by the model, and unconstrained trace variables, which can be assigned
to any trace. In comparison, hyperlogics such as HyperLTL have only constrained
trace quantifiers. We use hypertrace logic to study how different quantifier
patterns affect the decidability of the satisfiability problem. We prove that
hypertrace logic without constrained trace quantifiers is equivalent to monadic
second-order logic of one successor (S1S), and therefore satisfiable, and that
the trace-prefixed fragment (all trace quantifiers precede all time
quantifiers) is equivalent to HyperQPTL. Moreover, we show that all hypertrace
formulas where the only alternation between constrained trace quantifiers is
from an existential to a universal quantifier are equisatisfiable to formulas
without constraints on their trace variables and, therefore, decidable as well.
Our framework allows us to study also time-prefixed hyperlogics, for which we
provide new decidability and undecidability results

</details>


### [26] [On the Formal Metatheory of the Pure Type Systems using One-sorted Variable Names and Multiple Substitutions](https://arxiv.org/abs/2510.12300)
*Sebastián Urciuoli*

Main category: cs.LO

TL;DR: 开发了使用一阶语法和Stoughton多重替换的Church风格lambda项与Pi类型的转换形式理论，并在Agda系统中机器验证了纯类型系统的基本元理论性质。


<details>
  <summary>Details</summary>
Motivation: 证明使用传统语法且不识别alpha等价lambda项来机械化依赖类型理论的可行性。

Method: 使用一阶语法和Stoughton多重替换开发Church风格lambda项与Pi类型的转换形式理论，并在Agda系统中形式化纯类型系统及其元理论性质。

Result: 成功实现了依赖类型理论的机械化，验证了弱化、语法有效性、alpha转换闭包和替换闭包等基本元理论性质。

Conclusion: 使用传统语法且不识别alpha等价项的依赖类型理论机械化是可行的，为相关形式化工作提供了新方法。

Abstract: We develop formal theories of conversion for Church-style lambda-terms with
Pi-types in first-order syntax using one-sorted variables names and Stoughton's
multiple substitutions. We then formalize the Pure Type Systems along some
fundamental metatheoretic properties: weakening, syntactic validity, closure
under alpha-conversion and substitution. Finally, we compare our formalization
with others related. The whole development has been machine-checked using the
Agda system. Our work demonstrates that the mechanization of dependent type
theory by using conventional syntax and without identifying alpha-convertible
lambda-terms is feasible.

</details>


### [27] [CoLF Logic Programming as Infinitary Proof Exploration](https://arxiv.org/abs/2510.12302)
*Zhibo Chen,Frank Pfenning*

Main category: cs.LO

TL;DR: 本文提出了一种在CoLF^ω_1（一阶CoLF^ω片段）上实现计算即证明构造的方法，通过将逻辑变量解释为通信通道，将计算建模为并发消息传递，并开发了从CoLF^ω_1到Sax语言的编译器。


<details>
  <summary>Details</summary>
Motivation: 现有的逻辑框架如λ-Prolog、Elf等虽然支持计算即证明构造的范式，但都不直接支持无穷对象或证明的表达。CoLF^ω最近发展出了支持无穷对象的能力，本文旨在利用这一特性构建新的计算框架。

Method: 将逻辑变量解释为通信通道，将计算建模为并发消息传递过程。开发了从CoLF^ω_1到Sax语言的编译器，Sax是基于半公理化序列演算证明归约的并行编程语言。

Result: 提出了一个工作进展报告，描述了在CoLF^ω_1上实现计算即证明构造的方法框架，包括逻辑变量到通信通道的映射和相应的编译器设计。

Conclusion: 通过将逻辑变量重新解释为通信通道，可以在支持无穷对象的CoLF^ω_1框架中实现计算即证明构造的范式，这为在逻辑框架中表达并发计算提供了新的可能性。

Abstract: Logical Frameworks such as Automath [de Bruijn, 1968] or LF [Harper et al.,
1993] were originally conceived as metalanguages for the specification of
foundationally uncommitted deductive systems, yielding generic proof checkers.
Their high level of abstraction was soon exploited to also express algorithms
over deductive systems such as theorem provers, type-checkers, evaluators,
compilers, proof transformers, etc. in the paradigm of
computation-as-proof-construction. This has been realized in languages such as
$\lambda$-Prolog [Miller et al., 1991] or Elf [Pfenning, 1991] based on
backward chaining, and LolliMon [Lopez et al., 2005] or Celf [Schack-Nielsen
and Schuermann, 2008], which integrated forward chaining. None of these early
frameworks supported the direct expression of infinitary objects or proofs,
which are available in the recently developed CoLF$^\omega$ [Chen, 2023]. In
this work-in-progress report, we sketch an approach to
computation-as-proof-construction over the first-order fragment of
CoLF$^\omega$ (called CoLF$^\omega_1$ ) that already includes infinitary
objects and proofs. A key idea is the interpretation of logic variables as
communication channels and computation as concurrent message-passing. This is
realized in a concrete compiler from CoLF$^\omega_1$ to Sax, a
proof-theoretically inspired parallel programming language based on the
proof-reduction in the semi-axiomatic sequent calculus [DeYoung et al., 2020].

</details>


### [28] [Type Theory with Single Substitutions](https://arxiv.org/abs/2510.12303)
*Ambrus Kaposi,Szumi Xie*

Main category: cs.LO

TL;DR: 本文提出了单替换演算(SSC)作为类型理论的替代定义，与传统的并行替换演算相比更加简单和最小化，并证明了SSC语法与类别族(CwF)语法在具有依赖函数空间和宇宙层次的理论中是同构的。


<details>
  <summary>Details</summary>
Motivation: 现有的类型理论代数定义大多基于并行替换演算，其中替换形成一个范畴。本文旨在提供一个仅包含单替换和单弱化的更简单、最小化的替代方案。

Method: 定义了一个单替换演算(SSC)，只包含单替换和单弱化，以及八个相关方程：四个描述如何替换变量，四个用于类型检查。通过证明SSC语法与CwF语法之间的同构关系来验证该方法的有效性。

Result: 证明了SSC语法与CwF语法在具有依赖函数空间和宇宙层次的理论中是同构的。SSC与CwF的关系类似于扩展组合演算与lambda演算的关系：前者有更多模型，但语法等价。

Conclusion: SSC提供了一个简单、最小化的替代方案来定义类型理论，与并行替换演算或B系统相比更加简洁。在具有额外类型构造子的情况下，SSC模型可以导出CwF。

Abstract: Type theory can be described as a generalised algebraic theory. This
automatically gives a notion of model and the existence of the syntax as the
initial model, which is a quotient inductive-inductive type. Algebraic
definitions of type theory include Ehrhard's definition of model, categories
with families (CwFs), contextual categories, Awodey's natural models,
C-systems, B-systems. With the exception of B-systems, these notions are based
on a parallel substitution calculus where substitutions form a category. In
this paper we define a single substitution calculus (SSC) for type theory and
show that the SSC syntax and the CwF syntax are isomorphic for a theory with
dependent function space and a hierarchy of universes. SSC only includes single
substitutions and single weakenings, and eight equations relating these: four
equations describe how to substitute variables, and there are four equations on
types which are needed to typecheck the other equations. SSC provides a simple,
minimalistic alternative to parallel substitution calculi or B-systems for
defining type theory. SSC relates to CwF as extensional combinatory calculus
relates to lambda calculus: there are more models of the former, but the
syntaxes are equivalent. If we have some additional type formers, we show that
an SSC model gives rise to a CwF.

</details>


### [29] [Substitution Without Copy and Paste](https://arxiv.org/abs/2510.12304)
*Thorsten Altenkirch,Nathaniel Burke,Philip Wadler*

Main category: cs.LO

TL;DR: 提出一种轻量级方法，避免在带绑定器的语言中重复定义替换和重命名操作，构建了与初始简单类型CwF同构的简单类型CwF。


<details>
  <summary>Details</summary>
Motivation: 在带绑定器的语言中定义替换操作需要重复定义替换和重命名，验证其范畴性质时需要多次重复相同论证，这导致重复工作。

Method: 使用轻量级方法避免重复，通过literate Agda脚本实现，构建简单类型类别与族(CwF)。

Result: 成功构建了与初始简单类型CwF同构的简单类型CwF，避免了重复定义和验证。

Conclusion: 该方法有效解决了带绑定器语言中定义替换操作时的重复问题，为相关理论研究提供了更简洁的实现方式。

Abstract: Defining substitution for a language with binders like the simply typed
$\lambda$-calculus requires repetition, defining substitution and renaming
separately. To verify the categorical properties of this calculus, we must
repeat the same argument many times. We present a lightweight method that
avoids repetition and that gives rise to a simply typed category with families
(CwF) isomorphic to the initial simply typed CwF. Our paper is a literate Agda
script.

</details>


### [30] [Dependently Sorted Nominal Signatures](https://arxiv.org/abs/2510.12305)
*Maribel Fernández,Miguel Pagano,Nora Szasz,Álvaro Tasistro*

Main category: cs.LO

TL;DR: 本文扩展了名义多类签名系统，引入具有实例化形式的抽象（称为广义具体化）作为消除算子，构建了一个支持α-转换但不允许名称携带抽象类型的一阶依赖类型系统。


<details>
  <summary>Details</summary>
Motivation: 为需要处理原始表达式和α-等价的形式系统提供完全形式化处理的基础，能够表示多种有趣演算的判断形式和推理规则。

Method: 扩展名义多类签名，引入广义具体化作为抽象消除算子，构建一阶依赖类型系统，该系统支持α-转换但限制名称不能携带抽象类型。

Result: 提出了系统的规则和性质，进行了表示实验，展示了系统能够表示多种演算的判断和推理规则。

Conclusion: 该系统为构建类型理论提供了基础，能够在其中对具有α-等价的原始表达式进行完全形式化处理。

Abstract: We investigate an extension of nominal many-sorted signatures in which
abstraction has a form of instantiation, called generalised concretion, as
elimination operator (similarly to lambda-calculi). Expressions are then
classified using a system of sorts and sort families that respects
alpha-conversion (similarly to dependently-typed lambda-calculi) but not
allowing names to carry abstraction sorts, thus constituting a first-order
dependent sort system. The system can represent forms of judgement and rules of
inference of several interesting calculi. We present rules and properties of
the system as well as experiments of representation, and discuss how it
constitutes a basis on which to build a type theory where raw expressions with
alpha-equivalence are given a completely formal treatment.

</details>


### [31] [Proceedings of the International Workshop on Verification of Scientific Software](https://arxiv.org/abs/2510.12314)
*Stephen F. Siegel,Ganesh Gopalakrishnan*

Main category: cs.LO

TL;DR: VSS 2025研讨会论文集，聚焦科学软件验证，汇集了软件验证与科学计算领域的研究者，包含5篇同行评审论文、3篇特邀贡献和一系列挑战问题，涵盖演绎验证、浮点误差分析等主题。


<details>
  <summary>Details</summary>
Motivation: 解决大规模科学代码的正确性和可靠性挑战，基于Supercomputing的正确性研讨会系列和2023年NSF/DOE关于科学软件正确性的报告。

Method: 通过研讨会形式汇集研究者，采用同行评审论文、特邀贡献和挑战问题等多种方式，涵盖演绎验证、浮点误差分析、耦合模型规范和领域感知测试等方法。

Result: 展示了该重要领域的广泛视角、问题和解决方案进展，挑战问题有潜力将不同的验证工具整合为协同行动。

Conclusion: VSS 2025作为科学软件验证领域的重要快照，成功汇集了研究者并展示了该领域的多样化方法和进展，为未来工具整合和协同验证提供了基础。

Abstract: This volume contains the proceedings of the Verification of Scientific
Software (VSS 2025) workshop, held on 4 May 2025 at McMaster University,
Canada, as part of ETAPS 2025. VSS brings together researchers in software
verification and scientific computing to address challenges in ensuring the
correctness and reliability of large-scale scientific codes. The program
featured five peer-reviewed papers, three invited contributions, and a set of
challenge problems, covering themes such as deductive verification,
floating-point error analysis, specification of coupled models, and
domain-aware testing. VSS builds on the Correctness Workshop series at
Supercomputing and the 2023 NSF/DOE report on scientific software correctness.
It serves as yet another snapshot of this important area, showcasing a wide
range of perspectives, problems and their solutions in progress, with the
challenge problems having the potential to bring together separate verification
tools into concerted action.

</details>
