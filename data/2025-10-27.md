<div id=toc></div>

# Table of Contents

- [cs.FL](#cs.FL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 12]


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [1] [Language Equivalence is Undecidable in VASS with Restricted Nondeterminism](https://arxiv.org/abs/2510.21514)
*Wojciech Czerwiński,Łukasz Orlikowski*

Main category: cs.FL

TL;DR: 本文证明了二维向量加法系统(VASS)的语言等价性是不可判定的，即使其中一个系统是确定性的，另一个是历史确定性的。


<details>
  <summary>Details</summary>
Motivation: 扩展二维VASS语言等价性不可判定性的研究范围，探索在更受限系统类型下的可判定性边界。

Method: 通过构造性证明，展示即使一个系统是确定性的，另一个是历史确定性的，语言等价性问题仍然不可判定。

Result: 证明了二维VASS的语言等价性在所述条件下是不可判定的，并将结果扩展到双向模拟和语言等价之间的任何等价关系。

Conclusion: 二维VASS的语言等价性即使在高度受限的情况下也是不可判定的，这对相关验证问题的复杂性有重要影响。

Abstract: In this work, we extend undecidability of language equivalence for
two-dimensional Vector Addition System with States (VASS) accepting by
coverability condition. We show that the problem is undecidable even when one
of the two-dimensional VASSs is deterministic and the other is
history-deterministic. Moreover, we observe, that the languages of two
history-deterministic VASSs are equal if and only if each can simulate the
other. This observation allows us to extend the undecidability to any
equivalence relation between two-sided simulation and language equivalence.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [AgentArcEval: An Architecture Evaluation Method for Foundation Model based Agents](https://arxiv.org/abs/2510.21031)
*Qinghua Lu,Dehai Zhao,Yue Liu,Hao Zhang,Liming Zhu,Xiwei Xu,Angela Shi,Tristan Tan,Rick Kazman*

Main category: cs.SE

TL;DR: 提出了AgentArcEval方法，专门用于评估基于基础模型的智能体架构，并提供了智能体特定通用场景目录来指导架构设计和评估。


<details>
  <summary>Details</summary>
Motivation: 传统评估方法无法满足智能体架构的评估需求，因为智能体具有复合架构、自主非确定性行为和持续演化等独特特性。

Method: 开发了AgentArcEval评估方法和智能体特定通用场景目录，通过真实税务助手Luna的案例研究验证其有效性。

Result: 成功展示了AgentArcEval方法和场景目录在评估真实世界智能体架构中的实用性。

Conclusion: AgentArcEval为基于基础模型的智能体架构评估提供了专门解决方案，能够有效应对智能体的复杂特性。

Abstract: The emergence of foundation models (FMs) has enabled the development of
highly capable and autonomous agents, unlocking new application opportunities
across a wide range of domains. Evaluating the architecture of agents is
particularly important as the architectural decisions significantly impact the
quality attributes of agents given their unique characteristics, including
compound architecture, autonomous and non-deterministic behaviour, and
continuous evolution. However, these traditional methods fall short in
addressing the evaluation needs of agent architecture due to the unique
characteristics of these agents. Therefore, in this paper, we present
AgentArcEval, a novel agent architecture evaluation method designed specially
to address the complexities of FM-based agent architecture and its evaluation.
Moreover, we present a catalogue of agent-specific general scenarios, which
serves as a guide for generating concrete scenarios to design and evaluate the
agent architecture. We demonstrate the usefulness of AgentArcEval and the
catalogue through a case study on the architecture evaluation of a real-world
tax copilot, named Luna.

</details>


### [3] [BDiff: Block-aware and Accurate Text-based Code Differencing](https://arxiv.org/abs/2510.21094)
*Yao Lu,Wanwei Liu,Tanghaoran Zhang,Kang Yang,Yang Zhang,Wenyu Xu,Longfei Sun,Xinjun Mao,Shuzheng Gao,Michael R. Lyu*

Main category: cs.SE

TL;DR: BDiff是一种基于文本的代码差异分析算法，能够识别块级和行级编辑操作，相比现有工具能生成更高质量的差异结果。


<details>
  <summary>Details</summary>
Motivation: 现有代码差异分析工具在处理跨多行的块级编辑操作时存在局限，只能将其表示为离散的行级操作序列，这严重影响了开发人员理解代码变更的效率。

Method: 基于传统差异算法构建包含所有可能行映射和块映射的候选集，使用Kuhn-Munkres算法计算最优映射集，最小化编辑脚本大小并贴近开发者意图。

Result: 实验表明BDiff在差异结果质量上优于包括大语言模型在内的五种最先进工具，同时保持竞争力的运行时性能。

Conclusion: BDiff能有效识别块级编辑操作，提高代码差异分析质量，同时揭示了LLM在代码差异任务中的不可靠性和运行效率问题。

Abstract: Code differencing is a fundamental technique in software engineering practice
and research. While researchers have proposed text-based differencing
techniques capable of identifying line changes over the past decade, existing
methods exhibit a notable limitation in identifying edit actions (EAs) that
operate on text blocks spanning multiple lines. Such EAs are common in
developers' practice, such as moving a code block for conditional branching or
duplicating a method definition block for overloading. Existing tools represent
such block-level operations as discrete sequences of line-level EAs, compelling
developers to manually correlate them and thereby substantially impeding the
efficiency of change comprehension. To address this issue, we propose BDiff, a
text-based differencing algorithm capable of identifying two types of
block-level EAs and five types of line-level EAs. Building on traditional
differencing algorithms, we first construct a candidate set containing all
possible line mappings and block mappings. Leveraging the Kuhn-Munkres
algorithm, we then compute the optimal mapping set that can minimize the size
of the edit script (ES) while closely aligning with the original developer's
intent. To validate the effectiveness of BDiff, we selected five
state-of-the-art tools, including large language models (LLMs), as baselines
and adopted a combined qualitative and quantitative approach to evaluate their
performance in terms of ES size, result quality, and running time. Experimental
results show that BDiff produces higher-quality differencing results than
baseline tools while maintaining competitive runtime performance. Our
experiments also show the unreliability of LLMs in code differencing tasks
regarding result quality and their infeasibility in terms of runtime
efficiency. We have implemented a web-based visual differencing tool.

</details>


### [4] [R2ComSync: Improving Code-Comment Synchronization with In-Context Learning and Reranking](https://arxiv.org/abs/2510.21106)
*Zhen Yang,Hongyi Lin,Xiao Yu,Jacky Wai Keung,Shuo Liu,Pak Yuen Patrick Chan,Yicheng Sun,Fengji Zhang*

Main category: cs.SE

TL;DR: R2ComSync是一个基于检索和重排的代码-注释同步方法，通过集成混合检索和多轮重排策略，利用大语言模型提升代码注释同步的性能。


<details>
  <summary>Details</summary>
Motivation: 现有代码-注释同步方法存在泛化能力不足或需要大量特定任务学习资源的问题，而大语言模型在该领域表现不佳，主要原因是缺乏有效的上下文学习示例和正确候选排序机制。

Method: 提出R2ComSync方法，包含两个创新点：(1) 集成混合检索，同时考虑代码-注释语义和变更模式的相似性来创建有效的ICL提示；(2) 多轮重排策略，通过大规模样本分析得出三个重要规则，逐步利用这些规则对LLM推理结果进行重排，优先选择正确率高的候选。

Result: 在三个CCS数据集（涵盖Java和Python）上使用五个最新LLM进行评估，并与五个SOTA方法比较。大量实验证明R2ComSync性能优于其他方法，定量和定性分析均表明该方法同步的注释质量显著更高。

Conclusion: R2ComSync通过检索增强的上下文学习和多轮重排策略，有效解决了LLM在代码-注释同步任务中的局限性，实现了优于现有方法的性能表现。

Abstract: Code-Comment Synchronization (CCS) aims to synchronize the comments with code
changes in an automated fashion, thereby significantly reducing the workload of
developers during software maintenance and evolution. While previous studies
have proposed various solutions that have shown success, they often exhibit
limitations, such as a lack of generalization ability or the need for extensive
task-specific learning resources. This motivates us to investigate the
potential of Large Language Models (LLMs) in this area. However, a pilot
analysis proves that LLMs fall short of State-Of-The-Art (SOTA) CCS approaches
because (1) they lack instructive demonstrations for In-Context Learning (ICL)
and (2) many correct-prone candidates are not prioritized.To tackle the above
challenges, we propose R2ComSync, an ICL-based code-Comment Synchronization
approach enhanced with Retrieval and Re-ranking. Specifically, R2ComSync
carries corresponding two novelties: (1) Ensemble hybrid retrieval. It equally
considers the similarity in both code-comment semantics and change patterns
when retrieval, thereby creating ICL prompts with effective examples. (2)
Multi-turn re-ranking strategy. We derived three significant rules through
large-scale CCS sample analysis. Given the inference results of LLMs, it
progressively exploits three re-ranking rules to prioritize relatively
correct-prone candidates. We evaluate R2ComSync using five recent LLMs on three
CCS datasets covering both Java and Python programming languages, and make
comparisons with five SOTA approaches. Extensive experiments demonstrate the
superior performance of R2ComSync against other approaches. Moreover, both
quantitative and qualitative analyses provide compelling evidence that the
comments synchronized by our proposal exhibit significantly higher quality.}

</details>


### [5] [GreenMalloc: Allocator Optimisation for Industrial Workloads](https://arxiv.org/abs/2510.21405)
*Aidan Dakhama,W. B. Langdon,Hector D. Menendez,Karine Even-Mendoza*

Main category: cs.SE

TL;DR: GreenMalloc是一个基于多目标搜索的框架，用于自动配置内存分配器，使用NSGA II算法和rand_malloc作为轻量级代理基准测试工具，在保持运行时效率的同时显著减少堆使用量。


<details>
  <summary>Details</summary>
Motivation: 传统内存分配器配置通常需要手动调优，效率低下且难以找到最优配置。GreenMalloc旨在自动化这一过程，通过搜索算法找到既能减少内存使用又不会影响性能的最佳配置。

Method: 使用NSGA II多目标遗传算法，结合rand_malloc作为轻量级代理基准测试工具，从执行轨迹中高效探索分配器参数，并将最佳配置转移到gem5系统模拟器中进行验证。

Result: 在多种工作负载下，实验结果显示平均堆使用量最多减少4.1%，同时运行时效率没有损失，反而有0.25%的提升。

Conclusion: GreenMalloc框架能够有效自动化内存分配器配置过程，在保持性能的同时显著减少内存使用，证明了搜索算法在系统优化中的实用价值。

Abstract: We present GreenMalloc, a multi objective search-based framework for
automatically configuring memory allocators. Our approach uses NSGA II and
rand_malloc as a lightweight proxy benchmarking tool. We efficiently explore
allocator parameters from execution traces and transfer the best configurations
to gem5, a large system simulator, in a case study on two allocators: the GNU
C/CPP compiler's glibc malloc and Google's TCMalloc. Across diverse workloads,
our empirical results show up to 4.1 percantage reduction in average heap usage
without loss of runtime efficiency; indeed, we get a 0.25 percantage reduction.

</details>


### [6] [Context Engineering for AI Agents in Open-Source Software](https://arxiv.org/abs/2510.21413)
*Seyedmoein Mohsenimofidi,Matthias Galster,Christoph Treude,Sebastian Baltes*

Main category: cs.SE

TL;DR: 本研究调查了466个开源项目中AI配置文件（如AGENTS.md）的采用情况，发现目前尚无统一结构，开发者以多种方式提供上下文信息。


<details>
  <summary>Details</summary>
Motivation: 随着基于代理的AI编码助手的发展，如何为AI代理提供足够的软件项目上下文信息成为一个挑战。需要了解开发者是否及如何采用AGENTS.md等配置文件格式。

Method: 对466个开源软件项目进行初步研究，分析AI配置文件的采用情况、提供的信息类型、呈现方式以及随时间演变的情况。

Result: 研究发现AI配置文件尚未形成统一结构，开发者在提供上下文信息时存在很大差异（描述性、规定性、禁止性、解释性、条件性等）。

Conclusion: AI配置文件的采用为研究真实世界的提示和上下文工程提供了独特机会，研究文件结构和呈现方式的改进如何影响生成内容质量具有很大潜力。

Abstract: GenAI-based coding assistants have disrupted software development. Their next
generation is agent-based, operating with more autonomy and potentially without
human oversight. One challenge is to provide AI agents with sufficient context
about the software projects they operate in. Like humans, AI agents require
contextual information to develop solutions that are in line with the target
architecture, interface specifications, coding guidelines, standard workflows,
and other project-specific policies. Popular AI agents for software development
(e.g., Claude Code) advocate for maintaining tool-specific version-controlled
Markdown files that cover aspects such as the project structure, building and
testing, or code style. The content of these files is automatically added to
each prompt. AGENTS.md has emerged as a potential standard that consolidates
tool-specific formats. However, little is known about whether and how
developers adopt this format. Therefore, in this paper, we present the results
of a preliminary study investigating the adoption of AI configuration files in
466 open-source software projects, what information developers provide in these
files, how they present that information, and how they evolve over time. Our
findings indicate that there is no established structure yet, and that there is
a lot of variation in terms of how context is provided (descriptive,
prescriptive, prohibitive, explanatory, conditional). We see great potential in
studying which modifications in structure or presentation can positively affect
the quality of the generated content. Finally, our analysis of commits that
have modified AGENTS.md files provides first insights into how projects
continuously extend and maintain these files. We conclude the paper by
outlining how the adoption of AI configuration files in provides a unique
opportunity to study real-world prompt and context engineering.

</details>


### [7] [Does Model Size Matter? A Comparison of Small and Large Language Models for Requirements Classification](https://arxiv.org/abs/2510.21443)
*Mohammad Amin Zadenoori,Vincenzo De Martino,Jacek Dabrowski,Xavier Franch,Alessio Ferrari*

Main category: cs.SE

TL;DR: 该研究比较了大型语言模型(LLMs)和小型语言模型(SLMs)在需求工程任务中的表现，发现SLMs虽然比LLMs小300倍，但在需求分类任务中性能接近甚至在某些方面超过LLMs，且差异无统计学意义。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在需求工程任务中表现出色，但存在计算成本高、数据共享风险和依赖外部服务等问题。小型语言模型提供了轻量级、本地部署的替代方案，但其在需求工程任务中的准确性表现尚不清楚。

Method: 比较了8个模型（3个LLMs和5个SLMs）在PROMISE、PROMISE Reclass和SecReq数据集上的需求分类任务表现。

Result: LLMs平均F1分数比SLMs高2%，但差异无统计学意义。SLMs在所有数据集上几乎达到LLMs的性能，在PROMISE Reclass数据集上的召回率甚至超过LLMs，尽管模型大小最多小300倍。数据集特征对性能的影响比模型大小更重要。

Conclusion: SLMs是需求分类任务中LLMs的有效替代方案，在隐私保护、成本和本地部署方面具有优势。

Abstract: [Context and motivation] Large language models (LLMs) show notable results in
natural language processing (NLP) tasks for requirements engineering (RE).
However, their use is compromised by high computational cost, data sharing
risks, and dependence on external services. In contrast, small language models
(SLMs) offer a lightweight, locally deployable alternative. [Question/problem]
It remains unclear how well SLMs perform compared to LLMs in RE tasks in terms
of accuracy. [Results] Our preliminary study compares eight models, including
three LLMs and five SLMs, on requirements classification tasks using the
PROMISE, PROMISE Reclass, and SecReq datasets. Our results show that although
LLMs achieve an average F1 score of 2% higher than SLMs, this difference is not
statistically significant. SLMs almost reach LLMs performance across all
datasets and even outperform them in recall on the PROMISE Reclass dataset,
despite being up to 300 times smaller. We also found that dataset
characteristics play a more significant role in performance than model size.
[Contribution] Our study contributes with evidence that SLMs are a valid
alternative to LLMs for requirements classification, offering advantages in
privacy, cost, and local deployability.

</details>


### [8] [Scalpel: Automotive Deep Learning Framework Testing via Assembling Model Components](https://arxiv.org/abs/2510.21451)
*Yinglong Zou,Juan Zhai,Chunrong Fang,An Guo,Jiawei Liu,Zhenyu Chen*

Main category: cs.SE

TL;DR: Scalpel是一种针对自动驾驶深度学习框架的测试方法，通过组件级模型生成来检测框架特有的质量问题，如内存崩溃和分配错误。


<details>
  <summary>Details</summary>
Motivation: 现有DL框架测试方法无法检测自动驾驶系统中框架特有的质量问题，如内存限制导致的崩溃，因为生成的测试模型缺乏多输入/输出张量处理、多模态数据处理和多级数据特征提取等必要能力。

Method: Scalpel在模型组件级别生成测试输入模型，通过维护和更新模型组件库（头、颈、骨干网络），选择、变异和组装这些组件来生成支持自动驾驶系统所需能力的模型。

Result: 成功生成的模型被添加回组件库以丰富资源，新生成的模型通过差分测试在自动驾驶系统中部署，以测试汽车DL框架。

Conclusion: Scalpel填补了现有测试方法的空白，能够有效检测自动驾驶DL框架特有的质量问题，提高框架的可靠性和安全性。

Abstract: Deep learning (DL) plays a key role in autonomous driving systems. DL models
support perception modules, equipped with tasks such as object detection and
sensor fusion. These DL models enable vehicles to process multi-sensor inputs
to understand complex surroundings. Deploying DL models in autonomous driving
systems faces stringent challenges, including real-time processing, limited
computational resources, and strict power constraints. To address these
challenges, automotive DL frameworks (e.g., PaddleInference) have emerged to
optimize inference efficiency. However, these frameworks encounter unique
quality issues due to their more complex deployment environments, such as
crashes stemming from limited scheduled memory and incorrect memory allocation.
Unfortunately, existing DL framework testing methods fail to detect these
quality issues due to the failure in deploying generated test input models, as
these models lack three essential capabilities: (1) multi-input/output tensor
processing, (2) multi-modal data processing, and (3) multi-level data feature
extraction. These capabilities necessitate specialized model components, which
existing testing methods neglect during model generation. To bridge this gap,
we propose Scalpel, an automotive DL frameworks testing method that generates
test input models at the model component level. Scalpel generates models by
assembling model components (heads, necks, backbones) to support capabilities
required by autonomous driving systems. Specifically, Scalpel maintains and
updates a repository of model components, generating test inputs by selecting,
mutating, and assembling them. Successfully generated models are added back to
enrich the repository. Newly generated models are then deployed within the
autonomous driving system to test automotive DL frameworks via differential
testing.

</details>


### [9] [Towards Socio-Technical Topology-Aware Adaptive Threat Detection in Software Supply Chains](https://arxiv.org/abs/2510.21452)
*Thomas Welsh,Kristófer Finnsson,Brynjólfur Stefánsson,Helmut Neukirchen*

Main category: cs.SE

TL;DR: 提出利用社会技术模型来支持软件供应链的自适应威胁检测，通过分析XZ Utils攻击案例，强调监控技术和社交数据可以识别可疑行为趋势


<details>
  <summary>Details</summary>
Motivation: 软件供应链攻击日益增多，但现有方法主要关注技术层面的依赖监控和组件控制，缺乏对社会技术动态的理解来指导威胁检测

Method: 提出研究愿景，开发社会技术模型来支持自适应威胁检测，通过分析XZ Utils攻击案例展示该方法的价值

Result: 识别出监控技术和社交数据可以检测可疑行为趋势，从而指导有针对性的漏洞评估

Conclusion: 需要进一步研究开发者分析、软件分析、去中心化适应等技术，并建立软件供应链安全研究测试平台

Abstract: Software supply chains (SSCs) are complex systems composed of dynamic,
heterogeneous technical and social components which collectively achieve the
production and maintenance of software artefacts. Attacks on SSCs are
increasing, yet pervasive vulnerability analysis is challenging due to their
complexity. Therefore, threat detection must be targeted, to account for the
large and dynamic structure, and adaptive, to account for its change and
diversity. While current work focuses on technical approaches for monitoring
supply chain dependencies and establishing component controls, approaches which
inform threat detection through understanding the socio-technical dynamics are
lacking. We outline a position and research vision to develop and investigate
the use of socio-technical models to support adaptive threat detection of SSCs.
We motivate this approach through an analysis of the XZ Utils attack whereby
malicious actors undermined the maintainers' trust via the project's GitHub and
mailing lists. We highlight that monitoring technical and social data can
identify trends which indicate suspicious behaviour to then inform targeted and
intensive vulnerability assessment. We identify challenges and research
directions to achieve this vision considering techniques for developer and
software analysis, decentralised adaptation and the need for a test bed for
software supply chain security research.

</details>


### [10] [Risk Management for Mitigating Benchmark Failure Modes: BenchRisk](https://arxiv.org/abs/2510.21460)
*Sean McGregor,Victor Lu,Vassil Tashev,Armstrong Foundjem,Aishwarya Ramasethu,Sadegh AlMahdi Kazemi Zarkouei,Chris Knotz,Kongtao Chen,Alicia Parrish,Anka Reuel,Heather Frase*

Main category: cs.SE

TL;DR: 该研究提出了BenchRisk框架，用于评估大型语言模型基准测试的风险，识别了57种潜在故障模式和196种缓解策略，帮助用户更准确地判断LLM的适用性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM基准测试存在各种故障模式，可能影响基准的偏差、方差、覆盖范围和用户理解能力，导致部署决策不可靠。

Method: 基于NIST风险管理流程，迭代分析26个流行基准，识别故障模式和缓解策略，开发BenchRisk评分系统评估基准风险。

Result: 所有26个基准在五个维度（全面性、可理解性、一致性、正确性、持久性）中至少一个存在显著风险，BenchRisk可作为元评估基准。

Conclusion: LLM基准测试领域存在重要研究空白，BenchRisk工作流程支持基准间比较，开源工具促进风险识别和缓解策略共享。

Abstract: Large language model (LLM) benchmarks inform LLM use decisions (e.g., "is
this LLM safe to deploy for my use case and context?"). However, benchmarks may
be rendered unreliable by various failure modes that impact benchmark bias,
variance, coverage, or people's capacity to understand benchmark evidence.
Using the National Institute of Standards and Technology's risk management
process as a foundation, this research iteratively analyzed 26 popular
benchmarks, identifying 57 potential failure modes and 196 corresponding
mitigation strategies. The mitigations reduce failure likelihood and/or
severity, providing a frame for evaluating "benchmark risk," which is scored to
provide a metaevaluation benchmark: BenchRisk. Higher scores indicate that
benchmark users are less likely to reach an incorrect or unsupported conclusion
about an LLM. All 26 scored benchmarks present significant risk within one or
more of the five scored dimensions (comprehensiveness, intelligibility,
consistency, correctness, and longevity), which points to important open
research directions for the field of LLM benchmarking. The BenchRisk workflow
allows for comparison between benchmarks; as an open-source tool, it also
facilitates the identification and sharing of risks and their mitigations.

</details>


### [11] [Wisdom and Delusion of LLM Ensembles for Code Generation and Repair](https://arxiv.org/abs/2510.21513)
*Fernando Vallecillos Ruiz,Max Hort,Leon Moonen*

Main category: cs.SE

TL;DR: 研究表明，单个大型语言模型在软件工程任务中存在局限性，而通过多样性策略组合多个模型可以实现比最佳单模型高83%的性能提升，避免共识策略的"流行度陷阱"。


<details>
  <summary>Details</summary>
Motivation: 当前追求单一大型语言模型处理所有软件工程任务既资源密集又忽视了不同模型互补性的潜力，但如何最大化集成模型潜力尚不明确。

Method: 在三个软件工程基准上实证比较了来自五个家族的十个LLM及其集成，评估模型间的互补性，并测试了多种从集成候选池中选择正确答案的启发式方法。

Result: 集成模型的理论性能上限比最佳单模型高83%，基于共识的策略会放大常见但错误的输出，而基于多样性的策略能实现理论潜力的95%，即使在小型双模型集成中也有效。

Conclusion: 基于多样性的集成策略提供了一种成本效益高的方式，通过利用多个LLM来提升性能，避免共识策略的缺陷。

Abstract: Today's pursuit of a single Large Language Model (LMM) for all software
engineering tasks is resource-intensive and overlooks the potential benefits of
complementarity, where different models contribute unique strengths. However,
the degree to which coding LLMs complement each other and the best strategy for
maximizing an ensemble's potential are unclear, leaving practitioners without a
clear path to move beyond single-model systems.
  To address this gap, we empirically compare ten individual LLMs from five
families, and three ensembles of these LLMs across three software engineering
benchmarks covering code generation and program repair. We assess the
complementarity between models and the performance gap between the best
individual model and the ensembles. Next, we evaluate various selection
heuristics to identify correct solutions from an ensemble's candidate pool.
  We find that the theoretical upperbound for an ensemble's performance can be
83% above the best single model. Our results show that consensus-based
strategies for selecting solutions fall into a "popularity trap," amplifying
common but incorrect outputs. In contrast, a diversity-based strategy realizes
up to 95% of this theoretical potential, and proves effective even in small
two-model ensembles, enabling a cost-efficient way to enhance performance by
leveraging multiple LLMs.

</details>


### [12] [Lights-Out: An Automated Ground Segment for unstaffed Satellite Operations](https://arxiv.org/abs/2510.21516)
*Marvin Böcker,Ralph Biggins,Michael Schmeing*

Main category: cs.SE

TL;DR: 德国Heinrich Hertz卫星任务实现了首个周期性无人值守的全自动化地面段系统，支持卫星平台自动化操作、用户实验自主调度和24/7自助服务。


<details>
  <summary>Details</summary>
Motivation: 为德国Heinrich Hertz卫星通信任务开发全自动化地面段，实现非工作时间完全自动化操作，提高任务效率和用户服务灵活性。

Method: 采用自动化跟踪、遥测和指令系统，预规划自动执行计划，用户任务与主任务计划分离并自动协调，配置化自动监控和响应机制，提供自助服务门户。

Result: 成功实现了卫星平台在非工作时间的全自动化操作，用户可通过门户24/7灵活安排实验，支持长期规划和快速响应（小于1分钟）的载荷重配置。

Conclusion: 该自动化地面段概念在Heinrich Hertz任务中成功应用，证明了周期性无人值守卫星操作的可行性，为未来卫星任务提供了高效灵活的操作模式。

Abstract: We present our approach for a periodically unstaffed, fully automated ground
segment. The concept is in use for the first time on the German satellite
communications mission Heinrich Hertz on behalf of the German Space Agency at
DLR. Heinrich Hertz was launched in July 2023 and offers access to scientific
and technical experiments to its users. The mission utilizes major automation
concepts for the satellite platform operations, allowing fully automated
operations outside of office hours. The concept includes tracking, telemetry
and commanding (TTC) of the satellite. Pre-planned and automatically executed
schedules enable commanding without human interaction. The user mission
schedule is planned separately from the main mission schedule and is
automatically de-conflicted. The automatic monitoring concept monitors the
systems of the satellite and all assets in the ground segment and triggers
reactions in operator-configurable ways depending on the mission needs, for
example emergency notifications or automated execution of flight operation
procedures. Additionally, the concept also puts special emphasis on a
self-service user portal that provides flexible access 24/7, even when the
control center is not staffed. The portal allows external users of the payload
to schedule pre-defined experiments, monitor the live execution of the
experiment with browser-based displays and access ground station telemetry and
dedicated RF test equipment during the time of their scheduled experiment.
Tasks can be planned long in advance as well as with a short reaction time
(less than 1 minute), which allows, for example, the reconfiguration of the
payload during a running experiment.

</details>


### [13] [Privacy by Design: Aligning GDPR and Software Engineering Specifications with a Requirements Engineering Approach](https://arxiv.org/abs/2510.21591)
*Oleksandr Kosenkov,Ehsan Zabardast,Davide Fucci,Daniel Mendez,Michael Unterkalmsteiner*

Main category: cs.SE

TL;DR: 本文研究了GDPR合规性中需求与系统规范的联合规范方法，通过建模GDPR法律概念来支持隐私设计，确保问题空间与解决方案空间的关联性。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分处理GDPR中问题空间与解决方案空间的复杂交叉，缺乏对隐私设计规范目标和实践者视角的理解。

Method: 回顾了相关研究并进行实践者访谈，开发并评估了基于GDPR法律概念建模的需求与系统规范方法。

Result: 基于GDPR原始法律概念建模的方法有助于捕获法律知识、支持规范透明度和可追溯性，有效支持隐私设计。

Conclusion: GDPR需求需要在工程生命周期的不同抽象层次中解决，法律知识应被捕获在规范中以满足不同利益相关者需求并确保合规性。

Abstract: Context: Consistent requirements and system specifications are essential for
the compliance of software systems towards the General Data Protection
Regulation (GDPR). Both artefacts need to be grounded in the original text and
conjointly assure the achievement of privacy by design (PbD). Objectives: There
is little understanding of the perspectives of practitioners on specification
objectives and goals to address PbD. Existing approaches do not account for the
complex intersection between problem and solution space expressed in GDPR. In
this study we explore the demand for conjoint requirements and system
specification for PbD and suggest an approach to address this demand. Methods:
We reviewed secondary and related primary studies and conducted interviews with
practitioners to (1) investigate the state-of-practice and (2) understand the
underlying specification objectives and goals (e.g., traceability). We
developed and evaluated an approach for requirements and systems specification
for PbD, and evaluated it against the specification objectives. Results: The
relationship between problem and solution space, as expressed in GDPR, is
instrumental in supporting PbD. We demonstrate how our approach, based on the
modeling GDPR content with original legal concepts, contributes to
specification objectives of capturing legal knowledge, supporting specification
transparency, and traceability. Conclusion: GDPR demands need to be addressed
throughout different levels of abstraction in the engineering lifecycle to
achieve PbD. Legal knowledge specified in the GDPR text should be captured in
specifications to address the demands of different stakeholders and ensure
compliance. While our results confirm the suitability of our approach to
address practical needs, we also revealed specific needs for the future
effective operationalization of the approach.

</details>
