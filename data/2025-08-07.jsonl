{"id": "2508.03830", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.03830", "abs": "https://arxiv.org/abs/2508.03830", "authors": ["Hanwen Guo", "Ben Greenman"], "title": "If-T: A Benchmark for Type Narrowing", "comment": null, "summary": "**Context:** The design of static type systems that can validate\ndynamically-typed programs (**gradually**) is an ongoing challenge. A key\ndifficulty is that dynamic code rarely follows datatype-driven design. Programs\ninstead use runtime tests to narrow down the proper usage of incoming data.\nType systems for dynamic languages thus need a **type narrowing** mechanism\nthat refines the type environment along individual control paths based on\ndominating tests, a form of flow-sensitive typing. In order to express\nrefinements, the type system must have some notion of sets and subsets. Since\nset-theoretic types are computationally and ergonomically complex, the need for\ntype narrowing raises design questions about how to balance precision and\nperformance. **Inquiry:** To date, the design of type narrowing systems has\nbeen driven by intuition, past experience, and examples from users in various\nlanguage communities. There is no standard that captures desirable and\nundesirable behaviors. Prior formalizations of narrowing are also significantly\nmore complex than a standard type system, and it is unclear how the extra\ncomplexity pays off in terms of concrete examples. This paper addresses the\nproblems through If-T, a language-agnostic **design benchmark** for type\nnarrowing that characterizes the abilities of implementations using simple\nprograms that draw attention to fundamental questions. Unlike a traditional\nperformance-focused benchmark, If-T measures a narrowing system's ability to\nvalidate correct code and reject incorrect code. Unlike a test suite, systems\nare not required to fully conform to If-T. Deviations are acceptable provided\nthey are justified by well-reasoned design considerations, such as compile-time\nperformance. **Approach:** If-T is guided by the literature on type narrowing,\nthe documentation of gradual languages such as TypeScript, and experiments with\ntypechecker implementations. We have identified a set of core technical\ndimensions for type narrowing. For each dimension, the benchmark contains a set\nof topics and (at least) two characterizing programs per topic: one that should\ntypecheck and one that should not typecheck. **Knowledge:** If-T provides a\nbaseline to measure type narrowing systems. For researchers, it provides\ncriteria to categorize future designs via its collection of positive and\nnegative examples. For language designers, the benchmark demonstrates the\npayoff of typechecker complexity in terms of concrete examples. Designers can\nuse the examples to decide whether supporting a particular example is\nworthwhile. Both the benchmark and its implementations are freely available\nonline. **Grounding:** We have implemented the benchmark for five typecheckers:\nTypeScript, Flow, Typed Racket, mypy, and Pyright. The results highlight\nimportant differences, such as the ability to track logical implications among\nprogram variables and typechecking for user-defined narrowing predicates.\n**Importance:** Type narrowing is essential for gradual type systems, but the\ntradeoffs between systems with different complexity have been unclear. If-T\nclarifies these tradeoffs by illustrating the benefits and limitations of each\nlevel of complexity. With If-T as a way to assess implementations in a fair,\ncross-language manner, future type system designs can strive for a better\nbalance among precision, annotation burden, and performance."}
{"id": "2508.03831", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.03831", "abs": "https://arxiv.org/abs/2508.03831", "authors": ["Chinmayi Prabhu Baramashetru", "Paola Giannini", "Silvia Lizeth Tapia Tarifa", "Olaf Owe"], "title": "A Type System for Data Privacy Compliance in Active Object Languages", "comment": null, "summary": "Data protection laws such as GDPR aim to give users unprecedented control\nover their personal data. Compliance with these regulations requires\nsystematically considering information flow and interactions among entities\nhandling sensitive data. Privacy-by-design principles advocate embedding data\nprotection into system architectures as a default. However, translating these\nabstract principles into concrete, explicit methods remains a significant\nchallenge. This paper addresses this gap by proposing a language-based approach\nto privacy integration, combining static and runtime techniques. By employing\ntype checking and type inference in an active object language, the framework\nenables the tracking of authorised data flows and the automatic generation of\nconstraints checked at runtime based on user consent. This ensures that\npersonal data is processed in compliance with GDPR constraints. The key\ncontribution of this work is a type system that gather the compliance checks\nand the changes to users consent and integrates data privacy compliance\nverification into system execution. The paper demonstrates the feasibility of\nthis approach through a soundness proof and several examples, illustrating how\nthe proposed language addresses common GDPR requirements, such as user consent,\npurpose limitation, and data subject rights. This work advances the state of\nthe art in privacy-aware system design by offering a systematic and automated\nmethod for integrating GDPR compliance into programming languages. This\ncapability has implications for building trustworthy systems in domains such as\nhealthcare or finance, where data privacy is crucial."}
{"id": "2508.03832", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.03832", "abs": "https://arxiv.org/abs/2508.03832", "authors": ["Andreas Pointner", "Josef Pichler", "Herbert Pr√§hofer"], "title": "Generating Inputs for Grammar Mining using Dynamic Symbolic Execution", "comment": null, "summary": "A vast number of software systems include components that parse and process\nstructured input. In addition to programming languages, which are analyzed by\ncompilers or interpreters, there are numerous components that process\nstandardized or proprietary data formats of varying complexity. Even if such\ncomponents were initially developed and tested based on a specification, such\nas a grammar, numerous modifications and adaptations over the course of\nsoftware evolution can make it impossible to precisely determine which inputs\nthey actually accept. In this situation, grammar mining can be used to\nreconstruct the specification in the form of a grammar. Established approaches\nalready produce useful results, provided that sufficient input data is\navailable to fully cover the input language. However, achieving this\ncompleteness is a major challenge. In practice, only input data recorded during\nthe operation of the software systems is available. If this data is used for\ngrammar mining, the resulting grammar reflects only the actual processed inputs\nbut not the complete grammar of the input language accepted by the software\ncomponent. As a result, edge cases or previously supported features that no\nlonger appear in the available input data are missing from the generated\ngrammar. This work addresses this challenge by introducing a novel approach for\nthe automatic generation of inputs for grammar mining. Although input\ngenerators have already been used for fuzz testing, it remains unclear whether\nthey are also suitable for grammar miners. Building on the grammar miner Mimid,\nthis work presents a fully automated approach to input generation. The approach\nleverages Dynamic Symbolic Execution (DSE) and extends it with two mechanisms\nto overcome the limitations of DSE regarding structured input parsers. First,\nthe search for new inputs is guided by an iterative expansion that starts with\na single-character input and gradually extends it. Second, input generation is\nstructured into a novel three-phase approach, which separates the generation of\ninputs for parser functions. The proposed method was evaluated against a\ndiverse set of eleven benchmark applications from the existing literature.\nResults demonstrate that the approach achieves precision and recall for\nextracted grammars close to those derived from state-of-the-art grammar miners\nsuch as Mimid. Notably, it successfully uncovers subtle features and edge cases\nin parsers that are typically missed by such grammar miners. The effectiveness\nof the method is supported by empirical evidence, showing that it can achieve\nhigh performance in various domains without requiring prior input samples. This\ncontribution is significant for researchers and practitioners in software\nengineering, offering an automated, scalable, and precise solution for grammar\nmining. By eliminating the need for manual input generation, the approach not\nonly reduces workload but also enhances the robustness and comprehensiveness of\nthe extracted grammars. Following this approach, software engineers can\nreconstruct specification from existing (legacy) parsers."}
{"id": "2508.04115", "categories": ["cs.PL", "A.1; C.1.2; D.3.1; F.3.1; F.3.2"], "pdf": "https://arxiv.org/pdf/2508.04115", "abs": "https://arxiv.org/abs/2508.04115", "authors": ["Roger C. Su", "Robert J. Colvin"], "title": "Weak Memory Model Formalisms: Introduction and Survey", "comment": null, "summary": "Memory consistency models define the order in which accesses to shared memory\nin a concurrent system may be observed to occur. Such models are a necessity\nsince program order is not a reliable indicator of execution order, due to\nmicroarchitectural features or compiler transformations. Concurrent\nprogramming, already a challenging task, is thus made even harder when weak\nmemory effects must be addressed. A rigorous specification of weak memory\nmodels is therefore essential to make this problem tractable for developers of\nsafety- and security-critical, low-level software.\n  In this paper we survey the field of formalisations of weak memory models,\nincluding their specification, their effects on execution, and tools and\ninference systems for reasoning about code. To assist the discussion we also\nprovide an introduction to two styles of formal representation found commonly\nin the literature (using a much simplified version of Intel's x86 as the\nexample): a step-by-step construction of traces of the system (operational\nsemantics); and with respect to relations between memory events (axiomatic\nsemantics). The survey covers some long-standing hardware features that lead to\nobservable weak behaviours, a description of historical developments in\npractice and in theory, an overview of computability and complexity results,\nand outlines current and future directions in the field."}
{"id": "2508.03826", "categories": ["cs.FL", "F.1.1; F.4.2; F.4.3"], "pdf": "https://arxiv.org/pdf/2508.03826", "abs": "https://arxiv.org/abs/2508.03826", "authors": ["Smayan Agarwal", "Shobhit Singh", "Aalok Thakkar"], "title": "Identity Testing for Stochastic Languages", "comment": null, "summary": "Determining whether an unknown distribution matches a known reference is a\ncornerstone problem in distributional analysis. While classical results\nestablish a rigorous framework in the case of distributions over finite\ndomains, real-world applications in computational linguistics, bioinformatics,\nand program analysis demand testing over infinite combinatorial structures,\nparticularly strings. In this paper, we initiate the theoretical study of\nidentity testing for stochastic languages, bridging formal language theory with\nmodern distribution property testing.\n  We first propose a polynomial-time algorithm to verify if a finite state\nmachine represents a stochastic language, and then prove that rational\nstochastic languages can approximate an arbitrary probability distribution.\nBuilding on these representations, we develop a truncation-based identity\ntesting algorithm that distinguishes between a known and an unknown\ndistributions with sample complexity $\\widetilde{\\Theta}\\left(\n\\frac{\\sqrt{n}}{\\varepsilon^2} + \\frac{n}{\\log n} \\right)$ where $n$ is the\nsize of the truncated support. Our approach leverages the exponential decay\ninherent in rational stochastic languages to bound truncation error, then\napplies classical finite-domain testers to the restricted problem.\n  This work establishes the first identity testing framework for infinite\ndiscrete distributions, opening new directions in probabilistic formal methods\nand statistical analysis of structured data."}
{"id": "2508.03846", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03846", "abs": "https://arxiv.org/abs/2508.03846", "authors": ["Hashini Gunatilake", "John Grundy", "Rashina Hoda", "Ingo Mueller"], "title": "Empathy Guidelines for Improving Practitioner Well-being & Software Engineering Practices", "comment": null, "summary": "Empathy is a powerful yet often overlooked element in software engineering\n(SE), supporting better teamwork, smoother communication, and effective\ndecision-making. In our previous study, we identified a range of practitioner\nstrategies for fostering empathy in SE contexts. Building on these insights,\nthis paper introduces 17 actionable empathy guidelines designed to support\npractitioners, teams, and organisations. We also explore how these guidelines\ncan be implemented in practice by examining real-world applications,\nchallenges, and strategies to overcome them shared by software practitioners.\nTo support adoption, we present a visual prioritisation framework that\ncategorises the guidelines based on perceived importance, ease of\nimplementation, and willingness to adopt. The findings offer practical and\nflexible suggestions for integrating empathy into everyday SE work, helping\nteams move from principles to sustainable action."}
{"id": "2508.03947", "categories": ["cs.LO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.03947", "abs": "https://arxiv.org/abs/2508.03947", "authors": ["Vishnu Murali", "Mohammed Adib Oumer", "Majid Zamani"], "title": "Control Closure Certificates", "comment": "28 pages, 4 figures, 6 Tables. To appear in International Symposium\n  on Automated Technology for Verification and Analysis (ATVA), 2025", "summary": "This paper introduces the notion of control closure certificates to\nsynthesize controllers for discrete-time control systems against\n$\\omega$-regular specifications. Typical functional approaches to synthesize\ncontrollers against $\\omega$-regular specifications rely on combining inductive\ninvariants (for example, via barrier certificates) with proofs of\nwell-foundedness (for example, via ranking functions). Transition invariants,\nprovide an alternative where instead of standard well-foundedness arguments one\nmay instead search for disjunctive well-foundedness arguments that together\nensure a well-foundedness argument. Closure certificates, functional analogs of\ntransition invariants, provide an effective, automated approach to verify\ndiscrete-time dynamical systems against linear temporal logic and\n$\\omega$-regular specifications. We build on this notion to synthesize\ncontrollers to ensure the satisfaction of $\\omega$-regular specifications. To\ndo so, we first illustrate how one may construct control closure certificates\nto visit a region infinitely often (or only finitely often) via disjunctive\nwell-founded arguments. We then combine these arguments to provide an argument\nfor parity specifications. Thus, finding an appropriate control closure\ncertificate over the product of the system and a parity automaton specifying a\ndesired $\\omega$-regular specification ensures that there exists a controller\n$\\kappa$ to enforce the $\\omega$-regular specification. We propose a\nsum-of-squares optimization approach to synthesize such certificates and\ndemonstrate their efficacy in designing controllers over some case studies."}
{"id": "2508.04458", "categories": ["cs.FL"], "pdf": "https://arxiv.org/pdf/2508.04458", "abs": "https://arxiv.org/abs/2508.04458", "authors": ["Hiroya Fujinami", "Masaki Waga", "Jie An", "Kohei Suenaga", "Nayuta Yanagisawa", "Hiroki Iseri", "Ichiro Hasuo"], "title": "Componentwise Automata Learning for System Integration (Extended Version)", "comment": null, "summary": "Compositional automata learning is attracting attention as an analysis\ntechnique for complex black-box systems. It exploits a target system's internal\ncompositional structure to reduce complexity. In this paper, we identify system\nintegration -- the process of building a new system as a composite of\npotentially third-party and black-box components -- as a new application domain\nof compositional automata learning. Accordingly, we propose a new problem\nsetting, where the learner has direct access to black-box components. This is\nin contrast with the usual problem settings of compositional learning, where\nthe target is a legacy black-box system and queries can only be made to the\nwhole system (but not to components). We call our problem componentwise\nautomata learning for distinction. We identify a challenge there called\ncomponent redundancies: some parts of components may not contribute to\nsystem-level behaviors, and learning them incurs unnecessary effort. We\nintroduce a contextual componentwise learning algorithm that systematically\nremoves such redundancies. We experimentally evaluate our proposal and show its\npractical relevance."}
{"id": "2508.03856", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.03856", "abs": "https://arxiv.org/abs/2508.03856", "authors": ["Richard Hegewald", "Rebecca Beyer"], "title": "Evaluating Software Supply Chain Security in Research Software", "comment": "Accepted at conference GI SKILL 2025", "summary": "The security of research software is essential for ensuring the integrity and\nreproducibility of scientific results. However, research software security is\nstill largely unexplored. Due to its dependence on open source components and\ndistributed development practices, research software is particularly vulnerable\nto supply chain attacks. This study analyses 3,248 high-quality, largely\npeer-reviewed research software repositories using the OpenSSF Scorecard. We\nfind a generally weak security posture with an average score of 3.5/10.\nImportant practices, such as signed releases and branch protection, are rarely\nimplemented. Finally, we present actionable, low-effort recommendations that\ncan help research teams improve software security and mitigate potential\nthreats to scientific integrity."}
{"id": "2508.04438", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2508.04438", "abs": "https://arxiv.org/abs/2508.04438", "authors": ["Mark Chevallier", "Filip Smola", "Richard Schmoetten", "Jacques D. Fleuriot"], "title": "GradSTL: Comprehensive Signal Temporal Logic for Neurosymbolic Reasoning and Learning", "comment": "Accepted for presentation at TIME 2025", "summary": "We present GradSTL, the first fully comprehensive implementation of signal\ntemporal logic (STL) suitable for integration with neurosymbolic learning. In\nparticular, GradSTL can successfully evaluate any STL constraint over any\nsignal, regardless of how it is sampled. Our formally verified approach\nspecifies smooth STL semantics over tensors, with formal proofs of soundness\nand of correctness of its derivative function. Our implementation is generated\nautomatically from this formalisation, without manual coding, guaranteeing\ncorrectness by construction. We show via a case study that using our\nimplementation, a neurosymbolic process learns to satisfy a pre-specified STL\nconstraint. Our approach offers a highly rigorous foundation for integrating\nsignal temporal logic and learning by gradient descent."}
{"id": "2508.03881", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03881", "abs": "https://arxiv.org/abs/2508.03881", "authors": ["Martin Obaidi", "Kushtrim Qengaj", "Jakob Droste", "Hannah Deters", "Marc Herrmann", "Jil Kl√ºnder", "Elisa Schmid", "Kurt Schneider"], "title": "From App Features to Explanation Needs: Analyzing Correlations and Predictive Potential", "comment": "This paper has been accepted at the 33rd IEEE International\n  Requirements Engineering Workshop (REW 2025)", "summary": "In today's digitized world, software systems must support users in\nunderstanding both how to interact with a system and why certain behaviors\noccur. This study investigates whether explanation needs, classified from user\nreviews, can be predicted based on app properties, enabling early consideration\nduring development and large-scale requirements mining. We analyzed a gold\nstandard dataset of 4,495 app reviews enriched with metadata (e.g., app\nversion, ratings, age restriction, in-app purchases). Correlation analyses\nidentified mostly weak associations between app properties and explanation\nneeds, with moderate correlations only for specific features such as app\nversion, number of reviews, and star ratings. Linear regression models showed\nlimited predictive power, with no reliable forecasts across configurations.\nValidation on a manually labeled dataset of 495 reviews confirmed these\nfindings. Categories such as Security & Privacy and System Behavior showed\nslightly higher predictive potential, while Interaction and User Interface\nremained most difficult to predict. Overall, our results highlight that\nexplanation needs are highly context-dependent and cannot be precisely inferred\nfrom app metadata alone. Developers and requirements engineers should therefore\nsupplement metadata analysis with direct user feedback to effectively design\nexplainable and user-centered software systems."}
{"id": "2508.03922", "categories": ["cs.SE", "cs.HC", "D.2.1"], "pdf": "https://arxiv.org/pdf/2508.03922", "abs": "https://arxiv.org/abs/2508.03922", "authors": ["Soroush Heydari"], "title": "A Human Centric Requirements Engineering Framework for Assessing Github Copilot Output", "comment": "8 pages", "summary": "The rapid adoption of Artificial Intelligence(AI) programming assistants such\nas GitHub Copilot introduces new challenges in how these software tools address\nhuman needs. Many existing evaluation frameworks address technical aspects such\nas code correctness and efficiency, but often overlook crucial human factors\nthat affect the successful integration of AI assistants in software development\nworkflows. In this study, I analyzed GitHub Copilot's interaction with users\nthrough its chat interface, measured Copilot's ability to adapt explanations\nand code generation to user expertise levels, and assessed its effectiveness in\nfacilitating collaborative programming experiences. I established a\nhuman-centered requirements framework with clear metrics to evaluate these\nqualities in GitHub Copilot chat. I discussed the test results and their\nimplications for future analysis of human requirements in automated\nprogramming."}
{"id": "2508.03931", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03931", "abs": "https://arxiv.org/abs/2508.03931", "authors": ["Everton Guimaraes", "Nathalia Nascimento", "Chandan Shivalingaiah", "Asish Nelapati"], "title": "Analyzing Prominent LLMs: An Empirical Study of Performance and Complexity in Solving LeetCode Problems", "comment": "11 pages, 13 figures, 29th International Conference on Evaluation and\n  Assessment in Software Engineering (EASE)", "summary": "Large Language Models (LLMs) like ChatGPT, Copilot, Gemini, and DeepSeek are\ntransforming software engineering by automating key tasks, including code\ngeneration, testing, and debugging. As these models become integral to\ndevelopment workflows, a systematic comparison of their performance is\nessential for optimizing their use in real world applications. This study\nbenchmarks these four prominent LLMs on one hundred and fifty LeetCode problems\nacross easy, medium, and hard difficulties, generating solutions in Java and\nPython. We evaluate each model based on execution time, memory usage, and\nalgorithmic complexity, revealing significant performance differences. ChatGPT\ndemonstrates consistent efficiency in execution time and memory usage, while\nCopilot and DeepSeek show variability as task complexity increases. Gemini,\nalthough effective on simpler tasks, requires more attempts as problem\ndifficulty rises. Our findings provide actionable insights into each model's\nstrengths and limitations, offering guidance for developers selecting LLMs for\nspecific coding tasks and providing insights on the performance and complexity\nof GPT-like generated solutions."}
{"id": "2508.03949", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.03949", "abs": "https://arxiv.org/abs/2508.03949", "authors": ["Md. Abdul Awal", "Mrigank Rochan", "Chanchal K. Roy"], "title": "Model Compression vs. Adversarial Robustness: An Empirical Study on Language Models for Code", "comment": null, "summary": "Transformer-based language models for code have shown remarkable performance\nin various software analytics tasks, but their adoption is hindered by high\ncomputational costs, slow inference speeds, and substantial environmental\nimpact. Model compression techniques such as pruning, quantization, and\nknowledge distillation have gained traction in addressing these challenges.\nHowever, the impact of these strategies on the robustness of compressed\nlanguage models for code in adversarial scenarios remains poorly understood.\nUnderstanding how these compressed models behave under adversarial attacks is\nessential for their safe and effective deployment in real-world applications.\nTo bridge this knowledge gap, we conduct a comprehensive evaluation of how\ncommon compression strategies affect the adversarial robustness of compressed\nmodels. We assess the robustness of compressed versions of three widely used\nlanguage models for code across three software analytics tasks, using six\nevaluation metrics and four commonly used classical adversarial attacks. Our\nfindings indicate that compressed models generally maintain comparable\nperformance to their uncompressed counterparts. However, when subjected to\nadversarial attacks, compressed models exhibit significantly reduced\nrobustness. These results reveal a trade-off between model size reduction and\nadversarial robustness, underscoring the need for careful consideration when\ndeploying compressed models in security-critical software applications. Our\nstudy highlights the need for further research into compression strategies that\nstrike a balance between computational efficiency and adversarial robustness,\nwhich is essential for deploying reliable language models for code in\nreal-world software applications."}
{"id": "2508.04125", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04125", "abs": "https://arxiv.org/abs/2508.04125", "authors": ["Sangwon Hyun", "Hyunjun Kim", "Jinhyuk Jang", "Hyojin Choi", "M. Ali Babar"], "title": "Experimental Analysis of Productive Interaction Strategy with ChatGPT: User Study on Function and Project-level Code Generation Tasks", "comment": "The benchmark repository has not been publicly released yet due to\n  the IP policy in our institutions. If you would like to use the benchmark or\n  collaborate on extension, please contact \"dr.sangwon.hyun@gmail.com\"", "summary": "The application of Large Language Models (LLMs) is growing in the productive\ncompletion of Software Engineering tasks. Yet, studies investigating the\nproductive prompting techniques often employed a limited problem space,\nprimarily focusing on well-known prompting patterns and mainly targeting\nfunction-level SE practices. We identify significant gaps in real-world\nworkflows that involve complexities beyond class-level (e.g., multi-class\ndependencies) and different features that can impact Human-LLM Interactions\n(HLIs) processes in code generation. To address these issues, we designed an\nexperiment that comprehensively analyzed the HLI features regarding the code\ngeneration productivity. Our study presents two project-level benchmark tasks,\nextending beyond function-level evaluations. We conducted a user study with 36\nparticipants from diverse backgrounds, asking them to solve the assigned tasks\nby interacting with the GPT assistant using specific prompting patterns. We\nalso examined the participants' experience and their behavioral features during\ninteractions by analyzing screen recordings and GPT chat logs. Our statistical\nand empirical investigation revealed (1) that three out of 15 HLI features\nsignificantly impacted the productivity in code generation; (2) five primary\nguidelines for enhancing productivity for HLI processes; and (3) a taxonomy of\n29 runtime and logic errors that can occur during HLI processes, along with\nsuggested mitigation plans."}
{"id": "2508.04295", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.04295", "abs": "https://arxiv.org/abs/2508.04295", "authors": ["Chaofan Wang", "Tingrui Yu", "Jie Wang", "Dong Chen", "Wenrui Zhang", "Yuling Shi", "Xiaodong Gu", "Beijun Shen"], "title": "EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust Translation", "comment": null, "summary": "Rust's compile-time safety guarantees make it ideal for safety-critical\nsystems, creating demand for translating legacy C codebases to Rust. While\nvarious approaches have emerged for this task, they face inherent trade-offs:\nrule-based solutions face challenges in meeting code safety and idiomaticity\nrequirements, while LLM-based solutions often fail to generate semantically\nequivalent Rust code, due to the heavy dependencies of modules across the\nentire codebase. Recent studies have revealed that both solutions are limited\nto small-scale programs. In this paper, we propose EvoC2Rust, an automated\nframework for converting entire C projects to equivalent Rust ones. EvoC2Rust\nemploys a skeleton-guided translation strategy for project-level translation.\nThe pipeline consists of three evolutionary stages: 1) it first decomposes the\nC project into functional modules, employs a feature-mapping-enhanced LLM to\ntransform definitions and macros and generates type-checked function stubs,\nwhich form a compilable Rust skeleton; 2) it then incrementally translates the\nfunction, replacing the corresponding stub placeholder; 3) finally, it repairs\ncompilation errors by integrating LLM and static analysis. Through evolutionary\naugmentation, EvoC2Rust combines the advantages of both rule-based and\nLLM-based solutions. Our evaluation on open-source benchmarks and six\nindustrial projects demonstrates EvoC2Rust's superior performance in\nproject-level C-to-Rust translation. On average, it achieves 17.24% and 14.32%\nimprovements in syntax and semantic accuracy over the LLM-based approaches,\nalong with a 96.79% higher code safety rate than the rule-based tools. At the\nmodule level, EvoC2Rust reaches 92.25% compilation and 89.53% test pass rates\non industrial projects, even for complex codebases and long functions."}
{"id": "2508.04352", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.04352", "abs": "https://arxiv.org/abs/2508.04352", "authors": ["Dragana Sunaric", "Charlotte Verbruggen", "Dominik Bork"], "title": "Vanilla-Converter: A Tool for Converting Camunda 7 BPMN Models into Camunda 8 Models", "comment": null, "summary": "As organizations prepare for the end-of-life of Camunda 7, manual migration\nremains complex due to fundamental differences between the two platforms. We\npresent Vanilla-Converter, a command-line tool that facilitates the migration\nof BPMN models from Camunda 7 to Camunda 8. Vanilla-Converter automates the\ntransformation process, supports a wide range of BPMN elements, and produces a\ntransformed model and a detailed transformation log indicating automatic\nchanges and remaining manual conversion tasks. The tool's effectiveness is\ndemonstrated through three case studies with real industrially used Camunda 7\nmodels, confirming its ability to convert these models into valid and\nexecutable Camunda 8 models."}
{"id": "2508.04408", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.04408", "abs": "https://arxiv.org/abs/2508.04408", "authors": ["Carlos Andr√©s Ram√≠rez Cata√±o", "Makoto Itoh"], "title": "Breaking New Ground in Software Defect Prediction: Introducing Practical and Actionable Metrics with Superior Predictive Power for Enhanced Decision-Making", "comment": "16 pages, 2 figures, 2 formulas, 12 tables", "summary": "Software defect prediction using code metrics has been extensively researched\nover the past five decades. However, prediction harnessing non-software metrics\nis under-researched. Considering that the root cause of software defects is\noften attributed to human error, human factors theory might offer key\nforecasting metrics for actionable insights. This paper explores automated\nsoftware defect prediction at the method level based on the developers' coding\nhabits. First, we propose a framework for deciding the metrics to conduct\npredictions. Next, we compare the performance of our metrics to that of the\ncode and commit history metrics shown by research to achieve the highest\nperformance to date. Finally, we analyze the prediction importance of each\nmetric. As a result of our analyses of twenty-one critical infrastructure\nlarge-scale open-source software projects, we have presented: (1) a human\nerror-based framework with metrics useful for defect prediction at method\nlevel; (2) models using our proposed metrics achieve better average prediction\nperformance than the state-of-the-art code metrics and history measures; (3)\nthe prediction importance of all metrics distributes differently with each of\nthe novel metrics having better average importance than code and history\nmetrics; (4) the novel metrics dramatically enhance the explainability,\npracticality, and actionability of software defect prediction models,\nsignificantly advancing the field. We present a systematic approach to\nforecasting defect-prone software methods via a human error framework. This\nwork empowers practitioners to act on predictions, empirically demonstrating\nhow developer coding habits contribute to defects in software systems."}
{"id": "2508.04448", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.04448", "abs": "https://arxiv.org/abs/2508.04448", "authors": ["Damian Gnieciak", "Tomasz Szandala"], "title": "Large Language Models Versus Static Code Analysis Tools: A Systematic Benchmark for Vulnerability Detection", "comment": null, "summary": "Modern software relies on a multitude of automated testing and quality\nassurance tools to prevent errors, bugs and potential vulnerabilities. This\nstudy sets out to provide a head-to-head, quantitative and qualitative\nevaluation of six automated approaches: three industry-standard rule-based\nstatic code-analysis tools (SonarQube, CodeQL and Snyk Code) and three\nstate-of-the-art large language models hosted on the GitHub Models platform\n(GPT-4.1, Mistral Large and DeepSeek V3). Using a curated suite of ten\nreal-world C# projects that embed 63 vulnerabilities across common categories\nsuch as SQL injection, hard-coded secrets and outdated dependencies, we measure\nclassical detection accuracy (precision, recall, F-score), analysis latency,\nand the developer effort required to vet true positives. The language-based\nscanners achieve higher mean F-1 scores,0.797, 0.753 and 0.750, than their\nstatic counterparts, which score 0.260, 0.386 and 0.546, respectively. LLMs'\nadvantage originates from superior recall, confirming an ability to reason\nacross broader code contexts. However, this benefit comes with substantial\ntrade-offs: DeepSeek V3 exhibits the highest false-positive ratio, all language\nmodels mislocate issues at line-or-column granularity due to tokenisation\nartefacts. Overall, language models successfully rival traditional static\nanalysers in finding real vulnerabilities. Still, their noisier output and\nimprecise localisation limit their standalone use in safety-critical audits. We\ntherefore recommend a hybrid pipeline: employ language models early in\ndevelopment for broad, context-aware triage, while reserving deterministic\nrule-based scanners for high-assurance verification. The open benchmark and\nJSON-based result harness released with this paper lay a foundation for\nreproducible, practitioner-centric research into next-generation automated code\nsecurity."}
{"id": "2508.04479", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2508.04479", "abs": "https://arxiv.org/abs/2508.04479", "authors": ["Hashini Gunatilake", "John Grundy", "Rashina Hoda", "Ingo Mueller"], "title": "Manifestations of Empathy in Software Engineering: How, Why, and When It Matters", "comment": null, "summary": "Empathy plays a crucial role in software engineering (SE), influencing\ncollaboration, communication, and decision-making. While prior research has\nhighlighted the importance of empathy in SE, there is limited understanding of\nhow empathy manifests in SE practice, what motivates SE practitioners to\ndemonstrate empathy, and the factors that influence empathy in SE work. Our\nstudy explores these aspects through 22 interviews and a large scale survey\nwith 116 software practitioners. Our findings provide insights into the\nexpression of empathy in SE, the drivers behind empathetic practices, SE\nactivities where empathy is perceived as useful or not, and the other factors\nthat influence empathy. In addition, we offer practical implications for SE\npractitioners and researchers, offering a deeper understanding of how to\neffectively integrate empathy into SE processes."}
