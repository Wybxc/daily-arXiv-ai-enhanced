<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 28]
- [cs.LO](#cs.LO) [Total: 5]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.FL](#cs.FL) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [TestWeaver: Execution-aware, Feedback-driven Regression Testing Generation with Large Language Models](https://arxiv.org/abs/2508.01255)
*Cuong Chi Le,Cuong Duc Van,Tung Duy Vu,Thai Minh Pham Vu,Hoang Nhat Phan,Huy Nhat Phan,Tien N. Nguyen*

Main category: cs.SE

TL;DR: TestWeaver是一种基于LLM的新方法，通过轻量级程序分析指导测试生成，解决了现有LLM方法在回归测试中的覆盖停滞问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在回归测试中因缺乏程序执行推理能力导致覆盖停滞，TestWeaver旨在通过更有效的测试生成方法解决这一问题。

Method: TestWeaver结合轻量级程序分析，提供目标行的后向切片、相似测试用例和执行内联注释，以增强LLM的推理能力。

Result: 实验证明，TestWeaver能加速代码覆盖增长并生成更有效的回归测试用例。

Conclusion: TestWeaver通过针对性输入和上下文增强，显著提升了LLM在回归测试中的表现。

Abstract: Regression testing ensures that code changes do not unintentionally break
existing functionality. While recent advances in large language models (LLMs)
have shown promise in automating test generation for regression testing, they
often suffer from limited reasoning about program execution, resulting in
stagnated coverage growth - a phenomenon known as the coverage plateau. In this
paper, we present TestWeaver, a novel LLM-based approach that integrates
lightweight program analysis to guide test generation more effectively.
TestWeaver introduces three key innovations: (1) it reduces hallucinations and
improves focus by supplying the LLM with the backward slice from the target
line instead of full program context; (2) it identifies and incorporates close
test cases - those that share control-flow similarities with the path to the
target line - to provide execution context within the LLM's context window; and
(3) it enhances LLM's reasoning with execution in-line annotations that encode
variable states as comments along executed paths. By equipping LLMs with these
targeted and contextualized inputs, TestWeaver improves coverage-guided test
generation and mitigates redundant explorations. Empirical results demonstrate
that TestWeaver accelerates code coverage growth and generates more effective
regression test cases than existing LLM-based approaches.

</details>


### [2] [Flow Sensitivity without Control Flow Graph: An Efficient Andersen-Style Flow-Sensitive Pointer Analysis](https://arxiv.org/abs/2508.01974)
*Jiahao Zhang,Xiao Cheng,Yuxiang Lei*

Main category: cs.SE

TL;DR: CG-FSPTA提出了一种基于流敏感约束图的指针分析方法，显著提高了效率，同时保持了精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于控制流图的流敏感指针分析方法存在计算效率低的问题。

Method: CG-FSPTA采用流敏感的约束图（FSConsG），结合图优化和动态求解技术。

Result: 实验表明，CG-FSPTA平均减少33.05%内存使用，加速7.27倍。

Conclusion: CG-FSPTA为大规模软件系统分析提供了高效且精确的解决方案。

Abstract: Flow-sensitive pointer analysis constitutes an essential component of precise
program analysis for accurately modeling pointer behaviors by incorporating
control flows. Flow-sensitive pointer analysis is extensively used in alias
analysis, taint analysis, program understanding, compiler optimization, etc.
Existing flow-sensitive pointer analysis approaches, which are conducted based
on control flow graphs, have significantly advanced the precision of pointer
analysis via sophisticated techniques to leverage control flow information.
However, they inevitably suffer from computational inefficiencies when
resolving points-to information due to the inherent complex structures of
control flow graphs. We present CG-FSPTA, a Flow-Sensitive Constraint Graph
(FSConsG) based flow-sensitive pointer analysis to overcome the inefficiency of
control-flow-graph-based analysis. CG-FSPTA uses a flow-sensitive variant to
leverage the structural advantages of set-constraint graphs (which are commonly
used in flow-insensitive pointer analysis) while keeping the flow sensitivity
of variable definitions and uses, allowing the incorporation of sophisticated
graph optimization and dynamic solving techniques. In this way, CG-FSPTA
achieves significant efficiency improvements while keeping the precision of
flow-sensitive analysis. Experimental evaluations on benchmark programs
demonstrate that CG-FSPTA, significantly reduces both memory usage and
execution time while maintaining precision. In particular, by solving in the
FSConsG, CG-FSPTA achieves an average memory reduction of 33.05\% and
accelerates flow-sensitive pointer analysis by 7.27x compared to the
state-of-art method. These experimental results underscore the efficacy of
CG-FSPTA as a scalable solution to analyze large-scale software systems,
establishing a robust foundation for future advancements in efficient program
analysis frameworks.

</details>


### [3] [Screencast-Based Analysis of User-Perceived GUI Responsiveness](https://arxiv.org/abs/2508.01337)
*Wei Liu,Linqiang Guo,Yi Wen Heng,Chenglin Li,Tse-Hsun,Chen,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 提出了一种名为\tool的轻量级黑盒技术，通过分析移动设备屏幕录像直接测量GUI响应性，解决了传统方法难以捕捉用户感知延迟的问题。


<details>
  <summary>Details</summary>
Motivation: GUI响应性对移动应用用户体验至关重要，但现有方法难以准确检测和量化用户感知的延迟，尤其是在大规模工业测试环境中。

Method: \tool利用计算机视觉技术检测用户交互，并通过分析帧级视觉变化计算两个关键指标：响应时间（从用户操作到首次视觉反馈）和完成时间（直到视觉反馈稳定）。

Result: 在2,458次交互的基准测试中，\tool的交互检测精度为0.96，召回率为0.93；响应时间和完成时间的误差分别控制在50ms和100ms以内，覆盖89%以上的交互。

Conclusion: \tool已部署于工业测试流程中，能够高效发现传统工具遗漏的响应性问题，显著提升性能调试效率。

Abstract: GUI responsiveness is critical for a positive user experience in mobile
applications. Even brief delays in visual feedback can frustrate users and lead
to negative reviews. However, detecting and quantifying such user-perceived
delays remains challenging, especially in industrial testing pipelines that
evaluate thousands of apps daily across diverse devices and OS versions.
Existing techniques based on static analysis or system metrics, while useful,
may not accurately capture user-perceived issues or scale effectively.
  In this experience paper, we present \tool, a lightweight and black-box
technique that measures GUI responsiveness directly from mobile screencasts --
video recordings captured during automated GUI testing. \tool detects user
interactions and visual delays, helping developers identify GUI performance
issues that affect the user experience. It uses computer vision to detect user
interactions and analyzes frame-level visual changes to compute two key
metrics: response time (from user action to first visual feedback) and finish
time (until visual feedback stabilizes). We evaluate \tool on a manually
annotated benchmark of 2,458 interactions from 64 popular Android apps. \tool
achieves 0.96 precision and 0.93 recall in detecting interactions, and measures
response and finish times within 50\,ms and 100\,ms error, respectively, for
over 89\% of interactions. The tool has been deployed in an industrial testing
pipeline and analyzes thousands of screencasts daily, uncovering responsiveness
issues missed by traditional tools and improving performance debugging
efficiency.

</details>


### [4] [HyClone: Bridging LLM Understanding and Dynamic Execution for Semantic Code Clone Detection](https://arxiv.org/abs/2508.01357)
*Yunhao Liang,Ruixuan Ying,Takuya Taniguchi,Guwen Lyu,Zhe Cui*

Main category: cs.SE

TL;DR: 提出了一种结合LLM筛选和基于执行的验证的两阶段框架，用于检测Python程序中的语义克隆，显著提升了检测效果。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以捕捉功能等效的语义克隆（Type 4），而直接应用LLM效果不佳。

Method: 两阶段框架：LLM筛选语义相似代码对，执行验证功能等效性。

Result: 实验表明，框架在精确率、召回率和F1分数上显著优于直接LLM检测。

Conclusion: 框架有效识别语义克隆，未来可扩展至跨语言检测和优化大规模应用。

Abstract: Code clone detection is a critical task in software engineering, aimed at
identifying duplicated or similar code fragments within or across software
systems. Traditional methods often fail to capture functional equivalence,
particularly for semantic clones (Type 4), where code fragments implement
identical functionality despite differing syntactic structures. Recent advances
in large language models (LLMs) have shown promise in understanding code
semantics. However, directly applying LLMs to code clone detection yields
suboptimal results due to their sensitivity to syntactic differences. To
address these challenges, we propose a novel two-stage framework that combines
LLM-based screening with execution-based validation for detecting semantic
clones in Python programs. In the first stage, an LLM evaluates code pairs to
filter out obvious non-clones based on semantic analysis. For pairs not
identified as clones, the second stage employs an execution-based validation
approach, utilizing LLM-generated test inputs to assess functional equivalence
through cross-execution validation. Our experimental evaluation demonstrates
significant improvements in precision, recall, and F1-score compared to direct
LLM-based detection, highlighting the framework's effectiveness in identifying
semantic clones. Future work includes exploring cross-language clone detection
and optimizing the framework for large-scale applications.

</details>


### [5] [An Empirical Validation of Open Source Repository Stability Metrics](https://arxiv.org/abs/2508.01358)
*Elijah Kayode Adejumo,Brittany Johnson*

Main category: cs.SE

TL;DR: 论文通过实证验证了基于控制理论的Composite Stability Index (CSI)在开源项目稳定性评估中的可行性，并提出改进方法。


<details>
  <summary>Details</summary>
Motivation: 开源软件在全球软件供应链中的重要性日益增加，但缺乏对其稳定性和可持续性的实证评估方法。

Method: 使用100个高排名GitHub仓库进行实证研究，改进CSI的测量方法（如周提交频率、中位数统计推断）。

Result: 研究发现周提交频率更可行，中位数统计推断能提升问题与拉取请求的稳定性指数。

Conclusion: 实证结果支持控制理论在开源健康评估中的应用，并为实际项目监控工具提供了改进建议。

Abstract: Over the past few decades, open source software has been continuously
integrated into software supply chains worldwide, drastically increasing
reliance and dependence. Because of the role this software plays, it is
important to understand ways to measure and promote its stability and potential
for sustainability. Recent work proposed the use of control theory to
understand repository stability and evaluate repositories' ability to return to
equilibrium after a disturbance such as the introduction of a new feature
request, a spike in bug reports, or even the influx or departure of
contributors. This approach leverages commit frequency patterns, issue
resolution rate, pull request merge rate, and community activity engagement to
provide a Composite Stability Index (CSI). While this framework has theoretical
foundations, there is no empirical validation of the CSI in practice. In this
paper, we present the first empirical validation of the proposed CSI by
experimenting with 100 highly ranked GitHub repositories. Our results suggest
that (1) sampling weekly commit frequency pattern instead of daily is a more
feasible measure of commit frequency stability across repositories and (2)
improved statistical inferences (swapping mean with median), particularly with
ascertaining resolution and review times in issues and pull request, improves
the overall issue and pull request stability index. Drawing on our empirical
dataset, we also derive data-driven half-width parameters that better align
stability scores with real project behavior. These findings both confirm the
viability of a control-theoretic lens on open-source health and provide
concrete, evidence-backed applications for real-world project monitoring tools.

</details>


### [6] [From Technical Excellence to Practical Adoption: Lessons Learned Building an ML-Enhanced Trace Analysis Tool](https://arxiv.org/abs/2508.01430)
*Kaveh Shahedi,Matthew Khouzam,Heng Li,Maxime Lamothe,Foutse Khomh*

Main category: cs.SE

TL;DR: 论文探讨了复杂软件行为分析工具在工业应用中的采用障碍，提出了“卓越悖论”，并通过TMLL的设计解决了这些问题。


<details>
  <summary>Details</summary>
Motivation: 研究工业环境中高级跟踪分析工具采用率低的根本原因，特别是专家知识与实际应用之间的鸿沟。

Method: 通过与Ericsson合作开发TMLL，结合用户反馈和调查（40名行业与学术专家），分析采用障碍并提出解决方案。

Result: 发现用户更注重结果质量与信任（77.5%）而非技术复杂性，67.5%偏好半自动化分析。TMLL通过嵌入专家知识、透明解释和渐进式采用解决了问题。

Conclusion: 可持续采用需转向以采用为中心的设计，强调认知兼容性、嵌入专家知识和透明信任，挑战传统以能力为导向的工具开发模式。

Abstract: System tracing has become essential for understanding complex software
behavior in modern systems, yet sophisticated trace analysis tools face
significant adoption gaps in industrial settings. Through a year-long
collaboration with Ericsson Montr\'eal, developing TMLL (Trace-Server Machine
Learning Library, now in the Eclipse Foundation), we investigated barriers to
trace analysis adoption. Contrary to assumptions about complexity or automation
needs, practitioners struggled with translating expert knowledge into
actionable insights, integrating analysis into their workflows, and trusting
automated results they could not validate. We identified what we called the
Excellence Paradox: technical excellence can actively impede adoption when
conflicting with usability, transparency, and practitioner trust. TMLL
addresses this through adoption-focused design that embeds expert knowledge in
interfaces, provides transparent explanations, and enables incremental
adoption. Validation through Ericsson's experts' feedback, Eclipse Foundation's
integration, and a survey of 40 industry and academic professionals revealed
consistent patterns: survey results showed that 77.5% prioritize quality and
trust in results over technical sophistication, while 67.5% prefer
semi-automated analysis with user control, findings supported by qualitative
feedback from industrial collaboration and external peer review. Results
validate three core principles: cognitive compatibility, embedded expertise,
and transparency-based trust. This challenges conventional capability-focused
tool development, demonstrating that sustainable adoption requires
reorientation toward adoption-focused design with actionable implications for
automated software engineering tools.

</details>


### [7] [Tuning LLM-based Code Optimization via Meta-Prompting: An Industrial Perspective](https://arxiv.org/abs/2508.01443)
*Jingzhi Gong,Rafail Giavrimis,Paul Brookes,Vardan Voskanyan,Fan Wu,Mari Ashiga,Matthew Truscott,Mike Basios,Leslie Kanthan,Jie Xu,Zheng Wang*

Main category: cs.SE

TL;DR: MPCO框架通过元提示技术自动生成跨LLM的高质量代码优化提示，解决了工业平台中多LLM部署的提示工程瓶颈，性能提升达19.06%。


<details>
  <summary>Details</summary>
Motivation: 工业平台中多LLM部署时，针对单一LLM优化的提示在其他LLM上效果不佳，导致高昂的模型特定提示工程成本，限制了多LLM优化系统的实际应用。

Method: MPCO利用元提示技术动态合成上下文感知的优化提示，整合项目元数据、任务需求和LLM特定上下文，并在ARTEMIS平台上实现自动化验证和扩展。

Result: 在五个真实代码库的366小时运行基准测试中，MPCO性能提升最高达19.06%，96%的优化来自有效编辑。

Conclusion: MPCO证明了元提示技术的有效性，全面上下文整合是关键，且三大LLM均可作为元提示器，为工业实践提供了实用见解。

Abstract: There is a growing interest in leveraging large language models (LLMs) for
automated code optimization. However, industrial platforms deploying multiple
LLMs face a critical challenge: prompts optimized for one LLM often fail with
others, requiring expensive model-specific prompt engineering. This cross-model
prompt engineering bottleneck severely limits the practical deployment of
multi-LLM optimization systems in production environments. To address this, we
introduce Meta-Prompted Code Optimization (MPCO), a framework that
automatically generates high-quality, task-specific prompts across diverse LLMs
while maintaining industrial efficiency requirements. MPCO leverages
meta-prompting to dynamically synthesize context-aware optimization prompts by
integrating project metadata, task requirements, and LLM-specific contexts, and
it seamlessly deploys on the ARTEMIS industrial platform for automated
validation and scaling.
  Our comprehensive evaluation on five real-world codebases with 366 hours of
runtime benchmarking demonstrates MPCO's effectiveness: it achieves overall
performance improvements up to 19.06% with the best statistical rank across all
systems compared to baseline methods. Analysis shows that 96% of the
top-performing optimizations stem from meaningful edits. Through systematic
ablation studies and meta-prompter sensitivity analysis, we identify that
comprehensive context integration is essential for effective meta-prompting,
and that all three major LLMs can serve effectively as meta-prompters,
providing actionable insights for industrial practitioners.

</details>


### [8] [Directed Grammar-Based Test Generation](https://arxiv.org/abs/2508.01472)
*Lukas Kirschner,Ezekiel Soremekun*

Main category: cs.SE

TL;DR: FdLoop是一种自动化测试生成方法，通过迭代学习输入属性来生成目标特定的测试输入，显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有语法测试生成器难以针对特定测试目标生成有效输入，FdLoop旨在解决这一问题。

Method: FdLoop通过迭代选择、演化和学习输入分布，结合测试反馈和概率语法，生成目标特定的测试输入。

Result: 在86%的情况下，FdLoop优于五种基线方法，且在诱导错误行为方面效果是最好基线的两倍。

Conclusion: FdLoop能有效实现单一和多个测试目标，且其核心组件对其效果有积极贡献。

Abstract: To effectively test complex software, it is important to generate
goal-specific inputs, i.e., inputs that achieve a specific testing goal.
However, most state-of-the-art test generators are not designed to target
specific goals. Notably, grammar-based test generators, which (randomly)
produce syntactically valid inputs via an input specification (i.e., grammar)
have a low probability of achieving an arbitrary testing goal. This work
addresses this challenge by proposing an automated test generation approach
(called FdLoop) which iteratively learns relevant input properties from
existing inputs to drive the generation of goal-specific inputs. Given a
testing goal, FdLoop iteratively selects, evolves and learn the input
distribution of goal-specific test inputs via test feedback and a probabilistic
grammar. We concretize FdLoop for four testing goals, namely unique code
coverage, input-to-code complexity, program failures (exceptions) and long
execution time. We evaluate FdLoop using three (3) well-known input formats
(JSON, CSS and JavaScript) and 20 open-source software. In most (86%) settings,
FdLoop outperforms all five tested baselines namely the baseline grammar-based
test generators (random, probabilistic and inverse-probabilistic methods),
EvoGFuzz and DynaMosa. FdLoop is (up to) twice (2X) as effective as the best
baseline (EvoGFuzz) in inducing erroneous behaviors. In addition, we show that
the main components of FdLoop (i.e., input mutator, grammar mutator and test
feedbacks) contribute positively to its effectiveness. Finally, our evaluation
demonstrates that FdLoop effectively achieves single testing goals (revealing
erroneous behaviors, generating complex inputs, or inducing long execution
time) and scales to multiple testing goals across varying parameter settings.

</details>


### [9] [GitHub Marketplace: Driving Automation and Fostering Innovation in Software Development](https://arxiv.org/abs/2508.01489)
*SK. Golam Saroar,Waseefa Ahmed,Elmira Onagh,Maleknaz Nayebi*

Main category: cs.SE

TL;DR: 本文分析了GitHub Marketplace在开源软件生态系统中的作用，探讨了自动化工具的趋势与学术研究的脱节，并提出弥合差距的方法。


<details>
  <summary>Details</summary>
Motivation: GitHub Marketplace作为自动化工具平台，对开源软件生态系统产生了深远影响，但学术界与行业实践之间存在脱节，需要系统分析以弥合差距。

Method: 通过系统分析GitHub Marketplace的趋势和特点，并与学术文献中的进展进行比较。

Result: 研究发现行业工具与学术研究之间存在差异，并识别了学术界可以贡献的实际创新领域。

Conclusion: 研究为弥合学术界与行业实践之间的差距提供了方向，强调了学术研究对实际创新的潜在贡献。

Abstract: GitHub, a central hub for collaborative software development, has
revolutionized the open-source software (OSS) ecosystem through its GitHub
Marketplace, a platform launched in 2017 to host automation tools aimed at
enhancing the efficiency and scalability of software projects. As the adoption
of automation in OSS production grows, understanding the trends,
characteristics, and underlying dynamics of this marketplace has become vital.
Furthermore, despite the rich repository of academic research on software
automation, a disconnect persists between academia and industry practices. This
study seeks to bridge this gap by providing a systematic analysis of the GitHub
Marketplace, comparing trends observed in industry tools with advancements
reported in academic literature, and identifying areas where academia can
contribute to practical innovation.

</details>


### [10] [OpenLambdaVerse: A Dataset and Analysis of Open-Source Serverless Applications](https://arxiv.org/abs/2508.01492)
*Angel C. Chavez-Moreno,Cristina L. Abad*

Main category: cs.SE

TL;DR: OpenLambdaVerse提供了一个基于Serverless Framework和AWS Lambda的GitHub仓库数据集，分析了当前无服务器架构的现状，包括应用规模、语言、触发方式、项目成熟度和安全性。


<details>
  <summary>Details</summary>
Motivation: 随着无服务器计算的快速发展，需要了解实际项目中这些工具的使用情况，填补现有研究的空白。

Method: 基于Wonderless数据集的方法，通过新的过滤步骤创建了一个使用Serverless Framework和AWS Lambda的GitHub仓库数据集，并对其进行分析。

Result: 数据集揭示了当前无服务器应用的规模、复杂性、语言选择、触发方式、项目成熟度和安全性实践。

Conclusion: OpenLambdaVerse为实践者和研究人员提供了有价值的资源，帮助他们更好地理解无服务器工作负载的演变。

Abstract: Function-as-a-Service (FaaS) is at the core of serverless computing, enabling
developers to easily deploy applications without managing computing resources.
With an Infrastructure-as-Code (IaC) approach, frameworks like the Serverless
Framework use YAML configurations to define and deploy APIs, tasks, workflows,
and event-driven applications on cloud providers, promoting zero-friction
development. As with any rapidly evolving ecosystem, there is a need for
updated insights into how these tools are used in real-world projects. Building
on the methodology established by the Wonderless dataset for serverless
computing (and applying multiple new filtering steps), OpenLambdaVerse
addresses this gap by creating a dataset of current GitHub repositories that
use the Serverless Framework in applications that contain one or more AWS
Lambda functions. We then analyze and characterize this dataset to get an
understanding of the state-of-the-art in serverless architectures based on this
stack. Through this analysis we gain important insights on the size and
complexity of current applications, which languages and runtimes they employ,
how are the functions triggered, the maturity of the projects, and their
security practices (or lack of). OpenLambdaVerse thus offers a valuable,
up-to-date resource for both practitioners and researchers that seek to better
understand evolving serverless workloads.

</details>


### [11] [Exploring Direct Instruction and Summary-Mediated Prompting in LLM-Assisted Code Modification](https://arxiv.org/abs/2508.01523)
*Ningzhi Tang,Emory Smith,Yu Huang,Collin McMillan,Toby Jia-Jun Li*

Main category: cs.SE

TL;DR: 研究探讨了使用大语言模型（LLM）修改代码的两种提示策略：直接指令提示和摘要中介提示，发现开发者选择策略受任务目标和上下文影响。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM生成代码已被广泛研究，但其在代码修改中的作用尚不明确，且构建有效的修改提示存在挑战。

Method: 通过15名开发者在多种场景下使用两种提示策略（直接指令提示和摘要中介提示）完成修改任务的探索性研究。

Result: 直接指令提示更灵活易指定，摘要中介提示有助于理解和控制；开发者选择策略受任务目标和上下文影响。

Conclusion: 研究强调需要更易用的提示交互，包括可调摘要粒度、可靠的摘要-代码可追溯性和生成摘要的一致性。

Abstract: This paper presents a study of using large language models (LLMs) in
modifying existing code. While LLMs for generating code have been widely
studied, their role in code modification remains less understood. Although
"prompting" serves as the primary interface for developers to communicate
intents to LLMs, constructing effective prompts for code modification
introduces challenges different from generation. Prior work suggests that
natural language summaries may help scaffold this process, yet such approaches
have been validated primarily in narrow domains like SQL rewriting. This study
investigates two prompting strategies for LLM-assisted code modification:
Direct Instruction Prompting, where developers describe changes explicitly in
free-form language, and Summary-Mediated Prompting, where changes are made by
editing the generated summaries of the code. We conducted an exploratory study
with 15 developers who completed modification tasks using both techniques
across multiple scenarios. Our findings suggest that developers followed an
iterative workflow: understanding the code, localizing the edit, and validating
outputs through execution or semantic reasoning. Each prompting strategy
presented trade-offs: direct instruction prompting was more flexible and easier
to specify, while summary-mediated prompting supported comprehension, prompt
scaffolding, and control. Developers' choice of strategy was shaped by task
goals and context, including urgency, maintainability, learning intent, and
code familiarity. These findings highlight the need for more usable prompt
interactions, including adjustable summary granularity, reliable summary-code
traceability, and consistency in generated summaries.

</details>


### [12] [RepoForge: Training a SOTA Fast-thinking SWE Agent with an End-to-End Data Curation Pipeline Synergizing SFT and RL at Scale](https://arxiv.org/abs/2508.01550)
*Zhilong Chen,Chengzong Zhao,Boyuan Chen,Dayi Lin,Yihao Chen,Arthur Leung,Gopi Krishnan Rajbahadur,Gustavo A. Oliva,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: RepoForge是一个自动化端到端流水线，用于生成、评估和训练软件工程（SWE）代理，解决了基础设施昂贵、评估效率低、数据稀缺和质量控制成本高等问题。


<details>
  <summary>Details</summary>
Motivation: 解决SWE LLMs训练中的瓶颈问题，包括昂贵的基础设施、低效的评估流程、稀缺的训练数据和高质量控制的成本。

Method: RepoForge通过统一的存储高效沙盒、Ray驱动的评估工具、自动化数据生成、基于SPICE的标注和无气泡RL框架，实现了高效的SWE代理训练。

Result: RepoForge-8B-Agent在SWE-Bench-Verified上达到17.4%的准确率，存储减少14倍，评估速度提升70%，标注成本降低19,000倍。

Conclusion: RepoForge展示了即使是≤8B的模型也能在SWE-Bench-Verified等要求高的基准测试中达到最新技术水平，解决了SWE代理训练中的关键瓶颈。

Abstract: Training software engineering (SWE) LLMs is bottlenecked by expensive
infrastructure, inefficient evaluation pipelines, scarce training data, and
costly quality control. We present RepoForge, an autonomous, end-to-end
pipeline that generates, evaluates, and trains SWE agents at scale. Our key
contributions include: (1) RepoForge-8B-Agent, achieving 17.4\% on
SWE-Bench-Verified~\citep{swebench_verified2024}, establishing new
state-of-the-art for $\leq$8B non-thinking LLMs; (2) 7,304 executable
environments auto-generated from real GitHub commits with zero manual
intervention; (3) 14$\times$ storage reduction (1.4GB $\rightarrow$ 102MB per
instance) via intelligent dependency management and image pruning; (4) $>$70\%
faster evaluation using a Ray-powered~\citep{ray2018} distributed RepoForge
harness; (5) 19,000$\times$ cheaper labeling through our automated
SPICE~\citep{spice2024} difficulty assessment technique. By unifying
storage-efficient sandboxing, Ray-powered evaluation harness, automated data
generation, SPICE-based labeling, and bubble-free RL scaffold, we demonstrate
that even $\leq$8B models can reach new state-of-the-art performance on
demanding benchmarks like SWE-Bench-Verified. Our approach addresses critical
bottlenecks in SWE agent training: high storage costs of container-based
evaluation, inefficient sequential reward pipelines, limited availability of
high-quality training data, expensive manual labeling, and multi-turn RL
pipeline bottlenecks.

</details>


### [13] [PCREQ: Automated Inference of Compatible Requirements for Python Third-party Library Upgrades](https://arxiv.org/abs/2508.02023)
*Huashan Lei,Guanping Xiao,Yepang Liu,Zheng Zheng*

Main category: cs.SE

TL;DR: PCREQ是一种自动化工具，用于解决Python第三方库升级中的版本和代码兼容性问题，通过整合多个模块实现高效兼容性推断，显著优于现有工具。


<details>
  <summary>Details</summary>
Motivation: Python第三方库升级常引发兼容性问题，现有工具未能全面解决版本和代码兼容性问题，亟需自动化解决方案。

Method: PCREQ结合版本和代码兼容性分析，通过六个模块（如知识获取、兼容性评估等）自动推断兼容版本并生成修复报告。

Result: 在REQBench基准测试中，PCREQ成功率达94.03%，远超其他工具，平均处理时间为60.79秒。

Conclusion: PCREQ显著减少手动调试工作，推动了Python依赖维护的自动化进程。

Abstract: Python third-party libraries (TPLs) are essential in modern software
development, but upgrades often cause compatibility issues, leading to system
failures. These issues fall into two categories: version compatibility issues
(VCIs) and code compatibility issues (CCIs). Existing tools mainly detect
dependency conflicts but overlook code-level incompatibilities, with no
solution fully automating the inference of compatible versions for both VCIs
and CCIs. To fill this gap, we propose PCREQ, the first approach to
automatically infer compatible requirements by combining version and code
compatibility analysis. PCREQ integrates six modules: knowledge acquisition,
version compatibility assessment, invoked APIs and modules extraction, code
compatibility assessment, version change, and missing TPL completion. PCREQ
collects candidate versions, checks for conflicts, identifies API usage,
evaluates code compatibility, and iteratively adjusts versions to generate a
compatible requirements.txt with a detailed repair report. To evaluate PCREQ,
we construct REQBench, a large-scale benchmark with 2,095 upgrade test cases
(including 406 unsolvable by pip). Results show PCREQ achieves a 94.03%
inference success rate, outperforming PyEGo (37.02%), ReadPyE (37.16%), and
LLM-based approaches (GPT-4o, DeepSeek V3/R1) by 18-20%. PCREQ processes each
case from REQBench in 60.79s on average, demonstrating practical efficiency.
PCREQ significantly reduces manual effort in troubleshooting upgrades,
advancing Python dependency maintenance automation.

</details>


### [14] [BiFuzz: A Two-Stage Fuzzing Tool for Open-World Video Games](https://arxiv.org/abs/2508.02144)
*Yusaku Kato,Norihiro Yoshida,Erina Makihara,Katsuro Inoue*

Main category: cs.SE

TL;DR: BiFuzz是一种针对开放世界视频游戏的两阶段模糊测试工具，通过逐步变异游戏策略和测试用例，能够检测“卡住”故障。


<details>
  <summary>Details</summary>
Motivation: 开放世界视频游戏的搜索空间较大，传统测试自动化方法面临挑战。

Method: 提出BiFuzz，一种两阶段模糊测试工具，通过逐步变异游戏策略和测试用例生成新输入。

Result: BiFuzz能够检测“卡住”故障，并展示了其有效性。

Conclusion: BiFuzz为开放世界视频游戏的自动化测试提供了一种有效工具。

Abstract: Open-world video games present a broader search space than other games,
posing challenges for test automation. Fuzzing, which generates new inputs by
mutating an initial input, is commonly used to uncover failures. In this study,
we proposed BiFuzz, a two-stage fuzzer designed for automated testing of
open-world video games, and investigated its effectiveness. The results
revealed that BiFuzz mutated the overall strategy of gameplay and test cases,
including actual movement paths, step by step. Consequently, BiFuzz can detect
`stucking' failures. The tool and its video are at
https://github.com/Yusaku-Kato/BiFuzz.

</details>


### [15] [An MLIR-based Compilation Framework for Control Flow Management on CGRAs](https://arxiv.org/abs/2508.02167)
*Yuxuan Wang,Cristian Tirelli,Giovanni Ansaloni,Laura Pozzi,David Atienza*

Main category: cs.SE

TL;DR: 论文提出了一种针对粗粒度可重构阵列（CGRA）的编译框架，通过优化控制流管理，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有CGRA编译器主要关注数据流，缺乏对控制流的支持，限制了其适用范围和性能。本文旨在解决这一问题。

Method: 提出模块化编译框架，包含转换和优化步骤，支持任意控制流的应用，并引入新的映射方法以解决硬件资源限制。

Result: 框架性能提升高达2.1倍，优于现有方法。

Conclusion: 通过编译优化，控制流管理可以高效实现，且硬件无关，适用于广泛的应用场景。

Abstract: Coarse Grained Reconfigurable Arrays (CGRAs) present both high flexibility
and efficiency, making them well-suited for the acceleration of intensive
workloads. Nevertheless, a key barrier towards their widespread adoption is
posed by CGRA compilation, which must cope with a multi-dimensional space
spanning both the spatial and the temporal domains. Indeed, state-of-the-art
compilers are limited in scope as they mostly deal with the data flow of
applications, while having little or no support for control flow. Hence, they
mostly target the mapping of single loops and/or delegate the management of
control flow divergences to ad-hoc hardware units.
  Conversely, in this paper we show that control flow can be effectively
managed and optimized at the compilation level, allowing for a broad set of
applications to be targeted while being hardware-agnostic and achieving high
performance. We embody our methodology in a modular compilation framework
consisting of transformation and optimization passes, enabling support for
applications with arbitrary control flows running on abstract CGRA meshes. We
also introduce a novel mapping methodology that acts as a compilation back-end,
addressing the limitations in available CGRA hardware resources and
guaranteeing a feasible solution in the compilation process. Our framework
achieves up to 2.1X speedups over state-of-the-art approaches, purely through
compilation optimizations.

</details>


### [16] [Highly Interactive Testing for Uninterrupted Development Flow](https://arxiv.org/abs/2508.02176)
*Andrew Tropin*

Main category: cs.SE

TL;DR: 本文介绍了一个库，用于在高度交互的开发环境（HIDE）中无缝集成测试，减少开发流程中的中断。


<details>
  <summary>Details</summary>
Motivation: 传统测试方法在HIDE中隔离了测试工具，导致开发流程中断，影响开发者的注意力。

Method: 提出一个提供测试运行时表示的库，实现与HIDE的紧密集成，并在测试失败时提供即时访问HIDE工具的能力。

Result: 展示了增强的测试开发工作流程，实现了亚秒级的测试重新执行时间，有助于保持开发者专注。

Conclusion: 该库显著改善了HIDE中的测试体验，减少了开发流程中的中断。

Abstract: Highly interactive development environments (HIDEs) enable uninterrupted
development flow through continuous program evolution and rapid hypothesis
checking. However, traditional testing approaches -- typically executed
separately via CLI -- isolate tests from HIDE tooling (interactive debuggers,
value and stack inspectors, etc.) and introduce disruptive delays due to coarse
execution granularity and lack of runtime context. This disconnect breaks
development flow by exceeding critical attention thresholds. In this paper we
present a library that provides runtime representation for tests, allowing
tight integration with HIDEs, and enabling immediate access to HIDE tooling in
the context of test failure. We then describe development workflows enhanced
with testing and demonstrate how they achieve subsecond test reexecution times
crucial for maintaining developer focus.

</details>


### [17] [A Methodological Framework for LLM-Based Mining of Software Repositories](https://arxiv.org/abs/2508.02233)
*Vincenzo De Martino,Joel Castaño,Fabio Palomba,Xavier Franch,Silverio Martínez-Fernández*

Main category: cs.SE

TL;DR: 该论文探讨了大型语言模型（LLMs）在软件工程研究中的应用，特别是挖掘软件仓库（MSR）任务中的方法论整合问题。通过混合方法研究，提出了PRIMES 2.0框架，以支持透明和可重复的LLM-based MSR研究。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在MSR研究中越来越受欢迎，但其方法论整合仍缺乏深入理解。现有研究多关注特定能力或性能基准，未能全面展示LLMs在整个研究流程中的应用。

Method: 采用混合方法研究，结合快速文献综述和问卷调查，分析LLM4MSR领域中的方法论和威胁。

Result: 识别了15种方法论、9种主要威胁及25种缓解策略，并提出了PRIMES 2.0框架，包含6个阶段和23个子步骤。

Conclusion: PRIMES 2.0框架为LLM-based MSR研究提供了透明和可重复的方法论支持，填补了现有研究的空白。

Abstract: Large Language Models (LLMs) are increasingly used in software engineering
research, offering new opportunities for automating repository mining tasks.
However, despite their growing popularity, the methodological integration of
LLMs into Mining Software Repositories (MSR) remains poorly understood.
Existing studies tend to focus on specific capabilities or performance
benchmarks, providing limited insight into how researchers utilize LLMs across
the full research pipeline. To address this gap, we conduct a mixed-method
study that combines a rapid review and questionnaire survey in the field of
LLM4MSR. We investigate (1) the approaches and (2) the threats that affect the
empirical rigor of researchers involved in this field. Our findings reveal 15
methodological approaches, nine main threats, and 25 mitigation strategies.
Building on these findings, we present PRIMES 2.0, a refined empirical
framework organized into six stages, comprising 23 methodological substeps,
each mapped to specific threats and corresponding mitigation strategies,
providing prescriptive and adaptive support throughout the lifecycle of
LLM-based MSR studies. Our work contributes to establishing a more transparent
and reproducible foundation for LLM-based MSR research.

</details>


### [18] [Dialogue Systems Engineering: A Survey and Future Directions](https://arxiv.org/abs/2508.02279)
*Mikio Nakano,Hironori Takeuchi,Sadahiro Yoshikawa,Yoichi Matsuyama,Kazunori Komatani*

Main category: cs.SE

TL;DR: 本文提出将对话系统生命周期的软件工程领域称为对话系统工程，并综述该领域及其未来方向。随着大语言模型的进步，对话系统技术有望解决社会问题和商业需求，需正确高效地构建、运营和改进。本文基于SWEBOK 4.0列举对话系统工程的知识领域，并探讨未探索主题和未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，对话系统技术进步显著，需适应社会与商业需求，因此需专门针对对话系统演进软件工程方法。

Method: 基于SWEBOK 4.0列举对话系统工程的知识领域，并综述各领域。

Result: 识别了各知识领域中的未探索主题。

Conclusion: 讨论了对话系统工程的未来发展方向。

Abstract: This paper proposes to refer to the field of software engineering related to
the life cycle of dialogue systems as Dialogue Systems Engineering, and surveys
this field while also discussing its future directions. With the advancement of
large language models, the core technologies underlying dialogue systems have
significantly progressed. As a result, dialogue system technology is now
expected to be applied to solving various societal issues and in business
contexts. To achieve this, it is important to build, operate, and continuously
improve dialogue systems correctly and efficiently. Accordingly, in addition to
applying existing software engineering knowledge, it is becoming increasingly
important to evolve software engineering tailored specifically to dialogue
systems. In this paper, we enumerate the knowledge areas of dialogue systems
engineering based on those of software engineering, as defined in the Software
Engineering Body of Knowledge (SWEBOK) Version 4.0, and survey each area. Based
on this survey, we identify unexplored topics in each area and discuss the
future direction of dialogue systems engineering.

</details>


### [19] [Interoperable verification and dissemination of software assets in repositories using COAR Notify](https://arxiv.org/abs/2508.02335)
*Matteo Cancellieri,Martin Docekal,David Pride,Morane Gruenpeter,David Douard,Petr Knoth*

Main category: cs.SE

TL;DR: SoFAIR项目（2024-2025）提出了一种利用机器学习工具从研究论文中提取软件提及的工作流，结合COAR Notify协议，提升研究软件的可见性和可信度。


<details>
  <summary>Details</summary>
Motivation: 开放研究软件的发现、归属和重用性常因其在学术文献中的隐蔽性而受限。

Method: 项目采用机器学习工具提取软件提及，并整合存储库系统、作者及服务（如HAL和Software Heritage），结合COAR Notify协议实现系统间互操作通信。

Result: 通过自动化流程确保研究软件的正确归档、引用和可访问性，符合FAIR原则。

Conclusion: SoFAIR工作流和COAR Notify协议的整合有望显著提升研究软件作为一流文献记录的可见性和可信度。

Abstract: The discoverability, attribution, and reusability of open research software
are often hindered by its obscurity within academic manuscripts. To address
this, the SoFAIR project (2024-2025) introduces a comprehensive workflow
leveraging machine learning tools for extracting software mentions from
research papers. The project integrates repository systems, authors, and
services like HAL and Software Heritage to ensure proper archiving, citation,
and accessibility of research software in alignment with FAIR principles. To
enable interoperable communication across the various systems we present an
integration of the COAR Notify Protocol, which facilitates automated,
interoperable communication among repositories and authors to validate and
disseminate software mentions. This paper outlines the SoFAIR workflow and the
implementation of the COAR Notify Protocol, emphasising its potential to
enhance the visibility and credibility of research software as first-class
bibliographic records.

</details>


### [20] [Vision Language Model-based Testing of Industrial Autonomous Mobile Robots](https://arxiv.org/abs/2508.02338)
*Jiahui Wu,Chengjie Lu,Aitor Arrieta,Shaukat Ali,Thomas Peyrucain*

Main category: cs.SE

TL;DR: 提出了一种基于视觉语言模型（VLM）的测试方法RVSG，用于生成违反工业AMR功能和安全要求的多样化人类行为，以测试AMR的安全性。


<details>
  <summary>Details</summary>
Motivation: 由于人类行为不可预测且实际测试成本高、风险大，需要一种高效的方法来测试AMR的安全性。

Method: 利用VLM生成违反要求的人类行为，并在模拟器中评估AMR的反应。

Result: RVSG能有效生成违反要求的场景，并增加机器人行为的多样性，揭示其不确定性。

Conclusion: RVSG为AMR的安全测试提供了一种高效且实用的解决方案。

Abstract: Autonomous Mobile Robots (AMRs) are deployed in diverse environments (e.g.,
warehouses, retail spaces, and offices), where they work alongside humans.
Given that human behavior can be unpredictable and that AMRs may not have been
trained to handle all possible unknown and uncertain behaviors, it is important
to test AMRs under a wide range of human interactions to ensure their safe
behavior. Moreover, testing in real environments with actual AMRs and humans is
often costly, impractical, and potentially hazardous (e.g., it could result in
human injury). To this end, we propose a Vision Language Model (VLM)-based
testing approach (RVSG) for industrial AMRs developed by PAL Robotics in Spain.
Based on the functional and safety requirements, RVSG uses the VLM to generate
diverse human behaviors that violate these requirements. We evaluated RVSG with
several requirements and navigation routes in a simulator using the latest AMR
from PAL Robotics. Our results show that, compared with the baseline, RVSG can
effectively generate requirement-violating scenarios. Moreover, RVSG-generated
scenarios increase variability in robot behavior, thereby helping reveal their
uncertain behaviors.

</details>


### [21] [JC-Finder: Detecting Java Clone-based Third-Party Library by Class-level Tree Analysis](https://arxiv.org/abs/2508.02397)
*Lida Zhao,Chaofan Li,Yueming Wu,Lyuye Zhang,Jiahui Wu,Chengwei Liu,Sen Chen,Yutao Hu,Zhengzi Xu,Yi Liu,Jingquan Ge,Jun Sun,Yang Liu*

Main category: cs.SE

TL;DR: JC-Finder是一个针对Java项目的克隆检测工具，用于识别第三方库（TPL）的代码重用，解决了现有工具在Java中的不足，并在准确性和效率上表现优异。


<details>
  <summary>Details</summary>
Motivation: 第三方库（TPL）的混乱管理和未经授权的代码使用带来了维护和伦理问题，而现有的克隆检测工具在Java中缺乏针对性。

Method: JC-Finder通过类级特征捕获、维护函数间关系并排除冗余元素，实现了高效且准确的TPL重用检测。

Result: 在测试中，JC-Finder的F1得分为0.818，比同类工具高0.427，处理速度提升9倍，并成功检测到26.20%未声明的TPL。

Conclusion: JC-Finder填补了Java克隆检测工具的空白，为TPL重用提供了高效且全面的解决方案。

Abstract: While reusing third-party libraries (TPL) facilitates software development,
its chaotic management has brought great threats to software maintenance and
the unauthorized use of source code also raises ethical problems such as
misconduct on copyrighted code. To identify TPL reuse in projects, Software
Composition Analysis (SCA) is employed, and two categories of SCA techniques
are used based on how TPLs are introduced: clone-based SCA and
package-manager-based SCA (PM-based SCA). Although introducing TPLs by clones
is prevalent in Java, no clone-based SCA tools are specially designed for Java.
Also, directly applying clone-based SCA techniques from other tools is
problematic. To fill this gap, we introduce JC-Finder, a novel clone-based SCA
tool that aims to accurately and comprehensively identify instances of TPL
reuse introduced by source code clones in Java projects. JC-Finder achieves
both accuracy and efficiency in identifying TPL reuse from code cloning by
capturing features at the class level, maintaining inter-function
relationships, and excluding trivial or duplicated elements. To evaluate the
efficiency of JC-Finder, we applied it to 9,965 most popular Maven libraries as
reference data and tested the TPL reuse of 1,000 GitHub projects. The result
shows that JC-Finder achieved an F1-score of 0.818, outperforming the other
function-level tool by 0.427. The average time taken for resolving TPL reuse is
14.2 seconds, which is approximately 9 times faster than the other tool. We
further applied JC-Finder to 7,947 GitHub projects, revealing TPL reuse by code
clones in 789 projects (about 9.89% of all projects) and identifying a total of
2,142 TPLs. JC-Finder successfully detects 26.20% more TPLs that are not
explicitly declared in package managers.

</details>


### [22] [Quantum Machine Learning-based Test Oracle for Autonomous Mobile Robots](https://arxiv.org/abs/2508.02407)
*Xinyi Wang,Qinghua Xu,Paolo Arcaini,Shaukat Ali,Thomas Peyrucain*

Main category: cs.SE

TL;DR: 论文提出了一种基于量子机器学习的测试预言框架QuReBot，用于自主移动机器人的回归测试，相比传统神经网络降低了15%的预测误差。


<details>
  <summary>Details</summary>
Motivation: 机器人软件升级后需要回归测试，但由于环境不确定性，测试预言难以确定。量子机器学习提供了一种快速训练和高精度的解决方案。

Method: 提出混合框架QuReBot，结合量子储备计算（QRC）和简单神经网络，预测机器人行为。

Result: QRC单独使用时无法收敛，而QuReBot降低了15%的预测误差。

Conclusion: QuReBot在机器人软件测试中表现优异，并提供了优化配置的实用建议。

Abstract: Robots are increasingly becoming part of our daily lives, interacting with
both the environment and humans to perform their tasks. The software of such
robots often undergoes upgrades, for example, to add new functionalities, fix
bugs, or delete obsolete functionalities. As a result, regression testing of
robot software becomes necessary. However, determining the expected correct
behavior of robots (i.e., a test oracle) is challenging due to the potentially
unknown environments in which the robots must operate. To address this
challenge, machine learning (ML)-based test oracles present a viable solution.
This paper reports on the development of a test oracle to support regression
testing of autonomous mobile robots built by PAL Robotics (Spain), using
quantum machine learning (QML), which enables faster training and the
construction of more precise test oracles. Specifically, we propose a hybrid
framework, QuReBot, that combines both quantum reservoir computing (QRC) and a
simple neural network, inspired by residual connection, to predict the expected
behavior of a robot. Results show that QRC alone fails to converge in our case,
yielding high prediction error. In contrast, QuReBot converges and achieves 15%
reduction of prediction error compared to the classical neural network
baseline. Finally, we further examine QuReBot under different configurations
and offer practical guidance on optimal settings to support future robot
software testing.

</details>


### [23] [TreeRanker: Fast and Model-agnostic Ranking System for Code Suggestions in IDEs](https://arxiv.org/abs/2508.02455)
*Daniele Cipollone,Egor Bogomolov,Arie van Deursen,Maliheh Izadi*

Main category: cs.SE

TL;DR: 提出了一种基于语言模型的轻量级、模型无关的代码补全排序方法，通过前缀树和贪婪解码提升补全建议的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的代码补全系统依赖手工启发式或轻量级机器学习模型，难以捕捉上下文信息并泛化到不同项目和编码风格。

Method: 将所有有效补全组织成前缀树，通过单次贪婪解码收集令牌级分数，实现无需束搜索或模型调整的精确排序。

Result: 该方法快速、架构无关，且兼容现有代码补全模型，为IDE工具提供了更智能的开发者辅助。

Conclusion: 该研究为语言模型集成到现有IDE工具提供了一条实用且有效的路径，提升了代码补全的响应性和智能性。

Abstract: Token-level code completion is one of the most critical features in modern
Integrated Development Environments (IDEs). It assists developers by suggesting
relevant identifiers and APIs during coding. While completions are typically
derived from static analysis, their usefulness depends heavily on how they are
ranked, as correct predictions buried deep in the list are rarely seen by
users. Most current systems rely on hand-crafted heuristics or lightweight
machine learning models trained on user logs, which can be further improved to
capture context information and generalize across projects and coding styles.
In this work, we propose a new scoring approach to ranking static completions
using language models in a lightweight and model-agnostic way. Our method
organizes all valid completions into a prefix tree and performs a single greedy
decoding pass to collect token-level scores across the tree. This enables a
precise token-aware ranking without needing beam search, prompt engineering, or
model adaptations. The approach is fast, architecture-agnostic, and compatible
with already deployed models for code completion. These findings highlight a
practical and effective pathway for integrating language models into already
existing tools within IDEs, and ultimately providing smarter and more
responsive developer assistance.

</details>


### [24] [An Efficient and Adaptive Next Edit Suggestion Framework with Zero Human Instructions in IDEs](https://arxiv.org/abs/2508.02473)
*Xinfang Chen,Siyang Xiao,Xianying Zhu,Junhong Xie,Ming Liang,Dajun Chen,Wei Jiang,Yong Li,Peng Di*

Main category: cs.SE

TL;DR: 提出了一种名为NES的LLM驱动代码编辑框架，通过分析开发者历史编辑模式实现无指令、低延迟的代码编辑建议。


<details>
  <summary>Details</summary>
Motivation: 现有基于自然语言指令的代码编辑工具依赖人工输入且延迟高，难以融入开发者工作流。

Method: 采用双模型架构，利用SFT和DAPO数据集训练，优化推理以减少延迟。

Result: 在预测编辑位置任务中准确率达75.6%和81.6%，意图对齐编辑的ES和EMR分别为91.36%和27.7%，优于现有模型。

Conclusion: NES是一种可扩展的行业解决方案，显著提升开发效率，其数据集对开源CodeLLMs性能有提升作用。

Abstract: Code editing, including modifying, refactoring, and maintaining existing
code, is the most frequent task in software development and has garnered
significant attention from AI-powered tools. However, existing solutions that
translate explicit natural language instructions into code edits face critical
limitations, such as heavy reliance on human instruction input and high
latency, which hinder their effective integration into a developer's workflow.
We observe that developers' habitual behaviors and coding objectives are often
reflected in their historical editing patterns, making this data key to
addressing existing limitations. To leverage these insights, we propose NES
(Next Edit Suggestion), an LLM-driven code editing framework that delivers an
instruction-free and low-latency experience. Built on a dual-model architecture
and trained with our high-quality SFT and DAPO datasets, NES enhances
productivity by understanding developer intent while optimizing inference to
minimize latency. NES is a scalable, industry-ready solution with a continuous
Tab key interaction workflow, seamlessly adopted by a FinTech company with over
20,000 developers. Evaluations on real-world datasets show NES achieves 75.6%
and 81.6% accuracy in two tasks of predicting next edit locations, alongside
91.36% ES and 27.7% EMR for intent-aligned edits, outperforming SOTA models.
Our open-sourced SFT and DAPO datasets have been demonstrated to enhance the
performance of open-source CodeLLMs. The demonstration of NES is available at
https://youtu.be/yGoyYOe6fbY.

</details>


### [25] [Commit Stability as a Signal for Risk in Open-Source Projects](https://arxiv.org/abs/2508.02487)
*Elijah Kayode Adejumo,Brittany Johnson,Mariam Guizani*

Main category: cs.SE

TL;DR: 论文研究了开源软件项目的韧性，提出通过提交频率模式评估项目稳定性，发现仅有少数项目能在不同时间粒度上保持稳定，并识别了影响稳定性的关键因素。


<details>
  <summary>Details</summary>
Motivation: 随着开源软件在全球技术基础设施中的重要性提升，理解项目演化及其韧性（即应对干扰后恢复正常的能力）变得至关重要。现有指标对项目健康的评估不足，尤其是韧性维度。

Method: 基于复合稳定性指数（CSI）框架，对100个高排名仓库的提交频率模式进行实证分析，评估其在不同时间粒度（日、周、月）上的稳定性。

Result: 结果显示，仅2%的仓库实现日稳定性，29%实现周稳定性，50%实现月稳定性，而一半仓库在所有时间粒度上均不稳定。编程语言和区块链应用表现最稳定。

Conclusion: 项目稳定性与治理模式、持续贡献者和开发流程相关，未来可通过结合问题解决时间、PR合并率等指标进一步丰富韧性评估。

Abstract: Open source software (OSS) generates trillions of dollars in economic value
and has become essential to technical infrastructures worldwide. As
organizations increasingly depend on OSS, understanding project evolution is
critical. While existing metrics provide insights into project health, one
dimension remains understudied: project resilience -- the ability to return to
normal operations after disturbances such as contributor departures, security
vulnerabilities, and bug report spikes. We hypothesize that stable commit
patterns reflect underlying project characteristics such as mature governance,
sustained contributors, and robust development processes that enable
resilience. Building on the Composite Stability Index (CSI) framework, we
empirically validate commit frequency patterns across 100 highly ranked
repositories. Our findings reveal that only 2\% of repositories exhibit daily
stability, 29\% achieve weekly stability, and 50\% demonstrate monthly
stability, while half remain unstable across all temporal levels. Programming
languages and blockchain applications were the most stable. We identified two
exemplary repositories that achieved stability at all three granularities,
whose governance models, CI cadence, and release policies could serve as
reference frameworks. We observed that large yearly commit throughput does not
necessarily correlate with stability. Beyond commits, stability can be enriched
with issue-resolution times, PR merge rates, and community-engagement metrics
to broaden resilience assessment and sharpen stability-based risk evaluation.

</details>


### [26] [Bridging Language Gaps in Open-Source Documentation with Large-Language-Model Translation](https://arxiv.org/abs/2508.02497)
*Elijah Kayode Adejumo,Brittany Johnson,Mariam Guizani*

Main category: cs.SE

TL;DR: 论文探讨了大型语言模型（LLMs）在开源技术文档翻译中的潜力与挑战，发现其能提供准确翻译但存在格式和结构保留问题，并提出了TRIFID框架以评估翻译保真度。


<details>
  <summary>Details</summary>
Motivation: 开源社区的技术文档多仅以英语提供，限制了全球贡献者的参与。研究旨在探索LLMs在多语言技术文档翻译中的能力。

Method: 评估了50个README文件的英德翻译，使用ChatGPT 4和Claude模型，并分析了翻译保真度。

Result: LLMs能提供准确翻译，但在保留超链接和格式一致性上表现不佳。

Conclusion: LLMs在文档国际化中有潜力，但需解决保真度问题；TRIFID框架为自动化翻译支持奠定了基础。

Abstract: While open source communities attract diverse contributors globally, few
repositories provide essential documentation in languages other than English.
Large language models (LLMs) have demonstrated remarkable capabilities in
software engineering tasks and translations across domains. However, little is
known about LLM capabilities in translating open-source technical
documentation, which mixes natural language, code, URLs, and markdown
formatting. To understand the need and potential for LLMs in technical
documentation translation, we evaluated community translation activity and
English-to-German translations of 50 README files using OpenAI's ChatGPT 4 and
Anthropic's Claude. We found scarce translation activity, mostly in larger
repositories and community-driven in nature. LLM performance comparison
suggests they can provide accurate translations. However, analysis revealed
fidelity challenges: both models struggled to preserve structural components
(e.g., hyperlinks) and exhibited formatting inconsistencies. These findings
highlight both promise and challenges of LLM-assisted documentation
internationalization. As a first step toward translation-aware continuous
integration pipelines, we introduce TRIFID, an early-stage translation fidelity
scoring framework that automatically checks how well translations preserve
code, links, and formatting. Our efforts provide a foundation for automated
LLM-driven support for creating and maintaining open source documentation.

</details>


### [27] [Automatic Identification of Machine Learning-Specific Code Smells](https://arxiv.org/abs/2508.02541)
*Peter Hamfelt,Ricardo Britto,Lincoln Rocha,Camilo Almendra*

Main category: cs.SE

TL;DR: 研究开发了基于代码异味标准的静态代码分析工具MLpylint，用于识别ML应用中的特定代码异味，并通过专家验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对ML应用中特定代码异味的识别工具和研究，填补这一空白。

Method: 采用设计科学方法，通过文献综述和专家咨询设计工具，并在160个开源ML应用上评估。

Result: MLpylint被证明有效且实用，专家调查进一步验证了其价值。

Conclusion: 计划将MLpylint无缝集成到开发流程中，提升开发效率和创新性。

Abstract: Machine learning (ML) has rapidly grown in popularity, becoming vital to many
industries. Currently, the research on code smells in ML applications lacks
tools and studies that address the identification and validity of ML-specific
code smells. This work investigates suitable methods and tools to design and
develop a static code analysis tool (MLpylint) based on code smell criteria.
This research employed the Design Science Methodology. In the problem
identification phase, a literature review was conducted to identify ML-specific
code smells. In solution design, a secondary literature review and
consultations with experts were performed to select methods and tools for
implementing the tool. We evaluated the tool on data from 160 open-source ML
applications sourced from GitHub. We also conducted a static validation through
an expert survey involving 15 ML professionals. The results indicate the
effectiveness and usefulness of the MLpylint. We aim to extend our current
approach by investigating ways to introduce MLpylint seamlessly into
development workflows, fostering a more productive and innovative developer
environment.

</details>


### [28] [Meta-RAG on Large Codebases Using Code Summarization](https://arxiv.org/abs/2508.02611)
*Vali Tawosia,Salwa Alamir,Xiaomo Liu,Manuela Veloso*

Main category: cs.SE

TL;DR: 论文提出了一种名为Meta-RAG的多智能体系统，用于通过信息检索和大型语言模型（LLM）定位大型代码库中的错误。


<details>
  <summary>Details</summary>
Motivation: 软件开发的复杂性不仅限于代码实现，还包括代码维护。现有方法在定位大型代码库中的错误时效率不足，因此需要一种更高效的方法。

Method: 采用了一种新颖的检索增强生成（RAG）方法Meta-RAG，通过摘要将代码库压缩79.8%，生成紧凑的结构化自然语言表示，并利用LLM智能体确定与错误解决相关的关键代码部分。

Result: 在SWE-bench Lite数据集上，Meta-RAG在文件级别和函数级别的正确定位率分别达到84.67%和53.0%，性能达到当前最优水平。

Conclusion: Meta-RAG通过高效的代码库压缩和智能定位，显著提升了大型代码库中错误定位的准确性和效率。

Abstract: Large Language Model (LLM) systems have been at the forefront of applied
Artificial Intelligence (AI) research in a multitude of domains. One such
domain is software development, where researchers have pushed the automation of
a number of code tasks through LLM agents. Software development is a complex
ecosystem, that stretches far beyond code implementation and well into the
realm of code maintenance. In this paper, we propose a multi-agent system to
localize bugs in large pre-existing codebases using information retrieval and
LLMs. Our system introduces a novel Retrieval Augmented Generation (RAG)
approach, Meta-RAG, where we utilize summaries to condense codebases by an
average of 79.8\%, into a compact, structured, natural language representation.
We then use an LLM agent to determine which parts of the codebase are critical
for bug resolution, i.e. bug localization. We demonstrate the usefulness of
Meta-RAG through evaluation with the SWE-bench Lite dataset. Meta-RAG scores
84.67 % and 53.0 % for file-level and function-level correct localization
rates, respectively, achieving state-of-the-art performance.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [29] [Expressive Power of Graph Transformers via Logic](https://arxiv.org/abs/2508.01067)
*Veeti Ahvonen,Maurice Funk,Damian Heiman,Antti Kuusisto,Carsten Lutz*

Main category: cs.LO

TL;DR: 研究了图变换器（GTs）和GPS网络在软注意力和平均硬注意力下的表达能力，分别在实数理论和浮点实践场景下进行了分析。


<details>
  <summary>Details</summary>
Motivation: 探讨现代大型语言模型基础——变换器在图上的精确表达能力。

Method: 分析GTs和GPS网络在软注意力和平均硬注意力下的表现，比较实数理论和浮点实践场景。

Result: GPS网络在实数下与带全局模态的GML表达能力相同，浮点下与带计数全局模态的GML等价；GTs在实数下与带全局模态的命题逻辑等价，浮点下与带计数全局模态的命题逻辑等价。

Conclusion: 研究揭示了GTs和GPS网络在不同场景下的表达能力，为图变换器的理论理解提供了重要见解。

Abstract: Transformers are the basis of modern large language models, but relatively
little is known about their precise expressive power on graphs. We study the
expressive power of graph transformers (GTs) by Dwivedi and Bresson (2020) and
GPS-networks by Ramp\'asek et al. (2022), both under soft-attention and average
hard-attention. Our study covers two scenarios: the theoretical setting with
real numbers and the more practical case with floats. With reals, we show that
in restriction to vertex properties definable in first-order logic (FO),
GPS-networks have the same expressive power as graded modal logic (GML) with
the global modality. With floats, GPS-networks turn out to be equally
expressive as GML with the counting global modality. The latter result is
absolute, not restricting to properties definable in a background logic. We
also obtain similar characterizations for GTs in terms of propositional logic
with the global modality (for reals) and the counting global modality (for
floats).

</details>


### [30] [Relative Completeness of Incorrectness Separation Logic](https://arxiv.org/abs/2508.01535)
*Yeonseok Lee,Koji Nakazawa*

Main category: cs.LO

TL;DR: ISL是一种针对堆操作程序中下近似问题的证明系统，专注于错误检测。本研究通过最弱后条件的表达能力证明了其相对完备性。


<details>
  <summary>Details</summary>
Motivation: 解决传统逻辑（如Hoare逻辑或分离逻辑）中上近似方法无法处理的下近似问题，特别是堆操作程序中的错误检测。

Method: 利用最弱后条件的表达能力，引入包含变量别名的规范化方法，并允许无限析取范式。

Result: 证明了ISL的相对完备性。

Conclusion: ISL通过最弱后条件的表达能力和规范化方法，解决了堆操作程序中的下近似问题，并证明了其相对完备性。

Abstract: Incorrectness Separation Logic (ISL) is a proof system that is tailored
specifically to resolve problems of under-approximation in programs that
manipulate heaps, and it primarily focuses on bug detection. This approach is
different from the over-approximation methods that are used in traditional
logics such as Hoare Logic or Separation Logic. Although the soundness of ISL
has been established, its completeness remains unproven. In this study, we
establish relative completeness by leveraging the expressiveness of the weakest
postconditions; expressiveness is a factor that is critical to demonstrating
relative completeness in Reverse Hoare Logic. In our ISL framework, we allow
for infinite disjunctions in disjunctive normal forms, where each clause
comprises finite symbolic heaps with existential quantifiers. To compute the
weakest postconditions in ISL, we introduce a canonicalization that includes
variable aliasing.

</details>


### [31] [Causality and Decision-making: A Logical Framework for Systems and Security Modelling](https://arxiv.org/abs/2508.01758)
*Pinaki Chakraborty,Tristan Caulfield,David Pym*

Main category: cs.LO

TL;DR: 论文提出了一种基于最小结构假设的战略推理理论，用于建模复杂生态系统中的因果关系，结合了干预操作符和分离合取，与Halpern和Pearl的结构因果模型兼容。


<details>
  <summary>Details</summary>
Motivation: 理解复杂生态系统中决策制定的因果关系对安全性等问题至关重要，需要一种形式化的理论框架。

Method: 采用转移系统方法和模态逻辑，引入干预操作符和分离合取，结合Halpern和Pearl的结构因果模型。

Result: 通过分布式系统中的微服务决策案例验证了框架的适用性，并证明了等价定理。

Conclusion: 该工作为研究复杂交互系统中的因果决策提供了逻辑基础，统一了形式化的系统行为与反事实推理理论。

Abstract: Causal reasoning is essential for understanding decision-making about the
behaviour of complex `ecosystems' of systems that underpin modern society, with
security -- including issues around correctness, safety, resilience, etc. --
typically providing critical examples. We present a theory of strategic
reasoning about system modelling based on minimal structural assumptions and
employing the methods of transition systems, supported by a modal logic of
system states in the tradition of van Benthem, Hennessy, and Milner, and
validated through equivalence theorems. Our framework introduces an
intervention operator and a separating conjunction to capture actual causal
relationships between component systems of the ecosystem, aligning naturally
with Halpern and Pearl's counterfactual approach based on Structural Causal
Models. We illustrate the applicability through examples of of decision-making
about microservices in distributed systems. We discuss localized
decision-making through a separating conjunction. This work unifies a formal,
minimalistic notion of system behaviour with a Halpern--Pearl-compatible theory
of counterfactual reasoning, providing a logical foundation for studying
decision making about causality in complex interacting systems.

</details>


### [32] [Separation Logic of Generic Resources via Sheafeology](https://arxiv.org/abs/2508.01866)
*Berend van Starkenburg,Henning Basold,Chase Ford*

Main category: cs.LO

TL;DR: 本文提出了一种基于资源感知的一阶逻辑框架，通过范畴逻辑和sheafeology（层论）实现分离逻辑的通用化，适用于多种资源模型。


<details>
  <summary>Details</summary>
Motivation: 现有分离逻辑方法未能统一适用于不同资源结构的程序逻辑，缺乏通用框架。

Method: 利用范畴逻辑和层论（sheafeology）开发资源感知的一阶逻辑，构建内部纤维化模型以支持谓词和分离连接词。

Result: 提出了一个通用分离逻辑框架，可实例化为多种资源模型（如内存模型和随机变量）。

Conclusion: 通过层论和范畴逻辑，实现了分离逻辑的通用化，为不同资源结构的程序验证提供了统一框架。

Abstract: Separation logic was conceived in order to make the verification of pointer
programs scalable to large systems and it has proven extremely effective. The
key idea is that programs typically access only small parts of memory, allowing
for local reasoning. This idea is implemented in separation logic by extending
first-order logic with separating connectives, which inspect local regions of
memory. It turns that this approach not only applies to pointer programs, but
also to programs involving other resource structures. Various theories have
been put forward to extract and apply the ideas of separation logic more
broadly. This resulted in algebraic abstractions of memory and many variants of
separation logic for, e.g., concurrent programs and stochastic processes.
However, none of the existing approaches formulate the combination of
first-order logic with separating connectives in a theory that could
immediately yield program logics for different resources. In this paper, we
propose a framework based on the idea that separation logic can obtained by
making first-order logic resource-aware. First-order logic can be understood in
terms of categorical logic, specifically fibrations. Our contribution is to
make these resource-aware by developing categorical logic internally in
categories of sheaves, which is what we call sheafeology. The role of sheaves
is to model views on resources, through which resources can be localised and
combined, which enables the scalability promised by separation logic. We
contribute constructions of an internal fibration in sheaf categories that
models predicates on resources, and that admits first-order and separating
connectives. Thereby, we attain a general framework of separation logic for
generic resources, a claim we substantiate by instantiating our framework to
various memory models and random variables.

</details>


### [33] [Monitoring Hyperproperties over Observed and Constructed Traces](https://arxiv.org/abs/2508.02301)
*Marek Chalupa,Thomas A. Henzinger,Ana Oliveira da Costa*

Main category: cs.LO

TL;DR: 该论文研究了如何通过运行时监控验证系统是否满足由超属性定义的规范，引入了包含被动和主动跟踪量词的规范，并提出了相应的监控算法。


<details>
  <summary>Details</summary>
Motivation: 解决运行时监控系统是否满足超属性（如线性一致性或非干涉性变体）的问题，扩展了监控能力。

Method: 扩展了超节点逻辑，引入跟踪量词和生成器函数，设计并实现了监控算法。

Result: 实现了对异步超属性的监控，首次支持交替跟踪量词的监控。

Conclusion: 提出的方法扩展了超属性监控的范围，适用于并发和安全应用。

Abstract: We study the problem of monitoring at runtime whether a system fulfills a
specification defined by a hyperproperty, such as linearizability or variants
of non-interference. For this purpose, we introduce specifications with both
passive and active quantification over traces. While passive trace quantifiers
range over the traces that are observed, active trace quantifiers are
instantiated with \emph{generator functions}, which are part of the
specification. Generator functions enable the monitor to construct traces that
may never be observed at runtime, such as the linearizations of a concurrent
trace. As specification language, we extend hypernode logic with trace
quantifiers over generator functions and interpret these hypernode formulas
over possibly infinite domains. We present a corresponding monitoring
algorithm, which we implemented and evaluated on a range of hyperproperties for
concurrency and security applications. Our method enables, for the first time,
the monitoring of asynchronous hyperproperties that contain alternating trace
quantifiers.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [34] [Efficient compilation and execution of synchronous programs via type-state programming](https://arxiv.org/abs/2508.01199)
*Avinash Malik*

Main category: cs.PL

TL;DR: 提出了一种线性时间编译技术，用于同步程序的自动机编译，通过图重写规则和模板元编程生成高效的有限状态机（FSM），实验显示执行速度比现有编译器快31-60%。


<details>
  <summary>Details</summary>
Motivation: 同步程序在安全关键嵌入式软件中广泛应用，但多FSM同步组合会导致状态空间爆炸问题，需要高效的编译方法。

Method: 引入基于图的核编程构造重写规则，线性时间算法生成FSM，并通过C++模板元编程编码为类型状态程序。

Result: 编译时间和二进制大小与现有技术相当，执行速度平均快31-60%。

Conclusion: 该方法显著提升了同步程序的执行效率，适用于嵌入式系统。

Abstract: Synchronous programs are used extensively in implementation of safety
critical embedded software. Imperative synchronous programming languages model
multiple Finite State Machines (FSMs) executing in lockstep at logical clock
ticks. The synchronous view of time along with the FSM based design enables
easier formal verification. The synchronous composition of multiple FSMs,
during compilation, results in the well known state space explosion problem.
Hence, efficiently compiling imperative synchronous programs into small and
fast executables is challenging. This paper introduces a novel linear time
compilation technique for automata based compilation of synchronous programs.
Graph based rewrite rules for kernel programming constructs are introduced. A
linear time algorithm applies these rules to produce a FSM. The FSM is then
encoded into a type-state program using template meta-programming in C++.
Experimental results show that the compilation time and generated binary size
is comparable, while the execution times are on average 31-60% faster than
current state-of-the-art compilers.

</details>


### [35] [Proceedings 14th International Workshop on Trends in Functional Programming in Education](https://arxiv.org/abs/2508.02305)
*Rose Bohrer*

Main category: cs.PL

TL;DR: TFPIE是一个专注于教育中函数式编程的研讨会，旨在汇集研究者、教师和专业人士，讨论新想法、课堂实践和进行中的工作。


<details>
  <summary>Details</summary>
Motivation: 促进教育中函数式编程的应用，提供一个开放的讨论平台。

Method: 通过一天的研讨会形式，鼓励开放讨论，并在会后进行论文评审。

Result: 为参与者提供一个分享和交流函数式编程教育经验的平台。

Conclusion: TFPIE通过研讨会形式成功促进了教育中函数式编程的讨论与合作。

Abstract: The goal of TFPIE is to gather researchers, teachers and professionals that
use, or are interested in the use of, functional programming in education.
TFPIE aims to be a venue where novel ideas, classroom-tested ideas and
work-in-progress on the use of functional programming in education are
discussed. The one-day workshop will foster a spirit of open discussion by
having a review process for publication after the workshop.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [36] [Runtime Consultants](https://arxiv.org/abs/2508.01821)
*Dana Fisman,Elina Sudit*

Main category: cs.FL

TL;DR: 论文提出了一种运行时顾问的概念，用于在系统执行过程中提供建议以避免违规或优化目标值。


<details>
  <summary>Details</summary>
Motivation: 动机是解决运行时监控的被动性问题，通过主动提供建议来优化系统行为。

Method: 方法包括定义运行时顾问，并针对常见的值函数和ω-正则属性设计顾问算法。

Result: 结果表明，几乎所有常见值函数的运行时顾问都能在常数时间内工作。

Conclusion: 结论是运行时顾问能够有效提升系统行为的主动性和优化能力。

Abstract: In this paper we introduce the notion of a runtime consultant. A runtime
consultant is defined with respect to some value function on infinite words.
Similar to a runtime monitor, it runs in parallel to an execution of the system
and provides inputs at every step of the run. While a runtime monitor alerts
when a violation occurs, the idea behind a consultant is to be pro-active and
provide recommendations for which action to take next in order to avoid
violation (or obtain a maximal value for quantitative objectives). It is
assumed that a runtime-controller can take these recommendations into
consideration. The runtime consultant does not assume that its recommendations
are always followed. Instead, it adjusts to the actions actually taken (similar
to a vehicle navigation system). We show how to compute a runtime consultant
for common value functions used in verification, and that almost all have a
runtime consultant that works in constant time. We also develop consultants for
$\omega$-regular properties, under both their classical Boolean semantics and
their recently proposed quantitative interpretation.

</details>


### [37] [A Myhill-Nerode Theorem for Generalized Automata, with Applications to Pattern Matching and Compression](https://arxiv.org/abs/2302.06506)
*Nicola Cotumaccio*

Main category: cs.FL

TL;DR: 论文研究了广义自动机的确定性及其最小化问题，提出了一个集合$\mathcal{W(A)}$来解释最小化唯一性的缺失，并扩展了Myhill-Nerode定理。此外，论文还展示了$\mathcal{W(A)}$在模式匹配和数据压缩中的应用，特别是对Wheeler自动机的扩展。


<details>
  <summary>Details</summary>
Motivation: 广义自动机比传统自动机更简洁，但其确定性最小化的唯一性缺失问题尚未被完全理解。论文旨在解释这一现象，并探索广义自动机在模式匹配和数据压缩中的潜在应用。

Method: 通过引入集合$\mathcal{W(A)}$，论文重新定义了广义自动机的确定性，并扩展了Myhill-Nerode定理。此外，论文还研究了如何将Wheeler自动机的压缩和查询技术扩展到广义自动机。

Result: 论文证明了通过固定$\mathcal{W(A)}$，可以推导出广义自动机的完整Myhill-Nerode定理。同时，展示了广义Wheeler自动机在存储和模式匹配中的高效性。

Conclusion: 论文不仅解决了广义自动机最小化唯一性的理论问题，还展示了其在模式匹配和数据压缩中的实际应用价值。

Abstract: The model of generalized automata, introduced by Eilenberg in 1974, allows
representing a regular language more concisely than conventional automata by
allowing edges to be labeled not only with characters, but also strings.
Giammaresi and Montalbano introduced a notion of determinism for generalized
automata [STACS 1995]. While generalized deterministic automata retain many
properties of conventional deterministic automata, the uniqueness of a minimal
generalized deterministic automaton is lost.
  In the first part of the paper, we show that the lack of uniqueness can be
explained by introducing a set $ \mathcal{W(A)} $ associated with a generalized
automaton $ \mathcal{A} $. By fixing $ \mathcal{W(A)} $, we are able to derive
for the first time a full Myhill-Nerode theorem for generalized automata, which
contains the textbook Myhill-Nerode theorem for conventional automata as a
degenerate case.
  In the second part of the paper, we show that the set $ \mathcal{W(A)} $
leads to applications for pattern matching and data compression. Wheeler
automata [TCS 2017, SODA 2020] are a popular class of automata that can be
compactly stored using $ e \log \sigma (1 + o(1)) + O(e) $ bits ($ e $ being
the number of edges, $ \sigma $ being the size of the alphabet) in such a way
that pattern matching queries can be solved in $ \tilde{O}(m) $ time ($ m $
being the length of the pattern). In the paper, we show how to extend these
results to generalized automata. More precisely, a Wheeler generalized automata
can be stored using $ \mathfrak{e} \log \sigma (1 + o(1)) + O(e + rn) $ bits so
that pattern matching queries can be solved in $ \tilde{O}(r m) $ time, where $
\mathfrak{e} $ is the total length of all edge labels, $ r $ is the maximum
length of an edge label and $ n $ is the number of states.

</details>
