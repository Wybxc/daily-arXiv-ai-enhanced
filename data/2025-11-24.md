<div id=toc></div>

# Table of Contents

- [cs.LO](#cs.LO) [Total: 5]
- [cs.SE](#cs.SE) [Total: 12]


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [1] [A Coq-based Axiomatization of Tarski's Mereogeometry](https://arxiv.org/abs/2511.16705)
*Patrick Barlatier,Richard Dapoigny*

Main category: cs.LO

TL;DR: 本文提出了一种基于Lesniewski原始思想的类型论空间表示方法，使用Coq语言实现，为定性空间推理中的mereogeometry提供了更清晰的基础。


<details>
  <summary>Details</summary>
Motivation: 现有mereogeometry方法偏离了Lesniewski的原始理论，使用集合论基础，限制了逻辑能力，并需要引入连接关系。Tarski的原始论文基础不清晰，需要更符合Lesniewski理论的扩展。

Method: 采用类型论方法表示空间，使用Coq语言实现，基于Lesniewski的原始思想，仅需三个公理而非四个。

Result: 该方法能够为mereogeometry提供更清晰的基础，减少公理数量，并完全符合Lesniewski系统，可作为空间推理的基础。

Conclusion: 提出的类型论方法为mereogeometry提供了更坚实和符合原始理论的基础，支持完整的空间推理能力。

Abstract: During the last decade, the domain of Qualitative Spatial Reasoning, has known a renewal of interest for mereogeometry, a theory that has been initiated by Tarski. Mereogeometry relies on mereology, the Lesniewski's theory of parts and wholes that is further extended with geometrical primitives and appropriate definitions. However, most approaches (i) depart from the original Lesniewski's mereology which does not assume usual sets as a basis, (ii) restrict the logical power of mereology to a mere theory of part-whole relations and (iii) require the introduction of a connection relation. Moreover, the seminal paper of Tarki shows up unclear foundations and we argue that mereogeometry as it is introduced by Tarski, can be more suited to extend the whole theory of Lesniewski. For that purpose, we investigate a type-theoretical representation of space more closely related with the original ideas of Lesniewski and expressed with the Coq language. We show that (i) it can be given a more clear foundation, (ii) it can be based on three axioms instead of four and (iii) it can serve as a basis for spatial reasoning with full compliance with Lesniewski's systems.

</details>


### [2] [Tableau methodology for propositional logics](https://arxiv.org/abs/2511.16781)
*T. Jarmuzek,R. Gore*

Main category: cs.LO

TL;DR: 提出了一种为命题逻辑构建表系统的一般方法，通过表元理论为不同语义或公式的表系统提供通用形式化概念。


<details>
  <summary>Details</summary>
Motivation: 为各种命题语言和语义的表方法提供更深入和通用的处理方式，将Smullyan和Fitting的抽象一致性性质从模态情况推广到其他情况。

Method: 建立表元理论，提供表系统的通用形式化概念，通过检查表规则的具体属性来构建完整的表系统。

Result: 证明了与特定命题逻辑应用无关的事实，包括简化构建完整表系统过程的表元定理。

Conclusion: 该方法对于各种命题语言和语义的表方法处理至关重要，提供了通用且深入的理论框架。

Abstract: We set out a general methodology for producing tableau systems for propositional logics via a tableau metatheory that provides general and formal notions for different tableau systems that vary by semantics or formulae. Moreover, by dint of these general notions, some facts, independent of their applications to a particular propositional logic, can be proved. One of the examples is the tableau metatheorem that simplifies the process of constructing a complete tableau system for a given logic, just reducing it to checking specific properties of the tableau rules within the analyzed, particular system. In our paper we generalize an abstract consistency property proposed by R. Smullyan and M. Fitting from the modal case to the others. Such a methodology is essential for a deeper and universal treatment of tableau methods for various propositional languages and semantics.

</details>


### [3] [Cubical coherent confluence, cubical $ω$-groupoids and the cube equation](https://arxiv.org/abs/2511.16852)
*Philippe Malbos,Tanguy Massacrier,Georg Struth*

Main category: cs.LO

TL;DR: 本文在立方体范畴内研究抽象重写系统的合流性质，通过引入立方体收缩和构造立方体多面体分辨率，建立了立方体证明框架，证明了Newman引理、Church-Rosser定理和Squier相干定理等基本重写结果。


<details>
  <summary>Details</summary>
Motivation: 研究抽象重写系统在立方体范畴内的合流性质，将传统重写理论推广到更高维度的范畴论框架中。

Method: 引入立方体收缩作为正规形式约简的高维推广，构造收敛重写系统的立方体多面体分辨率，通过粘贴立方体相干单元进行证明。

Result: 在纯范畴论术语中推导出λ演算和Garside理论中的立方体定律，证明每个收敛抽象重写系统自由生成一个非循环立方体群胚。

Conclusion: 建立了一个统一的立方体范畴框架，能够处理重写理论中的高阶相干性问题，为传统重写结果提供了新的高维证明方法。

Abstract: We study the confluence property of abstract rewriting systems internal to cubical categories. We introduce cubical contractions, a higher-dimensional generalisation of reductions to normal forms, and employ them to construct cubical polygraphic resolutions of convergent rewriting systems. Within this categorical framework, we establish cubical proofs of fundamental rewriting results -- Newman's lemma, the Church-Rosser theorem, and Squier's coherence theorem -- via the pasting of cubical coherence cells. We moreover derive, in purely categorical terms, the cube law known from the $λ$-calculus and Garside theory. As a consequence, we show that every convergent abstract rewriting system freely generates an acyclic cubical groupoid, in which higher-dimensional generators can be replaced by degenerate cells beyond dimension two.

</details>


### [4] [An Efficient Computational Framework for Discrete Fuzzy Numbers Based on Total Orders](https://arxiv.org/abs/2511.17080)
*Arnau Mir,Alejandro Mus,Juan Vicente Riera*

Main category: cs.LO

TL;DR: 本文提出了计算离散模糊数位置函数及其逆函数的精确算法，复杂度为O(n²m log n)，显著降低了计算成本，使得在离散模糊数集合上的代数运算（如聚合和蕴含）能够高效实现。


<details>
  <summary>Details</summary>
Motivation: 离散模糊数在模糊系统中被广泛用于表示语言信息，研究其全序关系有助于构建新的逻辑连接词。位置函数作为双射函数，在确定离散模糊数位置方面发挥关键作用，但现有方法计算效率有待提升。

Method: 引入利用全序组合结构的算法来计算位置函数及其逆函数，通过分析离散模糊数的组合特性，实现精确的位置映射计算。

Result: 提出的算法复杂度为O(n²m log n)，其中n是基础链的大小，m是隶属度级别的数量。该算法在隶属度粒度方面具有良好的可扩展性。

Conclusion: 该公式显著降低了计算成本，使得在离散模糊数集合上的代数运算能够高效实现，为模糊系统的实际应用提供了更高效的计算工具。

Abstract: Discrete fuzzy numbers, and in particular those defined over a finite chain $L_n = \{0, \ldots, n\}$, have been effectively employed to represent linguistic information within the framework of fuzzy systems. Research on total (admissible) orderings of such types of fuzzy subsets, and specifically those belonging to the set $\mathcal{D}_1^{L_n\rightarrow Y_m}$ consisting of discrete fuzzy numbers $A$ whose support is a closed subinterval of the finite chain $L_n = \{0, 1, \ldots, n\}$ and whose membership values $A(x)$, for $x \in L_n$, belong to the set $Y_m = \{ 0 = y_1 < y_2 < \cdots < y_{m-1} < y_m = 1 \}$, has facilitated the development of new methods for constructing logical connectives, based on a bijective function, called $\textit{pos function}$, that determines the position of each $A \in \mathcal{D}_1^{L_n\rightarrow Y_m}$. For this reason, in this work we revisit the problem by introducing algorithms that exploit the combinatorial structure of total (admissible) orders to compute the $\textit{pos}$ function and its inverse with exactness. The proposed approach achieves a complexity of $\mathcal{O}(n^{2} m \log n)$, which is quadratic in the size of the underlying chain ($n$) and linear in the number of membership levels ($m$). The key point is that the dominant factor is $m$, ensuring scalability with respect to the granularity of membership values. The results demonstrate that this formulation substantially reduces computational cost and enables the efficient implementation of algebraic operations -- such as aggregation and implication -- on the set of discrete fuzzy numbers.

</details>


### [5] [Characterizing Sets of Theories That Can Be Disjointly Combined](https://arxiv.org/abs/2511.17374)
*Benjamin Przybocki,Guilherme V. Toledo,Yoni Zohar*

Main category: cs.LO

TL;DR: 该论文研究了允许一阶理论进行不相交组合的性质，包括稳定无限性、光泽性、强礼貌性和温和性。通过建立可判定理论集合之间的伽罗瓦连接，精确描述了能与满足已知理论组合性质的理论集合组合的最大可判定理论集合。


<details>
  <summary>Details</summary>
Motivation: 动机是研究理论组合的性质，特别是要回答关于改进现有理论组合方法以适用于更大理论集合的长期开放问题，并提供一个系统框架来生成新的组合方法。

Method: 方法是通过建立可判定理论集合之间的伽罗瓦连接，形成一个完整的理论组合性质格，通过取格中元素的交和并来生成新的组合定理。

Result: 结果是否定了几个关于改进现有理论组合方法的长期开放问题，精确描述了能与已知理论组合性质集合组合的理论集合，并引入了新的组合定理。

Conclusion: 结论是建立的伽罗瓦连接产生了一个完整的理论组合性质格，为系统生成新的理论组合方法提供了框架，并将新旧组合方法置于该格中统一处理。

Abstract: We study properties that allow first-order theories to be disjointly combined, including stable infiniteness, shininess, strong politeness, and gentleness. Specifically, we describe a Galois connection between sets of decidable theories, which picks out the largest set of decidable theories that can be combined with a given set of decidable theories. Using this, we exactly characterize the sets of decidable theories that can be combined with those satisfying well-known theory combination properties. This strengthens previous results and answers in the negative several long-standing open questions about the possibility of improving existing theory combination methods to apply to larger sets of theories. Additionally, the Galois connection gives rise to a complete lattice of theory combination properties, which allows one to generate new theory combination methods by taking meets and joins of elements of this lattice. We provide examples of this process, introducing new combination theorems. We situate both new and old combination methods within this lattice.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [6] [Large language models for automated PRISMA 2020 adherence checking](https://arxiv.org/abs/2511.16707)
*Yuki Kataoka,Ryuhei So,Masahiro Banno,Yasushi Tsujimoto,Tomohiro Takayama,Yosuke Yamagishi,Takahiro Tsuge,Norio Yamamoto,Chiaki Suda,Toshi A. Furukawa*

Main category: cs.SE

TL;DR: 构建了一个包含108篇系统评价的版权感知基准，评估了10个LLM在5种输入格式下对PRISMA 2020指南的依从性评估能力。


<details>
  <summary>Details</summary>
Motivation: 解决同行评审过程中PRISMA 2020指南依从性评估缺乏可共享基准的问题。

Method: 使用108篇CC许可的系统评价构建基准，评估10个LLM在5种输入格式（Markdown、JSON、XML、纯文本、仅手稿）下的表现，并在独立验证队列中验证结果。

Result: 提供结构化PRISMA 2020检查表时准确率达78.7-79.7%，显著高于仅手稿输入的45.21%。Qwen3-Max模型在完整数据集上达到95.1%敏感性和49.3%特异性。

Conclusion: 结构化检查表显著改善LLM的PRISMA评估能力，但编辑决策前仍需人类专家验证。

Abstract: Evaluating adherence to PRISMA 2020 guideline remains a burden in the peer review process. To address the lack of shareable benchmarks, we constructed a copyright-aware benchmark of 108 Creative Commons-licensed systematic reviews and evaluated ten large language models (LLMs) across five input formats. In a development cohort, supplying structured PRISMA 2020 checklists (Markdown, JSON, XML, or plain text) yielded 78.7-79.7% accuracy versus 45.21% for manuscript-only input (p less than 0.0001), with no differences between structured formats (p>0.9). Across models, accuracy ranged from 70.6-82.8% with distinct sensitivity-specificity trade-offs, replicated in an independent validation cohort. We then selected Qwen3-Max (a high-sensitivity open-weight model) and extended evaluation to the full dataset (n=120), achieving 95.1% sensitivity and 49.3% specificity. Structured checklist provision substantially improves LLM-based PRISMA assessment, though human expert verification remains essential before editorial decisions.

</details>


### [7] [Multi-Agent Code Verification with Compound Vulnerability Detection](https://arxiv.org/abs/2511.16708)
*Shreshth Rajan*

Main category: cs.SE

TL;DR: CodeX-Verify是一个多代理系统，使用四个专门代理检测不同类型的代码错误，在76.1%的准确率下捕获错误，比单代理系统提高39.7个百分点，且运行速度更快无需测试执行。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成的代码存在严重错误：29.6%的SWE-bench补丁失败，62%的BaxBench解决方案有漏洞，现有工具只能捕获65%的错误且假阳性率35%。

Method: 构建多代理系统，使用四个专门代理检测不同类型的错误，通过数学证明组合不同检测模式的代理能发现更多错误，测试了15种不同代理组合。

Result: 在99个带验证标签的代码样本上，系统捕获76.1%的错误，匹配最佳现有方法但运行更快；多代理比单代理准确率提高39.7个百分点；最佳双代理组合达到79.3%准确率；在300个真实补丁上测试，每个样本运行时间低于200ms。

Conclusion: 多代理方法能显著提高代码错误检测准确率，且具有实际生产可行性，同时发现同一代码中的多个漏洞会带来指数级风险增加。

Abstract: LLMs generate buggy code: 29.6% of SWE-bench "solved" patches fail, 62% of BaxBench solutions have vulnerabilities, and existing tools only catch 65% of bugs with 35% false positives. We built CodeX-Verify, a multi-agent system that uses four specialized agents to detect different types of bugs. We prove mathematically that combining agents with different detection patterns finds more bugs than any single agent when the agents look for different problems, confirmed by measuring agent correlation of p = 0.05--0.25. We also show that multiple vulnerabilities in the same code create exponentially more risk than previously thought--SQL injection plus exposed credentials creates 15x more danger (risk 300 vs. 20) than traditional models predict. Testing on 99 code samples with verified labels shows our system catches 76.1% of bugs, matching the best existing method while running faster and without test execution. We tested 15 different agent combinations and found that using multiple agents improves accuracy by 39.7 percentage points (from 32.8% to 72.4%) compared to single agents, with gains of +14.9pp, +13.5pp, and +11.2pp for agents 2, 3, and 4. The best two-agent combination reaches 79.3% accuracy. Testing on 300 real patches from Claude Sonnet 4.5 runs in under 200ms per sample, making this practical for production use.

</details>


### [8] [Is the Cure Still Worse Than the Disease? Test Overfitting by LLMs in Automated Program Repair](https://arxiv.org/abs/2511.16858)
*Toufique Ahmed,Jatin Ganhotra,Avraham Shinnar,Martin Hirzel*

Main category: cs.SE

TL;DR: 研究现代程序修复中的测试过拟合问题，特别是在SWE-bench仓库级任务上的表现


<details>
  <summary>Details</summary>
Motivation: 自动程序修复存在测试过拟合问题，即修复后的代码在已见测试上通过但在隐藏测试集上失败。随着大语言模型的兴起，需要重新评估这个问题是否仍然存在

Method: 使用SWE-bench仓库级任务进行实验研究，评估测试过拟合的程度

Result: 实验结果表明测试过拟合在现代程序修复中仍然是一个显著问题

Conclusion: 尽管大语言模型技术发展，测试过拟合仍然是自动程序修复需要关注的重要挑战

Abstract: Automated program repair has been shown to be susceptible to generating repaired code that passes on seen tests but fails on a hold-out set of hidden tests. This problem, dubbed test overfitting, has been identified and studied before the rise of large language models. We experimentally study how much test overfitting is still a problem today, using repository-level SWE-bench tasks.

</details>


### [9] [MOOT: a Repository of Many Multi-Objective Optimization Tasks](https://arxiv.org/abs/2511.16882)
*Tim Menzies,Tao Chen,Yulong Ye,Kishan Kumar Ganguly,Amirali Rayegan,Srinath Srinivasan,Andre Lustosa*

Main category: cs.SE

TL;DR: MOOT是一个多目标优化任务库，包含120多个来自软件工程研究论文的任务，用于帮助研究者和从业者探索软件工程中的权衡决策。


<details>
  <summary>Details</summary>
Motivation: 软件工程师需要在竞争性目标之间做出权衡决策（如速度vs成本、安全vs可用性、准确vs可解释等），但目前缺乏有效的工具来探索这些权衡，导致研究困难和实践中的次优产品。

Method: 收集近期软件工程研究论文中的多目标优化任务，构建MOOT仓库，涵盖软件配置、云调优、项目健康、过程建模、超参数优化等领域。

Result: MOOT目前包含120多个任务，在MIT许可下免费提供，支持多种新颖研究问题的探索。

Conclusion: MOOT为软件工程多目标优化研究提供了重要资源，能够促进该领域的研究发展，并邀请社区贡献更多任务。

Abstract: Software engineers must make decisions that trade off competing goals (faster vs. cheaper, secure vs. usable, accurate vs. interpretable, etc.). Despite MSR's proven techniques for exploring such goals, researchers still struggle with these trade-offs. Similarly, industrial practitioners deliver sub-optimal products since they lack the tools needed to explore these trade-offs.
  To enable more research in this important area, we introduce MOOT, a repository of multi-objective optimization tasks taken from recent SE research papers. MOOT's tasks cover software configuration, cloud tuning, project health, process modeling, hyperparameter optimization, and more. Located at github.com/timm/moot, MOOT's current 120+ tasks are freely available under an MIT license (and we invite community contributions). As shown here, this data enables dozens of novel research questions.

</details>


### [10] [ReVul-CoT: Towards Effective Software Vulnerability Assessment with Retrieval-Augmented Generation and Chain-of-Thought Prompting](https://arxiv.org/abs/2511.17027)
*Zhijie Chen,Xiang Chen,Ziming Li,Jiacheng Xue,Chaoyang Gao*

Main category: cs.SE

TL;DR: ReVul-CoT框架结合检索增强生成(RAG)和思维链(COT)提示，通过动态检索权威漏洞数据库信息和逐步推理，显著提升了基于大语言模型的软件漏洞评估性能。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在软件漏洞评估中的两个主要局限：缺乏领域特定知识，以及依赖浅层模式匹配而非深度上下文推理。

Method: 提出ReVul-CoT框架，集成RAG模块从本地知识库动态检索漏洞信息，结合COT提示引导LLM进行分步推理，评估可利用性、影响范围等因素。

Result: 在12,070个漏洞数据集上，ReVul-CoT在MCC指标上超越最先进基线16.50%-42.26%，在准确率、F1分数和MCC上分别领先10.43%、15.86%和16.50%。

Conclusion: RAG与COT提示的结合显著增强了基于LLM的软件漏洞评估，为未来研究指明了有前景的方向。

Abstract: Context: Software Vulnerability Assessment (SVA) plays a vital role in evaluating and ranking vulnerabilities in software systems to ensure their security and reliability. Objective: Although Large Language Models (LLMs) have recently shown remarkable potential in SVA, they still face two major limitations. First, most LLMs are trained on general-purpose corpora and thus lack domain-specific knowledge essential for effective SVA. Second, they tend to rely on shallow pattern matching instead of deep contextual reasoning, making it challenging to fully comprehend complex code semantics and their security implications. Method: To alleviate these limitations, we propose a novel framework ReVul-CoT that integrates Retrieval-Augmented Generation (RAG) with Chain-of-Thought (COT) prompting. In ReVul-CoT, the RAG module dynamically retrieves contextually relevant information from a constructed local knowledge base that consolidates vulnerability data from authoritative sources (such as NVD and CWE), along with corresponding code snippets and descriptive information. Building on DeepSeek-V3.1, CoT prompting guides the LLM to perform step-by-step reasoning over exploitability, impact scope, and related factors Results: We evaluate ReVul-CoT on a dataset of 12,070 vulnerabilities. Experimental results show that ReVul-CoT outperforms state-of-the-art SVA baselines by 16.50%-42.26% in terms of MCC, and outperforms the best baseline by 10.43%, 15.86%, and 16.50% in Accuracy, F1-score, and MCC, respectively. Our ablation studies further validate the contributions of considering dynamic retrieval, knowledge integration, and CoT-based reasoning. Conclusion: Our results demonstrate that combining RAG with CoT prompting significantly enhances LLM-based SVA and points out promising directions for future research.

</details>


### [11] [UI-CUBE: Enterprise-Grade Computer Use Agent Benchmarking Beyond Task Accuracy to Operational Reliability](https://arxiv.org/abs/2511.17131)
*Horia Cristescu,Charles Park,Trong Canh Nguyen,Sergiu Talmacel,Alexandru-Gabriel Ilie,Stefan Adam*

Main category: cs.SE

TL;DR: UI-CUBE是一个系统性基准测试，包含226个任务，旨在揭示当前计算机使用代理在企业部署准备度方面的根本架构限制。评估显示代理在简单UI交互上表现尚可(67-85%)，但在复杂工作流中性能急剧下降至9-19%，表明存在根本性架构问题而非可通过训练解决的增量能力差距。


<details>
  <summary>Details</summary>
Motivation: 现有计算机使用代理基准主要衡量任务完成度，但缺乏对企业部署准备度的评估，过于强调功能正确性而忽视了生产系统所需的操作可靠性。

Method: 开发了UI-CUBE基准测试，包含226个任务，分为两个难度层级：简单UI交互(136个任务)和复杂工作流(90个任务，包括复制粘贴任务和企业应用场景)，采用系统界面变化覆盖、多分辨率测试和通过应用状态自动验证任务成功的方法。

Result: 五个最先进模型的评估显示能力悬崖而非渐进性能下降：简单UI交互成功率67-85%(人类97.9%)，复杂工作流骤降至9-19%。人类评估者在复杂任务上仅达61.2%，表明当前代理仅能达到人类在简单任务上68-87%的性能，但在复杂工作流上仅15-32%。

Conclusion: UI-CUBE作为企业准备度诊断工具，揭示了当前计算机使用代理虽然能操作单个界面元素，但无法作为可靠的工作流自动化工具。这些发现为开发能够管理复杂多步骤企业流程的生产就绪代理提供了必要的架构洞见。

Abstract: While current Computer Use Agent (CUA) benchmarks measure task completion effectively, they provide limited assessment of enterprise deployment readiness, emphasizing functional correctness over the operational reliability required for production systems. We present UI-CUBE (UiPath Computer Use BEnchmark), a systematic benchmark comprising 226 tasks across two difficulty tiers designed to expose fundamental architectural limitations in current CUAs. Our evaluation covers simple UI interactions (136 tasks) and complex workflows including copy-paste tasks (50 tasks) and enterprise application scenarios (40 tasks), with systematic interface variation coverage, multi-resolution testing and automated validation of task success through the application state. Evaluation of five state-of-the-art models reveals a sharp capability cliff rather than gradual performance degradation. Simple UI interactions achieve 67-85% success rates (compared to 97.9% human performance), but complex workflows drop precipitously to 9-19%. Human evaluators with no prior application experience achieve only 61.2% on complex tasks despite near-perfect performance on simple tasks, establishing realistic performance ceilings. This discontinuous performance pattern -- where agents achieve 68-87% of human performance on simple tasks but only 15-32% on complex workflows -- indicates fundamental architectural limitations in memory management, hierarchical planning, and state coordination rather than incremental capability gaps addressable through better training or prompting. UI-CUBE functions as an enterprise-readiness diagnostic, revealing that while current CUAs can manipulate individual interface elements, they cannot yet function as reliable workflow automation tools. These findings provide architectural insights essential for developing production-ready CUAs capable of managing complex, multi-step enterprise processes.

</details>


### [12] [SlsReuse: LLM-Powered Serverless Function Reuse](https://arxiv.org/abs/2511.17262)
*Jinfeng Wen,Yuehan Sun*

Main category: cs.SE

TL;DR: SlsReuse是一个基于大语言模型的服务器less函数重用框架，通过语义增强表示和意图感知发现，显著提高了函数推荐的准确性。


<details>
  <summary>Details</summary>
Motivation: 服务器less计算虽然降低了运维负担，但对新手开发者来说，从头开发函数需要适应异构的平台特定编程风格，过程耗时且容易出错。函数重用是解决这些挑战的有前景方案，但现有技术由于任务描述和异构函数实现之间的语义鸿沟而不足。

Method: SlsReuse首先构建可重用函数库作为知识基础，然后通过有效的提示工程和少样本提示学习异构函数的统一语义增强表示，最后给定自然语言任务查询时，执行意图感知发现结合多级剪枝策略和相似度匹配。

Result: 在包含110个任务查询的数据集上评估，基于ChatGPT-4o的SlsReuse实现了91.20%的Recall@10，比最先进的基线方法提高了24.53个百分点。

Conclusion: SlsReuse是首个LLM驱动的服务器less函数重用框架，通过利用大语言模型的能力有效弥合了任务需求和函数语义之间的差距，显著提升了函数推荐的性能。

Abstract: Serverless computing has rapidly emerged as a popular cloud computing paradigm. It enables developers to implement function-level tasks, i.e., serverless functions, without managing infrastructure. While reducing operational overhead, it poses challenges, especially for novice developers. Developing functions from scratch requires adapting to heterogeneous, platform-specific programming styles, making the process time-consuming and error-prone. Function reuse offers a promising solution to address these challenges. However, research on serverless computing lacks a dedicated approach for function recommendation. Existing techniques from traditional contexts remain insufficient due to the semantic gap between task descriptions and heterogeneous function implementations. Advances in large language models (LLMs), pre-trained on large-scale corpora, create opportunities to bridge this gap by aligning developer requirements with function semantics.
  This paper presents SlsReuse, the first LLM-powered framework for serverless function reuse. Specifically, SlsReuse first constructs a reusable function repository serving as a foundational knowledge base. Then, it learns unified semantic-enhanced representations of heterogeneous functions through effective prompt engineering with few-shot prompting, capturing implicit code intent, target platforms, programming languages, and cloud services. Finally, given a natural language task query, SlsReuse performs intent-aware discovery combined with a multi-level pruning strategy and similarity matching. We evaluate SlsReuse on a curated dataset of 110 task queries. Built on ChatGPT-4o, one of the most representative LLMs, SlsReuse achieves Recall@10 of 91.20%, exceeding the state-of-the-art baseline by 24.53 percentage points.

</details>


### [13] [Detecting Performance-Relevant Changes in Configurable Software Systems](https://arxiv.org/abs/2511.17271)
*Sebastian Böhm,Florian Sattler,Norbert Siegmund,Sven Apel*

Main category: cs.SE

TL;DR: ConfFLARE通过识别与性能相关代码的数据流交互来检测性能回归，并基于相关特征选择配置子集进行测试，显著减少测试配置数量。


<details>
  <summary>Details</summary>
Motivation: 软件系统性能具有波动性，频繁的性能分析成本高昂，特别是在可配置系统中需要测量多个配置。配置采样方法无法保证完整性，可能遗漏仅影响少数配置的性能回归。

Method: ConfFLARE通过分析代码变更与性能相关代码的数据流交互，识别可能影响性能的特征，并基于这些特征选择相关配置子集进行性能测试。

Result: 在合成和真实软件系统的研究中，ConfFLARE几乎在所有情况下正确检测到性能回归，在除两个案例外的所有情况下识别出相关特征，平均减少79%（合成）和70%（真实场景）的测试配置数量，节省数小时性能测试时间。

Conclusion: ConfFLARE是一种有效的性能回归检测方法，通过智能选择相关配置显著降低测试成本，同时保持检测准确性。

Abstract: Performance is a volatile property of a software system and frequent performance profiling is required to keep the knowledge about a software system's performance behavior up to date. Repeating all performance measurements after every revision is a cost-intensive task, especially in the presence of configurability, where one has to measure multiple configurations to obtain a comprehensive picture. Configuration sampling is a common approach to control the measurement cost. However, it cannot guarantee completeness and might miss performance regressions, especially if they only affect few configurations. As an alternative to solve the cost reduction problem, we present ConfFLARE: ConfFLARE estimates whether a change potentially impacts performance by identifying data-flow interactions with performance-relevant code and extracts which software features participate in such interactions. Based on these features, we can select a subset of relevant configurations to focus performance profiling efforts on. In a study conducted on both, synthetic and real-world software systems, ConfFLARE correctly detects performance regressions in almost all cases and identifies relevant features in all but two cases, reducing the number of configurations to be tested on average by $79\%$ for synthetic and by $70\%$ for real-world regression scenarios saving hours of performance testing time.

</details>


### [14] [Framework Matters: Energy Efficiency of UI Automation Testing Frameworks](https://arxiv.org/abs/2511.17303)
*Timmie M. R. Lagermann,Kristina Sophia Carter,Su Mei Gwen Ho,Luís Cruz,Kerstin Eder,Maja H. Kirkeby*

Main category: cs.SE

TL;DR: 比较了四个Web UI自动化测试框架的能耗表现，发现不同框架执行相同UI操作的能耗差异可达6倍，Puppeteer在多数操作中最节能，Nightwatch最耗能。


<details>
  <summary>Details</summary>
Motivation: 研究UI自动化测试框架的能耗特性，为开发者提供能耗透明的信息，帮助他们在测试特定UI操作时做出节能的框架选择决策。

Method: 在受控的客户端-服务器环境中使用外部功率计，对每个UI操作（刷新、点击变体、复选框、拖放、文本输入、滚动）重复执行35次，测量能耗。

Result: Puppeteer在左键点击、右键点击、双击、复选框和文本输入操作中最节能；Selenium在刷新和滚动操作中最节能；Nightwatch通常能耗最高；相同操作在不同框架间的能耗差异可达6倍。

Conclusion: 提供UI自动化测试框架的能耗透明度，使开发者能够为特定UI操作测试做出明智的节能决策。

Abstract: We examine per action energy consumption across four web user interface (UI) automation testing frameworks to determine whether consistent tendencies can guide energy-aware test design. Using a controlled client-server setup with external power metering, we repeat each UI action (refresh, click variants, checkbox, drag&drop, input-text, scroll) 35 times. Across each of the actions, energy costs vary by both framework and action. Puppeteer is the most efficient for left-click, right-click, double-click, checkbox, and input-text; Selenium is the most efficient for refresh and scroll; Nightwatch is generally the least energy efficient. The energy cost of performing the same action varied by up to a factor of six depending on the framework. This indicates that providing transparency of energy consumption for UI automation testing frameworks allows developers to make informed, energy-aware decisions when testing a specific UI action.

</details>


### [15] [Agentic Program Verification](https://arxiv.org/abs/2511.17330)
*Haoxin Tu,Huan Zhao,Yahui Song,Mehtab Zafar,Ruijie Meng,Abhik Roychoudhury*

Main category: cs.SE

TL;DR: AutoRocq是首个用于程序验证的LLM智能体，通过迭代精化循环与Rocq定理证明器协作，实现自主证明构建。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成代码的普及，需要AI智能体来验证大量自动生成的代码。程序验证比一般数学推理更具结构性和上下文丰富性，形成有吸引力的研究方向。

Method: 采用LLM智能体与Rocq定理证明器协作的迭代精化方法，智能体在飞行中学习并通过与证明器的通信获得上下文和反馈来改进证明。

Result: 在SV-COMP基准测试和Linux内核模块上的实验评估显示，该方法在实现自动化程序验证方面具有良好效果。

Conclusion: AutoRocq可以与AI编码智能体集成，实现生成-验证循环，向可信自动编程的愿景迈进。

Abstract: Automatically generated code is gaining traction recently, owing to the prevalence of Large Language Models (LLMs). Further, the AlphaProof initiative has demonstrated the possibility of using AI for general mathematical reasoning. Reasoning about computer programs (software) can be accomplished via general mathematical reasoning; however, it tends to be more structured and richer in contexts. This forms an attractive proposition, since then AI agents can be used to reason about voluminous code that gets generated by AI.
  In this work, we present a first LLM agent, AutoRocq, for conducting program verification. Unlike past works, which rely on extensive training of LLMs on proof examples, our agent learns on-the-fly and improves the proof via an iterative refinement loop. The iterative improvement of the proof is achieved by the proof agent communicating with the Rocq (formerly Coq) theorem prover to get additional context and feedback. The final result of the iteration is a proof derivation checked by the Rocq theorem prover. In this way, our proof construction involves autonomous collaboration between the proof agent and the theorem prover. This autonomy facilitates the search for proofs and decision-making in deciding on the structure of the proof tree.
  Experimental evaluation on SV-COMP benchmarks and on Linux kernel modules shows promising efficacy in achieving automated program verification. As automation in code generation becomes more widespread, we posit that our proof agent can be potentially integrated with AI coding agents to achieve a generate and validate loop, thus moving closer to the vision of trusted automatic programming.

</details>


### [16] [Exploring Scientific Debt: Harnessing AI for SATD Identification in Scientific Software](https://arxiv.org/abs/2511.17368)
*Eric L. Melin,Ahmed Musa Awon,Nasir U. Eisty,Neil A. Ernst,Shurui Zhou*

Main category: cs.SE

TL;DR: 本研究探索科学软件中的自认技术债务，发现科学软件比通用软件包含9.25倍的科学债务和4.93倍的SATD，并开发了基于Transformer的SATD识别模型。


<details>
  <summary>Details</summary>
Motivation: 科学软件中的技术债务对研究结果的准确性和可重现性构成威胁，但SATD与科学软件之间的关系尚未得到充分研究。

Method: 分析27个科学和通用软件仓库，在67,066个标注代码注释上微调比较10个基于Transformer的模型（1亿-70亿参数）。

Result: 科学软件包含显著更多的科学债务和SATD，最佳模型性能优于现有方法。

Conclusion: 科学软件中的SATD与通用软件存在显著差异，影响软件质量和科学有效性，需要针对性管理策略。

Abstract: Developers often leave behind clues in their code, admitting where it falls short, known as Self-Admitted Technical Debt (SATD). In the world of Scientific Software (SSW), where innovation moves fast and collaboration is key, such debt is not just common but deeply impactful. As research relies on accurate and reproducible results, accumulating SATD can threaten the very foundations of scientific discovery. Yet, despite its significance, the relationship between SATD and SSW remains largely unexplored, leaving a crucial gap in understanding how to manage SATD in this critical domain. This study explores SATD in SSW repositories, comparing SATD in scientific versus general-purpose open-source software and evaluating transformer-based models for SATD identification. We analyzed SATD in 27 scientific and general-purpose repositories across multiple domains and languages. We fine-tuned and compared 10 transformer-based models (100M-7B parameters) on 67,066 labeled code comments. SSW contains 9.25x more Scientific Debt and 4.93x more SATD than general-purpose software due to complex computations, domain constraints, and evolving research needs. Furthermore, our best model outperforms existing ones. This study uncovers how SATD in SSW differs from general software, revealing its impact on quality and scientific validity. By recognizing these challenges, developers and researchers can adopt smarter strategies to manage debt and safeguard the integrity of scientific discovery.

</details>


### [17] [CREST: Improving Interpretability and Effectiveness of Troubleshooting at Ericsson through Criterion-Specific Trouble Report Retrieval](https://arxiv.org/abs/2511.17417)
*Soroush Javdan,Pragash Krishnamoorthy,Olga Baysal*

Main category: cs.SE

TL;DR: CREST方法通过为不同故障报告标准训练专门模型并集成其输出，显著提升了电信故障报告检索系统的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 电信行业快速发展需要高效的故障排除流程，但故障报告的复杂性和多样性给检索系统带来挑战，现有方法难以同时处理不同标准的多方面故障特征。

Method: 提出CREST方法，为不同故障报告标准训练专门化模型，然后集成这些专门模型的输出来捕获多样化和互补的信号。

Result: 在爱立信内部故障报告数据集上，基于标准的专门化模型在关键评估指标上显著优于单一模型方法。

Conclusion: 所有目标标准对于优化检索系统性能都很重要，CREST方法通过提供每个标准的相关性分数提高了检索准确性和可解释性。

Abstract: The rapid evolution of the telecommunication industry necessitates efficient troubleshooting processes to maintain network reliability, software maintainability, and service quality. Trouble Reports (TRs), which document issues in Ericsson's production system, play a critical role in facilitating the timely resolution of software faults. However, the complexity and volume of TR data, along with the presence of diverse criteria that reflect different aspects of each fault, present challenges for retrieval systems. Building on prior work at Ericsson, which utilized a two-stage workflow, comprising Initial Retrieval (IR) and Re-Ranking (RR) stages, this study investigates different TR observation criteria and their impact on the performance of retrieval models. We propose \textbf{CREST} (\textbf{C}riteria-specific \textbf{R}etrieval via \textbf{E}nsemble of \textbf{S}pecialized \textbf{T}R models), a criterion-driven retrieval approach that leverages specialized models for different TR fields to improve both effectiveness and interpretability, thereby enabling quicker fault resolution and supporting software maintenance. CREST utilizes specialized models trained on specific TR criteria and aggregates their outputs to capture diverse and complementary signals. This approach leads to enhanced retrieval accuracy, better calibration of predicted scores, and improved interpretability by providing relevance scores for each criterion, helping users understand why specific TRs were retrieved. Using a subset of Ericsson's internal TRs, this research demonstrates that criterion-specific models significantly outperform a single model approach across key evaluation metrics. This highlights the importance of all targeted criteria used in this study for optimizing the performance of retrieval systems.

</details>
