<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.LO](#cs.LO) [Total: 3]
- [cs.SE](#cs.SE) [Total: 8]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Data Race Detection by Digest-Driven Abstract Interpretation (Extended Version)](https://arxiv.org/abs/2511.11055)
*Michael Schwarz,Julian Erhard*

Main category: cs.PL

TL;DR: 该论文提出了一种基于摘要的静态数据竞争检测方法，通过摘要捕获冲突访问不能并行发生的条件，在Goblint静态分析器中实现并在SV-COMP基准测试中评估。


<details>
  <summary>Details</summary>
Motivation: 利用静态分析证明数据竞争的缺失，通过确定不会同时发生冲突的内存访问。重新利用摘要概念来为线程模块化值分析带来可调并发敏感性。

Method: 使用摘要来捕获冲突访问不能并行发生的条件，在线程模块化局部跟踪语义中定义数据竞争，并将潜在冲突的排除标准表示为摘要。在Goblint静态分析器中实现摘要驱动的数据竞争检测。

Result: 在SV-COMP基准测试中评估，结合锁集摘要与线程ID和线程连接推理的摘要，相比仅使用锁集推理，正确解决任务数量增加了五倍以上。

Conclusion: 摘要驱动的数据竞争检测方法显著提高了静态分析器在检测数据竞争方面的效果，特别是通过结合多种摘要类型来增强分析能力。

Abstract: Sound static analysis can prove the absence of data races by establishing that no two conflicting memory accesses can occur at the same time. We repurpose the concept of digests -- summaries of computational histories originally introduced to bring tunable concurrency-sensitivity to thread-modular value analysis by abstract interpretation, extending this idea to race detection: We use digests to capture the conditions under which conflicting accesses may not happen in parallel. To formalize this, we give a definition of data races in the thread-modular local trace semantics and show how exclusion criteria for potential conflicts can be expressed as digests. We report on our implementation of digest-driven data race detection in the static analyzer Goblint, and evaluate it on the SV-COMP benchmark suite. Combining the lockset digest with digests reasoning on thread ids and thread joins increases the number of correctly solved tasks by more than a factor of five compared to lockset reasoning alone.

</details>


### [2] [Optimising Density Computations in Probabilistic Programs via Automatic Loop Vectorisation](https://arxiv.org/abs/2511.11070)
*Sangho Lim,Hyoungjin Lim,Wonyeol Lee,Xavier Rival,Hongseok Yang*

Main category: cs.PL

TL;DR: 提出了一种自动向量化概率程序中循环的方法，通过推测性并行执行和定点检查来提升性能，在Pyro PPL中实现并取得了1.1-6倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 概率推理成本高昂，其中一个瓶颈是处理大型数据集或长序列随机样本的循环迭代。现有向量化技术要么需要手动操作容易出错，要么无法处理一般重复结构如嵌套循环和数据依赖控制流。

Method: 采用推测性并行执行循环迭代，通过定点检查保持原始循环语义，将命令式PPL转换为支持向量化原语的低级目标语言。

Result: 在Pyro PPL中实现并在多个概率模型上评估，相比现有向量化基线获得1.1-6倍速度提升，减少GPU内存使用，且能处理所有测试模型。

Conclusion: 该方法能有效自动向量化概率程序中的循环，显著提升性能，解决了现有技术在处理一般重复结构时的局限性。

Abstract: Probabilistic programming languages (PPLs) are a popular tool for high-level modelling across many fields. They provide a range of algorithms for probabilistic inference, which analyse models by learning their parameters from a dataset or estimating their posterior distributions. However, probabilistic inference is known to be very costly. One of the bottlenecks of probabilistic inference stems from the iteration over entries of a large dataset or a long series of random samples. Vectorisation can mitigate this cost, but manual vectorisation is error-prone, and existing automatic techniques are often ad-hoc and limited, unable to handle general repetition structures, such as nested loops and loops with data-dependent control flow, without significant user intervention. To address this bottleneck, we propose a sound and effective method for automatically vectorising loops in probabilistic programs. Our method achieves high throughput using speculative parallel execution of loop iterations, while preserving the semantics of the original loop through a fixed-point check. We formalise our method as a translation from an imperative PPL into a lower-level target language with primitives geared towards vectorisation. We implemented our method for the Pyro PPL and evaluated it on a range of probabilistic models. Our experiments show significant performance gains against an existing vectorisation baseline, achieving $1.1$--$6\times$ speedups and reducing GPU memory usage in many cases. Unlike the baseline, which is limited to a subset of models, our method effectively handled all the tested models.

</details>


### [3] [Kleene Algebra](https://arxiv.org/abs/2511.11264)
*Tobias Kappé,Alexandra Silva,Jana Wagemaker*

Main category: cs.PL

TL;DR: 本小册子介绍Kleene代数(KA)，这是一套用于研究程序等价性的定律。它讨论了如何用正则表达式建模程序，表达式与自动机的对应关系，以及如何利用这种对应关系证明KA的核心结果：正则表达式等价当且仅当可以用KA定律证明。


<details>
  <summary>Details</summary>
Motivation: 研究程序等价性是一个重要问题，Kleene代数提供了一套形式化的定律来系统地研究程序之间的等价关系。

Method: 通过将程序建模为正则表达式，建立正则表达式与自动机之间的对应关系，并利用这种对应关系来证明Kleene代数的核心定理。

Result: 证明了Kleene代数的核心结果：正则表达式的等价性当且仅当可以用KA的定律来证明。

Conclusion: Kleene代数为研究程序等价性提供了一个强大的形式化框架，通过正则表达式和自动机的对应关系，可以系统地验证程序的等价性。

Abstract: This booklet serves as an introduction to Kleene Algebra (KA), a set of laws that can be used to study general equivalences between programs. It discusses how general programs can be modeled using regular expressions, how those expressions correspond to automata, and how this correspondence can be exploited to obtain the central result of KA, namely that an equivalence of regular expressions is true if and only if it can be proved using the laws of KA. Each chapter closes with a set of exercises to further build intuition and understanding, and there is an optional chapter that develops automata theory through the lens of coalgebra.

</details>


### [4] [The Jasmin Compiler Preserves Cryptographic Security](https://arxiv.org/abs/2511.11292)
*Santiago Arranz-Olmos,Gilles Barthe,Lionel Blatter,Benjamin Grégoire,Vincent Laporte,Paolo Torrini*

Main category: cs.PL

TL;DR: 本文显著增强了Jasmin编译器（用于开发高效、形式验证的密码学实现）的安全性保证，证明其前端25个编译过程能够保持密码学安全性。


<details>
  <summary>Details</summary>
Motivation: Jasmin编译器虽然已被证明功能正确，但该正确性声明不适用于密码学中必需的非终止或概率计算，因此需要增强其密码学安全性保证。

Method: 首先定义适用于编译器正确性证明的关系式Hoare逻辑，并基于交互树建立新的Jasmin程序指称语义；然后使用该程序逻辑证明Jasmin编译器的功能正确性；最后形式化密码学安全性（聚焦IND-CCA）并证明编译器保持该安全性。

Result: 在Rocq证明器中成功证明了Jasmin编译器前端25个编译过程能够保持密码学安全性，特别是IND-CCA安全性。

Conclusion: 通过新的关系式Hoare逻辑和基于交互树的语义，成功增强了Jasmin编译器的安全性保证，使其能够处理密码学中必需的非终止和概率计算，并保持密码学安全性。

Abstract: Jasmin is a programming and verification framework for developing efficient, formally verified, cryptographic implementations. A main component of the framework is the Jasmin compiler, which empowers programmers to write efficient implementations of state-of-the-art cryptographic primitives, including post-quantum cryptographic standards. The Jasmin compiler is proven functionally correct in the Rocq prover. However, this functional correctness statement does not apply to nonterminating or probabilistic computations, which are essential features in cryptography.
  In this paper, we significantly enhance the guarantees of the compiler by showing, in the Rocq prover, that its front-end (25 out of 30 passes) preserves cryptographic security. To this end, we first define a Relational Hoare Logic tailored for compiler correctness proofs. We prove the soundness of our logic w.r.t. a new denotational semantics of Jasmin programs based on interaction trees. Secondly, we use our program logic to prove the functional correctness of the (unmodified) Jasmin compiler w.r.t. said semantics. Lastly, we formalize cryptographic security -- focusing on IND-CCA -- with interaction trees and prove that the Jasmin compiler preserves cryptographic security.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [5] [Automata-less Monitoring via Trace-Checking (Extended Version)](https://arxiv.org/abs/2511.11072)
*Andrea Brunello,Luca Geatti,Angelo Montanari,Nicola Saccomanno*

Main category: cs.FL

TL;DR: 该论文提出了一种无需构建DFA的运行时监控方法，通过识别有意安全和共安全公式，直接对系统轨迹进行评估，避免了双重指数级的复杂度爆炸。


<details>
  <summary>Details</summary>
Motivation: 在运行时验证中，从LTL或LTLf规范合成监控器（通常是DFA）会导致双重指数级的规模爆炸，这在实际应用中不可行。

Method: 基于Kupferman和Vardi提出的有意安全和共安全公式概念，证明对这些公式的监控可以通过轨迹检查直接完成，即直接在当前系统轨迹上评估公式。

Result: 对于LTLf的安全和共安全片段，所有公式都是有意安全和共安全的，无需额外检查；对于LTL，识别问题在PSPACE复杂度内，显著优于完整LTL的EXPSPACE复杂度。

Conclusion: 该方法通过直接轨迹检查实现了多项式复杂度的监控，避免了DFA合成的双重指数级开销，为运行时验证提供了更高效的解决方案。

Abstract: In runtime verification, monitoring consists of analyzing the current execution of a system and determining, on the basis of the observed finite trace, whether all its possible continuations satisfy or violate a given specification. This is typically done by synthesizing a monitor--often a Deterministic Finite State Automaton (DFA)--from logical specifications expressed in Linear Temporal Logic (LTL) or in its finite-word variant (LTLf). Unfortunately, the size of the resulting DFA may incur a doubly exponential blow-up in the size of the formula. In this paper, we identify some conditions under which monitoring can be done without constructing such a DFA. We build on the notion of intentionally safe and cosafe formulas, introduced in [Kupferman & Vardi, FMSD, 2001], to show that monitoring of these formulas can be carried out through trace-checking, that is, by directly evaluating them on the current system trace, with a polynomial complexity in the size of both the trace and the formula. In addition, we investigate the complexity of recognizing intentionally safe and cosafe formulas for the safety and cosafety fragments of LTL and LTLf. As for LTLf, we show that all formulas in these fragments are intentionally safe and cosafe, thus removing the need for the check. As for LTL, we prove that the problem is in PSPACE, significantly improving over the EXPSPACE complexity of full LTL.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [6] [Knowledge Reasoning Involving Four Types of Syllogisms](https://arxiv.org/abs/2511.10916)
*Long Wei,Liheng Hao*

Main category: cs.LO

TL;DR: 该论文从知识推理角度研究涉及Square{most}和Square{all}量词的非平凡广义三段论的有效性和话语推理，通过形式化证明AMI-1的有效性，并从中推导出多种三段论类型。


<details>
  <summary>Details</summary>
Motivation: 研究广义三段论的有效性和话语推理，为英语语言信息处理提供理论基础，并为研究其他三段论系统提供方法论见解。

Method: 首先提出这些三段论的知识表示，形式化证明广义三段论AMI-1的有效性，然后基于演绎推理从AMI-1推导出多种三段论类型。

Result: 成功推导出19个非平凡广义三段论、22个非平凡有效广义模态三段论、8个有效经典三段论和24个有效经典模态三段论，并讨论了如何判断这些三段论嵌套的话语推理有效性。

Conclusion: 这种形式化推导不仅为英语语言信息处理提供了理论基础，还为研究其他三段论系统提供了方法论启示。

Abstract: This paper studies the validity and discourse reasoning of non-trivial generalized syllogisms involving the quantifiers in Square{most} and Square{all} from the perspective of knowledge reasoning. Firstly, this paper presents knowledge representations for these syllogisms and formally proves the validity of generalized syllogism AMI-1. Subsequently, 19 non-trivial generalized syllogisms, 22 non-trivial valid generalized modal syllogisms, 8 valid classical syllogisms, and 24 valid classical modal syllogisms are respectively deduced from the valid generalized syllogism AMI-1 on the basis of deductive reasoning. Additionally, this paper discusses how to judge the validity of discourse reasoning nested by the above four types of syllogisms, which have four types of figures and different forms. In conclusion, such formal deductions not only provide a theoretical foundation for English language information processing, but also provide methodological insights for studying other syllogistic systems.

</details>


### [7] [Arity hierarchies for quantifiers closed under partial polymorphisms](https://arxiv.org/abs/2511.11326)
*Anuj Dawar,Lauri Hella,Benedikt Pago*

Main category: cs.LO

TL;DR: 该论文研究了在部分多态条件下封闭的广义量词的表达能力，回答了Dawar和Hella (2024)提出的问题，建立了基于部分近一致性多态性和量词元数的层次结构，并证明了具有部分Maltsev多态性的量词的不表达能力。


<details>
  <summary>Details</summary>
Motivation: 研究约束满足问题中由部分多态条件封闭的广义量词的表达能力，解决Dawar和Hella (2024)工作中提出的若干问题。

Method: 使用类似Cai-Fürer-Immerman风格的新颖代数构造和Dawar-Hella (2024)的量词卵石游戏来证明分离结果。

Result: 建立了部分近一致性多态性元数与量词元数之间的层次关系：在ℓ元部分近一致性多态性下封闭的(ℓ+1)元量词的表达能力严格介于所有ℓ-1元量词和ℓ元量词之间；还建立了基于固定部分近一致性多态性元数的量词元数的无限层次结构。

Conclusion: 部分多态条件对广义量词的表达能力产生重要影响，建立了明确的层次结构，并为具有部分Maltsev多态性的量词提供了不表达能力证明。

Abstract: We investigate the expressive power of generalized quantifiers closed under partial polymorphism conditions motivated by the study of constraint satisfaction problems. We answer a number of questions arising from the work of Dawar and Hella (CSL 2024) where such quantifiers were introduced. For quantifiers closed under partial near-unanimity polymorphisms, we establish hierarchy results clarifying the interplay between the arity of the polymorphisms and of the quantifiers: The expressive power of $(\ell+1)$-ary quantifiers closed under $\ell$-ary partial near-unanimity polymorphisms is strictly between the class of all quantifiers of arity $\ell-1$ and $\ell$. We also establish an infinite hierarchy based on the arity of quantifiers with a fixed arity of partial near-unanimity polymorphisms. Finally, we prove inexpressiveness results for quantifiers with a partial Maltsev polymorphism. The separation results are proved using novel algebraic constructions in the style of Cai-Fürer-Immerman and the quantifier pebble games of Dawar and Hella (2024).

</details>


### [8] [Universal Safety Controllers with Learned Prophecies](https://arxiv.org/abs/2511.11390)
*Bernd Finkbeiner,Niklas Metzger,Satya Prakash Nayak,Anne-Kathrin Schmuck*

Main category: cs.LO

TL;DR: 提出一种近似算法用于通用安全控制器（USC）合成，通过从示例植物中学习计算树逻辑（CTL）公式作为预言表示，替代计算复杂的精确预言，从而提高效率、可解释性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统USC合成中精确预言的计算和验证计算复杂度高，限制了其实际应用。需要一种更高效、可解释的方法来构建通用安全控制器。

Method: 使用学习方法来近似USC合成：从小的示例植物计算预言的下界和上界近似，推断CTL公式作为预言表示，通过验证步骤泛化到未见植物。

Result: 实验结果表明，学习的预言保持泛化能力，同时比精确树自动机表示更紧凑和可解释。

Conclusion: 提出的近似USC合成方法通过CTL公式表示预言，在保持泛化能力的同时显著提高了效率、紧凑性和可解释性。

Abstract: \emph{Universal Safety Controllers (USCs)} are a promising logical control framework that guarantees the satisfaction of a given temporal safety specification when applied to any realizable plant model. Unlike traditional methods, which synthesize one logical controller over a given detailed plant model, USC synthesis constructs a \emph{generic controller} whose outputs are conditioned by plant behavior, called \emph{prophecies}. Thereby, USCs offer strong generalization and scalability benefits over classical logical controllers. However, the exact computation and verification of prophecies remain computationally challenging. In this paper, we introduce an approximation algorithm for USC synthesis that addresses these limitations via learning. Instead of computing exact prophecies, which reason about sets of trees via automata, we only compute under- and over-approximations from (small) example plants and infer computation tree logic (CTL) formulas as representations of prophecies. The resulting USC generalizes to unseen plants via a verification step and offers improved efficiency and explainability through small and concise CTL prophecies, which remain human-readable and interpretable. Experimental results demonstrate that our learned prophecies remain generalizable, yet are significantly more compact and interpretable than their exact tree automata representations.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [9] [Peer Code Review in Research Software Development: The Research Software Engineer Perspective](https://arxiv.org/abs/2511.10781)
*Md Ariful Islam Malik,Jeffrey C. Carver,Nasir U. Eisty*

Main category: cs.SE

TL;DR: 本研究调查了研究软件工程师对同行代码审查的看法，发现尽管同行代码审查对提高研究软件质量至关重要，但RSEs在采用过程中面临独特挑战，需要通过结构化流程、改进工具和针对性培训来提升采用率和效果。


<details>
  <summary>Details</summary>
Motivation: 研究软件对科研发现至关重要，但不断变化的需求、复杂输入和遗留依赖阻碍了软件质量和可维护性。虽然同行代码审查可以改善软件质量，但其在研究软件工程师中的采用情况尚未被探索。

Method: 通过调查问卷收集研究软件工程师对同行代码审查的见解，调查设计与先前研究保持一致以进行对比分析，同时包含针对RSEs的额外问题。

Result: 收到61份有效回复，发现结果与先前研究一致，同时揭示了RSEs相比更广泛开发者群体面临的独特挑战和实践差异。

Conclusion: 同行代码审查对提高研究软件的质量、可维护性和可靠性至关重要。尽管RSEs面临独特挑战，但通过结构化流程、改进工具和针对性培训可以增强研究软件开发中同行审查的采用和有效性。

Abstract: Background: Research software is crucial for enabling research discoveries and supporting data analysis, simulation, and interpretation across domains. However, evolving requirements, complex inputs, and legacy dependencies hinder the software quality and maintainability. While peer code review can improve software quality, its adoption by research software engineers (RSEs) remains unexplored. Aims: This study explores RSE perspectives on peer code review, focusing on their practices, challenges, and potential improvements. Building on prior work, it aims to uncover how RSEs insights differ from those of other research software developers and identify factors that can enhance code review adoption in this domain. Method: We surveyed RSEs to gather insights into their perspectives on peer code review. The survey design aligned with previous research to enable comparative analysis while including additional questions tailored to RSEs. Results: We received 61 valid responses from the survey. The findings align with prior research while uncovering unique insights about the challenges and practices of RSEs compared to broader developer groups. Conclusions: Peer code review is vital in improving research software's quality, maintainability, and reliability. Despite the unique challenges RSEs face, addressing these through structured processes, improved tools, and targeted training can enhance peer review adoption and effectiveness in research software development.

</details>


### [10] [Towards a Human-in-the-Loop Framework for Reliable Patch Evaluation Using an LLM-as-a-Judge](https://arxiv.org/abs/2511.10865)
*Sherry Shi,Renyao Wei,Michele Tufano,José Cambronero,Runxiang Cheng,Franjo Ivančić,Pat Rondon*

Main category: cs.SE

TL;DR: 提出了一种结合人类参与的LLM补丁有效性评估方法，通过生成评估标准、人工审查和LLM判断来降低手动标注成本。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化程序修复评估方法依赖执行测试，无法准确捕捉补丁有效性，而手动标注成本高昂。

Method: 采用人类参与循环的方法：首先生成每个bug的评估标准，进行一次性人工审查和优化，然后用LLM基于优化后的标准判断补丁有效性。

Result: 在人类评分者一致同意的补丁上，该方法与人类共识达成高度一致（Cohen's kappa 0.75），召回率0.94，精确率0.80。

Conclusion: 该方法能显著降低评估成本，在人类一致同意的案例中表现良好，但在存在分歧的案例中仍有改进空间。

Abstract: Reliable evaluation is crucial for advancing Automated Program Repair (APR), but prevailing benchmarks rely on execution-based evaluation methods (unit test pass@k), which fail to capture true patch validity. Determining validity can require costly manual annotation. To reduce this cost, we introduce a human-in-the-loop approach to LLM-based patch validity judgment. Inspired by the observation that human judgment is better aligned when using a shared rubric, we first employ an LLM to generate a per-bug rubric, followed by a one-time human review and optional refinement to this rubric, and then employ an LLM to judge patches using the refined rubric. We apply this approach to assign binary validity labels to patches for issues found by Google sanitizer tools. Our results show that this approach yields substantial agreement with human consensus (Cohen's kappa 0.75), high recall (0.94) and high precision (0.80), when considering patches that have unanimous agreement from 3 human raters on the validity labels. On the full dataset including patches where human raters disagree, we find this approach can still be further improved (Cohen's kappa 0.57, recall 0.93, precision 0.65) and identify possible future directions.

</details>


### [11] [Architecting software monitors for control-flow anomaly detection through large language models and conformance checking](https://arxiv.org/abs/2511.10876)
*Francesco Vitale,Francesco Flammini,Mauro Caporuscio,Nicola Mazzocca*

Main category: cs.SE

TL;DR: 提出了一种基于大语言模型和一致性检查的软件监控方法，用于检测运行时控制流异常，在铁路系统案例中实现了高覆盖率和检测性能。


<details>
  <summary>Details</summary>
Motivation: 现代计算机系统复杂度高，设计时验证无法完全覆盖运行时可能出现的控制流异常，需要运行时监控来检测未知异常。

Method: 利用大语言模型连接设计时模型和实现代码，自动化源代码插桩生成事件日志，然后通过一致性检查技术分析日志检测控制流异常。

Result: 在ERTMS/ETCS铁路系统案例中，LLM源代码插桩实现了84.775%的控制流覆盖率，异常检测达到96.610% F1分数和93.515% AUC。

Conclusion: 结合领域知识指导LLM进行源代码插桩能够获得高质量的软件日志，并通过一致性检查实现有效的控制流异常检测。

Abstract: Context: Ensuring high levels of dependability in modern computer-based systems has become increasingly challenging due to their complexity. Although systems are validated at design time, their behavior can be different at run-time, possibly showing control-flow anomalies due to "unknown unknowns".
  Objective: We aim to detect control-flow anomalies through software monitoring, which verifies run-time behavior by logging software execution and detecting deviations from expected control flow.
  Methods: We propose a methodology to develop software monitors for control-flow anomaly detection through Large Language Models (LLMs) and conformance checking. The methodology builds on existing software development practices to maintain traditional V&V while providing an additional level of robustness and trustworthiness. It leverages LLMs to link design-time models and implementation code, automating source-code instrumentation. The resulting event logs are analyzed via conformance checking, an explainable and effective technique for control-flow anomaly detection.
  Results: We test the methodology on a case-study scenario from the European Railway Traffic Management System / European Train Control System (ERTMS/ETCS), which is a railway standard for modern interoperable railways. The results obtained from the ERTMS/ETCS case study demonstrate that LLM-based source-code instrumentation can achieve up to 84.775% control-flow coverage of the reference design-time process model, while the subsequent conformance checking-based anomaly detection reaches a peak performance of 96.610% F1-score and 93.515% AUC.
  Conclusion: Incorporating domain-specific knowledge to guide LLMs in source-code instrumentation significantly allowed obtaining reliable and quality software logs and enabled effective control-flow anomaly detection through conformance checking.

</details>


### [12] [Beyond Accuracy: Behavioral Dynamics of Agentic Multi-Hunk Repair](https://arxiv.org/abs/2511.11012)
*Noor Nashid,Daniel Ding,Keheliya Gallaba,Ahmed E. Hassan,Ali Mesbah*

Main category: cs.SE

TL;DR: 本文首次系统研究了LLM驱动的代码代理在修复多块错误方面的表现，发现修复准确率从25.8%到93.3%不等，且随着错误分散度和复杂度增加而下降。高表现代理展现更好的语义一致性，而失败修复消耗更多资源。


<details>
  <summary>Details</summary>
Motivation: 传统程序修复主要关注单块错误，而忽略了现实系统中普遍存在的多块错误。修复这些错误需要在多个不连续代码区域进行协调编辑，带来了更大的挑战。

Method: 在Hunk4J数据集的372个多块错误上评估了Claude Code、Codex、Gemini-cli和Qwen Code等LLM驱动代码代理，使用细粒度指标分析1,488个修复轨迹，并开发了Maple工具提供仓库级上下文。

Result: 修复准确率范围从25.8%(Qwen Code)到93.3%(Claude Code)，随着错误分散度和复杂度增加而下降。高表现代理实现正向回归减少，而低表现代理常引入新测试失败。失败修复消耗39%-343%更多token和43%-427%更长时间。Maple将Gemini-cli修复准确率提高了30%。

Conclusion: 通过细粒度指标和轨迹级分析，本研究超越了准确率，解释了代码代理在多块修复过程中如何定位、推理和行动，为理解LLM在复杂程序修复任务中的行为提供了深入见解。

Abstract: Automated program repair has traditionally focused on single-hunk defects, overlooking multi-hunk bugs that are prevalent in real-world systems. Repairing these bugs requires coordinated edits across multiple, disjoint code regions, posing substantially greater challenges. We present the first systematic study of LLM-driven coding agents (Claude Code, Codex, Gemini-cli, and Qwen Code) on this task. We evaluate these agents on 372 multi-hunk bugs from the Hunk4J dataset, analyzing 1,488 repair trajectories using fine-grained metrics that capture localization, repair accuracy, regression behavior, and operational dynamics. Results reveal substantial variation: repair accuracy ranges from 25.8% (Qwen Code) to 93.3% (Claude Code) and consistently declines with increasing bug dispersion and complexity. High-performing agents demonstrate superior semantic consistency, achieving positive regression reduction, whereas lower-performing agents often introduce new test failures. Notably, agents do not fail fast; failed repairs consume substantially more resources (39%-343% more tokens) and require longer execution time (43%-427%). Additionally, we developed Maple to provide agents with repository-level context. Empirical results show that Maple improves the repair accuracy of Gemini-cli by 30% through enhanced localization. By analyzing fine-grained metrics and trajectory-level analysis, this study moves beyond accuracy to explain how coding agents localize, reason, and act during multi-hunk repair.

</details>


### [13] [Utilizing LLMs for Industrial Process Automation: A Case Study on Modifying RAPID Programs](https://arxiv.org/abs/2511.11125)
*Salim Fares,Steffen Herbold*

Main category: cs.SE

TL;DR: 本文研究了如何利用大型语言模型（LLM）处理工业过程自动化领域的专业编程语言，通过少量样本提示方法解决简单问题，同时确保敏感公司数据的本地保护。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注通用编程语言，而工业过程自动化领域使用的专业语言通常只在专有环境中使用，LLM在这些领域的应用潜力尚未充分探索。

Method: 采用少量样本提示方法，在不进行大量领域特定语言模型训练的情况下，利用现有LLM解决专业语言中的简单问题。

Result: 研究表明，少量样本提示方法足以解决专业语言中的简单问题，且可在本地部署确保数据安全。

Conclusion: 企业无需投入大量资源训练专业语言模型，通过少量样本提示方法即可在本地环境中有效利用LLM处理工业自动化领域的专业编程任务。

Abstract: How to best use Large Language Models (LLMs) for software engineering is covered in many publications in recent years. However, most of this work focuses on widely-used general purpose programming languages. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, is still underexplored. Within this paper, we study enterprises can achieve on their own without investing large amounts of effort into the training of models specific to the domain-specific languages that are used. We show that few-shot prompting approaches are sufficient to solve simple problems in a language that is otherwise not well-supported by an LLM and that is possible on-premise, thereby ensuring the protection of sensitive company data.

</details>


### [14] [SQuaD: The Software Quality Dataset](https://arxiv.org/abs/2511.11265)
*Mikel Robredo,Matteo Esposito,Davide Taibi,Rafael Peñaloza,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: SQuaD是一个多维度的软件质量数据集，从450个成熟开源项目中提取质量指标，整合了9种静态分析工具，提供超过700个独特指标，支持大规模软件质量研究。


<details>
  <summary>Details</summary>
Motivation: 现有软件质量数据集通常只关注有限的维度（如代码异味、技术债务或重构活动），限制了跨时间和质量维度的综合分析。

Method: 通过整合9种最先进的静态分析工具（SonarQube、CodeScene、PMD等），从450个成熟开源项目中提取多维度质量指标，涵盖方法、类、文件和项目级别。

Result: 创建了包含63,586个项目版本的SQuaD数据集，提供版本控制历史、问题跟踪历史、软件漏洞数据以及过程指标，支持即时缺陷预测。

Conclusion: SQuaD支持可维护性、技术债务、软件演化和质量评估的大规模实证研究，并提出了自动化数据集更新和跨项目质量建模等新兴研究方向。

Abstract: Software quality research increasingly relies on large-scale datasets that measure both the product and process aspects of software systems. However, existing resources often focus on limited dimensions, such as code smells, technical debt, or refactoring activity, thereby restricting comprehensive analyses across time and quality dimensions. To address this gap, we present the Software Quality Dataset (SQuaD), a multi-dimensional, time-aware collection of software quality metrics extracted from 450 mature open-source projects across diverse ecosystems, including Apache, Mozilla, FFmpeg, and the Linux kernel. By integrating nine state-of-the-art static analysis tools, i.e., SonarQube, CodeScene, PMD, Understand, CK, JaSoMe, RefactoringMiner, RefactoringMiner++, and PyRef, our dataset unifies over 700 unique metrics at method, class, file, and project levels. Covering a total of 63,586 analyzed project releases, SQuaD also provides version control and issue-tracking histories, software vulnerability data (CVE/CWE), and process metrics proven to enhance Just-In-Time (JIT) defect prediction. The SQuaD enables empirical research on maintainability, technical debt, software evolution, and quality assessment at unprecedented scale. We also outline emerging research directions, including automated dataset updates and cross-project quality modeling to support the continuous evolution of software analytics. The dataset is publicly available on ZENODO (DOI: 10.5281/zenodo.17566690).

</details>


### [15] [SCRUTINEER: Detecting Logic-Level Usage Violations of Reusable Components in Smart Contracts](https://arxiv.org/abs/2511.11411)
*Xingshuang Lin,Binbin Zhao,Jinwen Wang,Qinge Xie,Xibin Zhao,Shouling Ji*

Main category: cs.SE

TL;DR: SCRUTINEER是一个自动化系统，用于检测智能合约可重用组件(SCRs)的逻辑级使用违规，通过复合特征提取、LLM驱动的知识构建、RAG驱动的检查和冲突检查器实现高精度检测。


<details>
  <summary>Details</summary>
Motivation: 智能合约可重用组件(SCRs)虽然促进了模块化和代码重用，但逻辑级使用违规风险日益严重。这种违规发生在SCR符合使用规则但与特定业务逻辑不匹配时，导致重大漏洞，需要深度语义理解来检测。

Method: 1. 复合特征提取方法生成三种互补特征表示；2. LLM驱动的知识构建框架提取逻辑级使用并构建SCR知识库；3. RAG驱动的检查器结合快速检索和综合分析；4. 逻辑级使用违规分析引擎集成相似性检查和快照推理冲突检查。

Result: 在3个真实数据集上的评估显示，SCRUTINEER在检测SCR逻辑级使用违规方面达到80.77%的精确率、82.35%的召回率和81.55%的F1分数。

Conclusion: SCRUTINEER是首个自动化且实用的系统，能够有效检测智能合约可重用组件的逻辑级使用违规，为智能合约安全提供了重要保障。

Abstract: Smart Contract Reusable Components(SCRs) play a vital role in accelerating the development of business-specific contracts by promoting modularity and code reuse. However, the risks associated with SCR usage violations have become a growing concern. One particular type of SCR usage violation, known as a logic-level usage violation, is becoming especially harmful. This violation occurs when the SCR adheres to its specified usage rules but fails to align with the specific business logic of the current context, leading to significant vulnerabilities. Detecting such violations necessitates a deep semantic understanding of the contract's business logic, including the ability to extract implicit usage patterns and analyze fine-grained logical behaviors. To address these challenges, we propose SCRUTINEER, the first automated and practical system for detecting logic-level usage violations of SCRs. First, we design a composite feature extraction approach that produces three complementary feature representations, supporting subsequent analysis. We then introduce a Large Language Model-powered knowledge construction framework, which leverages comprehension-oriented prompts and domain-specific tools to extract logic-level usage and build the SCR knowledge base. Next, we develop a Retrieval-Augmented Generation-driven inspector, which combines a rapid retrieval strategy with both comprehensive and targeted analysis to identify potentially insecure logic-level usages. Finally, we implement a logic-level usage violation analysis engine that integrates a similarity-based checker and a snapshot-based inference conflict checker to enable accurate and robust detection. We evaluate SCRUTINEER from multiple perspectives on 3 ground-truth datasets. The results show that SCRUTINEER achieves a precision of 80.77%, a recall of 82.35%, and an F1-score of 81.55% in detecting logic-level usage violations of SCRs.

</details>


### [16] [CertiA360: Enhance Compliance Agility in Aerospace Software Development](https://arxiv.org/abs/2511.11550)
*J. Antonio Dantas Macedo,Hugo Fernandes,J. Eduardo Ferreira Ribeiro*

Main category: cs.SE

TL;DR: 提出了CertiA360工具，将敏捷方法的灵活性与航空航天安全关键系统的严格认证要求（DO-178C）相结合，通过自动化变更管理和需求追踪来确保合规性。


<details>
  <summary>Details</summary>
Motivation: 敏捷方法在灵活性方面具有优势，但在航空航天等安全关键系统开发中面临严格合规要求（如DO-178C标准）的挑战，需要将敏捷的灵活性与认证指南的严谨性相结合。

Method: 开发CertiA360工具，通过与行业专家合作设计验证，自动化需求变更管理和追踪，确保在整个软件开发生命周期中的监管合规性。

Result: 反馈显示CertiA360的自动化功能可以减少人工工作量，在响应需求变化的同时确保DO-178C合规性，尽管工具尚未通过DO-330认证。

Conclusion: 适当调整的敏捷方法不仅能够与航空航天等高监管领域的安全系统开发和认证要求共存，还能提高效率。

Abstract: Agile methods are characterised by iterative and incremental processes with a strong focus on flexibility and accommodating changing requirements based on either technical, regulatory, or stakeholder feedback. However, integrating Agile methods into safety-critical system development in the aerospace industry presents substantial challenges due to its strict compliance requirements, such as those outlined in the DO-178C standard. To achieve this vision, the flexibility of Agile must align with the rigorous certification guidelines, which emphasize documentation, traceability of requirements across different levels and disciplines, and comprehensive verification and validation (V&V) activities. The research work described in this paper proposes a way of using the strengths of the flexible nature of Agile methods to automate and manage change requests throughout the whole software development lifecycle, ensuring robust traceability, regulatory compliance and ultimately facilitating successful certification. This study proposes CertiA360, a tool designed to help teams improve requirement maturity, automate the changes in traceability, and align with the regulatory objectives. The tool was designed and validated in close collaboration with aerospace industry experts, using their feedback to ensure practical application and real-life effectiveness. The feedback collected demonstrated that the automation given by CertiA360 may reduce manual effort and allow response to changing requirements while ensuring compliance with DO-178C. While the tool is not yet qualified under DO-330 (Tool Qualification), findings suggest that when tailored appropriately, Agile methods can not only coexist with the requirements of safety-system development and certification in highly regulated domains like aerospace, but also add efficiency.

</details>
