<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.FL](#cs.FL) [Total: 3]
- [cs.SE](#cs.SE) [Total: 25]
- [cs.LO](#cs.LO) [Total: 14]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Nice to Meet You: Synthesizing Practical MLIR Abstract Transformers](https://arxiv.org/abs/2512.06442)
*Xuanyu Peng,Dominic Kennedy,Yuyou Fan,Ben Greenman,John Regehr,Loris D'Antoni*

Main category: cs.PL

TL;DR: NiceToMeetYou是一个程序合成框架，用于为编译器中的整数抽象域自动生成抽象变换器，无需人工草图，通过分解合成问题并验证正确性。


<details>
  <summary>Details</summary>
Motivation: 静态分析在编译中至关重要，但实现正确、精确且高效的抽象变换器很困难。LLVM和GCC都曾因不正确的抽象变换器导致错误编译，且LLVM仍有数百条指令缺乏抽象变换器。

Method: 采用程序合成框架，将合成问题分解为多个部分：每个变换器是多个简单、正确变换器的交（meet），每个新部分填补最终变换器的精度缺口。无需草图，通过将验证降低到MLIR的SMT方言来验证变换器。

Result: 合成的变换器被证明是正确的，其中17%比LLVM提供的变换器更精确。实现了批量自动化生成，解决了生产编译器整数抽象域变换器的合成问题。

Conclusion: NiceToMeetYou框架能够自动生成正确且精确的抽象变换器，解决了编译器开发中长期存在的抽象变换器实现难题，提高了编译器的可靠性和完整性。

Abstract: Static analyses play a fundamental role during compilation: they discover facts that are true in all executions of the code being compiled, and then these facts are used to justify optimizations and diagnostics. Each static analysis is based on a collection of abstract transformers that provide abstract semantics for the concrete instructions that make up a program. It can be challenging to implement abstract transformers that are sound, precise, and efficient, and in fact both LLVM and GCC have suffered from miscompilations caused by unsound abstract transformers. Moreover, even after more than 20 years of development, LLVM lacks abstract transformers for hundreds of instructions in its intermediate representation (IR). We developed NiceToMeetYou, a program synthesis framework for abstract transformers that are aimed at the kinds of non-relational integer abstract domains that are heavily used by today's production compilers. It exploits a simple but novel technique for breaking the synthesis problem into parts: each of our transformers is the meet of a collection of simpler, sound transformers that are synthesized such that each new piece fills a gap in the precision of the final transformer. Our design point is bulk automation: no sketches are required. Transformers are verified by lowering to a previously created SMT dialect of MLIR. Each of our synthesized transformers is provably sound and some (17 percent) are more precise than those provided by LLVM.

</details>


### [2] [PIP: Making Andersen's Points-to Analysis Sound and Practical for Incomplete C Programs](https://arxiv.org/abs/2512.07299)
*Håvard Rognebakke Krogstie,Helge Bahmann,Magnus Själander,Nico Reissmann*

Main category: cs.PL

TL;DR: 提出一种针对不完整C程序的安德森式指针分析，通过隐式指针目标跟踪实现高效且可靠的分析，比现有技术快15倍，并通过PIP技术进一步加速1.9倍。


<details>
  <summary>Details</summary>
Motivation: 现有指针分析技术需要完整程序才能保证可靠性，但在生产编译器中，文件单独编译时只能处理不完整程序，且缺乏摘要函数支持。需要一种既能保证可靠性又高效的指针分析方法。

Method: 采用安德森式指针分析框架，通过隐式跟踪外部模块可访问的内存位置和指针（在约束图中隐式实现），避免显式指针目标跟踪的开销。提出PIP技术进一步减少显式指针目标的使用。

Result: 隐式指针目标跟踪使约束求解器比五种最先进显式跟踪技术快15倍；PIP技术提供额外1.9倍加速；在别名分析客户端中，比LLVM BasicAA减少40%的MayAlias响应；内存可扩展性好，适合实际编译器优化。

Conclusion: 该方法首次实现了对不完整C程序的高效可靠指针分析，通过隐式指针目标跟踪和PIP技术显著提升性能，同时保持高精度和内存可扩展性，适合生产编译器使用。

Abstract: Compiling files individually lends itself well to parallelization, but forces the compiler to operate on incomplete programs. State-of-the-art points-to analyses guarantee sound solutions only for complete programs, requiring summary functions to describe any missing program parts. Summary functions are rarely available in production compilers, however, where soundness and efficiency are non-negotiable. This paper presents an Andersen-style points-to analysis that efficiently produces sound solutions for incomplete C programs. The analysis accomplishes soundness by tracking memory locations and pointers that are accessible from external modules, and efficiency by performing this tracking implicitly in the constraint graph. We show that implicit pointee tracking makes the constraint solver 15$\times$ faster than any combination of five different state-of-the-art techniques using explicit pointee tracking. We also present the Prefer Implicit Pointees (PIP) technique that further reduces the use of explicit pointees. PIP gives an additional speedup of 1.9$\times$, compared to the fastest solver configuration not benefiting from PIP. The precision of the analysis is evaluated in terms of an alias-analysis client, where it reduces the number of MayAlias-responses by 40% compared to LLVM's BasicAA pass alone. Finally, we show that the analysis is scalable in terms of memory, making it suitable for optimizing compilers in practice.

</details>


### [3] [Canonical bidirectional typechecking](https://arxiv.org/abs/2512.07511)
*Zanzi Mihejevs,Jules Hedges*

Main category: cs.PL

TL;DR: 论文展示了可检查/可综合的双向类型检查与极化System L中的对偶性相一致，正项和负余项可检查，负项和正余项可综合，并扩展到笛卡尔System L和线性-非线性组合。


<details>
  <summary>Details</summary>
Motivation: 探索双向类型检查中的可检查/可综合划分与极化System L（极化μμ̃-演算）中现有对偶性之间的关系，建立类型系统之间的深刻联系。

Method: 结合标准双向类型检查与Zeilberger的"共上下文"变体，扩展到笛卡尔System L使用McBride的共德布鲁因作用域表述，并以线性-非线性风格组合两者。

Result: 证明了极化System L的移位、LNL演算和双向演算之间存在显著的三重一致性，正类型对应线性类型，负类型对应笛卡尔类型。

Conclusion: 双向类型检查的划分与极化System L的对偶性本质相同，这一发现揭示了不同类型系统之间的深层结构联系，为类型理论提供了统一视角。

Abstract: We demonstrate that the checkable/synthesisable split in bidirectional typechecking coincides with existing dualities in polarised System L, also known as polarised $μ\tildeμ$-calculus. Specifically, positive terms and negative coterms are checkable, and negative terms and positive coterms are synthesisable. This combines a standard formulation of bidirectional typechecking with Zeilberger's `cocontextual' variant. We extend this to ordinary `cartesian' System L using Mc Bride's co-de Bruijn formulation of scopes, and show that both can be combined in a linear-nonlinear style, where linear types are positive and cartesian types are negative. This yields a remarkable 3-way coincidence between the shifts of polarised System L, LNL calculi, and bidirectional calculi.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [4] [Soft state reduction of fuzzy automata over residuated lattices](https://arxiv.org/abs/2512.06832)
*Linh Anh Nguyen,Son Thanh Cao,Stefan Stanimirović*

Main category: cs.FL

TL;DR: 提出软状态约简方法，通过阈值ε和可选的字长限制k，在非局部有限剩余格上近似约简非确定性模糊有限自动机


<details>
  <summary>Details</summary>
Motivation: 非确定性模糊有限自动机（FfA）在非局部有限剩余格（如乘积和Hamacher结构）上的状态约简具有挑战性，现有方法难以处理

Method: 引入软状态约简方法，使用阈值ε忽略小于ε的模糊值，使剩余格变为局部有限；定义近似不变性关系，允许在ε容忍度和可选字长k内合并几乎等价的状态；提出迭代应用这些不变性实现约简的算法

Result: 该方法能有效约简现有技术无法处理的FfA，在约简精度和计算可行性之间取得平衡

Conclusion: 软状态约简为处理非局部有限剩余格上的非确定性模糊有限自动机提供了一种实用的近似方法，扩展了模糊自动机约简的应用范围

Abstract: State reduction of finite automata plays a significant role in improving efficiency in formal verification, pattern recognition, and machine learning, where automata-based models are widely used. While deterministic automata have well-defined minimization procedures, reducing states in nondeterministic fuzzy finite automata (FfAs) remains challenging, especially for FfAs over non-locally finite residuated lattices like the product and Hamacher structures. This work introduces soft state reduction, an approximate method that leverages a small threshold $\varepsilon$ possibly combined with a word length bound $k$ to balance reduction accuracy and computational feasibility. By omitting fuzzy values smaller than $\varepsilon$, the underlying residuated lattice usually becomes locally finite, making computations more tractable. We introduce and study approximate invariances, which are fuzzy relations that allow merging of almost equivalent states of an FfA up to a tolerance level $\varepsilon$ and, optionally, to words of bounded length $k$. We further present an algorithm which iteratively applies such invariances to achieve reduction while preserving approximate language equivalence. Our method effectively reduces FfAs where existing techniques fail.

</details>


### [5] [An Analysis of Decision Problems for Relational Pattern Languages under Various Constraints](https://arxiv.org/abs/2512.07476)
*Klaus Jansen,Dirk Nowotka,Lis Pirotton,Corinna Wambsganz,Max Wiedenhöft*

Main category: cs.FL

TL;DR: 将模式语言从仅支持变量相等关系扩展到任意关系，研究成员、包含和等价问题的复杂性，发现即使使用更简单的关系，这些问题与经典情况的复杂性和不可判定性特征保持不变。


<details>
  <summary>Details</summary>
Motivation: 传统模式语言只允许变量通过相等关系关联（即相同变量多次出现），但实际应用中可能需要更复杂的关系。本文旨在将模式语言扩展到支持任意关系，并研究这种扩展对核心决策问题的影响。

Method: 提出关系模式和关系模式语言的概念，将变量间的关联从仅相等关系扩展到任意关系。在广泛的关系类型下，系统地研究模式语言的三个核心决策问题：成员问题、包含问题和等价问题。

Result: 研究表明，即使对于许多更简单或限制更少的关系，这些决策问题的复杂性和不可判定性特征与经典情况（仅使用相等关系）相比没有变化。

Conclusion: 将模式语言扩展到支持任意关系并不会改变核心决策问题的基本复杂性特征，这为关系模式语言的实际应用提供了理论基础，表明扩展关系能力不会带来额外的计算负担。

Abstract: Patterns are words with terminals and variables. The language of a pattern is the set of words obtained by uniformly substituting all variables with words that contain only terminals. In their original definition, patterns only allow for multiple distinct occurrences of some variables to be related by the equality relation, represented by using the same variable multiple times. In an extended notion, called relational patterns and relational pattern languages, variables may be related by arbitrary other relations. We extend the ongoing investigation of the main decision problems for patterns (namely, the membership problem, the inclusion problem, and the equivalence problem) to relational pattern languages under a wide range of individual relations. It is shown show that - even for many much simpler or less restrictive relations - the complexity and (un)decidability characteristics of these problems do not change compared to the classical case where variables are related only by equality.

</details>


### [6] [Specializing anti-unification for interaction models composition via gate connections](https://arxiv.org/abs/2512.07595)
*Joel Nguetoum,Boutheina Bannour,Pascale Le Gall,Erwan Mahe*

Main category: cs.FL

TL;DR: 提出一种基于反统一化的交互模型组合方法，通过特殊常量保持变体对齐局部模型的门，重构全局交互


<details>
  <summary>Details</summary>
Motivation: 分布式系统交互模型通常用代数项表示，局部模型通过门标记交互点。将局部模型组合成一致的全局模型需要对齐这些门，同时尊重交互算子的代数定律。现有方法在这方面存在不足。

Method: 专门化反统一化（泛化）方法，采用特殊常量保持变体，在泛化剩余结构时保留指定常量。开发基于规则的专用计算过程，证明其终止性、可靠性和完备性，扩展到模等式理论，并集成到标准反统一化框架中。

Result: 原型工具展示了该方法能够从部分视图中重构全局交互，验证了方法的有效性。

Conclusion: 提出的常量保持反统一化方法为分布式系统交互模型的组合提供了有效的代数基础，能够从局部视图重构一致的全局模型。

Abstract: Interaction models describe distributed systems as algebraic terms, with gates marking interaction points between local views. Composing local models into a coherent global one requires aligning these gates while respecting the algebraic laws of interaction operators. We specialize anti-unification (or generalization) via a special constant-preserving variant, which preserves designated constants while generalizing the remaining structure. We develop a dedicated rule-based procedure for computing these generalizations, prove its termination, soundness, and completeness, extend it modulo equational theories, and integrate it into a standard anti-unification framework. A prototype tool demonstrates the approach's ability to recompose global interactions from partial views.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [7] [Auto-SPT: Automating Semantic Preserving Transformations for Code](https://arxiv.org/abs/2512.06042)
*Ashish Hooda,Mihai Christodorescu,Chuangang Ren,Aaron Wilson,Kassem Fawaz,Somesh Jha*

Main category: cs.SE

TL;DR: Auto-SPT：基于LLM的自动语义保持变换生成框架，用于增强代码克隆检测模型的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现实世界代码经常经历各种语义保持变换（如重构、压缩、格式化、编译器优化），而现有代码克隆检测模型主要在干净、结构化的代码数据集上训练，导致训练与测试数据之间存在关键差距

Method: 提出Auto-SPT框架，利用大语言模型自动构建代码合成数据生成器：1）生成多样化的语义保持变换；2）为这些变换生成强实现；3）组合变换以产生强变换效果。理论分析表明变换多样性影响组合强度

Result: Auto-SPT比现有方法生成更多样化的语义保持变换，这些变换显著降低了最先进代码克隆检测器的性能。实验还表明Auto-SPT可用于增强训练数据集，产生对现实世界对抗性代码变换具有鲁棒性的代码克隆检测模型

Conclusion: Auto-SPT通过自动生成多样化的语义保持变换，有效弥合了代码克隆检测中训练与测试数据的差距，既能评估现有模型的脆弱性，又能增强训练数据以提升模型鲁棒性

Abstract: Machine learning (ML) models for code clone detection determine whether two pieces of code are semantically equivalent, which in turn is a key building block for software-engineering tasks like refactoring and security tasks like vulnerability and malware detection. While these models are predominantly trained on clean, structured code datasets, real-world code often undergoes a variety of semantic-preserving transformations, including refactoring, minification, automated formatting, and compiler optimizations. To address this critical gap between training and test data, we propose Auto-SPT, a novel framework to automatically construct synthetic-data generators for code. Auto-SPT is designed to produce Semantic Preserving Transformations (SPTs) that alter a program's syntactic structure while preserving its functionality and is instantiated on top of Large Language Models (LLMs). In particular, we use LLMs to craft a diverse set of SPTs, generate strong implementations for these SPTs, and compose them to result into strong transformations. Our formal analysis shows that the diversity of SPTs impacts the strength of their composition. We then empirically demonstrate that Auto-SPT generates more diverse SPTs than existing approaches and these SPTs significantly drop the performance of state-of-the-art code clone detectors. Further experiments show Auto-SPT can be used to enhance code datasets for training, to produce code-clone detection models that are robust to real-world, adversarial code transformations.

</details>


### [8] [Leveraging LLMs to support co-evolution between definitions and instances of textual DSLs](https://arxiv.org/abs/2512.06836)
*Weixing Zhang,Regina Hebig,Daniel Strüber*

Main category: cs.SE

TL;DR: LLM可用于小规模文本DSL实例的语法协同演化，能保留注释等辅助信息，但面临大规模实例的可扩展性挑战。


<details>
  <summary>Details</summary>
Motivation: 文本DSL语法演化时，现有MDE协同演化技术会丢失注释和布局等辅助信息，这些信息对软件理解和维护很重要。需要探索能保留这些信息的协同演化方法。

Method: 使用Claude-3.5和GPT-4o两种先进LLM，在7种案例语言上进行实验，评估LLM直接处理文本实例实现语法和实例协同演化的可行性。

Result: LLM在小规模、有限实例大小的案例中表现出良好的迁移能力，这些案例代表了实践中遇到的部分情况。但在处理更大实例时面临显著的可扩展性挑战。

Conclusion: LLM在文本DSL协同演化方面有潜力，特别是能保留辅助信息，但需要解决可扩展性问题。研究结果为未来研究提供了有用见解。

Abstract: Software languages evolve over time for various reasons, such as the addition of new features. When the language's grammar definition evolves, textual instances that originally conformed to the grammar become outdated. For DSLs in a model-driven engineering context, there exists a plethora of techniques to co-evolve models with the evolving metamodel. However, these techniques are not geared to support DSLs with a textual syntax -- applying them to textual language definitions and instances may lead to the loss of information from the original instances, such as comments and layout information, which are valuable for software comprehension and maintenance. This study explores the potential of Large Language Model (LLM)-based solutions in achieving grammar and instance co-evolution, with attention to their ability to preserve auxiliary information when directly processing textual instances. By applying two advanced language models, Claude-3.5 and GPT-4o, and conducting experiments across seven case languages, we evaluated the feasibility and limitations of this approach. Our results indicate a good ability of the considered LLMs for migrating textual instances in small-scale cases with limited instance size, which are representative of a subset of cases encountered in practice. In addition, we observe significant challenges with the scalability of LLM-based solutions to larger instances, leading to insights that are useful for informing future research.

</details>


### [9] [Beyond Prototyping: Autonomous, Enterprise-Grade Frontend Development from Pixel to Production via a Specialized Multi-Agent Framework](https://arxiv.org/abs/2512.06046)
*Ramprasath Ganesaraja,Swathika N,Saravanan AP,Kamalkumar Rathinasamy,Chetana Amancharla,Rahul Das,Sahil Dilip Panse,Aditya Batwe,Dileep Vijayan,Veena Ashok,Thanushree A P,Kausthubh J Rao,Alden Olivero,Roshan,Rajeshwar Reddy Manthena,Asmitha Yuga Sre A,Harsh Tripathi,Suganya Selvaraj,Vito Chin,Kasthuri Rangan Bhaskar,Kasthuri Rangan Bhaskar,Venkatraman R,Sajit Vijayakumar*

Main category: cs.SE

TL;DR: AI4UI是一个面向企业级应用交付的自主前端开发框架，专注于生产就绪性，通过设计阶段嵌入Gen-AI友好语法和后处理阶段专家优化，实现安全、可扩展、合规且可维护的UI代码生成。


<details>
  <summary>Details</summary>
Motivation: 现有通用代码助手主要面向快速原型开发，无法满足企业级应用对安全性、可扩展性、合规性和可维护性的严格要求。企业需要能够无缝集成到现有工作流程中的生产就绪UI代码生成解决方案。

Method: 采用目标导向的人机协同方法：1) 设计阶段在Figma原型中嵌入Gen-AI友好语法编码需求；2) 中间阶段完全自主运行，将设计转换为工程就绪的UI代码；3) 后处理阶段由领域专家进行精细化调整。技术贡献包括Figma语法、领域感知知识图谱、安全抽象/包代码集成策略、专家驱动的架构模板和面向变更的工作流程。

Result: 在大规模基准测试中，AI4UI实现了97.24%的平台兼容性、87.10%的编译成功率、86.98%的安全合规性、78.00%的功能实现成功率、73.50%的代码审查质量和73.36%的UI/UX一致性。在200名专家评估者的盲选偏好研究中，AI4UI成为领先解决方案之一。

Conclusion: AI4UI框架通过专门构建的自主前端开发代理，显著提升了企业级UI开发的效率和质量，能够在数周内生成数千个经过验证的UI屏幕，大幅压缩交付时间线，在竞争激烈的市场中展现出强大的竞争力。

Abstract: We present AI4UI, a framework of autonomous front-end development agents purpose-built to meet the rigorous requirements of enterprise-grade application delivery. Unlike general-purpose code assistants designed for rapid prototyping, AI4UI focuses on production readiness delivering secure, scalable, compliant, and maintainable UI code integrated seamlessly into enterprise workflows. AI4UI operates with targeted human-in-the-loop involvement: at the design stage, developers embed a Gen-AI-friendly grammar into Figma prototypes to encode requirements for precise interpretation; and at the post processing stage, domain experts refine outputs for nuanced design adjustments, domain-specific optimizations, and compliance needs. Between these stages, AI4UI runs fully autonomously, converting designs into engineering-ready UI code. Technical contributions include a Figma grammar for autonomous interpretation, domain-aware knowledge graphs, a secure abstract/package code integration strategy, expertise driven architecture templates, and a change-oriented workflow coordinated by specialized agent roles. In large-scale benchmarks against industry baselines and leading competitor systems, AI4UI achieved 97.24% platform compatibility, 87.10% compilation success, 86.98% security compliance, 78.00% feature implementation success, 73.50% code-review quality, and 73.36% UI/UX consistency. In blind preference studies with 200 expert evaluators, AI4UI emerged as one of the leaders demonstrating strong competitive standing among leading solutions. Operating asynchronously, AI4UI generates thousands of validated UI screens in weeks rather than months, compressing delivery timeline

</details>


### [10] [Reinforcement Learning Integrated Agentic RAG for Software Test Cases Authoring](https://arxiv.org/abs/2512.06060)
*Mohanakrishnan Hariharan*

Main category: cs.SE

TL;DR: 提出一个结合强化学习与自主智能体的框架，通过质量工程反馈持续改进从业务需求文档自动生成软件测试用例的过程，相比传统静态LLM系统实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统基于大语言模型的测试用例生成系统依赖静态知识库，无法随时间持续改进性能。需要一种能够从质量工程反馈中学习并不断优化的自动化测试生成方法。

Method: 提出强化学习增强的智能体RAG框架，结合专门智能体与混合向量-图知识库存储测试知识，使用PPO和DQN等强化学习算法，根据测试有效性、缺陷检测率和工作流指标优化智能体行为。

Result: 在企业级Apple项目上验证，测试生成准确率从94.8%提升至97.2%（提高2.4%），缺陷检测率提升10.8%。

Conclusion: 该框架建立了由质量工程专业知识驱动的持续知识精炼循环，逐步提升测试用例质量，增强而非替代人工测试能力。

Abstract: This paper introduces a framework that integrates reinforcement learning (RL) with autonomous agents to enable continuous improvement in the automated process of software test cases authoring from business requirement documents within Quality Engineering (QE) workflows. Conventional systems employing Large Language Models (LLMs) generate test cases from static knowledge bases, which fundamentally limits their capacity to enhance performance over time. Our proposed Reinforcement Infused Agentic RAG (Retrieve, Augment, Generate) framework overcomes this limitation by employing AI agents that learn from QE feedback, assessments, and defect discovery outcomes to automatically improve their test case generation strategies. The system combines specialized agents with a hybrid vector-graph knowledge base that stores and retrieves software testing knowledge. Through advanced RL algorithms, specifically Proximal Policy Optimization (PPO) and Deep Q-Networks (DQN), these agents optimize their behavior based on QE-reported test effectiveness, defect detection rates, and workflow metrics. As QEs execute AI-generated test cases and provide feedback, the system learns from this expert guidance to improve future iterations. Experimental validation on enterprise Apple projects yielded substantive improvements: a 2.4% increase in test generation accuracy (from 94.8% to 97.2%), and a 10.8% improvement in defect detection rates. The framework establishes a continuous knowledge refinement loop driven by QE expertise, resulting in progressively superior test case quality that enhances, rather than replaces, human testing capabilities.

</details>


### [11] [Toward Patch Robustness Certification and Detection for Deep Learning Systems Beyond Consistent Samples](https://arxiv.org/abs/2512.06123)
*Qilin Zhou,Zhengyuan Wei,Haipeng Wang,Zhuo Wang,W. K. Chan*

Main category: cs.SE

TL;DR: HiCert是一种新型的基于掩码的认证检测技术，能够为对抗性补丁攻击提供更全面的鲁棒性认证，特别是针对现有方法无法有效认证的误分类或不一致预测样本。


<details>
  <summary>Details</summary>
Motivation: 现有的认证检测方法在认证误分类样本或其突变体被不一致预测为不同标签的样本时效果不佳，这限制了对抗性补丁攻击防御的全面性。

Method: 通过形式化分析，HiCert建立了有害样本与其良性对应样本之间的形式关系，通过检查每个良性样本的潜在有害突变体的最大置信度边界，确保每个有害样本要么具有低于该边界的相同预测标签突变体的最小置信度，要么至少有一个突变体被预测为与有害样本本身不同的标签。

Result: HiCert在实验中表现出高效性，达到了新的最先进性能：认证了显著更多的良性样本（包括不一致和一致样本），在无警告样本上实现了显著更高的准确率，以及显著更低的虚假静默比率。

Conclusion: HiCert是首个能够为认证检测提供如此全面补丁鲁棒性认证的工作，通过系统性地认证不一致样本和大部分一致样本，显著提升了对抗性补丁攻击的防御能力。

Abstract: Patch robustness certification is an emerging kind of provable defense technique against adversarial patch attacks for deep learning systems. Certified detection ensures the detection of all patched harmful versions of certified samples, which mitigates the failures of empirical defense techniques that could (easily) be compromised. However, existing certified detection methods are ineffective in certifying samples that are misclassified or whose mutants are inconsistently pre icted to different labels. This paper proposes HiCert, a novel masking-based certified detection technique. By focusing on the problem of mutants predicted with a label different from the true label with our formal analysis, HiCert formulates a novel formal relation between harmful samples generated by identified loopholes and their benign counterparts. By checking the bound of the maximum confidence among these potentially harmful (i.e., inconsistent) mutants of each benign sample, HiCert ensures that each harmful sample either has the minimum confidence among mutants that are predicted the same as the harmful sample itself below this bound, or has at least one mutant predicted with a label different from the harmful sample itself, formulated after two novel insights. As such, HiCert systematically certifies those inconsistent samples and consistent samples to a large extent. To our knowledge, HiCert is the first work capable of providing such a comprehensive patch robustness certification for certified detection. Our experiments show the high effectiveness of HiCert with a new state-of the-art performance: It certifies significantly more benign samples, including those inconsistent and consistent, and achieves significantly higher accuracy on those samples without warnings and a significantly lower false silent ratio.

</details>


### [12] [Systematic Evaluation of Black-Box Checking for Fast Bug Detection](https://arxiv.org/abs/2512.07434)
*Bram Pellen,María Belén Rodríguez,Frits Vaandrager,Petra van den Bos*

Main category: cs.SE

TL;DR: 黑盒检查（BBC）在主动自动机学习过程中对中间假设模型进行模型检查，相比仅在最终模型检查的方法，能以更少查询（3.4%）快速发现规范违规，在RERS 2019工业基准中检测到94%的安全属性违规。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常在主动自动机学习的最后阶段才使用模型检查，而黑盒检查（BBC）的原始提议是在所有中间假设模型中都应用模型检查。本文旨在系统评估BBC在快速发现bug方面的能力。

Method: 基于77个来自真实协议实现和控制器的基准模型，这些模型都有可用的安全属性规范。系统评估BBC在以下方面的表现：1）完整模型可学习时发现规范违规的效率；2）完整模型不可学习时检测规范违规的能力；3）与现有基于模型的测试（MBT）算法在发现深度bug方面的比较。

Result: 1）完整模型可学习时，BBC仅需传统方法3.4%的查询就能检测到规范违规；2）即使完整模型无法学习，BBC仍能检测到许多规范违规，在RERS 2019工业LTL基准中检测到94%的安全属性违规；3）BBC在发现实现中的深度bug方面比现有MBT算法更有效。

Conclusion: 黑盒检查（BBC）在主动自动机学习过程中对中间假设模型进行模型检查，相比传统方法能显著提高bug发现效率，即使在完整模型无法学习的情况下也能有效检测规范违规，证明其在工业应用中的实用价值。

Abstract: Combinations of active automata learning, model-based testing and model checking have been successfully used in numerous applications, e.g., for spotting bugs in implementations of major network protocols and to support refactoring of embedded controllers. However, in the large majority of these applications, model checking is only used at the very end, when no counterexample can be found anymore for the latest hypothesis model. This contrasts with the original proposal of black-box checking (BBC) by Peled, Vardi & Yannakakis, which applies model checking for all hypotheses, also the intermediate ones. In this article, we present the first systematic evaluation of the ability of BBC to find bugs quickly, based on 77 benchmarks models from real protocol implementations and controllers for which specifications of safety properties are available. Our main finding are: (a) In cases where the full model can be learned, BBC detects violations of the specifications with just 3.4% of the queries needed by an approach in which model checking is only used for the full model. (b) Even when the full model cannot be learned, BBC is still able to detect many violations of the specification. In particular, BBC manages to detect 94% of the safety properties violations in the challenging RERS 2019 industrial LTL benchmarks. (c) Our results also confirm that BBC is way more effective than existing MBT algorithms in finding deep bugs in implementations.

</details>


### [13] [Systematically Thinking about the Complexity of Code Structuring Exercises at Introductory Level](https://arxiv.org/abs/2512.06178)
*Georgiana Haldeman,Peter Ohmann,Paul Denny*

Main category: cs.SE

TL;DR: 提出一个评估代码重构任务复杂度的框架，包含三个维度：重复、代码模式和数据依赖，用于系统化地教授分解与抽象技能。


<details>
  <summary>Details</summary>
Motivation: 分解与抽象是计算思维的核心，但在入门编程课程中常被忽视。随着生成式AI降低语法重要性，提升高层次代码推理能力，现在有机会重新强调分解与抽象的教学。

Method: 引入一个系统性评估代码重构任务复杂度的框架，包含三个维度：重复（重复程度）、代码模式（模式识别难度）、数据依赖（依赖关系复杂度），每个维度有多个层级。提供示例任务和交互工具来生成和探索分解与抽象问题。

Result: 开发了一个实用的框架和工具，支持教育工作者设计不同复杂度的分解与抽象任务，帮助学生在过程式编程范式中培养相关技能。

Conclusion: 该框架为系统化教授分解与抽象提供了理论基础和实践工具，有助于在AI时代重新重视高层次代码推理能力的培养。

Abstract: Decomposition and abstraction is an essential component of computational thinking, yet it is not always emphasized in introductory programming courses. In addition, as generative AI further reduces the focus on syntax and increases the importance of higher-level code reasoning, there is renewed opportunity to teach DA explicitly. In this paper, we introduce a framework for systematically assessing the complexity of code structuring tasks, where students must identify and separate meaningful abstractions within existing, unstructured code. The framework defines three dimensions of task complexity, each with multiple levels: repetition, code pattern, and data dependency. To support practical use, we provide example tasks mapped to these levels and offer an interactive tool for generating and exploring DA problems. The framework is designed to support the development of educational tasks that build students' skills with DA in the procedural paradigm.

</details>


### [14] [DUET: Agentic Design Understanding via Experimentation and Testing](https://arxiv.org/abs/2512.06247)
*Gus Henry Smith,Sandesh Adhikary,Vineet Thumuluri,Karthik Suresh,Vivek Pandit,Kartik Hegde,Hamid Shojaei,Chandra Bhagavatula*

Main category: cs.SE

TL;DR: DUET提出了一种通过实验测试来理解硬件设计的方法，模仿硬件专家通过EDA工具迭代测试来理解复杂RTL代码的行为，提升了AI代理在形式验证等任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在解决复杂软件工程任务方面表现出色，但在硬件设计任务上表现不佳。RTL代码使用SystemVerilog等低级语言特征编码复杂、动态、时间演化的行为，LLMs仅从RTL语法难以推断这些复杂行为，限制了其在代码补全、文档生成、验证等下游任务的能力。

Method: DUET（Design Understanding via Experimentation and Testing）方法模仿硬件设计专家理解复杂设计的方式：不是一次性阅读RTL代码，而是通过EDA工具（如仿真、波形检查、形式验证）进行迭代实验。该方法迭代生成假设，使用EDA工具测试这些假设，并整合结果以构建自底向上的设计理解。

Result: 评估显示，与没有实验的基线流程相比，DUET显著提高了AI代理在形式验证任务上的性能表现。

Conclusion: DUET通过模仿硬件专家的实验性学习方法，为LLMs理解复杂RTL设计提供了一种有效方法，解决了LLMs仅从语法难以推断硬件行为的问题，提升了AI代理在硬件设计任务上的能力。

Abstract: AI agents powered by large language models (LLMs) are being used to solve increasingly complex software engineering challenges, but struggle with hardware design tasks. Register Transfer Level (RTL) code presents a unique challenge for LLMs, as it encodes complex, dynamic, time-evolving behaviors using the low-level language features of SystemVerilog. LLMs struggle to infer these complex behaviors from the syntax of RTL alone, which limits their ability to complete all downstream tasks like code completion, documentation, or verification. In response to this issue, we present DUET: a general methodology for developing Design Understanding via Experimentation and Testing. DUET mimics how hardware design experts develop an understanding of complex designs: not just via a one-off readthrough of the RTL, but via iterative experimentation using a number of tools. DUET iteratively generates hypotheses, tests them with EDA tools (e.g., simulation, waveform inspection, and formal verification), and integrates the results to build a bottom-up understanding of the design. In our evaluations, we show that DUET improves AI agent performance on formal verification, when compared to a baseline flow without experimentation.

</details>


### [15] [CFCEval: Evaluating Security Aspects in Code Generated by Large Language Models](https://arxiv.org/abs/2512.06248)
*Cheng Cheng,Jinqiu Yang*

Main category: cs.SE

TL;DR: CFCEval是一个评估代码生成LLM质量和安全性的新框架，通过MLVBench基准和ELRM指标解决现有评估方法的数据集偏差和CodeBLEU局限性问题。


<details>
  <summary>Details</summary>
Motivation: 现有代码LLM评估协议缺乏方法严谨性和全面性，存在数据集偏差问题，且广泛使用的CodeBLEU指标存在分词不精确、结构限制和参考多样性低等关键缺陷。

Method: 提出CFCEval框架，通过创建新的基准MLVBench来缓解数据集偏差，并引入ELRM指标评估参考代码与生成代码之间的相关性。从编程质量、漏洞修复能力、后转换修复能力和相关性四个维度评估生成代码。

Result: 实验表明CFCEval能更有效地捕捉生成代码的质量和安全方面，其ELRM指标比CodeBLEU更符合人类判断，为代码LLM评估的未来发展铺平道路。

Conclusion: CFCEval为解决代码LLM评估中的关键挑战提供了一个全面框架，通过新的基准和指标改进了现有方法的局限性，推动了代码生成模型评估的进步。

Abstract: Code-focused Large Language Models (LLMs), such as CodeX and Star-Coder, have demonstrated remarkable capabilities in enhancing developer productivity through context-aware code generation. However, evaluating the quality and security of LLM-generated code remains a significant challenge. Existing evaluation protocols for Code LLMs lack both methodological rigor and comprehensive scope. A key limitation is dataset bias, which arises from unintentional overlap between training and testing data. Furthermore, while CodeBLEU, a BLEU-based metric, is widely used to assess code similarity, it suffers from critical shortcomings, including imprecise tokenization, structural limitations, and low reference diversity. To address these challenges, we introduce CFCEval, a novel framework for evaluating the quality and security of code generated by LLMs. CFCEval mitigates dataset bias by creating a new benchmark, MLVBench, and incorporates ELRM, a new metric designed to assess the relevance between reference code and generated code. CFCEval evaluates generated code across four dimensions: programming quality, vulnerability-fixing capability, post-transformation fixing capability, and relevance. Our experiments show that CFCEval not only captures both quality and security aspects of generated code more effectively but also that its ELRM aligns more closely with human judgments than CodeBLEU, thus paving the way for future advancements in Code LLMs evaluation.

</details>


### [16] [LLMCFG-TGen: Using LLM-Generated Control Flow Graphs to Automatically Create Test Cases from Use Cases](https://arxiv.org/abs/2512.06401)
*Zhenzhen Yang,Chenhui Cui,Tao Li,Rubing Huang,Nan Niu,Dave Towey,Shikai Guo*

Main category: cs.SE

TL;DR: 提出LLMCFG-TGen方法，使用LLM从自然语言用例生成控制流图，再枚举所有执行路径来生成测试用例，提高测试覆盖率和完整性。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的测试生成方法存在覆盖不全面、冗余、难以捕捉复杂条件逻辑的问题，需要一种更系统化的方法从自然语言需求生成完整测试用例。

Method: LLMCFG-TGen包含三步：1) LLM将自然语言用例转换为结构化控制流图；2) 探索CFG并枚举所有完整执行路径；3) 基于执行路径生成测试用例。

Result: 实验表明LLM能有效从自然语言用例构建结构良好的CFG，相比基线方法，LLMCFG-TGen实现全路径覆盖，提高完整性，生成清晰准确的测试用例。

Conclusion: 将LLM的语义推理与结构化建模相结合，能有效弥合自然语言需求与系统化测试生成之间的差距，大幅减少人工工作量。

Abstract: Appropriate test case generation is critical in software testing, significantly impacting the quality of the testing. Requirements-Based Test Generation (RBTG) derives test cases from software requirements, aiming to verify whether or not the system's behaviors align with user needs and expectations. Requirements are often documented in Natural Language (NL), with use-case descriptions being a popular method for capturing functional behaviors and interaction flows in a structured form. Large Language Models (LLMs) have shown strong potential for automating test generation directly from NL requirements. However, current LLM-based approaches may not provide comprehensive, non-redundant coverage. They may also fail to capture complex conditional logic in requirements, resulting in incomplete test cases. We propose a new approach that automatically generates test cases from NL use-case descriptions, called Test Generation based on LLM-generated Control Flow Graphs (LLMCFG-TGen). LLMCFG-TGen comprises three main steps: (1) An LLM transforms a use case into a structured CFG that encapsulates all potential branches; (2) The generated CFG is explored, and all complete execution paths are enumerated; and (3) The execution paths are then used to generate the test cases. To evaluate our proposed approach, we conducted a series of experiments. The results show that LLMs can effectively construct well-structured CFGs from NL use cases. Compared with the baseline methods, LLMCFG-TGen achieves full path coverage, improving completeness and ensuring clear and accurate test cases. Practitioner assessments confirm that LLMCFG-TGen produces logically consistent and comprehensive test cases, while substantially reducing manual effort. The findings suggest that coupling LLM-based semantic reasoning with structured modeling effectively bridges the gap between NL requirements and systematic test generation.

</details>


### [17] [Translating PL/I Macro Procedures into Java Using Automatic Templatization and Large Language Models](https://arxiv.org/abs/2512.06448)
*Takaaki Tateishi,Yasuharu Katsuno*

Main category: cs.SE

TL;DR: 提出模板化方法，使用符号执行生成代码模板作为中间表示，帮助LLM将PL/I宏过程翻译成可维护的Java代码


<details>
  <summary>Details</summary>
Motivation: 企业系统现代化需要将PL/I程序翻译成现代语言如Java，但PL/I宏过程作为字符串操作程序会生成PL/I代码，使得自动翻译更加复杂。现有的LLM方法难以将PL/I宏过程翻译成能重现原始PL/I代码行为的Java程序。

Method: 提出模板化方法：使用符号执行生成代码模板（带有命名占位符的代码）作为中间表示。将符号值视为宏生成代码的一部分，通过符号执行宏过程并生成代码模板，帮助LLM生成可读且可维护的Java代码。

Result: 在10个PL/I宏过程上的初步实验表明，通过模板化的LLM翻译成功生成了能够重现宏生成PL/I程序行为的Java程序。

Conclusion: 模板化方法通过符号执行生成代码模板作为中间表示，有效解决了LLM在翻译PL/I宏过程时的困难，能够生成行为正确的可维护Java代码。

Abstract: Modernizing legacy enterprise systems often involves translating PL/I programs into modern languages such as Java. This task becomes significantly more complex when PL/I macro procedures are involved. The PL/I macro procedures are considered string-manipulating programs that generate PL/I code, and they make automated translation more complex. Recently, large language models (LLMs) have been explored for automated code translation. However, LLM-based code translation struggles to translate the PL/I macro procedures to Java programs that reproduce the behavior of the plain PL/I code generated by the original PL/I macro procedures.
  This paper proposes a novel method called templatization, which uses symbolic execution to generate code templates (code with named placeholders) as an intermediate representation. In this approach, symbolic values are treated as parts of macro-generated code. By symbolically executing macro procedures and generating code templates, our approach facilitates LLMs to generate readable and maintainable Java code. Our preliminary experiment on ten PL/I macro procedures shows that the LLM-based translation through templatization successfully generates Java programs that reproduce the behavior of the macro-generated PL/I programs.

</details>


### [18] [METRION: A Framework for Accurate Software Energy Measurement](https://arxiv.org/abs/2512.06806)
*Benjamin Weigell,Simon Hornung,Bernhard Bauer*

Main category: cs.SE

TL;DR: METRION框架提出了一种线程级能耗归因模型，用于精确量化应用程序在CPU和DRAM上的能耗，考虑SMT、频率缩放、多插槽架构和NUMA等因素，实现跨平台应用。


<details>
  <summary>Details</summary>
Motivation: ICT行业占全球温室气体排放约1.4%和电力消耗4%，且持续增长。为减少环境影响，需要在IT基础设施和应用层面优化能耗。但有效优化需要首先识别主要能耗源，其次能够量化优化是否达到预期节能效果，因此需要精确确定应用级能耗。

Method: 提出一个能耗归因模型，在CPU和DRAM层面以线程级精度量化应用能耗，考虑同时多线程(SMT)、频率缩放、多插槽架构和非统一内存访问(NUMA)的影响。将该模型集成到可扩展框架METRION中，包括平台无关数据模型和针对Linux系统Intel CPU的初始实现。

Result: 在三种不同工作负载上评估METRION，能耗归因模型能够准确捕获仅针对CPU的应用的CPU能耗，平均绝对百分比误差为4.2%；对于针对DRAM的应用，DRAM能耗误差为16.1%。

Conclusion: METRION框架提供了一种精确量化应用级能耗的方法，有助于识别主要能耗源并验证优化效果，为减少ICT行业环境影响提供有效工具。

Abstract: The Information and Communication Technology sector accounted for approximately 1.4% of global greenhouse gas emissions and 4% of the world's electricity consumption in 2020, with both expected to rise. To reduce this environmental impact, optimization strategies are employed to reduce energy consumption at the IT infrastructure and application levels. However, effective optimization requires, firstly, the identification of major energy consumers and, secondly, the ability to quantify whether an optimization has achieved the intended energy savings. Accurate determination of application-level energy consumption is thus essential. Therefore, we introduce an energy attribution model that quantifies the energy consumption of applications on CPU and DRAM at the thread level, considering the influence of Simultaneous Multithreading, frequency scaling, multi-socket architectures, and Non-Uniform Memory Access. To ensure cross-platform applicability, we integrate the proposed model into an extensible framework, METRION, including a platform-independent data model and an initial implementation for Linux systems using Intel CPUs. We evaluate METRION across three different workloads and demonstrate that the energy attribution model can accurately capture the CPU energy consumption of applications targeting solely the CPU with a Mean Absolute Percentage Error of 4.2%, and the DRAM energy consumption of applications targeting DRAM with an 16.1% error.

</details>


### [19] [BabelCoder: Agentic Code Translation with Specification Alignment](https://arxiv.org/abs/2512.06902)
*Fazle Rabbi,Soumit Kanti Saha,Tri Minh Triet Pham,Song Wang,Jinqiu Yang*

Main category: cs.SE

TL;DR: BabelCoder是一个基于多智能体协作的代码翻译框架，通过翻译、测试、修复三个专门化智能体的分工合作，显著提升了跨语言代码翻译的准确率。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统演进，开发者需要在多种编程语言间工作，经常需要将代码从一种语言迁移到另一种语言。虽然自动代码翻译提供了有前景的解决方案，但长期以来一直是一项具有挑战性的任务。现有的大语言模型方法在准确性和利用代码上下文结构信息方面仍有局限，缺乏结构化的多智能体协作框架。

Method: BabelCoder是一个智能体框架，将代码翻译任务分解为三个专门化智能体：翻译智能体负责生成代码，测试智能体负责验证正确性，修复智能体负责修复错误。这些智能体协同工作，通过分工合作提高翻译质量。

Result: 在四个基准数据集上与四个最先进的基线方法比较，BabelCoder在94%的情况下优于现有方法，提升幅度为0.5%-13.5%，平均准确率达到94.16%。

Conclusion: BabelCoder通过多智能体协作框架有效解决了代码翻译中的准确性问题，为跨语言代码迁移提供了更可靠的自动化解决方案。

Abstract: As software systems evolve, developers increasingly work across multiple programming languages and often face the need to migrate code from one language to another. While automatic code translation offers a promising solution, it has long remained a challenging task. Recent advancements in Large Language Models (LLMs) have shown potential for this task, yet existing approaches remain limited in accuracy and fail to effectively leverage contextual and structural cues within the code. Prior work has explored translation and repair mechanisms, but lacks a structured, agentic framework where multiple specialized agents collaboratively improve translation quality. In this work, we introduce BabelCoder, an agentic framework that performs code translation by decomposing the task into specialized agents for translation, testing, and refinement, each responsible for a specific aspect such as generating code, validating correctness, or repairing errors. We evaluate BabelCoder on four benchmark datasets and compare it against four state-of-the-art baselines. BabelCoder outperforms existing methods by 0.5%-13.5% in 94% of cases, achieving an average accuracy of 94.16%.

</details>


### [20] [MINES: Explainable Anomaly Detection through Web API Invariant Inference](https://arxiv.org/abs/2512.06906)
*Wenjie Zhang,Yun Lin,Chun Fung Amos Kwok,Xiwen Teoh,Xiaofei Xie,Frank Liauw,Hongyu Zhang,Jin Song Dong*

Main category: cs.SE

TL;DR: MINES：通过从模式层面推断可解释的API不变量来进行异常检测，避免日志噪声干扰，实现高召回率和接近零误报


<details>
  <summary>Details</summary>
Motivation: 现代Web应用依赖API，异常检测面临挑战：异常日志与正常日志相似，缺乏关键信息（可能在数据库中），且日志存在噪声，导致现有解决方案学习虚假相关性，产生肤浅的检测模型

Method: 1) 将API签名转换为表模式以增强原始数据库模式；2) 在增强的数据库模式上推断潜在数据库约束，捕获API与数据库表之间的关系；3) 使用LLM基于两个表结构提取潜在关系；4) 使用正常日志实例拒绝或接受LLM生成的不变量；5) 将推断的约束转换为不变量并生成Python代码验证运行时日志

Result: 在TrainTicket、NiceFish、Gitea、Mastodon和NextCloud等基准上对Web篡改攻击进行广泛评估，结果显示MINES在异常检测上实现高召回率，同时引入几乎零误报，达到新的最先进水平

Conclusion: MINES通过从模式层面而非原始日志实例推断可解释的API不变量，能有效区分日志噪声，识别精确的正常行为，并检测超出日志记录范围的异常行为，为Web应用异常检测提供了新的有效方法

Abstract: Detecting the anomalies of web applications, important infrastructures for running modern companies and governments, is crucial for providing reliable web services. Many modern web applications operate on web APIs (e.g., RESTful, SOAP, and WebSockets), their exposure invites intended attacks or unintended illegal visits, causing abnormal system behaviors. However, such anomalies can share very similar logs with normal logs, missing crucial information (which could be in database) for log discrimination. Further, log instances can be also noisy, which can further mislead the state-of-the-art log learning solutions to learn spurious correlation, resulting superficial models and rules for anomaly detection. In this work, we propose MINES which infers explainable API invariants for anomaly detection from the schema level instead of detailed raw log instances, which can (1) significantly discriminate noise in logs to identify precise normalities and (2) detect abnormal behaviors beyond the instrumented logs. Technically, MINES (1) converts API signatures into table schema to enhance the original database shema; and (2) infers the potential database constraints on the enhanced database schema to capture the potential relationships between APIs and database tables. MINES uses LLM for extracting potential relationship based on two given table structures; and use normal log instances to reject and accept LLM-generated invariants. Finally, MINES translates the inferred constraints into invariants to generate Python code for verifying the runtime logs. We extensively evaluate MINES on web-tamper attacks on the benchmarks of TrainTicket, NiceFish, Gitea, Mastodon, and NextCloud against baselines such as LogRobust, LogFormer, and WebNorm. The results show that MINES achieves high recall for the anomalies while introducing almost zero false positives, indicating a new state-of-the-art.

</details>


### [21] [Multi-Docker-Eval: A `Shovel of the Gold Rush' Benchmark on Automatic Environment Building for Software Engineering](https://arxiv.org/abs/2512.06915)
*Kelin Fu,Tianyu Liu,Zeyu Shang,Yingwei Ma,Jian Yang,Jiaheng Liu,Kaigui Bian*

Main category: cs.SE

TL;DR: Multi-Docker-Eval基准测试评估自动化环境配置，发现当前LLM成功率低（最高37.7%），环境构建是主要瓶颈，开源模型表现有竞争力。


<details>
  <summary>Details</summary>
Motivation: 自动化环境配置是扩展软件工程自动化的关键瓶颈，需要可靠的评估标准来衡量系统在真实约束下的执行能力和效率。

Method: 提出Multi-Docker-Eval基准测试，包含40个真实仓库，涵盖9种编程语言，评估系统在实现可执行状态和效率方面的表现。

Result: 评估发现：1) 当前模型总体成功率低（最高37.7%），环境构建是主要瓶颈；2) 模型大小和推理长度不是决定性因素，开源模型DeepSeek-V3.1和Kimi-K2在效率和效果上具有竞争力；3) 代理框架和编程语言对成功率有显著影响。

Conclusion: 研究结果为构建可扩展的完全自动化软件工程流水线提供了实用指南，强调了环境配置自动化的重要性。

Abstract: Automated environment configuration is a critical bottleneck in scaling software engineering (SWE) automation. To provide a reliable evaluation standard for this task, we present Multi-Docker-Eval benchmark. It includes 40 real-world repositories spanning 9 programming languages and measures both success in achieving executable states and efficiency under realistic constraints. Our extensive evaluation of state-of-the-art LLMs and agent frameworks reveals key insights: (1) the overall success rate of current models is low (F2P at most 37.7%), with environment construction being the primary bottleneck; (2) model size and reasoning length are not decisive factors, and open-source models like DeepSeek-V3.1 and Kimi-K2 are competitive in both efficiency and effectiveness; (3) agent framework and programming language also have significantly influence on success rate. These findings provide actionable guidelines for building scalable, fully automated SWE pipelines.

</details>


### [22] [Reformulate, Retrieve, Localize: Agents for Repository-Level Bug Localization](https://arxiv.org/abs/2512.07022)
*Genevieve Caumartin,Glaucia Melo*

Main category: cs.SE

TL;DR: 使用LLM代理通过查询重构和摘要改进文件级bug定位，相比传统方法提升35%的首文件检索排名


<details>
  <summary>Details</summary>
Motivation: 传统基于信息检索的bug定位方法依赖未处理的bug描述，包含噪声信息导致检索准确率低。LLM在查询重构方面有进展，但对代理性能的影响尚未探索。

Method: 使用开源、未微调的LLM从bug报告中提取关键信息（如标识符和代码片段），进行检索前查询重构。代理使用BM25检索处理后的查询，自动化大规模定位工作流。

Result: 使用最佳查询重构技术，代理在首文件检索排名上比BM25基线提升35%，文件检索性能比SWE-agent提升高达22%。

Conclusion: LLM驱动的代理通过轻量级查询重构和摘要能显著改进文件级bug定位性能，展示了LLM在自动化软件工程任务中的潜力。

Abstract: Bug localization remains a critical yet time-consuming challenge in large-scale software repositories. Traditional information retrieval-based bug localization (IRBL) methods rely on unchanged bug descriptions, which often contain noisy information, leading to poor retrieval accuracy. Recent advances in large language models (LLMs) have improved bug localization through query reformulation, yet the effect on agent performance remains unexplored. In this study, we investigate how an LLM-powered agent can improve file-level bug localization via lightweight query reformulation and summarization. We first employ an open-source, non-fine-tuned LLM to extract key information from bug reports, such as identifiers and code snippets, and reformulate queries pre-retrieval. Our agent then orchestrates BM25 retrieval using these preprocessed queries, automating localization workflow at scale. Using the best-performing query reformulation technique, our agent achieves 35% better ranking in first-file retrieval than our BM25 baseline and up to +22% file retrieval performance over SWE-agent.

</details>


### [23] [RisConFix: LLM-based Automated Repair of Risk-Prone Drone Configurations](https://arxiv.org/abs/2512.07122)
*Liping Han,Tingting Nie,Le Yu,Mingzhe Hu,Tao Yue*

Main category: cs.SE

TL;DR: 提出基于大语言模型的无人机风险配置实时修复方法RisConFix，通过持续监控飞行状态，在检测到异常时利用LLM分析配置参数与飞行状态关系，生成修正参数更新，迭代修复直至恢复稳定。


<details>
  <summary>Details</summary>
Motivation: 无人机飞行控制软件通常包含大量可配置参数以适应任务多样性和环境不确定性，但即使使用推荐值，某些参数组合仍可能导致不稳定飞行行为，降低无人机鲁棒性。现有方法无法实时修复这些风险配置。

Method: 提出RisConFix方法：1) 持续监控无人机运行状态；2) 检测到异常飞行行为时自动触发修复机制；3) 利用LLM分析配置参数与飞行状态关系；4) 生成修正参数更新；5) 作为迭代过程，持续监控飞行状态，若异常持续则触发下一修复周期。

Result: 在ArduPilot案例研究中（包含1,421组错误配置），RisConFix实现了最佳修复成功率97%，最优平均修复次数1.17次，证明其能够有效且高效地实时修复风险配置。

Conclusion: RisConFix通过LLM驱动的实时修复机制，能够有效解决无人机配置参数组合导致的鲁棒性问题，为无人机安全稳定运行提供了创新解决方案。

Abstract: Flight control software is typically designed with numerous configurable parameters governing multiple functionalities, enabling flexible adaptation to mission diversity and environmental uncertainty. Although developers and manufacturers usually provide recommendations for these parameters to ensure safe and stable operations, certain combinations of parameters with recommended values may still lead to unstable flight behaviors, thereby degrading the drone's robustness. To this end, we propose a Large Language Model (LLM) based approach for real-time repair of risk-prone configurations (named RisConFix) that degrade drone robustness. RisConFix continuously monitors the drone's operational state and automatically triggers a repair mechanism once abnormal flight behaviors are detected. The repair mechanism leverages an LLM to analyze relationships between configuration parameters and flight states, and then generates corrective parameter updates to restore flight stability. To ensure the validity of the updated configuration, RisConFix operates as an iterative process; it continuously monitors the drone's flight state and, if an anomaly persists after applying an update, automatically triggers the next repair cycle. We evaluated RisConFix through a case study of ArduPilot (with 1,421 groups of misconfigurations). Experimental results show that RisConFix achieved a best repair success rate of 97% and an optimal average number of repairs of 1.17, demonstrating its capability to effectively and efficiently repair risk-prone configurations in real time.

</details>


### [24] [Towards Benchmarking Design Pattern Detection Under Obfuscation: Reproducing and Evaluating Attention-Based Detection Method](https://arxiv.org/abs/2512.07193)
*Manthan Shenoy,Andreas Rausch*

Main category: cs.SE

TL;DR: 该论文研究了基于注意力的设计模式检测分类器在语义鲁棒性方面的不足，发现它们过度依赖表层语法特征而非深层语义，在代码混淆后性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估基于注意力的设计模式检测方法（如DPDAtt）是否真正理解代码的语义，还是仅仅依赖表层语法特征（如标识符名称、字符串字面量）。

Method: 方法包括：1）复现DPDAtt（基于注意力的设计模式检测方法）；2）创建混淆版DPDAtt语料库，保留控制流、继承和逻辑但替换标识符名称和字符串字面量；3）评估混淆前后分类器的性能变化。

Result: 结果显示，DPDAtt分类器严重依赖表层语法特征，当这些特征通过混淆被移除时，分类性能显著下降，出现大量误分类。

Conclusion: 结论是当前基于注意力的设计模式检测工具缺乏语义鲁棒性，需要开发能够捕捉更深层语义含义的检测工具。作者提出的混淆语料库（包含34个Java源文件）可作为评估设计模式检测器真实语义泛化能力的基准。

Abstract: This paper investigates the semantic robustness of attention-based classifiers for design pattern detection, particularly focusing on their reliance on structural and behavioral semantics. We reproduce the DPDAtt, an attention-based design pattern detection approach using learning-based classifiers, and evaluate its performance under obfuscation. To this end, we curate an obfuscated version of the DPDAtt Corpus, where the name identifiers in code such as class names, method names, etc., and string literals like print statements and comment blocks are replaced while preserving control flow, inheritance, and logic. Our findings reveal that these trained classifiers in DPDAtt depend significantly on superficial syntactic features, leading to substantial misclassification when such cues are removed through obfuscation. This work highlights the need for more robust detection tools capable of capturing deeper semantic meanings in source code. We propose our curated Obfuscated corpus (containing 34 Java source files) as a reusable proof-of-concept benchmark for evaluating state-of-the-art design pattern detectors on their true semantic generalization capabilities.

</details>


### [25] [Automatic Syntax Error Repair for Discrete Controller Synthesis using Large Language Model](https://arxiv.org/abs/2512.07261)
*Yusei Ishimizu,Takuto Yamauchi,Sinan Chen,Jinyu Cai,Jialong Li,Kenji Tei*

Main category: cs.SE

TL;DR: 利用LLM修复离散控制器合成模型语法错误的自动化方法，通过知识引导提示策略实现高效修复


<details>
  <summary>Details</summary>
Motivation: 离散控制器合成(DCS)作为强大的形式化方法，其实际应用常受限于FSP/FLTL等专业建模语言的语法错误，这些错误成为开发瓶颈，干扰工作流程并分散高层语义设计注意力

Method: 提出基于大语言模型的自动化修复方法，通过系统实证研究识别常见错误模式，设计知识引导的提示策略，包含形式语法规则和示例，指导LLM进行准确修正

Result: 在系统注入真实语法错误的DCS模型基准上评估，方法在修复准确性和实用性方面表现高效，相比人工开发实现3.46倍的加速

Conclusion: 该方法能有效解决DCS模型语法错误修复问题，提高开发效率，实验复制套件已开源

Abstract: Discrete Controller Synthesis (DCS) is a powerful formal method for automatically generating specifications of discrete event systems. However, its practical adoption is often hindered by the highly specialized nature of formal models written in languages such as FSP and FLTL. In practice, syntax errors in modeling frequently become an important bottleneck for developers-not only disrupting the workflow and reducing productivity, but also diverting attention from higher-level semantic design. To this end, this paper presents an automated approach that leverages Large Language Models (LLMs) to repair syntax errors in DCS models using a well-designed, knowledge-informed prompting strategy. Specifically, the prompting is derived from a systematic empirical study of common error patterns, identified through expert interviews and student workshops. It equips the LLM with DCS-specific domain knowledge, including formal grammar rules and illustrative examples, to guide accurate corrections. To evaluate our method, we constructed a new benchmark by systematically injecting realistic syntax errors into validated DCS models. The quantitative evaluation demonstrates the high effectiveness of the proposed approach in terms of repair accuracy and its practical utility regarding time, achieving a speedup of 3.46 times compared to human developers. The experimental replication suite, including the benchmark and prompts, is available at https://github.com/Uuusay1432/DCSModelRepair.git

</details>


### [26] [The Human Need for Storytelling: Reflections on Qualitative Software Engineering Research With a Focus Group of Experts](https://arxiv.org/abs/2512.07293)
*Roberto Verdecchia,Justus Bogner*

Main category: cs.SE

TL;DR: 本文通过专家焦点小组讨论，回顾了软件工程中定性研究的现状、重要性、发展历程、当前面临的障碍以及未来展望。


<details>
  <summary>Details</summary>
Motivation: 软件工程研究长期以来以定量方法为主导，定性研究自1980年代末引入后逐渐发展。本文旨在反思定性软件工程研究的现状，探讨其重要性、演变过程、当前实践中的障碍以及未来发展方向。

Method: 采用专家焦点小组讨论的方法，邀请三位软件工程定性研究专家（Rashina Hoda、Carolyn Seaman、Klaas Stol）参与对话。讨论于2025年10月25日进行，内容经过主持和编辑后形成本文。

Result: 通过专家讨论，深入探讨了定性软件工程研究的重要性、历史演变、当前实践中的常见障碍，并对该领域的未来发展进行了展望。

Conclusion: 定性研究在软件工程领域已确立其重要地位，但仍面临挑战。专家讨论为理解该领域的现状和未来方向提供了宝贵见解，强调了定性研究在丰富软件工程知识体系中的持续价值。

Abstract: From its first adoption in the late 80s, qualitative research has slowly but steadily made a name for itself in what was, and perhaps still is, the predominantly quantitative software engineering (SE) research landscape. As part of our regular column on empirical software engineering (ACM SIGSOFT SEN-ESE), we reflect on the state of qualitative SE research with a focus group of experts. Among other things, we discuss why qualitative SE research is important, how it evolved over time, common impediments faced while practicing it today, and what the future of qualitative SE research might look like. Joining the conversation are Rashina Hoda (Monash University, Australia), Carolyn Seaman (University of Maryland, United States), and Klaas Stol (University College Cork, Ireland). The content of this paper is a faithful account of our conversation from October 25, 2025, which we moderated and edited for our column.

</details>


### [27] [Challenges in Developing Secure Software -- Results of an Interview Study in the German Software Industry](https://arxiv.org/abs/2512.07368)
*Alex R. Mattukat,Timo Langstrof,Horst Lichter*

Main category: cs.SE

TL;DR: 通过访谈12家跨行业公司的19位专家，研究发现软件开发面临的安全挑战主要源于高复杂性、安全意识不足、流程不当以及人才短缺，并提出了相应研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管存在许多安全软件开发工具和框架，但网络犯罪统计数据近年来并未改善。为了理解软件公司在开发安全软件时面临的实际挑战，需要进行实证研究。

Method: 采用访谈研究方法，对来自12家跨行业公司的19位行业专家进行了访谈研究。

Result: 研究发现安全软件开发的主要挑战包括：高复杂性、安全意识不足、不合适的流程，以及加剧这些问题的技能人才短缺。

Conclusion: 研究识别了行业面临的关键安全挑战，并基于这些发现提出了潜在的研究方向，以帮助改善安全软件开发实践。

Abstract: The damage caused by cybercrime makes the development of secure software inevitable. Although many tools and frameworks exist to support the development of secure software, statistics on cybercrime show no improvement in recent years. To understand the challenges software companies face in developing secure software, we conducted an interview study with 19 industry experts from 12 cross-industry companies. The results of our study show that the challenges are mainly due to high complexity, a lack of security awareness, and unsuitable processes, which are further exacerbated by an immediate lack of skilled personnel. This article presents our study and the challenges we identified, and derives potential research directions from them.

</details>


### [28] [Do LLMs Trust the Code They Write?](https://arxiv.org/abs/2512.07404)
*Francisco Ribeiro,Claudio Spiess,Prem Devanbu,Sarah Nadi*

Main category: cs.SE

TL;DR: LLMs内部存在代码正确性的表征，通过对比正确与错误代码的隐藏状态可以提取这种表征，用于提升代码生成质量，无需测试执行。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在代码生成方面有效，但经常输出错误代码。模型输出概率与正确性相关性不强，且只反映生成过程的最终输出。受LLMs内部编码真实性概念的启发，探索LLMs是否类似地表示代码正确性。

Method: 通过对比相同编程任务下正确与错误代码对的隐藏状态，识别LLMs内部的正确性表征。在四个LLMs上进行实验，提取这种内部正确性信号。

Result: 利用提取的正确性表征优于标准的对数似然排序和语言化模型置信度。这种内部正确性信号可用于选择更高质量的代码样本，无需测试执行。

Conclusion: 利用内部表征可以增强代码生成系统，使LLMs更可靠，从而提高对自动生成代码的信心。这项工作展示了如何通过内部表示提升代码生成质量。

Abstract: Despite the effectiveness of large language models (LLMs) for code generation, they often output incorrect code. One reason is that model output probabilities are often not well-correlated with correctness, and reflect only the final output of the generation process. Inspired by findings that LLMs internally encode concepts like truthfulness, this paper explores if LLMs similarly represent code correctness. Specifically, we identify a correctness representation inside LLMs by contrasting the hidden states between pairs of correct and incorrect code for the same programming tasks. By experimenting on four LLMs, we show that exploiting this extracted correctness representation outperforms standard log-likelihood ranking, as well as verbalized model confidence. Furthermore, we explore how this internal correctness signal can be used to select higher-quality code samples, without requiring test execution. Ultimately, this work demonstrates how leveraging internal representations can enhance code generation systems and make LLMs more reliable, thus improving confidence in automatically generated code.

</details>


### [29] [AutoICE: Automatically Synthesizing Verifiable C Code via LLM-driven Evolution](https://arxiv.org/abs/2512.07501)
*Weilin Luo,Xueyi Liang,Haotian Deng,Yanan Liu,Hai Wan*

Main category: cs.SE

TL;DR: AutoICE：基于LLM的进化搜索方法，用于从自然语言需求合成可验证的C代码，通过多样化初始化和协作交叉减少错误传播，在代码验证成功率上显著超越现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 从自然语言需求自动合成可验证代码能确保软件正确性和可靠性，同时降低形式化方法的应用门槛。现有方法因领域特定预训练数据稀缺和隐式知识形式化困难，存在严重的语法和语义错误。

Method: 提出AutoICE方法，采用LLM驱动的进化搜索，包含：1）多样化个体初始化；2）协作交叉实现多样化迭代更新；3）自反思变异促进隐式知识发现，从而缓解单智能体迭代中的错误传播问题。

Result: AutoICE成功验证了90.36%的代码，优于现有最佳方法。在开发者友好的数据集变体上，达到88.33%的验证成功率，显著超过现有最佳方法的65%。

Conclusion: AutoICE通过LLM驱动的进化搜索有效解决了从自然语言需求合成可验证C代码的问题，在代码验证成功率方面取得了显著提升，为自动形式化提供了新的有效方法。

Abstract: Automatically synthesizing verifiable code from natural language requirements ensures software correctness and reliability while significantly lowering the barrier to adopting the techniques of formal methods. With the rise of large language models (LLMs), long-standing efforts at autoformalization have gained new momentum. However, existing approaches suffer from severe syntactic and semantic errors due to the scarcity of domain-specific pre-training corpora and often fail to formalize implicit knowledge effectively. In this paper, we propose AutoICE, an LLM-driven evolutionary search for synthesizing verifiable C code. It introduces the diverse individual initialization and the collaborative crossover to enable diverse iterative updates, thereby mitigating error propagation inherent in single-agent iterations. Besides, it employs the self-reflective mutation to facilitate the discovery of implicit knowledge. Evaluation results demonstrate the effectiveness of AutoICE: it successfully verifies $90.36$\% of code, outperforming the state-of-the-art (SOTA) approach. Besides, on a developer-friendly dataset variant, AutoICE achieves a $88.33$\% verification success rate, significantly surpassing the $65$\% success rate of the SOTA approach.

</details>


### [30] [Understanding Privacy Risks in Code Models Through Training Dynamics: A Causal Approach](https://arxiv.org/abs/2512.07814)
*Hua Yang,Alejandro Velasco,Sen Fang,Bowen Xu,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 研究发现代码大语言模型中不同PII类型的泄露风险存在显著差异，且与训练动态相关，为开发类型感知防御提供因果证据。


<details>
  <summary>Details</summary>
Motivation: 代码大语言模型依赖包含大量个人身份信息的开源仓库，引发隐私担忧。现有研究将PII视为单一类别，忽视了不同类型PII的异质风险差异。

Method: 构建包含多种PII类型的数据集，微调不同规模的代表性模型，计算真实PII数据的训练动态，并构建结构因果模型来估计可学习性对泄露的因果效应。

Result: 泄露风险在不同PII类型间差异显著：易学习的实例（如IP地址）泄露率更高，而较难类型（如密钥和密码）泄露较少。模糊类型表现出混合行为。

Conclusion: 首次提供因果证据表明泄露风险具有类型依赖性，为开发类型感知和可学习性感知的代码大语言模型防御提供指导。

Abstract: Large language models for code (LLM4Code) have greatly improved developer productivity but also raise privacy concerns due to their reliance on open-source repositories containing abundant personally identifiable information (PII). Prior work shows that commercial models can reproduce sensitive PII, yet existing studies largely treat PII as a single category and overlook the heterogeneous risks among different types. We investigate whether distinct PII types vary in their likelihood of being learned and leaked by LLM4Code, and whether this relationship is causal. Our methodology includes building a dataset with diverse PII types, fine-tuning representative models of different scales, computing training dynamics on real PII data, and formulating a structural causal model to estimate the causal effect of learnability on leakage. Results show that leakage risks differ substantially across PII types and correlate with their training dynamics: easy-to-learn instances such as IP addresses exhibit higher leakage, while harder types such as keys and passwords leak less frequently. Ambiguous types show mixed behaviors. This work provides the first causal evidence that leakage risks are type-dependent and offers guidance for developing type-aware and learnability-aware defenses for LLM4Code.

</details>


### [31] [Studying the Role of Reusing Crowdsourcing Knowledge in Software Development](https://arxiv.org/abs/2512.07824)
*Rabe Abdalkareem*

Main category: cs.SE

TL;DR: 研究显示，重用Stack Overflow等众包平台的代码知识能提升开发效率，但会引入依赖开销和维护负担，需通过改进持续集成来保障软件质量。


<details>
  <summary>Details</summary>
Motivation: 众包平台（如Stack Overflow）改变了软件开发实践，但现有研究缺乏对软件质量的实证分析，不清楚开发者如何使用众包知识及其对软件质量的影响。

Method: 对Stack Overflow和npm等知名众包平台进行大规模实证研究，分析重用众包知识对软件项目的影响，并考察持续集成（CI）作为质量保障方法的有效性。

Result: 重用众包平台知识（特别是代码）能辅助开发实践，但会导致软件项目面临依赖开销增加、维护工作量上升等质量问题。

Conclusion: 需要基于数据驱动的决策来管理众包知识重用风险，改进持续集成（CI）方法可提升开发者生产力并节约资源，从而缓解对众包知识的依赖带来的质量问题。

Abstract: Crowdsourcing platforms, such as Stack Overflow, have changed and impacted the software development practice. In these platforms, developers share and reuse their software development and programming experience. Therefore, a plethora of research work focused on crowdsourcing in software engineering and showed that, among other things, crowdsourced development tends to increase developers' productivity and reduce time-to-market. However, in crowdsourcing, the empirical studies of software quality are lacking, and simple questions, such as what developers use the crowdsourcing knowledge for, are unanswered.
  Therefore, our research focused on studying the impact of reusing crowdsourcing knowledge on software projects. To do so, we conduct several large-scale empirical studies on some of the well-known crowdsourcing platforms, including Stack Overflow and npm. Our results showed that reusing knowledge from these crowdsourcing platforms has the potential to assist software development practice, specifically in the form of reusing crowdsourced code. However, using such knowledge affects the quality of the software in several aspects, such as making the software projects suffer from dependency overhead and increasing the maintenance effort. Based on these findings, we use the gained knowledge to make sound data-driven decisions where we examine software quality assurance methods to mitigate the risk of relying on crowd sourcing knowledge in software development. We examine the use of continuous integration (CI). Our analysis showed how CI can be improved to increase developers' productivity and save their resources.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [32] [Reasoning about concurrent loops and recursion with rely-guarantee rules](https://arxiv.org/abs/2512.06242)
*Ian J. Hayes,Larissa A. Meinicke,Cliff B. Jones*

Main category: cs.LO

TL;DR: 本文提出了一套通用的、经过机械验证的细化规则，用于在并发环境下推理递归程序和while循环，不假设表达式求值是原子的，采用rely-guarantee方法进行组合式并发推理。


<details>
  <summary>Details</summary>
Motivation: 在并发环境下对递归程序和循环进行形式化推理存在挑战，特别是当表达式求值不是原子操作时。现有方法通常假设原子性，这在实际并发系统中不成立。需要开发能够在非原子表达式求值条件下进行组合式并发推理的细化规则。

Method: 采用rely-guarantee并发推理方法，将递归程序定义为命令格上的不动点，开发不动点推理定律。循环通过不动点定义，从而将递归定律应用于循环推理。所有规则都经过机械验证。

Result: 提出了一套经过机械验证的细化规则，能够在非原子表达式求值条件下对并发程序中的递归和循环进行形式化推理。这些规则支持组合式并发分析，提高了并发程序验证的实用性和可靠性。

Conclusion: 通过将递归程序建模为命令格上的不动点，并应用rely-guarantee并发推理方法，成功开发了适用于非原子表达式求值环境的递归和循环细化规则，为并发程序的形式化验证提供了更实用的理论基础。

Abstract: The objective of this paper is to present general, mechanically verified, refinement rules for reasoning about recursive programs and while loops in the context of concurrency. Unlike many approaches to concurrency, we do not assume that expression evaluation is atomic. We make use of the rely-guarantee approach to concurrency that facilitates reasoning about interference from concurrent threads in a compositional manner. Recursive programs can be defined as fixed points over a lattice of commands and hence we develop laws for reasoning about fixed points. Loops can be defined in terms of fixed points and hence the laws for recursion can be applied to develop laws for loops.

</details>


### [33] [Formal State-Machine Models for Uniswap v3 Concentrated-Liquidity AMMs: Priced Timed Automata, Finite-State Transducers, and Provable Rounding Bounds](https://arxiv.org/abs/2512.06203)
*Julius Tranquilli,Naman Gupta*

Main category: cs.LO

TL;DR: 本文为Uniswap v3式CLAMMs提出形式化建模方法，使用PTA和FST进行模型检验，证明离散化单tick模型在无手续费交易下保持不变量，并通过TLA+实例验证。


<details>
  <summary>Details</summary>
Motivation: 虽然CLAMMs（如Uniswap v3）已成为DeFi常见原语，但现有研究多为经济和博弈论分析，缺乏可用于模型检验或定理证明的形式化状态机层面的工作。

Method: 使用(1)定价时间自动机网络(PTA)和(2)离散tick上的有限状态转换器(FST)对Uniswap v3式CLAMMs进行形式化建模。将流动性头寸视为有状态对象，仅在池价格跨越其活跃范围边界时转换。在PTA中编码分段常数乘积不变量、费用增长变量和tick跨越规则，并在TLA+中推导tick级FST抽象。

Result: 定义了离散化单tick CLAMM模型的显式tick-wise不变量，并证明在无手续费交易下该不变量在紧密加法舍入界内保持。在TLA+中实例化模型，使用TLC对结构忠实实例进行穷举检验，包括三tick集中流动性配置和双向单tick模型中的有界无舍入仅套利属性。

Conclusion: 该形式化建模方法为CLAMMs提供了严格的验证框架，证明了ε-slack在不变性性质中的合理性，展示了舍入作为受控扰动的作用，并可通过虚拟储备扩展到Uniswap v3的tick-wise结构。

Abstract: Concentrated-liquidity automated market makers (CLAMMs), as exemplified by Uniswap v3, are now a common primitive in decentralized finance frameworks. Their design combines continuous trading on constant-function curves with discrete tick boundaries at which liquidity positions change and rounding effects accumulate. While there is a body of economic and game-theoretic analysis of CLAMMs, there is negligible work that treats Uniswap v3 at the level of formal state machines amenable to model checking or theorem proving.
  In this paper we propose a formal modeling approach for Uniswap v3-style CLAMMs using (i) networks of priced timed automata (PTA), and (ii) finite-state transducers (FST) over discrete ticks. Positions are treated as stateful objects that transition only when the pool price crosses the ticks that bound their active range. We show how to encode the piecewise constant-product invariant, fee-growth variables, and tick-crossing rules in a PTA suitable for tools such as UPPAAL, and how to derive a tick-level FST abstraction for specification in TLA+.
  We define an explicit tick-wise invariant for a discretized, single-tick CLAMM model and prove that it is preserved up to a tight additive rounding bound under fee-free swaps. This provides a formal justification for the "$ε$-slack" used in invariance properties and shows how rounding enters as a controlled perturbation. We then instantiate these models in TLA+ and use TLC to exhaustively check the resulting invariants on structurally faithful instances, including a three-tick concentrated-liquidity configuration and a bounded no-rounding-only-arbitrage property in a bidirectional single-tick model. We discuss how these constructions lift to the tick-wise structure of Uniswap v3 via virtual reserves, and how the resulting properties can be phrased as PTA/TLA+ invariants about cross-tick behaviour and rounding safety.

</details>


### [34] [A finer reparameterisation theorem for MSO and FO queries on strings](https://arxiv.org/abs/2512.06466)
*Lê Thành Dũng Nguyên,Paweł Parys*

Main category: cs.LO

TL;DR: 该论文证明了关于有限单词上单子二阶k元查询的定理，表明如果二元字符串查询结果数量为O(0的数量×1的数量)，则每个结果可以从0位置、1位置和有限数据中MSO可定义地识别。


<details>
  <summary>Details</summary>
Motivation: 研究有限单词上单子二阶逻辑查询的结构特性，特别是当查询结果数量与输入字符串中0和1的数量乘积相关时，如何MSO可定义地识别每个结果。

Method: 通过定理证明方法，处理单子二阶逻辑和一阶逻辑/非周期幺半群的情况，使用形式逻辑和代数方法分析查询的结构特性。

Result: 证明了如果二元字符串查询结果数量为O(0的数量×1的数量)，则每个结果可以从0位置、1位置和有限数据中MSO可定义地识别。同时处理了一阶逻辑/非周期幺半群的情况，证明了维度最小化对于一阶字符串到字符串解释成立的民间定理。

Conclusion: 该研究建立了有限单词上单子二阶查询的结构特性与可定义性之间的关系，为一阶字符串解释的维度最小化提供了形式化证明，扩展了对逻辑查询结构特性的理解。

Abstract: We show a theorem on monadic second-order k-ary queries on finite words. It may be illustrated by the following example: if the number of results of a query on binary strings is O(number of 0s $\times$ number of 1s), then each result can be MSO-definably identified from a 0-position, a 1-position and some finite data.
  Our proofs also handle the case of first-order logic / aperiodic monoids. Thus we can state and prove the folklore theorem that dimension minimisation holds for first-order string-to-string interpretations.

</details>


### [35] [String Diagrams for Closed Symmetric Monoidal Categories](https://arxiv.org/abs/2512.06499)
*Callum Reader,Alessandro Di Giorgio*

Main category: cs.LO

TL;DR: 提出一种基于弦图的图形语言，用于闭对称幺半范畴，通过引入特殊的括号线表示内部hom，使内部hom函子的结构显式化。


<details>
  <summary>Details</summary>
Motivation: 现有弦图表示法在处理闭对称幺半范畴时，内部hom函子的结构不够直观和显式。需要一种更清晰的图形表示方法，使内部hom的结构更加明确，便于在范畴论、逻辑和编程语言语义等领域应用。

Method: 扩展弦图表示法，引入特殊的"括号线"来表示内部hom。定义了一套完整的图形规则，使标准态射线与括号线能够规范地交互。建立了该图形演算的可靠性和完备性。

Result: 成功开发了一种新的图形语言，能够显式地表示闭对称幺半范畴中的内部hom结构。该语言具有严格的数学基础（可靠且完备），并通过范畴论、逻辑和编程语言语义中的实例展示了其表达能力。

Conclusion: 提出的基于括号线的弦图扩展为闭对称幺半范畴提供了一种直观且形式化的图形表示方法，使内部hom的结构更加清晰，有助于在这些数学结构中进行推理和计算。

Abstract: We introduce a graphical language for closed symmetric monoidal categories based on an extension of string diagrams with special bracket wires representing internal homs. These bracket wires make the structure of the internal hom functor explicit, allowing standard morphism wires to interact with them through a well-defined set of graphical rules.
  We establish the soundness and completeness of the diagrammatic calculus, and illustrate its expressiveness through examples drawn from category theory, logic and programming language semantics.

</details>


### [36] [Comparing Knowledge: An Analysis of the Relative Epistemic Powers of Groups](https://arxiv.org/abs/2512.06542)
*Baltag Alexandru,Smets Sonja*

Main category: cs.LO

TL;DR: 使用比较知识断言的认知逻辑分析个体或群体的相对认知能力，研究不同群体在不同知识类型下的认知潜力比较


<details>
  <summary>Details</summary>
Motivation: 传统认知逻辑主要关注绝对知识状态，缺乏对相对认知能力的分析。本文旨在开发一种新的认知逻辑框架，能够表达和比较不同个体或群体在不同知识类型下的认知潜力，特别是关注群体对自身认知位置相对于他人的认知能力。

Method: 采用新型认知逻辑框架，引入比较知识断言，能够表达"一个群体能够（集体）知道另一个群体所能知道的一切"。分析不同知识类型（完全内省、正内省等）对应的模态认知条件（如S5、S4、KT），研究在这些条件下个体或群体的相对认知能力。

Result: 开发了一个能够表达相对认知能力的逻辑框架，能够形式化比较不同群体在不同知识类型下的认知潜力。特别关注了群体对自身认知位置相对于他人的认知能力，为分析分布式认知系统中的相对认知优势提供了理论基础。

Conclusion: 比较知识断言为分析个体或群体的相对认知能力提供了有力的逻辑工具，能够揭示在不同知识类型下群体认知潜力的差异，特别有助于理解群体对自身认知位置相对于他人的认知能力，为多智能体系统中的认知分析开辟了新方向。

Abstract: We use a novel type of epistemic logic, employing comparative knowledge assertions, to analyze the relative epistemic powers of individuals or groups of agents. Such comparative assertions can express that a group has the potential to (collectively) know everything that another group can know. Moreover, we look at comparisons involving various types of knowledge (fully introspective, positively introspective, etc.), satisfying the corresponding modal-epistemic conditions (e.g., $S5$, $S4$, $KT$). For each epistemic attitude, we are particularly interested in what agents or groups can know about their own epistemic position relative to that of others.

</details>


### [37] [Description Logics with Two Types of Definite Descriptions: Complexity, Expressiveness, and Automated Deduction](https://arxiv.org/abs/2512.06604)
*Michał Sochański,Przemysław Andrzej Wałęga,Michał Zawidzki*

Main category: cs.LO

TL;DR: 本文扩展了描述逻辑ALC，引入了局部和全局定指描述算子，建立了对应的互模拟概念，证明了表达力差异和相同的ExpTime复杂度，并实现了基于tableau的决策过程。


<details>
  <summary>Details</summary>
Motivation: 定指描述（"the unique x satisfying property C"）在ontology和查询语言中很重要，提供了比缺乏语义内容的专名（ID）更有意义的引用方式。需要研究如何在描述逻辑中形式化处理定指描述。

Method: 扩展描述逻辑ALC，引入局部定指描述ALCι_L和全局定指描述ALCι_G。定义了适当的互模拟概念来分析表达力。开发了基于tableau的决策过程，并实现了相应的算法。

Result: 两个逻辑在概念和ontology可满足性上都有紧的ExpTime复杂度界限，但ALCι_G比ALCι_L表达力更强。实验结果表明实现具有实际效用，并揭示了性能与输入公式结构特性之间的有趣相关性。

Conclusion: 成功将定指描述整合到描述逻辑中，建立了形式化框架，证明了理论性质，并提供了实用的决策过程实现，为ontology和查询语言中定指描述的应用奠定了基础。

Abstract: Definite descriptions are expressions of the form "the unique $x$ satisfying property $C$," which allow reference to objects through their distinguishing characteristics. They play a crucial role in ontology and query languages, offering an alternative to proper names (IDs), which lack semantic content and serve merely as placeholders.
  In this paper, we introduce two extensions of the well-known description logic $\mathcal{ALC}$ with local and global definite descriptions, denoted $\mathcal{ALC}ι_L$ and $\mathcal{ALC}ι_G$, respectively. We define appropriate bisimulation notions for these logics, enabling an analysis of their expressiveness. We show that although both logics share the same tight ExpTime complexity bounds for concept and ontology satisfiability, $\mathcal{ALC}ι_G$ is strictly more expressive than $\mathcal{ALC}ι_L$. Moreover, we present tableau-based decision procedures for satisfiability in both logics, provide their implementation, and report on a series of experiments. The empirical results demonstrate the practical utility of the implementation and reveal interesting correlations between performance and structural properties of the input formulas.

</details>


### [38] [FastLEC: Parallel Datapath Equivalence Checking with Hybrid Engines](https://arxiv.org/abs/2512.06627)
*Xindi Zhang,Furong Ye,Zhihan Chen,Shaowei Cai*

Main category: cs.LO

TL;DR: FastLEC是一个混合验证器，结合SAT、BDD和精确模拟三种形式验证引擎，通过智能调度、结构感知分区和GPU加速等技术，显著提升数据路径电路等价性检查的效率。


<details>
  <summary>Details</summary>
Motivation: 数据路径电路的组合等价性检查（CEC）面临挑战，因为复杂的算术结构使得SAT、BDD和精确模拟等独立技术的能力和可扩展性有限，需要更有效的验证方法。

Method: 1. 基于回归的引擎调度启发式方法预测求解器效果，实现更准确平衡的计算资源分配；2. 数据路径结构感知的分区策略结合动态分治SAT求解器，利用算术设计的规律性；3. 通过地址引用计数跟踪减少精确模拟的内存开销，并使用GPU后端加速模拟。

Result: 在368个数据路径电路上评估，使用32个CPU核心时，比广泛使用的ABC &cec工具多验证5.07倍电路；相比最新的串行和并行CEC验证器，在PAR-2时间上分别提升3.33倍和2.67倍，多解决74个电路；添加单个GPU后进一步提升4.07倍。

Conclusion: FastLEC通过统一三种形式推理引擎并引入创新策略，显著提高了数据路径电路等价性检查的效率和可扩展性，展示了在复杂算术电路验证中的优越性能。

Abstract: Combinational equivalence checking (CEC) remains a challenge EDA task in the formal verification of datapath circuits due to their complex arithmetic structures and the limited capability or scalability of SAT, BDD, and exact-simulation (ES) based techniques when used independently. This work presents FastLEC, a hybrid prover that unifies these three formal reasoning engines and introduces three strategies that substantially enhance verification efficiency. First, a regression-based engine-scheduling heuristic predicts solver effectiveness, enabling more accurate and balanced allocation of computational resources. Second, datapath-structure-aware partitioning strategies, along with a dynamic divide-and-conquer SAT prover, exploit the regularity of arithmetic designs while preserving completeness. Third, the memory overhead of ES is significantly reduced through address-reference-count tracking, and simulation is further accelerated through a GPU-enabled backend. FastLEC is evaluated across 368 datapath circuits. Using 32 CPU cores, it proves 5.07x more circuits than the widely used ABC &cec tool. Compared with the latest best datapath-oriented serial and parallel CEC provers, FastLEC outperforms them by 3.33x and 2.67x in PAR-2 time, demonstrating an improvement of 74 newly solved circuits. With the addition of a single GPU, it achieves a further 4.07x improvement. The prover also demonstrates excellent scalability.

</details>


### [39] [Functional Reduction to Speed Up Bounded Model Checking](https://arxiv.org/abs/2512.06643)
*Changyuan Yu,Wenbin Che,Hongce Zhang*

Main category: cs.LO

TL;DR: FRAIG-BMC：一种基于功能等价节点合并的有界模型检查技术，通过减少冗余提高SAT求解效率


<details>
  <summary>Details</summary>
Motivation: 传统BMC随着深度增加，SAT查询变得难以求解，而许多FPV问题涉及相关电路的多个副本，存在简化展开转换关系的机会

Method: 基于FRAIG技术，在展开过程中增量识别和合并功能等价节点，减少冗余

Result: FRAIG-BMC显著加速了BMC，在顺序等价检查、部分保留寄存器检测和信息流检查等应用中表现优异

Conclusion: 通过识别和合并功能等价节点，FRAIG-BMC有效提高了BMC的效率和可扩展性

Abstract: Bounded model checking (BMC) is a widely used technique for formal property verification (FPV), where the transition relation is repeatedly unrolled to increasing depths and encoded into Boolean satisfiability (SAT) queries. As the bound grows deeper, these SAT queries typically become more difficult to solve, posing scalability challenges. Howevefor, many FPV problems involve multiple copies of related circuits, creating opportunities to simplify the unrolled transition relation. Motivated by the functionally reduced and-inverter-graph (FRAIG) technique, we propose FRAIG-BMC, which incrementally identifies and merges functionally equivalent nodes during the unrolling process. By reducing redundancy, FRAIG-BMC improves the efficiency of SAT solving and accelerates property checking. Experiments demonstrate that FRAIG-BMC significantly speeds up BMC across a range of applications, including sequential equivalence checking, partial retention register detection, and information flow checking

</details>


### [40] [Formal that "Floats" High: Formal Verification of Floating Point Arithmetic](https://arxiv.org/abs/2512.06850)
*Hansa Mohanty,Vaisakh Naduvodi Viswambharan,Deepak Narayan Gadde*

Main category: cs.LO

TL;DR: 提出一种可扩展的浮点算术验证方法，采用直接RTL-to-RTL模型检查，结合分治策略和AI驱动的形式属性生成，相比传统方法获得更高覆盖率效率。


<details>
  <summary>Details</summary>
Motivation: 浮点算术的形式验证面临非线性算术行为和控制-数据路径紧密耦合的挑战。现有方法依赖高级C模型进行等价检查，但存在抽象差距、转换开销大且在RTL级别可扩展性有限的问题。

Method: 1) 直接RTL-to-RTL模型检查对比黄金参考模型；2) 分治策略将验证分解为模块化阶段；3) 反例引导的迭代精化定位实现缺陷；4) 针对性故障注入验证鲁棒性；5) 集成基于代理AI的形式属性生成，结合LLM驱动自动化和人工在环精化。

Result: 直接RTL-to-RTL模型检查相比独立验证获得更高覆盖率效率，需要更少断言，特别是结合经过人工在环精化的AI生成属性时效果更佳。

Conclusion: 该方法提供了一种可扩展、实用的浮点算术验证解决方案，通过直接RTL级验证减少抽象差距，结合AI自动化提高验证效率，为复杂算术单元验证提供了有效途径。

Abstract: Formal verification of floating-point arithmetic remains challenging due to non-linear arithmetic behavior and the tight coupling between control and datapath logic. Existing approaches often rely on high-level C models for equivalence checking against Register Transfer Level (RTL) designs, but this introduces abstraction gaps, translation overhead, and limits scalability at the RTL level. To address these challenges, this paper presents a scalable methodology for verifying floating-point arithmetic using direct RTL-to-RTL model checking against a golden reference model. The approach adopts a divide-and conquer strategy that decomposes verification into modular stages, each captured by helper assertions and lemmas that collectively prove a main correctness theorem. Counterexample (CEX)-guided refinement is used to iteratively localize and resolve implementation defects, while targeted fault injection validates the robustness of the verification process against precision-critical datapath errors. To assess scalability and practicality, the methodology is extended with agentic AI-based formal property generation, integrating large language model (LLM)-driven automation with Human-in-the-Loop (HITL) refinement. Coverage analysis evaluates the effectiveness of the approach by comparing handwritten and AI-generated properties in both RTL-to-RTL model checking and standalone RTL verification settings. Results show that direct RTL-to-RTL model checking achieves higher coverage efficiency and requires fewer assertions than standalone verification, especially when combined with AI-generated properties refined through HITL guidance.

</details>


### [41] [Resource-Bounded Type Theory: Compositional Cost Analysis via Graded Modalities](https://arxiv.org/abs/2512.06952)
*Mirco A. Mannucci,Corey Thuro*

Main category: cs.LO

TL;DR: 提出一个组合式框架，用于验证类型化程序的资源边界，通过抽象资源格统一处理时间、内存、gas等成本，并证明成本可靠性定理。


<details>
  <summary>Details</summary>
Motivation: 需要系统化地验证程序在各种资源（时间、内存、gas等）上的边界，确保程序执行成本可预测且可控，特别是在安全关键系统和资源受限环境中。

Method: 使用带合成边界的类型系统，引入分级可行性模态，建立语法成本可靠性定理，构建预层拓扑中的语法项模型，通过自然变换提取成本。

Result: 证明了递归自由简单类型片段的成本可靠性：若闭项在给定预算下合成边界为b，则其操作成本以b为界。建立了语法模型的初始性，并通过二分搜索案例展示了组合推理。

Conclusion: 该框架为程序资源边界验证提供了组合式、类型驱动的方法，能够统一处理多种资源成本，并通过语法模型保证了理论的坚实性，适用于实际程序验证。

Abstract: We present a compositional framework for certifying resource bounds in typed programs. Terms are typed with synthesized bounds drawn from an abstract resource lattice, enabling uniform treatment of time, memory, gas, and domain-specific costs.
  We introduce a graded feasibility modality with co-unit and monotonicity laws. Our main result is a syntactic cost soundness theorem for the recursion-free simply-typed fragment: if a closed term has synthesized bound b under a given budget, its operational cost is bounded by b. We provide a syntactic term model in the topos of presheaves over the lattice -- where resource bounds index a cost-stratified family of definable values -- with cost extraction as a natural transformation. We prove canonical forms via reification and establish initiality of the syntactic model: it embeds uniquely into all resource-bounded models.
  A case study demonstrates compositional reasoning for binary search using Lean's native recursion with separate bound proofs.

</details>


### [42] [Hereditary History-Preserving Bisimilarity: Characterizations via Backward Ready Multisets](https://arxiv.org/abs/2512.06959)
*Marco Bernardo,Andrea Esposito,Claudio A. Mezzina*

Main category: cs.LO

TL;DR: 论文提出了两种互补的遗传历史保持双模拟(HHPB)特征化方法：基于稳定配置结构的指称特征化和基于可逆进程演算的操作特征化，使用前向-反向双模拟加反向就绪多重集相等，比传统HHPB更轻量级。


<details>
  <summary>Details</summary>
Motivation: 传统HHPB需要唯一标识事件，这在实际应用中可能过于严格。本文旨在开发更轻量级的等价关系，通过计数相同标签事件的出现次数而非唯一标识，同时仍能区分自动并发和自动因果关系。

Method: 1) 基于稳定配置结构的指称特征化；2) 基于可逆进程演算的操作特征化；3) 使用前向-反向双模拟加反向就绪多重集相等；4) 开发事件标识逻辑和反向就绪多重集逻辑来建立逻辑基础。

Result: 提出的特征化方法能正确区分自动并发和自动因果关系，但仅在不存在非局部冲突时有效。新等价关系比传统HHPB更轻量级，将重点从事件唯一标识转移到计数相同标签事件的出现次数。

Conclusion: 论文成功开发了HHPB的两种互补特征化方法，提供了更轻量级的等价关系，并通过逻辑基础分析连接了传统事件标识逻辑和新开发的反向就绪多重集逻辑，为并发系统行为分析提供了新视角。

Abstract: We devise two complementary characterizations of hereditary history-preserving bisimilarity (HHPB): a denotational one, based on stable configuration structures, and an operational one, formulated in a reversible process calculus. Our characterizations rely on forward-reverse bisimilarity augmented with backward ready multiset equality. This shifts the emphasis from uniquely identifying events, as done in previous characterizations, to counting occurrences of identically labeled events associated with incoming transitions, which yields a more lightweight behavioral equivalence than HHPB. We show that our characterizations correctly distinguish between autoconcurrency and autocausation, but are valid only in the absence of non-local conflicts. We then study the logical foundations of these characterizations by relating event identifier logic, which captures the classical view of HHPB, and backward ready multiset logic, developed for our new equivalence.

</details>


### [43] [Extending Action Logic with Omega Iteration](https://arxiv.org/abs/2512.06985)
*Tikhon Pshenitsyn*

Main category: cs.LO

TL;DR: 扩展动作逻辑的证明系统，加入omega迭代作为无限乘法合取，证明了切割可容许性并建立了可证明性谓词的复杂度界限


<details>
  <summary>Details</summary>
Motivation: 动作逻辑是研究程序行为和资源推理的形式系统，但缺乏处理无限迭代的能力。omega迭代作为无限乘法合取，可以更精确地建模程序的无限行为，如循环和递归

Method: 扩展动作逻辑的证明系统，引入omega迭代作为无限乘法合取操作符。采用证明论方法，建立包含新规则的证明系统，并证明切割规则的可容许性

Result: 成功证明了扩展系统的切割可容许性，建立了可证明性谓词的复杂度界限，表明系统在计算上是可处理的

Conclusion: 扩展的动作逻辑系统在保持证明论良好性质的同时，增强了表达无限行为的能力，为程序验证和资源推理提供了更强大的形式工具

Abstract: We present a proof system that extends action logic by omega iteration, which is viewed as infinitary multiplicative conjunction. We prove cut admissibility and establish complexity bounds for the provability predicate.

</details>


### [44] [A Diagrammatic Basis for Computer Programming](https://arxiv.org/abs/2512.07240)
*Filippo Bonchi,Alessandro Di Giorgio,Elena Di Lavore*

Main category: cs.LO

TL;DR: 本文介绍了Kleene-Cartesian rig范畴，结合了笛卡尔双范畴和Kleene双范畴，用tape图处理命令式程序和程序逻辑。


<details>
  <summary>Details</summary>
Motivation: 现有的tape图虽然能表示rig范畴中的箭头，但缺乏对命令式程序和程序逻辑的专门支持。需要一种能同时处理这两种程序范式的图形化表示方法。

Method: 引入Kleene-Cartesian rig范畴，其中⊗运算构成笛卡尔双范畴，⊕运算构成Kleene双范畴。利用tape图作为图形化表示工具。

Result: 提出的Kleene-Cartesian rig范畴及其tape图表示能够方便地处理命令式程序和多种程序逻辑。

Conclusion: Kleene-Cartesian rig范畴为命令式程序和程序逻辑提供了统一的图形化表示框架，扩展了tape图的应用范围。

Abstract: Tape diagrams provide a convenient graphical notation for arrows of rig categories, i.e., categories equipped with two monoidal products, $\oplus$ and $\otimes$. In this work, we introduce Kleene-Cartesian rig categories, namely rig categories where $\otimes$ provides a Cartesian bicategory, while $\oplus$ a Kleene bicategory. We show that the associated tape diagrams can conveniently deal with imperative programs and various program logic.

</details>


### [45] [Symmetries in Sorting](https://arxiv.org/abs/2512.07349)
*Vikraman Choudhury,Wind Wong*

Main category: cs.LO

TL;DR: 该论文从抽象代数角度研究排序算法，将排序函数视为自由幺半群到自由交换幺半群的标准满射的截面，建立了排序函数与底层集合上的可判定全序之间的等价关系。


<details>
  <summary>Details</summary>
Motivation: 传统上排序算法被理解为根据底层集合的全序重新排列列表元素，但本文希望从更抽象的角度研究排序函数，不预设全序的存在，而是从代数结构的角度理解排序的本质。

Method: 1. 从函子签名的角度发展泛代数概念，在（单值）类型论中构造自由幺半群和自由交换幺半群。2. 将排序函数定义为自由幺半群到自由交换幺半群的标准满射的截面，并满足两个不预设全序的公理。3. 建立可判定全序与正确排序函数之间的等价关系。

Result: 证明了排序函数与底层集合上的可判定全序之间存在一一对应关系，为排序算法提供了严格的代数基础。所有理论都有Cubical Agda的形式化验证。

Conclusion: 排序算法可以从纯代数角度理解，无需预设全序概念。通过将排序函数定义为特定满射的截面，并满足两个自然公理，可以等价地推导出底层集合的全序结构，这为排序理论提供了新的数学基础。

Abstract: Sorting algorithms are fundamental to computer science, and their correctness criteria are well understood as rearranging elements of a list according to a specified total order on the underlying set of elements. As mathematical functions, they are functions on lists that perform combinatorial operations on the representation of the input list. In this paper, we study sorting algorithms conceptually as abstract sorting functions.
  There is a canonical surjection from the free monoid on a set (lists of elements) to the free commutative monoid on the same set (multisets of elements). We show that sorting functions determine a section (right inverse) to this surjection satisfying two axioms, that do not presuppose a total order on the underlying set. Then, we establish an equivalence between (decidable) total orders on the underlying set and correct sorting functions.
  The first part of the paper develops concepts from universal algebra from the point of view of functorial signatures, and gives constructions of free monoids and free commutative monoids in (univalent) type theory. Using these constructions, the second part of the paper develops the axiomatisation of sorting functions. The paper uses informal mathematical language, and comes with an accompanying formalisation in Cubical Agda.

</details>
