<div id=toc></div>

# Table of Contents

- [cs.LO](#cs.LO) [Total: 7]
- [cs.FL](#cs.FL) [Total: 4]
- [cs.SE](#cs.SE) [Total: 27]
- [cs.PL](#cs.PL) [Total: 6]


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [1] [Undecidability of Linear Logics without Weakening](https://arxiv.org/abs/2509.00644)
*Jun Suzuki,Katsuhiko Sano*

Main category: cs.LO

TL;DR: 本文证明了在经典命题线性逻辑CLL中省略指数模态的弱化规则的两个系统CLLR和CLLRR的不可判定性


<details>
  <summary>Details</summary>
Motivation: 研究在经典命题线性逻辑中完全省略指数模态弱化规则后系统的可判定性问题，填补CLLR系统不可判定性未知的研究空白

Method: 通过将CLLR的不可判定性归约到CLL的不可判定性，并利用单位元1和⊥模拟弱化规则；对于CLLRR系统，通过模拟Minsky的双计数器机来证明其不可判定性

Result: 成功证明了CLLR和CLLRR两个系统的不可判定性

Conclusion: 即使在经典命题线性逻辑中完全省略指数模态的弱化规则，系统的可判定性仍然无法保证，这扩展了对线性逻辑系统计算复杂性的理解

Abstract: The goal of this paper is to establish that it remains undecidable whether a
sequent is provable in two systems in which a weakening rule for an exponential
modality is completely omitted from classical propositional linear logic
$\mathbf{CLL}$ introduced by Girard (1987), which is shown to be undecidable by
Lincoln et al. (1992). We introduce two logical systems, $\mathbf{CLLR}$ and
$\mathbf{CLLRR}$. The first system, $\mathbf{CLLR}$, is obtained by omitting
the weakening rule for the exponential modality of $\mathbf{CLL}$. The system
$\mathbf{CLLR}$ has been studied by several authors, including
Meli\`es-Tabareau (2010), but its undecidability was unknown. This paper shows
the undecidability of $\mathbf{CLLR}$ by reducing it to the undecidability of
$\mathbf{CLL}$, where the units $\mathbf{1}$ and $\bot$ play a crucial role in
simulating the weakening rule. We also omit these units from the syntax and
inference rules of $\mathbf{CLLR}$ in order to define the second system,
$\mathbf{CLLRR}$. The undecidability of $\mathbf{CLLRR}$ is established by
showing that the system can simulate any two-counter machine proposed by Minsky
(1961).

</details>


### [2] [Formal Verification of Isothermal Chemical Reactors](https://arxiv.org/abs/2509.01130)
*Parivash Feyzishendi,Sophia Hamer,Jinyu Huang,Tyler R. Josephson*

Main category: cs.LO

TL;DR: 使用微分动态逻辑(dL)和KeYmaera X定理证明器对等温化学反应器进行达性分析，获得数学保证，但在CSTR中发现有限制


<details>
  <summary>Details</summary>
Motivation: 化学反应器的安全性、规定遵循性和经济效益取决于某些状态是否可达，传统使用数值模拟进行评估

Method: 利用微分动态逻辑(dL)和自动化定理证明器KeYmaera X，通过识别不变量对等温化学反应器进行符号达性分析

Result: 成功将方法应用于可解分析解的系统和Michaelis-Menten动力学等复杂模型，证明了某些条件的满足性，但CSTR中找不到有用不变量限制了可证明的反应网络复杂度

Conclusion: dL提供了一种有趣的符号逻辑方法来分析化学反应的达性问题，但获得的边界比数值达性分析更为宽松

Abstract: Chemical reactors are dynamic systems that can be described by systems of
ordinary differential equations (ODEs). Reactor safety, regulatory compliance,
and economics depend on whether certain states are reachable by the reactor,
and are generally assessed using numerical simulation. In this work, we show
how differential dynamic logic (dL), as implemented in the automated theorem
prover KeYmaera X, can be used to symbolically determine reachability in
isothermal chemical reactors, providing mathematical guarantees that certain
conditions are satisfied (for example, that an outlet concentration never
exceeds a regulatory threshold). First, we apply dL to systems whose dynamics
can be solved in closed form, such as first-order reactions in batch reactors,
proving that such reactors cannot exceed specified concentration limits. We
extend this method to reaction models as complex as Michaelis-Menten kinetics,
whose dynamics require approximations or numerical solutions. In all cases,
proofs are facilitated by identification of invariants; we find that
conservation of mass is both a principle proved from the ODEs describing mass
action kinetics as well as a useful relationship for proving other properties.
Useful invariants for continuous stirred tank reactors (CSTRs) were not found,
which limited the complexity of reaction networks that could be proved with dL.
While dL provides an interesting symbolic logic approach for reachability in
chemical reactions, the bounds we obtained are quite broad relative to those
typically achieved via numerical reachability analyses.

</details>


### [3] [Quantum Petri Nets with Event Structure semantics](https://arxiv.org/abs/2509.01423)
*Julien Saan Joachim,Marc de Visme,Stefan Haar*

Main category: cs.LO

TL;DR: 量子区块链网络(QPNs)的形式化框架，包含量子发生网络、展开语义和组合性，为量子并发提供严格的语义基础


<details>
  <summary>Details</summary>
Motivation: 现有的"量子区块链网络"缺乏严格的并发语义、分析工具和展开理论，需要建立一个类似于经典区块链网络的量子并发模型框架

Method: 通过引入配备量子评价的区块链网络(QPNs)，建立与量子事件结构语义相兼容的本地量子发生网络(LQONs)，并提供展开语义和组合性框架

Result: 建立了一个语义基础良好的量子并发模型，完整地展开了量子区块链网络的形式化语义

Conclusion: 该研究为量子并发提供了一个严格的模型化框架，完成了区块链网络理论与量子编程之间的桥梁

Abstract: Classical Petri nets provide a canonical model of concurrency, with unfolding
semantics linking nets, occurrence nets, and event structures. No comparable
framework exists for quantum concurrency: existing ''quantum Petri nets'' lack
rigorous concurrent and sound quantum semantics, analysis tools, and unfolding
theory. We introduce Quantum Petri Nets (QPNs), Petri nets equipped with a
quantum valuation compatible with the quantum event structure semantics of
Clairambault, De Visme, and Winskel (2019). Our contributions are: (i) a local
definition of Quantum Occurrence Nets (LQONs) compatible with quantum event
structures, (ii) a construction of QPNs with a well-defined unfolding
semantics, (iii) a compositional framework for QPNs. This establishes a
semantically well grounded model of quantum concurrency, bridging Petri net
theory and quantum programming.

</details>


### [4] [TREBL -- A Relative Complete Temporal Event-B Logic. Part I: Theory](https://arxiv.org/abs/2509.01462)
*Klaus-Dieter Schewe,Flavio Ferrarotti,Peter Rivière,Neeraj Kumar Singh,Guillaume Dupont,Yamine Aït Ameur*

Main category: cs.LO

TL;DR: 本文扩展了Event-B逻辑，提出了TREBL片段用于表达活性条件，并建立了完备的推导规则系统，证明了在充分精化条件下该系统的相对完备性。


<details>
  <summary>Details</summary>
Motivation: 验证活性条件是状态基础形式化方法的重要方面，需要扩展Event-B逻辑以表达Event-B机器迹的属性。

Method: 提出TREBL逻辑片段，定义了一套可靠的推导规则，证明在机器充分精化条件下这些规则的相对完备性。

Result: 建立了能够表达所有感兴趣活性条件的逻辑片段，并证明了推导系统的相对完备性。

Conclusion: TREBL逻辑片段为Event-B机器提供了表达和验证活性条件的有效框架，相关推导规则在精化条件下具有完备性。

Abstract: The verification of liveness conditions is an important aspect of state-based
rigorous methods. This article addresses the extension of the logic of Event-B
to a powerful logic, in which properties of traces of an Event-B machine can be
expressed. However, all formulae of this logic are still interpreted over
states of an Event-B machine rather than traces. The logic exploits that for an
Event-B machine $M$ a state $S$ determines all traces of $M$ starting in $S$.
We identify a fragment called TREBL of this logic, in which all liveness
conditions of interest can be expressed, and define a set of sound derivation
rules for the fragment. We further show relative completeness of these
derivation rules in the sense that for every valid entailment of a formula
$\varphi$ one can find a derivation, provided the machine $M$ is sufficiently
refined. The decisive property is that certain variant terms must be definable
in the refined machine. We show that such refinements always exist. Throughout
the article several examples from the field of security are used to illustrate
the theory.

</details>


### [5] [An Information-Flow Perspective on Explainability Requirements: Specification and Verification](https://arxiv.org/abs/2509.01479)
*Bernd Finkbeiner,Hadar Frenkel,Julian Siber*

Main category: cs.LO

TL;DR: 本文提出了一种使用认知时序逻辑和反事实因果量化来形式化验证可解释性系统的方法，既能确保系统提供足够的解释信息，又能保护隐私。


<details>
  <summary>Details</summary>
Motivation: 可解释性系统需要向用户展示为什么发生某些效果，但这种信息流需要被规范、验证，并与可能违反隐私的负面信息流进行平衡。

Method: 使用扩展的认知时序逻辑，加入反事实因果量化，来形式化指定多智能体系统的可解释性要求，并提供检查有限状态模型的算法。

Result: 开发了原型实现并在多个基准测试上评估，能够区分可解释和不可解释系统，并支持设置额外的隐私要求。

Conclusion: 该方法为可解释性系统的形式化验证提供了有效框架，能够在保证解释充分性的同时兼顾隐私保护需求。

Abstract: Explainable systems expose information about why certain observed effects are
happening to the agents interacting with them. We argue that this constitutes a
positive flow of information that needs to be specified, verified, and balanced
against negative information flow that may, e.g., violate privacy guarantees.
Since both explainability and privacy require reasoning about knowledge, we
tackle these tasks with epistemic temporal logic extended with quantification
over counterfactual causes. This allows us to specify that a multi-agent system
exposes enough information such that agents acquire knowledge on why some
effect occurred. We show how this principle can be used to specify
explainability as a system-level requirement and provide an algorithm for
checking finite-state models against such specifications. We present a
prototype implementation of the algorithm and evaluate it on several
benchmarks, illustrating how our approach distinguishes between explainable and
unexplainable systems, and how it allows to pose additional privacy
requirements.

</details>


### [6] [Derivation and Verification of Array Sorting by Merging, and its Certification in Dafny](https://arxiv.org/abs/2509.01758)
*Juan Pablo Carbonell,José E. Solsona,Nora Szasz,Álvaro Tasistro*

Main category: cs.LO

TL;DR: 本文在Dafny验证感知编程语言中提供了两种归并排序版本的完整认证，通过分治模式推导出递归和迭代实现，并展示了该方法对快速排序变体的适用性。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过分治模式系统性地推导和验证排序算法，特别是归并排序的递归和迭代版本，以提供形式化验证的方法论。

Method: 使用分治模式框架，从预条件和后条件规范推导归并排序算法，设计循环不变量来开发无需栈的迭代版本（自底向上归并排序），并在Dafny中进行形式化验证。

Result: 成功实现了归并排序的递归和迭代版本的完整形式化验证，证明了分治模式在算法推导和验证中的有效性。

Conclusion: 提供的分治模式框架能够有效指导排序算法的形式化推导和验证，该方法同样适用于快速排序等其他分治算法变体。

Abstract: We provide full certifications of two versions of merge sort of arrays in the
verification-aware programming language Dafny. We start by considering schemas
for applying the divide-and-conquer or partition method of solution to
specifications given by pre- and post-conditions involving linear arrays. We
then derive the merge sort and merging algorithms as instances of these
schemas, thereby arriving at a fully recursive formulation. Further, the
analysis of the tree of subproblems arising from the partition facilitates the
design of loop invariants that allow to derive a fully iterative version
(sometimes called bottom-up merge sort) that does not employ a stack. We show
how the use of the provided schemas conveniently conducts the formalization and
actual verification in Dafny. The whole method is also applicable to deriving
variants of quicksort, which we sketch.

</details>


### [7] [Probabilistically stable revision and comparative probability: a representation theorem and applications](https://arxiv.org/abs/2509.02495)
*Krzysztof Mierzewski*

Main category: cs.LO

TL;DR: 本文提出了概率稳定信念修正算子的表示定理，为Leitgeb的稳定性规则提供了完整的数学表征和选择函数语义，连接了概率论、信念修正理论和比较概率理论。


<details>
  <summary>Details</summary>
Motivation: 研究Leitgeb提出的稳定性规则（通过概率稳定命题来定义分类信念）所生成的信念修正算子的数学特性，为这种非单调逻辑提供完整的理论框架。

Method: 利用比较概率序理论，证明表示定理，提供选择函数语义，并建立概率稳定信念修正算子的完整表征。同时证明了两个独立的比较概率理论结果。

Result: 获得了概率稳定信念修正算子的完整表征定理，提供了选择函数语义，证明了该逻辑具有强单调性但违反AGM公设，并发现了在投票游戏理论和显示偏好理论中的应用。

Conclusion: 该研究为概率稳定的信念修正提供了坚实的数学基础，连接了概率论与信念修正理论，并在比较概率测量和投票理论等领域具有重要应用价值。

Abstract: The stability rule for belief, advocated by Leitgeb [Annals of Pure and
Applied Logic 164, 2013], is a rule for rational acceptance that captures
categorical belief in terms of $\textit{probabilistically stable
propositions}$: propositions to which the agent assigns resiliently high
credence. The stability rule generates a class of $\textit{probabilistically
stable belief revision}$ operators, which capture the dynamics of belief that
result from an agent updating their credences through Bayesian conditioning
while complying with the stability rule for their all-or-nothing beliefs. In
this paper, we prove a representation theorem that yields a complete
characterisation of such probabilistically stable revision operators and
provides a `qualitative' selection function semantics for the (non-monotonic)
logic of probabilistically stable belief revision. Drawing on the theory of
comparative probability orders, this result gives necessary and sufficient
conditions for a selection function to be representable as a
strongest-stable-set operator on a finite probability space. The resulting
logic of probabilistically stable belief revision exhibits strong monotonicity
properties while failing the AGM belief revision postulates and satisfying only
very weak forms of case reasoning. In showing the main theorem, we prove two
results of independent interest to the theory of comparative probability: the
first provides necessary and sufficient conditions for the joint representation
of a pair of (respectively, strict and non-strict) comparative probability
orders. The second result provides a method for axiomatising the logic of ratio
comparisons of the form ``event $A$ is at least $k$ times more likely than
event $B$''. In addition to these measurement-theoretic applications, we point
out two applications of our main result to the theory of simple voting games
and to revealed preference theory.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [8] [Generalised Möbius Categories and Convolution Kleene Algebras](https://arxiv.org/abs/2509.00168)
*James Cranch,Georg Struth,Jana Wagemaker*

Main category: cs.FL

TL;DR: 本文提出了一种基于广义Möbius范畴和形式幂级数星运算的卷积Kleene代数构造方法，解决了在广泛结构上构建卷积Kleene代数的难题


<details>
  <summary>Details</summary>
Motivation: 卷积代数在数学和科学中广泛应用，但在构建基于Kleene代数的卷积代数时，合适的星运算定义一直是主要障碍

Method: 通过将广义Möbius范畴与形式幂级数的经典星运算定义相结合，构建卷积Kleene代数，并扩展到多种变体

Result: 成功构建了测试卷积Kleene代数、模态卷积Kleene代数、并发卷积Kleene代数和更高维卷积Kleene代数等多种实例

Conclusion: 该构造方法为加权和概率程序的验证、高阶重写中的代数推理提供了重要工具，并与加权自动机领域的Conway半环相适应

Abstract: Convolution algebras on maps from structures such as monoids, groups or
categories into semirings, rings or fields abound in mathematics and the
sciences. Of special interest in computing are convolution algebras based on
variants of Kleene algebras, which are additively idempotent semirings equipped
with a Kleene star. Yet an obstacle to the construction of convolution Kleene
algebras on a wide class of structures has so far been the definition of a
suitable star. We show that a generalisation of M\"obius categories combined
with a generalisation of a classical definition of a star for formal power
series allow such a construction. We discuss several instances of this
construction on generalised M\"obius categories: convolution Kleene algebras
with tests, modal convolution Kleene algebras, concurrent convolution Kleene
algebras and higher convolution Kleene algebras (e.g. on strict higher
categories and higher relational monoids). These are relevant to the
verification of weighted and probabilistic sequential and concurrent programs,
using quantitative Hoare logics or predicate transformer algebras, as well as
for algebraic reasoning in higher-dimensional rewriting. We also adapt the
convolution Kleene algebra construction to Conway semirings, which is widely
studied in the context of weighted automata. Finally, we compare the
convolution Kleene algebra construction with a previous construction of
convolution quantales and present concrete example structures in preparation
for future applications.

</details>


### [9] [Computational Exploration of Finite Semigroupoids](https://arxiv.org/abs/2509.00837)
*Attila Egri-Nagy,Chrystopher L. Nehaniv*

Main category: cs.FL

TL;DR: 本文使用关系编程探索有限半群范畴，实现了枚举抽象半群范畴、寻找同态和构造变换表示的声明式解决方案。


<details>
  <summary>Details</summary>
Motivation: 代数自动机理论的最新算法进展引起了人们对半群范畴（半范畴）的关注，这些是类型化计算过程的数学描述，但在自动机背景下尚未得到系统研究。

Method: 使用关系编程来探索有限半群范畴，实现声明式解决方案来枚举抽象半群范畴（部分组合表）、寻找同态和构造（最小）变换表示。

Result: 证明了结合性和一致性类型化是不同的属性，区分了严格和更宽松的同态，并系统枚举了箭头类型半群范畴（具体化的类型结构）。

Conclusion: 通过关系编程方法成功探索了有限半群范畴，为理解这些计算模型提供了数学直觉，并建立了相关性质的区别和分类。

Abstract: Recent algorithmic advances in algebraic automata theory drew attention to
semigroupoids (semicategories). These are mathematical descriptions of typed
computational processes, but they have not been studied systematically in the
context of automata. Here, we use relational programming to explore finite
semigroupoids to improve our mathematical intuition about these models of
computation. We implement declarative solutions for enumerating abstract
semigroupoids (partial composition tables), finding homomorphisms, and
constructing (minimal) transformation representations. We show that
associativity and consistent typing are different properties, distinguish
between strict and more permissive homomorphisms, and systematically enumerate
arrow-type semigroupoids (reified type structures).

</details>


### [10] [A substitution lemma for multiple context-free languages](https://arxiv.org/abs/2509.02117)
*Andrew Duncan,Murray Elder,Lisa Frenkel,Mengfan Lyu*

Main category: cs.FL

TL;DR: 本文提出了一种新的替代引理来证明语言不是多重上下文无关的，并应用于多个语言实例，包括F2×F2的字问题。同时证明了具有多重上下文无关字问题的群具有有理子集成员和交集问题的可解性。


<details>
  <summary>Details</summary>
Motivation: 之前的研究表明无法将标准泵引理推广到多重上下文无关语言，且弱化的广义Ogden引理也不适用于此类语言，因此需要新的判别标准。

Method: 提出了替代引理作为新的判别标准，并应用于具体语言实例进行验证。

Result: 成功证明了包括F2×F2字问题在内的多个语言不是多重上下文无关的，并建立了群的字问题与有理子集问题之间的联系。

Conclusion: 替代引理是判别多重上下文无关语言的有效工具，为这类语言的理论研究提供了新的技术手段。

Abstract: We present a new criterion for proving that a language is not multiple
context-free, which we call a Substitution Lemma. We apply it to show a sample
selection of languages are not multiple context-free, including the word
problem of $F_2\times F_2$.
  Our result is in contrast to Kanazawa et al. [2014, Theory Comput. Syst.] who
proved that it was not possible to generalise the standard pumping lemma for
context-free languages to multiple context-free languages, and Kanazawa [2019,
Inform. and Comput.] who showed a weak variant of generalised Ogden's lemma
does not apply to multiple context-free languages.
  We also show that groups with multiple context-free word problem have
rational subset membership and intersection problems.

</details>


### [11] [DTMC Model Checking by Path Abstraction Revisited (extended version)](https://arxiv.org/abs/2509.02393)
*Arnd Hartmanns,Robert Modderman*

Main category: cs.FL

TL;DR: 本文完善和扩展了路径抽象方法，证明了在离散时间马尔可夫链中计算到达目标状态概率时，可以沿任意有限非目标状态集序列进行路径抽象分割计算，且结果与直接方法一致。


<details>
  <summary>Details</summary>
Motivation: 在概率模型检测中，计算到达目标状态的概率是核心任务。虽然可以直接计算所有有限路径的概率质量，但在细化反例时，计算路径子集的概率质量也很重要。Abraham等人2010年提出的路径抽象方法需要进一步完善和理论证明。

Method: 通过将DTMC解释为状态空间自由幺半群上的结构，提供了一种新颖的证明方法。证明了路径抽象可以沿着任意有限非目标状态集序列进行分割计算，而不需要遵循SCC结构。

Result: 证明了路径抽象分割计算与直接方法结果一致，且分割方式更加灵活。提供了PARI/GP中的紧凑参考实现。

Conclusion: 路径抽象方法具有理论保证和实际可行性，为概率模型检测中的路径概率计算提供了更灵活和有效的方法。

Abstract: Computing the probability of reaching a set of goal states G in a
discrete-time Markov chain (DTMC) is a core task of probabilistic model
checking. We can do so by directly computing the probability mass of the set of
all finite paths from the initial state to G; however, when refining
counterexamples, it is also interesting to compute the probability mass of
subsets of paths. This can be achieved by splitting the computation into path
abstractions that calculate "local" reachability probabilities as shown by
\'Abrah\'am et al. in 2010. In this paper, we complete and extend their work:
We prove that splitting the computation into path abstractions indeed yields
the same result as the direct approach, and that the splitting does not need to
follow the SCC structure. In particular, we prove that path abstraction can be
performed along any finite sequence of sets of non-goal states. Our proofs
proceed in a novel way by interpreting the DTMC as a structure on the free
monoid on its state space, which makes them clean and concise. Additionally, we
provide a compact reference implementation of path abstraction in PARI/GP.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [12] [LLM-based Triplet Extraction for Automated Ontology Generation in Software Engineering Standards](https://arxiv.org/abs/2509.00140)
*Songhui Yue*

Main category: cs.SE

TL;DR: 本文提出了一种基于大语言模型的开源方法，用于从软件工程标准文档中提取关系三元组，支持自动化本体生成。


<details>
  <summary>Details</summary>
Motivation: 软件工程标准文档长、非结构化且噪音高，传统方法难以处理。自动化本体生成需要先进的关系提取技术来支持知识表示和白盒推理。

Method: 提出了一个包含文档分割、候选术语挖掘、LLM基于关系推理、术辫规范化和跨部分对齐的有效工作流程，采用LLM辅助方式而非仅依赖提示工程。

Result: 构建了三个粒度级别的黄金标准测试集，评估结果显示该方法与OpenIE三元组提取方法相比具有可比性且潜在优势。

Conclusion: LLM辅助方法在处理软件工程标准文档时能够有效支持关系三元组提取，为自动化本体生成提供了一种有效的工作流程。

Abstract: Ontologies have supported knowledge representation and whitebox reasoning for
decades; thus, the automated ontology generation (AOG) plays a crucial role in
scaling their use. Software engineering standards (SES) consist of long,
unstructured text (with high noise) and paragraphs with domain-specific terms.
In this setting, relation triple extraction (RTE), together with term
extraction, constitutes the first stage toward AOG. This work proposes an
open-source large language model (LLM)-assisted approach to RTE for SES.
Instead of solely relying on prompt-engineering-based methods, this study
promotes the use of LLMs as an aid in constructing ontologies and explores an
effective AOG workflow that includes document segmentation, candidate term
mining, LLM-based relation inference, term normalization, and cross-section
alignment. Golden-standard benchmarks at three granularities are constructed
and used to evaluate the ontology generated from the study. The results show
that it is comparable and potentially superior to the OpenIE method of triple
extraction.

</details>


### [13] [LLM-Based Program Generation for Triggering Numerical Inconsistencies Across Compilers](https://arxiv.org/abs/2509.00256)
*Yutong Wang,Cindy Rubio-González*

Main category: cs.SE

TL;DR: LLM4FP是首个使用大语言模型生成浮点程序以触发编译器不一致性的框架，相比现有工具Varity检测到两倍多的不一致性，主要涉及实数值差异而非极端值。


<details>
  <summary>Details</summary>
Motivation: 不同编译器间的浮点不一致性会破坏数值软件的可靠性，需要有效工具来检测这些不一致性。

Method: 结合语法生成和基于反馈的变异，使用大语言模型生成多样化且有效的浮点程序。

Result: 在多个编译器和优化级别下，LLM4FP检测到的不一致性是Varity的两倍多，主要发现实数值差异，并在更广的优化级别范围和主机-设备编译器间发现最多不匹配。

Conclusion: LLM引导的程序生成显著提高了数值不一致性的检测能力。

Abstract: Floating-point inconsistencies across compilers can undermine the reliability
of numerical software. We present LLM4FP, the first framework that uses Large
Language Models (LLMs) to generate floating-point programs specifically
designed to trigger such inconsistencies. LLM4FP combines Grammar-Based
Generation and Feedback-Based Mutation to produce diverse and valid programs.
We evaluate LLM4FP across multiple compilers and optimization levels, measuring
inconsistency rate, time cost, and program diversity. LLM4FP detects over twice
as many inconsistencies compared to the state-of-the-art tool, Varity. Notably,
most of the inconsistencies involve real-valued differences, rather than
extreme values like NaN or infinities. LLM4FP also uncovers inconsistencies
across a wider range of optimization levels, and finds the most mismatches
between host and device compilers. These results show that LLM-guided program
generation improves the detection of numerical inconsistencies.

</details>


### [14] [JS-TOD: Detecting Order-Dependent Flaky Tests in Jest](https://arxiv.org/abs/2509.00466)
*Negar Hashemi,Amjed Tahir,Shawn Rasheed,August Shi,Rachel Blagojevic*

Main category: cs.SE

TL;DR: JS-TOD是一个用于检测Jest测试中顺序依赖导致测试不稳定的工具，通过随机化测试顺序来揭示测试间的依赖问题


<details>
  <summary>Details</summary>
Motivation: 测试顺序依赖是导致测试不稳定的主要原因之一，理想情况下每个测试应该独立运行且结果一致，但实践中测试结果会因执行顺序而变化

Method: 采用系统化方法随机化测试、测试套件和describe块的执行顺序，支持自定义排序次数和重运行次数（默认每个测试和测试套件10次重排序和10次重运行）

Result: 评估发现测试顺序依赖不稳定的两个主要原因：测试间共享文件和共享模拟状态

Conclusion: JS-TOD能够有效检测JavaScript测试中的顺序依赖问题，帮助识别和修复导致测试不稳定的根本原因

Abstract: We present JS-TOD (JavaScript Test Order-dependency Detector), a tool that
can extract, reorder, and rerun Jest tests to reveal possible order-dependent
test flakiness. Test order dependency is one of the leading causes of test
flakiness. Ideally, each test should operate in isolation and yield consistent
results no matter the sequence in which tests are run. However, in practice,
test outcomes can vary depending on their execution order. JS-TOD employed a
systematic approach to randomising tests, test suites, and describe blocks. The
tool is highly customisable, as one can set the number of orders and reruns
required (the default setting is 10 reorder and 10 reruns for each test and
test suite). Our evaluation using JS-TOD reveals two main causes of test order
dependency flakiness: shared files and shared mocking state between tests.

</details>


### [15] [Bug Whispering: Towards Audio Bug Reporting](https://arxiv.org/abs/2509.00785)
*Elena Masserini,Daniela Micucci,Leonardo Mariani*

Main category: cs.SE

TL;DR: 该论文探讨了通过音频消息记录和提交bug报告的方法，分析了音频bug报告的独特挑战，并呼吁进一步研究音频bug报告的收集和分析技术。


<details>
  <summary>Details</summary>
Motivation: 移动应用中的bug报告是重要功能，但现有文本报告方式可能限制用户参与。音频记录简单易用，有望增加bug报告数量，从而帮助开发者更快识别和修复问题。

Method: 基于初步实验探讨音频bug报告的特点，分析其对现有bug重现技术的挑战。

Result: 音频bug报告具有特定特征，这些特征对现有的bug重现技术构成了挑战，需要专门的处理方法。

Conclusion: 音频bug报告是一种有前景的bug报告方式，但需要进一步研究来开发专门的收集和分析技术，以克服其特有的挑战。

Abstract: Bug reporting is a key feature of mobile applications, as it enables
developers to collect information about faults that escaped testing and thus
affected end-users. This paper explores the idea of allowing end-users to
immediately report the problems that they experience by recording and
submitting audio messages. Audio recording is simple to implement and has the
potential to increase the number of bug reports that development teams can
gather, thus potentially improving the rate at which bugs are identified and
fixed. However, audio bug reports exhibit specific characteristics that
challenge existing techniques for reproducing bugs. This paper discusses these
challenges based on a preliminary experiment, and motivates further research on
the collection and analysis of audio-based bug reports

</details>


### [16] [REConnect: Participatory RE that Matters](https://arxiv.org/abs/2509.01006)
*Daniela Damian,Bachan Ghimire,Ze Shi Li*

Main category: cs.SE

TL;DR: REConnect提出了一种以人为中心的参与式需求工程方法，强调在文化和社会背景下建立信任关系、共同设计和赋能用户，确保系统符合人类价值观和实际需求。


<details>
  <summary>Details</summary>
Motivation: 当前流行的CrowdRE和AI辅助需求获取方法存在脱离文化、社会和政治背景的风险，可能导致需求工作与真实用户需求和价值观不符。

Method: 基于三个社会影响案例研究，提出REConnect框架，包含建立信任关系、与利益相关者共同设计、赋能用户作为变革推动者三个核心原则，以及一套可操作的实践方法(REActions)。

Result: REConnect能够产生文化基础扎实、社会合法且可持续的需求，在系统交付后仍能保持有效性。

Conclusion: 在生成式AI时代，AI集成必须由参与式实践指导，保持人类主体性，让人类成为价值观和伦理的守护者、包容性放大器、AI输出策展人以及迭代评审的共同反思者。

Abstract: Software increasingly shapes the infrastructures of daily life, making
requirements engineering (RE) central to ensuring that systems align with human
values and lived experiences. Yet, current popular practices such as CrowdRE
and AI-assisted elicitation strategies risk detaching requirements work from
the cultural, social, and political contexts that shape lived experiences,
human values, and real user needs. In this paper, we introduce REConnect that
re-centers RE on the human connection as central to the understanding of lived
experiences where impact is sought. REConnect advocates for a human-centered
participatory approach "that matters" to the communities and beneficiaries
involved, ensuring alignment with their values and aspirations. Drawing on
three case studies of societal impact: BloodSync in rural Nepal, Herluma
supporting women at risk of homelessness in Canada, and BridgingRoots to
revitalize Indigenous languages in the Canadian Arctic. REConnect argues that
three key principles and enablers: building trusting relationships,
co-designing with and alongside stakeholders, and empowering users as agents of
change, can yield requirements that are culturally grounded, socially
legitimate, and sustainable beyond system delivery. REConnect also proposes a
set of actionable practices (REActions) that embed relationality and ongoing
stakeholder engagement throughout requirements elicitation, analysis, and
validation of solution development. Finally, we situate REConnect in the era of
Generative AI. While AI can accelerate and scale certain RE tasks, its
integration must be guided by participatory practices that not only preserve
human agency but also empower humans' roles to become guardians of values and
ethics, inclusion amplifiers, curators of AI outputs, and co-reflectors in
iterative review cycles.

</details>


### [17] [Generative Goal Modeling](https://arxiv.org/abs/2509.01048)
*Ateeq Sharfuddin,Travis Breaux*

Main category: cs.SE

TL;DR: 使用GPT-4o从访谈记录中自动提取目标并构建目标模型，准确率达到62%的目标提取匹配度和98.7%的溯源准确性。


<details>
  <summary>Details</summary>
Motivation: 传统需求获取方法中，业务分析师需要手动审查访谈记录来识别和记录需求，这个过程耗时且容易出错。目标建模是表示早期利益相关者需求的流行技术，但手动构建目标模型效率低下。

Method: 采用文本蕴含(textual entailment)技术，利用GPT-4o从访谈记录中自动提取目标并构建目标模型。方法在15个访谈记录和29个应用领域进行了评估。

Result: GPT-4o能够可靠地从访谈记录中提取目标，与人工提取的目标匹配率达到62.0%，目标溯源准确性达到98.7%。在目标模型细化关系生成方面，人工评估显示准确率达到72.2%。

Conclusion: 该方法展示了使用大型语言模型自动化需求获取和目标建模的可行性，能够显著提高软件工程中需求获取的效率，同时保持较高的准确性。

Abstract: In software engineering, requirements may be acquired from stakeholders
through elicitation methods, such as interviews, observational studies, and
focus groups. When supporting acquisition from interviews, business analysts
must review transcripts to identify and document requirements. Goal modeling is
a popular technique for representing early stakeholder requirements as it lends
itself to various analyses, including refinement to map high-level goals into
software operations, and conflict and obstacle analysis. In this paper, we
describe an approach to use textual entailment to reliably extract goals from
interview transcripts and to construct goal models. The approach has been
evaluated on 15 interview transcripts across 29 application domains. The
findings show that GPT-4o can reliably extract goals from interview
transcripts, matching 62.0% of goals acquired by humans from the same
transcripts, and that GPT-4o can trace goals to originating text in the
transcript with 98.7% accuracy. In addition, when evaluated by human
annotators, GPT-4o generates goal model refinement relationships among
extracted goals with 72.2% accuracy.

</details>


### [18] [A Survey on the Techniques and Tools for Automated Requirements Elicitation and Analysis of Mobile Apps](https://arxiv.org/abs/2509.01068)
*Chong Wang,Haoning Wu,Peng Liang,Maya Daneva,Marten van Sinderen*

Main category: cs.SE

TL;DR: 本研究通过系统映射研究分析了73篇论文，发现移动应用自动化需求获取与分析主要使用半自动技术，最流行的工具是开源和非自研工具，主要应用于需求分析、挖掘和分类三大任务。


<details>
  <summary>Details</summary>
Motivation: 虽然RE研究人员和实践者提出了许多移动应用自动化需求获取与分析的技术和工具，但对其特性以及支持的RE任务缺乏系统了解，需要全面调查该领域的最新技术和工具状态。

Method: 采用Kitchenham等人的指南进行系统映射研究，分析了73篇相关论文。

Result: 发现最常用的技术是半自动技术，工具主要特点是开源和非自研，主要用于需求分析和文本预处理。需求分析、挖掘和分类是最受关注的三大RE任务。

Conclusion: 移动应用自动化需求获取与分析领域的技术和工具使用呈增长趋势，半自动技术占主导，需求分析、挖掘和分类是主要应用场景，开源和非自研工具最受欢迎。

Abstract: [Background:] Research on automated requirements elicitation and analysis of
mobile apps employed lots of techniques and tools proposed by RE researchers
and practitioners. However, little is known about the characteristics of these
techniques and tools as well as the RE tasks in requirements elicitation and
analysis that got supported with the help of respective techniques and tools.
[Aims:] The goal of this paper is to investigate the state-of-the-art of the
techniques and tools used in automated requirements elicitation and analysis of
mobile apps. [Method:] We carried out a systematic mapping study by following
the guidelines of Kitchenham et al. [Results:] Based on 73 selected papers, we
found the most frequently used techniques - semi-automatic techniques, and the
main characteristics of the tools - open-sourced and non-self-developed tools
for requirements analysis and text pre-processing. Plus, the most three
investigated RE tasks are requirements analysis, mining and classification.
[Conclusions:] Our most important conclusions are: (1) there is a growth in the
use of techniques and tools in automated requirements elicitation and analysis
of mobile apps, (2) semi-automatic techniques are mainly used in the
publications on this research topic, (3) requirements analysis, mining and
classification are the top three RE tasks with the support of automatic
techniques and tools, and (4) the most popular tools are open-sourced and
non-self-developed, and they are mainly used in requirements analysis and text
processing.

</details>


### [19] [Compiler Bugs Detection in Logic Synthesis Tools via Linear Upper Confidence Bound](https://arxiv.org/abs/2509.01149)
*Hui Zeng,Zhihao Xu,Hui Li,Siwen Wang,Qian Ma*

Main category: cs.SE

TL;DR: Lin-Hunter是一个针对FPGA逻辑综合工具的测试框架，通过元形态转换规则生成结构多样化的HDL测试用例，并采用LinUCB自适应策略选择机制提高bug发现效率。


<details>
  <summary>Details</summary>
Motivation: 现有的FPGA逻辑综合工具测试方法过度依赖随机选择策略，导致生成的HDL测试用例结构多样性不足，无法充分探索工具的功能空间，可能遗漏重要bug。

Method: 提出基于元形态转换规则的测试用例生成方法，创建功能等效但结构多样化的HDL变体；集成基于LinUCB的自适应策略选择机制，根据历史合成日志动态优先选择可能触发bug的转换策略。

Result: 在三个月实验中发现了18个独特bug，其中10个是先前未报告的缺陷，已获官方开发者确认；在测试用例多样性和bug发现效率方面均优于现有最先进方法。

Conclusion: Lin-Hunter通过系统化的测试用例多样性增强和自适应策略选择，显著提高了FPGA逻辑综合工具的验证效率和可靠性，为EDA工具的质量保障提供了有效解决方案。

Abstract: Field-Programmable Gate Arrays (FPGAs) play an indispensable role in
Electronic Design Automation (EDA), translating Register-Transfer Level (RTL)
designs into gate-level netlists. The correctness and reliability of FPGA logic
synthesis tools are critically important, as unnoticed bugs in these tools may
infect the final hardware implementations. However, recent approaches often
rely heavily on random selection strategies, limiting the structural diversity
of the generated HDL test cases and resulting in inadequate exploration of the
tool's feature space. To address this limitation, we propose Lin-Hunter, a
novel testing framework designed to systematically enhance the diversity of HDL
test cases and the efficiency of FPGA logic synthesis tool validation.
Specifically, Lin-Hunter introduces a principled set of metamorphic
transformation rules to generate functionally equivalent yet structurally
diverse HDL test case variants, effectively addressing the limited diversity of
existing test inputs. To further enhance bug discovery efficiency, Lin-Hunter
integrates an adaptive strategy selection mechanism based on the Linear Upper
Confidence Bound (LinUCB) method. This method leverages feedback from synthesis
logs of previously executed test cases to dynamically prioritize transformation
strategies that have empirically demonstrated a higher likelihood of triggering
synthesis bugs. Comprehensive experiments conducted over a three-month period
demonstrate the practical effectiveness of Lin-Hunter. Our method has
discovered 18 unique bugs, including 10 previously unreported defects, which
have been confirmed by official developers. Moreover, our method outperforms
state-of-the-art testing methods in both test-case diversity and bug-discovery
efficiency.

</details>


### [20] [Policy-driven Software Bill of Materials on GitHub: An Empirical Study](https://arxiv.org/abs/2509.01255)
*Oleksii Novikov,Davide Fucci,Oleksandr Adamov,Daniel Mendez*

Main category: cs.SE

TL;DR: 只有0.56%的GitHub仓库包含政策驱动的SBOM文件，这些SBOM中声明的依赖包含2202个独特漏洞，22%的依赖包缺少许可证信息


<details>
  <summary>Details</summary>
Motivation: 了解开源项目中政策驱动的SBOM（软件材料清单）的当前状况，这类SBOM主要用于安全目标而非工具配套或学术研究

Method: 通过挖掘软件仓库研究，收集和精选GitHub上的SBOM文件，使用描述性统计分析SBOM中的信息和漏洞

Result: 政策驱动SBOM在开源项目中应用极其稀缺，声明的依赖包含大量漏洞，且许可证信息缺失问题严重

Conclusion: 研究结果为SBOM在安全评估和许可证管理方面的应用提供了重要见解

Abstract: Background. The Software Bill of Materials (SBOM) is a machine-readable list
of all the software dependencies included in a software. SBOM emerged as way to
assist securing the software supply chain. However, despite mandates from
governments to use SBOM, research on this artifact is still in its early
stages. Aims. We want to understand the current state of SBOM in open-source
projects, focusing specifically on policy-driven SBOMs, i.e., SBOM created to
achieve security goals, such as enhancing project transparency and ensuring
compliance, rather than being used as fixtures for tools or artificially
generated for benchmarking or academic research purposes. Method. We performed
a mining software repository study to collect and carefully select SBOM files
hosted on GitHub. We analyzed the information reported in policy-driven SBOMs
and the vulnerabilities associated with the declared dependencies by means of
descriptive statistics. Results. We show that only 0.56% of popular GitHub
repositories contain policy-driven SBOM. The declared dependencies contain
2,202 unique vulnerabilities, while 22% of them do not report licensing
information. Conclusion. Our findings provide insights for SBOM usage to
support security assessment and licensing.

</details>


### [21] [Metamorphic Testing of Multimodal Human Trajectory Prediction](https://arxiv.org/abs/2509.01294)
*Helge Spieker,Nadjib Lazaar,Arnaud Gotlieb,Nassim Belmecheri*

Main category: cs.SE

TL;DR: 本文提出了一种基于蜕变测试的方法来测试多模态人类轨迹预测系统，解决了无测试预言的问题，通过定义五种蜕变关系来验证模型对输入变换的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 人类轨迹预测对自动驾驶系统安全至关重要，但由于多模态输入和随机输出特性，传统测试方法面临测试预言缺失的挑战，需要新的系统化测试方法。

Method: 提出了五种蜕变关系：1）保持标签的几何变换（镜像、旋转、缩放）；2）地图语义标签变换；3）引入障碍物。使用Wasserstein或Hellinger距离等概率分布度量来定义违反准则。

Result: 开发了MT框架工具，能够在不依赖真实轨迹的情况下评估多模态随机HTP系统的鲁棒性，有效解决了测试预言问题。

Conclusion: 蜕变测试为多模态人类轨迹预测系统提供了一种有效的无预言测试方法，能够系统化验证模型对输入变换和上下文变化的鲁棒性。

Abstract: Context: Predicting human trajectories is crucial for the safety and
reliability of autonomous systems, such as automated vehicles and mobile
robots. However, rigorously testing the underlying multimodal Human Trajectory
Prediction (HTP) models, which typically use multiple input sources (e.g.,
trajectory history and environment maps) and produce stochastic outputs
(multiple possible future paths), presents significant challenges. The primary
difficulty lies in the absence of a definitive test oracle, as numerous future
trajectories might be plausible for any given scenario. Objectives: This
research presents the application of Metamorphic Testing (MT) as a systematic
methodology for testing multimodal HTP systems. We address the oracle problem
through metamorphic relations (MRs) adapted for the complexities and stochastic
nature of HTP. Methods: We present five MRs, targeting transformations of both
historical trajectory data and semantic segmentation maps used as an
environmental context. These MRs encompass: 1) label-preserving geometric
transformations (mirroring, rotation, rescaling) applied to both trajectory and
map inputs, where outputs are expected to transform correspondingly. 2)
Map-altering transformations (changing semantic class labels, introducing
obstacles) with predictable changes in trajectory distributions. We propose
probabilistic violation criteria based on distance metrics between probability
distributions, such as the Wasserstein or Hellinger distance. Conclusion: This
study introduces tool, a MT framework for the oracle-less testing of
multimodal, stochastic HTP systems. It allows for assessment of model
robustness against input transformations and contextual changes without
reliance on ground-truth trajectories.

</details>


### [22] [Aligning Requirement for Large Language Model's Code Generation](https://arxiv.org/abs/2509.01313)
*Zhao Tian,Junjie Chen*

Main category: cs.SE

TL;DR: Specine是一种新颖的规范对齐技术，通过识别未对齐的输入规范、提升LLM感知的规范并进行对齐，显著提升LLM代码生成性能，在多个基准测试中平均提升29.60%


<details>
  <summary>Details</summary>
Motivation: 现有基于代理的技术忽视了规范感知这一关键问题，导致LLM生成的代码与编程规范持续存在不对齐问题，而准确的规范感知是LLM代码生成范式的基础

Method: 借鉴软件需求工程，提出Specine技术，包括识别未对齐输入规范、提升LLM感知规范、进行规范对齐三个关键步骤

Result: 在4个最先进LLM和5个具有挑战性的竞争基准测试中，与10个最先进基线相比，Specine表现优异，Pass@1指标平均提升29.60%

Conclusion: Specine通过规范对齐有效解决了LLM代码生成中的规范感知问题，显著提升了代码生成质量和对齐度

Abstract: Code generation refers to the automatic generation of source code based on a
given programming specification, which has garnered significant attention
particularly with the advancement of large language models (LLMs). However, due
to the inherent complexity of real-world problems, the LLM-generated code often
fails to fully align with the provided specification. While state-of-the-art
agent-based techniques have been proposed to enhance LLM code generation, they
overlook the critical issue of specification perception, resulting in
persistent misalignment issues. Given that accurate perception of programming
specifications serves as the foundation of the LLM-based code generation
paradigm, ensuring specification alignment is particularly crucial. In this
work, we draw on software requirements engineering to propose Specine, a novel
specification alignment technique for LLM code generation. Its key idea is to
identify misaligned input specifications, lift LLM-perceived specifications,
and align them to enhance the code generation performance of LLMs. Our
comprehensive experiments on four state-of-the-art LLMs across five challenging
competitive benchmarks by comparing with ten state-of-the-art baselines,
demonstrate the effectiveness of Specine. For example, Specine outperforms the
most effective baseline, achieving an average improvement of 29.60\% across all
subjects in terms of Pass@1.

</details>


### [23] [Leveraging SystemC-TLM-based Virtual Prototypes for Embedded Software Fuzzing](https://arxiv.org/abs/2509.01318)
*Chiara Ghinami,Jonas Winzer,Nils Bosbach,Lennart M. Reimann,Lukas Jünger,Simon Wörner,Rainer Leupers*

Main category: cs.SE

TL;DR: 一个提供SystemC虚拟原型与AFL模糊测试集成的框架，通过解耦模糊器和仿真器来支持嵌入式软件测试


<details>
  <summary>Details</summary>
Motivation: 解决现有测试方案中模糊器与仿真器耦合过突、缺乏硬件外设支持和灵活性不足的问题，以支持嵌入式软件的模糊测试

Method: 设计了一个解耦框架，包含一个harness来分离AFL模糊器和SystemC仿真器，拦截外设访问并向模糊器请求值，将外设行为与模糊器联系起来

Result: 实现了仿真环境中外设的灵活可替换性，支持不同SystemC虚拟原型的接口，通过与多个仿真器集成和测试多种软件证明了框架的灵活性

Conclusion: 该框架有效解决了嵌入式软件模糊测试的限制，提供了灵活的模糊器-仿真器集成方案，为嵌入式系统的自动化测试提供了新的可能性

Abstract: SystemC-based virtual prototypes have emerged as widely adopted tools to test
software ahead of hardware availability, reducing the time-to-market and
improving software reliability. Recently, fuzzing has become a popular method
for automated software testing due to its ability to quickly identify
corner-case errors. However, its application to embedded software is still
limited. Simulator tools can help bridge this gap by providing a more powerful
and controlled execution environment for testing. Existing solutions, however,
often tightly couple fuzzers with built-in simulators that lack support for
hardware peripherals and of- fer limited flexibility, restricting their ability
to test embedded software. To address these limitations, we present a framework
that allows the integration of American-Fuzzy-Lop-based fuzzers and
SystemC-based simulators. The framework provides a harness to decouple the
adopted fuzzer and simulator. In addition, it intercepts peripheral accesses
and queries the fuzzer for values, effectively linking peripheral behavior to
the fuzzer. This solution enables flexible interchangeability of peripher- als
within the simulation environment and supports the interfacing of different
SystemC-based virtual prototypes. The flexibility of the pro- posed solution is
demonstrated by integrating the harness with different simulators and by
testing various softwares.

</details>


### [24] [Towards Multi-Platform Mutation Testing of Task-based Chatbots](https://arxiv.org/abs/2509.01389)
*Diego Clerissi,Elena Masserini,Daniela Micucci,Leonardo Mariani*

Main category: cs.SE

TL;DR: MUTABOT是一个针对任务型聊天机器人的变异测试方法，通过在对话中注入故障来模拟缺陷，帮助发现测试套件的弱点。


<details>
  <summary>Details</summary>
Motivation: 任务型聊天机器人测试困难，传统测试方法难以覆盖所有可能的对话场景，导致一些错误行为无法被发现。

Method: 扩展MUTABOT方法到多个平台（Dialogflow和Rasa），通过变异测试在对话中注入故障，生成有缺陷的聊天机器人来模拟真实缺陷。

Result: 实验表明变异测试能够有效揭示Botium最先进测试生成器生成的测试套件中的弱点。

Conclusion: MUTABOT的扩展版本为多平台任务型聊天机器人提供了有效的变异测试方法，能够帮助发现测试覆盖不足的问题。

Abstract: Chatbots, also known as conversational agents, have become ubiquitous,
offering services for a multitude of domains. Unlike general-purpose chatbots,
task-based chatbots are software designed to prioritize the completion of tasks
of the domain they handle (e.g., flight booking). Given the growing popularity
of chatbots, testing techniques that can generate full conversations as test
cases have emerged. Still, thoroughly testing all the possible conversational
scenarios implemented by a task-based chatbot is challenging, resulting in
incorrect behaviors that may remain unnoticed. To address this challenge, we
proposed MUTABOT, a mutation testing approach for injecting faults in
conversations and producing faulty chatbots that emulate defects that may
affect the conversational aspects. In this paper, we present our extension of
MUTABOT to multiple platforms (Dialogflow and Rasa), and present experiments
that show how mutation testing can be used to reveal weaknesses in test suites
generated by the Botium state-of-the-art test generator.

</details>


### [25] [Non Technical Debt in Agile Software Development](https://arxiv.org/abs/2509.01445)
*Muhammad Ovais Ahmad,Tomas Gustavsson*

Main category: cs.SE

TL;DR: 非技术债务（NTD）是敏捷软件开发中的常见挑战，包括流程债务、社会债务、人员债务和组织债务四种形式。研究发现结构化团队、心理安全、清晰角色和有效流程对团队绩效至关重要。


<details>
  <summary>Details</summary>
Motivation: 研究非技术债务在大型敏捷软件开发环境中的影响，识别关键驱动因素及其对团队绩效的破坏性影响，为组织提供实践指导。

Method: 采用问卷调查、深度访谈和统计分析的方法，与瑞典工业合作伙伴合作，收集了大量软件专业人士的数据。

Result: 发现五个关键发现：1）结构化团队学习更快、适应更强；2）心理安全对创新至关重要；3）低效流程降低满意度；4）社会碎片化导致返工和延迟；5）人力资源不足限制组织能力。

Conclusion: 提出了基于证据的实践策略，包括优化团队组成、明确角色、培养心理安全、简化工作流程和将失败视为学习工具，帮助组织减少非技术债务并释放团队潜力。

Abstract: NonTechnical Debt (NTD) is a common challenge in agile software development,
manifesting in four critical forms, Process Debt, Social Debt, People Debt,
Organizational debt. NODLA project is a collaboration between Karlstad
University and four leading Swedish industrial partners, reveals how various
debt types disrupt large scale Agile Software Development (ASD) environments.
Through extensive surveys, indepth interviews, and statistical analyses
involving a diverse group of software professionals, we identified key drivers
of NTD and their impacts. Our findings emphasize (1) Well structured, highly
cohesive teams learn faster, adapt more effectively, and innovate consistently.
(2) Psychological safety, fostered by proactive leadership, is essential for
innovation, experimentation, and keeping employees. (3) Inefficient processes
and unclear roles contribute significantly to drops in job satisfaction,
productivity and team morale. (4) Social fragmentation, particularly in remote
and hybrid settings, breeds rework, delays, and increased costs. (5) Neglected
human resource needs, such as delayed hiring or insufficient training, limit an
organization ability to meet growing demands. This white paper distils these
insights into practical, evidence based strategies, such as refining team
composition, clarifying roles, fostering psychological safety, streamlining
workflows, and embracing failure as a learning tool. By implementing these
strategies, organizations can reduce NTD, reclaim agility, and unlock their
teams full potential.

</details>


### [26] [Benchmarking and Studying the LLM-based Code Review](https://arxiv.org/abs/2509.01494)
*Zhengran Zeng,Ruikai Shi,Keke Han,Yixin Li,Kaicheng Sun,Yidong Wang,Zhuohao Yu,Rui Xie,Wei Ye,Shikun Zhang*

Main category: cs.SE

TL;DR: SWRBench是一个新的自动化代码评审基准，包含1000个手动验证的GitHub Pull Requests，提供完整的项目上下文和基于LLM的客观评估方法，与人类判断高度一致（约90%）。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化代码评审基准无法反映真实世界的复杂性，经常关注细粒度代码单元，缺乏完整项目上下文，并使用不充分的评估指标，这阻碍了现代大型语言模型的评估。

Method: 引入SWRBench基准，包含1000个手动验证的GitHub Pull Requests，采用PR中心的评审方式，使用基于LLM的客观评估方法验证结构化真实问题是否在生成的评审中被覆盖。

Result: 当前ACR工具和LLM在SWRBench上表现不佳，ACR工具更擅长检测功能错误。提出的多评审聚合策略显著提升了ACR性能，F1分数最高提升43.67%。

Conclusion: SWRBench基准、其客观评估方法、对当前ACR能力的全面研究以及有效的增强方法，为推进ACR研究提供了有价值的见解。

Abstract: Automated Code Review (ACR) is crucial for software quality, yet existing
benchmarks often fail to reflect real-world complexities, hindering the
evaluation of modern Large Language Models (LLMs). Current benchmarks
frequently focus on fine-grained code units, lack complete project context, and
use inadequate evaluation metrics. To address these limitations, we introduce
SWRBench , a new benchmark comprising 1000 manually verified Pull Requests
(PRs) from GitHub, offering PR-centric review with full project context.
SWRBench employs an objective LLM-based evaluation method that aligns strongly
with human judgment (~90 agreement) by verifying if issues from a structured
ground truth are covered in generated reviews. Our systematic evaluation of
mainstream ACR tools and LLMs on SWRBench reveals that current systems
underperform, and ACR tools are more adept at detecting functional errors.
Subsequently, we propose and validate a simple multi-review aggregation
strategy that significantly boosts ACR performance, increasing F1 scores by up
to 43.67%. Our contributions include the SWRBench benchmark, its objective
evaluation method, a comprehensive study of current ACR capabilities, and an
effective enhancement approach, offering valuable insights for advancing ACR
research.

</details>


### [27] [A Privacy-Preserving Recommender for Filling Web Forms Using a Local Large Language Model](https://arxiv.org/abs/2509.01527)
*Amirreza Nayyeri,Abbas Rasoolzadegan*

Main category: cs.SE

TL;DR: 本文提出一种基于大语言模型的隐私保护式推荐器，用于本地化生成网页表单测试的有效字段值，避免云端LLM带来的数据泄漏风险。


<details>
  <summary>Details</summary>
Motivation: 网络应用在重要领域的普及导致对故障免息性能的需求增加，而手动生成表单测试值时间消耗大且容易出错。虽然LLM工具能更智能地处理这一任务，但云端模型在测试机密网页表单时存在数据泄漏风险。

Method: 开发了一种本地运行的隐私保护推荐器，通过分析表单的HTML结构、检测输入类型、根据字段类型和上下文内容提取约束条件，指导正确填写表单字段。

Result: 该工具能够在保护数据隐私的前提下，为测试人员提供有效的表单字段值建议，支持web表单测试。

Conclusion: 本文提出的本地化LLM工具有效解决了云端模型在测试机密网页表单时的数据泄漏风险，同时保持了LLM在生成有效测试数据方面的智能优势。

Abstract: Web applications are increasingly used in critical domains such as education,
finance, and e-commerce. This highlights the need to ensure their failure-free
performance. One effective method for evaluating failure-free performance is
web form testing, where defining effective test scenarios is key to a complete
and accurate evaluation. A core aspect of this process involves filling form
fields with suitable values to create effective test cases. However, manually
generating these values is time-consuming and prone to errors. To address this,
various tools have been developed to assist testers. With the appearance of
large language models (LLMs), a new generation of tools seeks to handle this
task more intelligently. Although many LLM-based tools have been introduced, as
these models typically rely on cloud infrastructure, their use in testing
confidential web forms raises concerns about unintended data leakage and
breaches of confidentiality. This paper introduces a privacy-preserving
recommender that operates locally using a large language model. The tool
assists testers in web form testing by suggesting effective field values. This
tool analyzes the HTML structure of forms, detects input types, and extracts
constraints based on each field's type and contextual content, guiding proper
field filling.

</details>


### [28] [WFC/WFD: Web Fuzzing Commons, Dataset and Guidelines to Support Experimentation in REST API Fuzzing](https://arxiv.org/abs/2509.01612)
*Omur Sahin,Man Zhang,Andrea Arcuri*

Main category: cs.SE

TL;DR: 提出了Web Fuzzing Commons (WFC)和Web Fuzzing Dataset (WFD)来解决REST API模糊测试中的三个关键挑战：认证处理、故障类型分类和标准化测试用例。


<details>
  <summary>Details</summary>
Motivation: 当前REST API模糊测试研究面临三个主要障碍：如何处理API认证、如何分类和比较不同模糊测试工具发现的故障类型，以及如何建立公平比较的基准测试用例。

Method: 开发了WFC（开源库和模式定义）来声明式指定认证信息和分类故障类型，以及WFD（包含36个开源API的数据集）来支持实验。使用EvoMaster等先进工具进行实验验证。

Result: WFC/WFD为REST API模糊测试提供了标准化框架和测试数据集，能够支持不同工具间的公平比较，并避免了常见的工具比较陷阱。

Conclusion: WFC和WFD解决了REST API模糊测试领域的关键挑战，为研究人员提供了统一的认证处理、故障分类和基准测试框架，促进了该领域的进一步发展。

Abstract: Fuzzing REST APIs is an important research problem, with practical
applications and impact in industry. As such, a lot of research work has been
carried out on this topic in the last few years. However, there are three major
issues that hinder further progress: how to deal with API authentication; how
to catalog and compare different fault types found by different fuzzers; and
what to use as case study to facilitate fair comparisons among fuzzers. To
address these important challenges, we present Web Fuzzing Commons (WFC) and
Web Fuzzing Dataset (WFD). WFC is a set of open-source libraries and schema
definitions to declaratively specify authentication info and catalog different
types of faults that fuzzers can automatically detect. WFD is a collection of
36 open-source APIs with all necessary scaffolding to easily run experiments
with fuzzers, supported by WFC. To show the usefulness of WFC/WFD, a set of
experiments is carried out with EvoMaster, a state-of-the-art fuzzer for Web
APIs. However, any fuzzer can benefit from WFC and WFD. We compare EvoMaster
with other state-of-the-art tools such as ARAT-RL, EmRest, LLamaRestTest,
RESTler, and Schemathesis. We discuss common pitfalls in tool comparisons, as
well as providing guidelines with support of WFC/WFD to avoid them.

</details>


### [29] [Automated Generation of Issue-Reproducing Tests by Combining LLMs and Search-Based Testing](https://arxiv.org/abs/2509.01616)
*Konstantinos Kitsios,Marco Castelluccio,Alberto Bacchelli*

Main category: cs.SE

TL;DR: BLAST是一个结合LLM和基于搜索的软件测试(SBST)的工具，能够从issue-patch对自动生成问题重现测试，在基准测试中达到35.4%的成功率，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 开发人员经常在没有问题重现测试的情况下提交补丁，这使得自动生成此类测试成为重要需求，以增强开发者对问题修复的信心并防止问题重新引入。

Method: 结合LLM和SBST技术：通过git历史分析、静态分析和SBST生成的测试提取相关上下文；使用LLM生成种子测试，然后通过SBST优化生成最终的问题重现测试。

Result: 在426个Python问题中成功生成151个测试(35.4%)，优于现有技术的23.5%；在真实GitHub部署中，32个PR-issue对中有11个成功生成测试。

Conclusion: BLAST证明了结合LLM和SBST自动生成问题重现测试的可行性，为研究者和工具开发者提供了有价值的见解和机会。

Abstract: Issue-reproducing tests fail on buggy code and pass once a patch is applied,
thus increasing developers' confidence that the issue has been resolved and
will not be re-introduced. However, past research has shown that developers
often commit patches without such tests, making the automated generation of
issue-reproducing tests an area of interest. We propose BLAST, a tool for
automatically generating issue-reproducing tests from issue-patch pairs by
combining LLMs and search-based software testing (SBST). For the LLM part, we
complement the issue description and the patch by extracting relevant context
through git history analysis, static analysis, and SBST-generated tests. For
the SBST part, we adapt SBST for generating issue-reproducing tests; the issue
description and the patch are fed into the SBST optimization through an
intermediate LLM-generated seed, which we deserialize into SBST-compatible
form. BLAST successfully generates issue-reproducing tests for 151/426 (35.4%)
of the issues from a curated Python benchmark, outperforming the
state-of-the-art (23.5%). Additionally, to measure the real-world impact of
BLAST, we built a GitHub bot that runs BLAST whenever a new pull request (PR)
linked to an issue is opened, and if BLAST generates an issue-reproducing test,
the bot proposes it as a comment in the PR. We deployed the bot in three
open-source repositories for three months, gathering data from 32 PRs-issue
pairs. BLAST generated an issue-reproducing test in 11 of these cases, which we
proposed to the developers. By analyzing the developers' feedback, we discuss
challenges and opportunities for researchers and tool builders. Data and
material: https://doi.org/10.5281/zenodo.16949042

</details>


### [30] [Tether: A Personalized Support Assistant for Software Engineers with ADHD](https://arxiv.org/abs/2509.01946)
*Aarsh Shah,Cleyton Magalhaes,Kiev Gama,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: Tether是一个基于LLM的桌面应用，专门为ADHD软件工程师设计，通过实时监控、RAG技术和游戏化机制提供个性化支持，改善专注力和任务执行能力。


<details>
  <summary>Details</summary>
Motivation: 当前软件工程中的公平性、多样性和包容性往往忽视神经多样性，特别是ADHD开发者的体验。虽然对该群体的认识在增长，但很少有工具专门支持他们在开发工作流程中的认知挑战。

Method: 结合本地活动监控、检索增强生成(RAG)和游戏化技术，开发了一个LLM驱动的桌面应用。系统集成操作系统级别的跟踪来提示参与，聊天机器人利用ADHD特定资源提供相关响应。

Result: 初步自我验证显示，通过迭代提示优化和RAG增强，上下文准确性得到改善。工具在软件特定工作流程和ADHD相关挑战方面表现出适应性和对齐性。

Conclusion: 这项工作为未来SE中的神经多样性感知工具奠定了基础，突显了LLM作为针对代表性不足认知需求的个性化支持系统的潜力，尽管尚未由目标用户评估。

Abstract: Equity, diversity, and inclusion in software engineering often overlook
neurodiversity, particularly the experiences of developers with Attention
Deficit Hyperactivity Disorder (ADHD). Despite the growing awareness about that
population in SE, few tools are designed to support their cognitive challenges
(e.g., sustained attention, task initiation, self-regulation) within
development workflows. We present Tether, an LLM-powered desktop application
designed to support software engineers with ADHD by delivering adaptive,
context-aware assistance. Drawing from engineering research methodology, Tether
combines local activity monitoring, retrieval-augmented generation (RAG), and
gamification to offer real-time focus support and personalized dialogue. The
system integrates operating system level system tracking to prompt engagement
and its chatbot leverages ADHD-specific resources to offer relevant responses.
Preliminary validation through self-use revealed improved contextual accuracy
following iterative prompt refinements and RAG enhancements. Tether
differentiates itself from generic tools by being adaptable and aligned with
software-specific workflows and ADHD-related challenges. While not yet
evaluated by target users, this work lays the foundation for future
neurodiversity-aware tools in SE and highlights the potential of LLMs as
personalized support systems for underrepresented cognitive needs.

</details>


### [31] [Automated Repair of C Programs Using Large Language Models](https://arxiv.org/abs/2509.01947)
*Mahdi Farzandway,Fatemeh Ghassemi*

Main category: cs.SE

TL;DR: 本研究提出了一个结合频谱故障定位、运行时反馈和思维链提示的自主修复框架，用于自动化修复C程序错误，在Codeflaws基准测试中达到44.93%的修复准确率，比现有最佳方法提升3.61%。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在自动化程序修复中的潜力，将统计程序分析与LLM推理能力相结合，提高自动程序修复的效率和准确性。

Method: 构建了一个集成框架，包含频谱故障定位(SBFL)、运行时反馈和思维链结构化提示的迭代修复循环。模型基于失败测试、可疑代码区域和先前补丁结果进行推理，生成新的候选补丁。

Result: 在Codeflaws基准测试的3,902个错误上实现了44.93%的修复准确率，相比GPT-4 with CoT等先进基线方法有3.61%的绝对提升。

Conclusion: 该方法为将统计程序分析与生成式AI在自动化调试中的集成提供了一条实用路径，通过迭代推理和反馈机制有效减少了重复错误并提高了修复成功率。

Abstract: This study explores the potential of Large Language Models (LLMs) in
automating the repair of C programs. We present a framework that integrates
spectrum-based fault localization (SBFL), runtime feedback, and
Chain-of-Thought-structured prompting into an autonomous repair loop. Unlike
prior approaches, our method explicitly combines statistical program analysis
with LLM reasoning. The iterative repair cycle leverages a structured
Chain-of-Thought (CoT) prompting approach, where the model reasons over failing
tests, suspicious code regions, and prior patch outcomes, before generating new
candidate patches. The model iteratively changes the code, evaluates the
results, and incorporates reasoning from previous attempts into subsequent
modifications, reducing repeated errors and clarifying why some bugs remain
unresolved. Our evaluation spans 3,902 bugs from the Codeflaws benchmark, where
our approach achieves 44.93% repair accuracy, representing a 3.61% absolute
improvement over strong state-of-the-art APR baselines such as GPT-4 with CoT.
This outcome highlights a practical pathway toward integrating statistical
program analysis with generative AI in automated debugging.

</details>


### [32] [ProbTest: Unit Testing for Probabilistic Programs (Extended Version)](https://arxiv.org/abs/2509.02012)
*Katrine Christensen,Mahsa Varshosaz,Raúl Pardo*

Main category: cs.SE

TL;DR: 提出ProbTest方法，基于优惠券收集问题理论，自动确定概率程序测试所需执行次数，提供统计保证


<details>
  <summary>Details</summary>
Motivation: 概率程序的随机性导致测试困难，需要多次执行才能验证期望结果，但传统方法无法确定合适的执行次数

Method: 基于优惠券收集问题理论的黑盒单元测试方法，自动计算所需测试执行次数，集成到PyTest框架中

Result: 实现了PyTest插件，在Gymnasium强化学习库和随机化数据结构案例中验证了方法的有效性

Conclusion: ProbTest为概率程序测试提供了统计保证的自动化解决方案，开发者可以像编写普通单元测试一样工作

Abstract: Testing probabilistic programs is non-trivial due to their stochastic nature.
Given an input, the program may produce different outcomes depending on the
underlying stochastic choices in the program. This means testing the expected
outcomes of probabilistic programs requires repeated test executions unlike
deterministic programs where a single execution may suffice for each test
input. This raises the following question: how many times should we run a
probabilistic program to effectively test it? This work proposes a novel
black-box unit testing method, ProbTest, for testing the outcomes of
probabilistic programs. Our method is founded on the theory surrounding a
well-known combinatorial problem, the coupon collector's problem. Using this
method, developers can write unit tests as usual without extra effort while the
number of required test executions is determined automatically with statistical
guarantees for the results. We implement ProbTest as a plug-in for PyTest, a
well-known unit testing tool for python programs. Using this plug-in,
developers can write unit tests similar to any other Python program and the
necessary test executions are handled automatically. We evaluate the method on
case studies from the Gymnasium reinforcement learning library and a randomized
data structure.

</details>


### [33] [Scalable Thread-Safety Analysis of Java Classes with CodeQL](https://arxiv.org/abs/2509.02022)
*Bjørnar Haugstad Jåtten,Simon Boye Jørgensen,Rasmus Petersen,Raúl Pardo*

Main category: cs.SE

TL;DR: 提出了一种基于Java内存模型的可扩展静态分析方法，用于检测Java类中的线程安全问题，在GitHub顶级项目中发现了数千个线程安全错误。


<details>
  <summary>Details</summary>
Motivation: 在面向对象语言中，开发人员依赖线程安全的类来实现并发应用，但判断类是否线程安全是一个具有挑战性的任务。

Method: 基于Java内存模型的正确性原则（数据竞争自由）定义了线程安全性，设计了一组确保线程安全的属性，并使用CodeQL静态分析工具自动分析Java源代码。

Result: 在GitHub前1000个仓库的3,632,865个Java类中进行分析，检测到数千个线程安全错误，运行时间在2分钟以内，已提交部分错误修复PR并获得开发者积极反馈。

Conclusion: 该方法在真实代码库中分析线程安全性具有适用性和可扩展性，CodeQL查询已提交到主仓库并即将作为GitHub Actions的一部分提供。

Abstract: In object-oriented languages software developers rely on thread-safe classes
to implement concurrent applications. However, determining whether a class is
thread-safe is a challenging task. This paper presents a highly scalable method
to analyze thread-safety in Java classes. We provide a definition of
thread-safety for Java classes founded on the correctness principle of the Java
memory model, data race freedom. We devise a set of properties for Java classes
that are proven to ensure thread-safety. We encode these properties in the
static analysis tool CodeQL to automatically analyze Java source code. We
perform an evaluation on the top 1000 GitHub repositories. The evaluation
comprises 3632865 Java classes; with 1992 classes annotated as @ThreadSafe from
71 repositories. These repositories include highly popular software such as
Apache Flink (24.6k stars), Facebook Fresco (17.1k stars), PrestoDB (16.2k
starts), and gRPC (11.6k starts). Our queries detected thousands of
thread-safety errors. The running time of our queries is below 2 minutes for
repositories up to 200k lines of code, 20k methods, 6000 fields, and 1200
classes. We have submitted a selection of detected concurrency errors as PRs,
and developers positively reacted to these PRs. We have submitted our CodeQL
queries to the main CodeQL repository, and they are currently in the process of
becoming available as part of GitHub actions. The results demonstrate the
applicability and scalability of our method to analyze thread-safety in
real-world code bases.

</details>


### [34] [Curiosity-Driven Testing for Sequential Decision-Making Process](https://arxiv.org/abs/2509.02025)
*Junda He,Zhou Yang,Jieke Shi,Chengran Yang,Kisub Kim,Bowen Xu,Xin Zhou,David Lo*

Main category: cs.SE

TL;DR: CureFuzz是一种新颖的好奇心驱动黑盒模糊测试方法，用于检测顺序决策过程中的不安全行为，通过多目标种子选择技术平衡探索新颖场景和生成崩溃触发场景。


<details>
  <summary>Details</summary>
Motivation: 顺序决策过程在安全关键应用中容易学习到不安全行为，现有测试框架难以识别多样化的崩溃触发场景，需要开发有效的测试方法。

Method: 提出好奇心机制探索新颖多样化场景，引入多目标种子选择技术平衡探索和崩溃场景生成，优化模糊测试过程。

Result: CureFuzz在故障总数和崩溃触发场景类型多样性方面显著优于现有最先进方法，发现的场景可用于修复顺序决策模型。

Conclusion: CureFuzz是测试顺序决策过程和优化其性能的有价值工具，能够有效识别和修复不安全行为。

Abstract: Sequential decision-making processes (SDPs) are fundamental for complex
real-world challenges, such as autonomous driving, robotic control, and traffic
management. While recent advances in Deep Learning (DL) have led to mature
solutions for solving these complex problems, SDMs remain vulnerable to
learning unsafe behaviors, posing significant risks in safety-critical
applications. However, developing a testing framework for SDMs that can
identify a diverse set of crash-triggering scenarios remains an open challenge.
To address this, we propose CureFuzz, a novel curiosity-driven black-box fuzz
testing approach for SDMs. CureFuzz proposes a curiosity mechanism that allows
a fuzzer to effectively explore novel and diverse scenarios, leading to
improved detection of crashtriggering scenarios. Additionally, we introduce a
multi-objective seed selection technique to balance the exploration of novel
scenarios and the generation of crash-triggering scenarios, thereby optimizing
the fuzzing process. We evaluate CureFuzz on various SDMs and experimental
results demonstrate that CureFuzz outperforms the state-of-the-art method by a
substantial margin in the total number of faults and distinct types of
crash-triggering scenarios. We also demonstrate that the crash-triggering
scenarios found by CureFuzz can repair SDMs, highlighting CureFuzz as a
valuable tool for testing SDMs and optimizing their performance.

</details>


### [35] [Txt2Sce: Scenario Generation for Autonomous Driving System Testing Based on Textual Reports](https://arxiv.org/abs/2509.02150)
*Pin Ji,Yang Feng,Zongtai Li,Xiangchi Zhou,Jia Liu,Jun Sun,Zhihong Zhao*

Main category: cs.SE

TL;DR: Txt2Sce是一个基于文本事故报告生成OpenSCENARIO格式测试场景的方法，通过LLM转换、场景分解、变异和重组来生成多样化测试场景，有效检测自动驾驶系统的异常行为。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖视觉数据且缺乏标准化格式，导致测试场景内存需求大、可移植性差。文本事故报告包含有价值的高风险场景信息但未被充分利用。

Method: 使用LLM将文本事故报告转换为OpenSCENARIO文件，通过场景分解、场景块变异和场景重组生成基于推导关系的场景文件树。

Result: 成功生成33个场景文件树共4,373个场景文件，有效检测了Autoware在安全性、智能性和平滑性方面的异常行为。

Conclusion: Txt2Sce能够有效利用文本报告生成标准化测试场景，提高场景多样性并成功识别自动驾驶系统的缺陷。

Abstract: With the rapid advancement of deep learning and related technologies,
Autonomous Driving Systems (ADSs) have made significant progress and are
gradually being widely applied in safety-critical fields. However, numerous
accident reports show that ADSs still encounter challenges in complex
scenarios. As a result, scenario-based testing has become essential for
identifying defects and ensuring reliable performance. In particular,
real-world accident reports offer valuable high-risk scenarios for more
targeted ADS testing. Despite their potential, existing methods often rely on
visual data, which demands large memory and manual annotation. Additionally,
since existing methods do not adopt standardized scenario formats (e.g.,
OpenSCENARIO), the generated scenarios are often tied to specific platforms and
ADS implementations, limiting their scalability and portability. To address
these challenges, we propose Txt2Sce, a method for generating test scenarios in
OpenSCENARIO format based on textual accident reports. Txt2Sce first uses a LLM
to convert textual accident reports into corresponding OpenSCENARIO scenario
files. It then generates a derivation-based scenario file tree through scenario
disassembly, scenario block mutation, and scenario assembly. By utilizing the
derivation relationships between nodes in the scenario tree, Txt2Sce helps
developers identify the scenario conditions that trigger unexpected behaviors
of ADSs. In the experiments, we employ Txt2Sce to generate 33 scenario file
trees, resulting in a total of 4,373 scenario files for testing the open-source
ADS, Autoware. The experimental results show that Txt2Sce successfully converts
textual reports into valid OpenSCENARIO files, enhances scenario diversity
through mutation, and effectively detects unexpected behaviors of Autoware in
terms of safety, smartness, and smoothness.

</details>


### [36] [Formalizing Operational Design Domains with the Pkl Language](https://arxiv.org/abs/2509.02221)
*Martin Skoglund,Fredrik Warg,Anders Thorsén,Sasikumar Punnekkat,Hans Hansson*

Main category: cs.SE

TL;DR: 本文提出使用Pkl语言形式化操作设计域(ODD)规范的方法，解决ODD规范化的核心挑战，提高可用性并通过汽车案例进行说明


<details>
  <summary>Details</summary>
Motivation: 随着自动化功能的发展，需要新的安全评估框架来证明这些功能在真实条件下不会带来不可接受的风险。ODD形式化面临保持规范格式灵活性同时确保一致性和可追溯性的挑战

Method: 使用Pkl配置语言来形式化ODD规范，利用其专门的语言特性来改进可用性，并通过汽车行业的实例进行说明

Result: 开发了一种能够保持规范格式灵活性、确保一致性和可追溯性，并能无缝集成到开发、验证和评估过程中的ODD形式化方法

Conclusion: 提出的Pkl语言ODD形式化方法可以有效解决ODD规范化的核心挑战，不仅适用于汽车行业，还可广泛应用于确保操作环境的严格评估

Abstract: The deployment of automated functions that can operate without direct human
supervision has changed safety evaluation in domains seeking higher levels of
automation. Unlike conventional systems that rely on human operators, these
functions require new assessment frameworks to demonstrate that they do not
introduce unacceptable risks under real-world conditions. To make a convincing
safety claim, the developer must present a thorough justification argument,
supported by evidence, that a function is free from unreasonable risk when
operated in its intended context. The key concept relevant to the presented
work is the intended context, often captured by an Operational Design Domain
specification (ODD). ODD formalization is challenging due to the need to
maintain flexibility in adopting diverse specification formats while preserving
consistency and traceability and integrating seamlessly into the development,
validation, and assessment. This paper presents a way to formalize an ODD in
the Pkl language, addressing central challenges in specifying ODDs while
improving usability through specialized configuration language features. The
approach is illustrated with an automotive example but can be broadly applied
to ensure rigorous assessments of operational contexts.

</details>


### [37] [Methodology for Test Case Allocation based on a Formalized ODD](https://arxiv.org/abs/2509.02311)
*Martin Skoglund,Fredrik Warg,Anders Thoren,Sasikumar Punnekkat,Hans Hansson*

Main category: cs.SE

TL;DR: 提出了一种基于ODD形式化的测试用例分配评估方法，用于评估不同测试环境对CCAM系统安全测试的适用性


<details>
  <summary>Details</summary>
Motivation: CCAM系统的出现改变了安全评估格局，需要新方法来评估其安全性。现有方法在将多样化测试环境需求与不同测试能力对齐方面存在挑战

Method: 扩展现有的ODD形式化方法，整合ODD参数和额外测试属性来捕获测试环境的相关能力，支持自动适用性评估

Result: 通过自动倒车卡车功能的案例研究验证了该方法，将系统实现保真度与ODD参数关联，实现基于环境对象检测传感器评估能力的自动测试用例分配

Conclusion: 该方法能够有效评估测试环境对特定测试用例的适用性，为CCAM系统的安全评估提供了系统化的测试分配解决方案

Abstract: The emergence of Connected, Cooperative, and Automated Mobility (CCAM)
systems has significantly transformed the safety assessment landscape. Because
they integrate automated vehicle functions beyond those managed by a human
driver, new methods are required to evaluate their safety. Approaches that
compile evidence from multiple test environments have been proposed for
type-approval and similar evaluations, emphasizing scenario coverage within the
systems Operational Design Domain (ODD). However, aligning diverse test
environment requirements with distinct testing capabilities remains
challenging. This paper presents a method for evaluating the suitability of
test case allocation to various test environments by drawing on and extending
an existing ODD formalization with key testing attributes. The resulting
construct integrates ODD parameters and additional test attributes to capture a
given test environments relevant capabilities. This approach supports automatic
suitability evaluation and is demonstrated through a case study on an automated
reversing truck function. The system's implementation fidelity is tied to ODD
parameters, facilitating automated test case allocation based on each
environments capacity for object-detection sensor assessment.

</details>


### [38] [ReCode: Improving LLM-based Code Repair with Fine-Grained Retrieval-Augmented Generation](https://arxiv.org/abs/2509.02330)
*Yicong Zhao,Shisong Chen,Jiacheng Zhang,Zhixu Li*

Main category: cs.SE

TL;DR: ReCode是一个细粒度检索增强的代码修复框架，通过算法感知检索和模块化双编码器架构，显著提高修复准确性并降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有代码修复方法存在训练成本高或推理计算昂贵的问题，传统检索策略无法捕捉代码结构细节，导致检索质量不佳。

Method: 提出算法感知检索策略缩小搜索空间，采用模块化双编码器架构分别处理代码和文本输入，实现细粒度语义匹配。

Result: 在RACodeBench和竞争性编程数据集上实验表明，ReCode实现了更高的修复准确性和显著降低的推理成本。

Conclusion: ReCode框架为现实世界代码修复场景提供了实用价值，展示了检索增强生成在代码修复任务中的有效性。

Abstract: Recent advances in large language models (LLMs) have demonstrated impressive
capabilities in code-related tasks, such as code generation and automated
program repair. Despite their promising performance, most existing approaches
for code repair suffer from high training costs or computationally expensive
inference. Retrieval-augmented generation (RAG), with its efficient in-context
learning paradigm, offers a more scalable alternative. However, conventional
retrieval strategies, which are often based on holistic code-text embeddings,
fail to capture the structural intricacies of code, resulting in suboptimal
retrieval quality. To address the above limitations, we propose ReCode, a
fine-grained retrieval-augmented in-context learning framework designed for
accurate and efficient code repair. Specifically, ReCode introduces two key
innovations: (1) an algorithm-aware retrieval strategy that narrows the search
space using preliminary algorithm type predictions; and (2) a modular
dual-encoder architecture that separately processes code and textual inputs,
enabling fine-grained semantic matching between input and retrieved contexts.
Furthermore, we propose RACodeBench, a new benchmark constructed from
real-world user-submitted buggy code, which addresses the limitations of
synthetic benchmarks and supports realistic evaluation. Experimental results on
RACodeBench and competitive programming datasets demonstrate that ReCode
achieves higher repair accuracy with significantly reduced inference cost,
highlighting its practical value for real-world code repair scenarios.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [39] [ChopChop: a Programmable Framework for Semantically Constraining the Output of Language Models](https://arxiv.org/abs/2509.00360)
*Shaan Nagy,Timothy Zhou,Nadia Polikarpova,Loris D'Antoni*

Main category: cs.PL

TL;DR: ChopChop是一个可编程的语义约束解码框架，通过连接词元级生成和抽象程序结构推理，确保语言模型生成的代码满足丰富的语义属性


<details>
  <summary>Details</summary>
Motivation: 现有约束解码方法仅限于浅层语法约束或依赖脆弱的语义编码，无法保证生成代码的正确性和语义等价性

Method: 使用基于共归纳的形式化方法，将约束执行简化为正则共数据上的可实现性问题，连接词元级生成与程序结构推理

Result: 在类型安全和程序等价约束下展示了通用性，提高了各种模型和任务的成功率，同时保持实用的解码延迟

Conclusion: ChopChop将语义约束解码从小众技术转变为系统化、原则性的语言模型扩展，实现了形式化方法与LM驱动代码生成的无缝集成

Abstract: Language models (LMs) can generate code, but cannot guarantee its
correctness--producing outputs that often violate type safety, program
invariants, or semantic equivalence. Constrained decoding offers a solution by
restricting generation to programs that satisfy desired properties. Yet,
existing methods are limited to shallow syntactic constraints or rely on
brittle, ad hoc encodings of semantics over token sequences.
  We present ChopChop, the first programmable framework for semantic
constrained decoding, enabling LMs to generate code that provably satisfies
rich semantic properties. ChopChop connects token-level generation with
reasoning over abstract program structures using a coinduction-based formalism
and reduces constraint enforcement to a realizability problem over regular
codata. We demonstrate ChopChop's generality through generation constrained by
type safety and program equivalence, showing how formal methods can be
seamlessly integrated into LM-driven code generation. ChopChop transforms
semantic constrained decoding from a niche technique into a systematic,
principled extension of LMs--improving success rates across models and tasks
while maintaining practical decoding latency.

</details>


### [40] [A Hoare Logic for Symmetry Properties](https://arxiv.org/abs/2509.00587)
*Vaibhav Mehta,Justin Hsu*

Main category: cs.PL

TL;DR: 提出了一个用于验证程序对称性属性的形式化方法，包括群动作语法设计和Hoare风格逻辑，并开发了原型工具SymVerif来验证对称性属性。


<details>
  <summary>Details</summary>
Motivation: 许多程序正确性属性可以用对称性来表达，但现有形式化方法缺乏对此类属性的推理支持。

Method: 设计群动作语法支持标准构造和蕴含关系，开发Hoare风格的逻辑来验证命令式程序的对称性属性，群动作取代传统的断言条件。

Result: 开发了原型工具SymVerif，在一系列手工制作的基准测试中验证了对称性属性，并发现了McLachlan和Quispel(2002)描述的动力学系统模型中的错误。

Conclusion: 该方法能够有效验证程序对称性属性，工具在实际案例中发现了现有模型中的错误，证明了方法的实用性。

Abstract: Many natural program correctness properties can be stated in terms of
  symmetries, but existing formal methods have little support for reasoning
  about such properties. We consider how to formally verify a broad class of
  symmetry properties expressed in terms of group actions. To specify these
  properties, we design a syntax for group actions, supporting standard
  constructions and a natural notion of entailment. Then, we develop a
  Hoare-style logic for verifying symmetry properties of imperative programs,
  where group actions take the place of the typical pre- and post-condition
  assertions. Finally, we develop a prototype tool $\mathsf{SymVerif}$, and use
  it to verify symmetry properties on a series of handcrafted benchmarks. Our
  tool uncovered an error in a model of a dynamical system described by
\citet{McLachlan_Quispel_2002}.

</details>


### [41] [Formalizing Linear Motion G-code for Invariant Checking and Differential Testing of Fabrication Tools](https://arxiv.org/abs/2509.00699)
*Yumeng He,Chandrakana Nandi,Sreepathi Pai*

Main category: cs.PL

TL;DR: 这篇论文提出了一种新的G代码分析算法，通过将G代码转换为立方体集合和点云表示，以支持三维打印制造流程中的错误检测和质量评估。


<details>
  <summary>Details</summary>
Motivation: 传统编译器有成熟的程序不变量检查技术，但三维打印制造流程缺乏类似方法。G代码作为最终机器代码，受到模型、切片软件和用户参数的多重影响，需要新方法来进行错误检测和质量评估。

Method: 提出一种新算法，将G代码程序升级为立方体集合，然后定义近似点云表示来高效操作这些立方体。这种表示方式为错误定位、切片器对比和网格修复工具评估提供了基础。

Result: 在58个实际CAD模型上评估原型工具GlitchFinder，结果显示它能有效识别小特征导致的切片问题，显示不同切片器（Cura和PrusaSlicer）的差异，并能发现网格修复工具（MeshLab和Meshmixer）在修复过程中引入的新错误。

Conclusion: 该算法为三维打印制造流程提供了类似传统编译器的程序分析能力，开启了CAD模型错误定位、切片器对比和网格修复工具评估等新的应用场景。

Abstract: The computational fabrication pipeline for 3D printing is much like a
compiler - users design models in Computer Aided Design (CAD) tools that are
lowered to polygon meshes to be ultimately compiled to machine code by 3D
slicers. For traditional compilers and programming languages, techniques for
checking program invariants are well-established. Similarly, methods like
differential testing are often used to uncover bugs in compilers themselves,
which makes them more reliable. The fabrication pipeline would benefit from
similar techniques but traditional approaches do not directly apply to the
representations used in this domain. Unlike traditional programs, 3D models
exist both as geometric objects as well as machine code that ultimately runs on
the hardware. The machine code, like in traditional compiling, is affected by
many factors like the model, the slicer being used, and numerous
user-configurable parameters that control the slicing process. In this work, we
propose a new algorithm for lifting G-code (a common language used in
fabrication pipelines) by denoting a G-code program to a set of cuboids, and
then defining an approximate point cloud representation for efficiently
operating on these cuboids. Our algorithm opens up new opportunities: we show
three use cases that demonstrate how it enables error localization in CAD
models through invariant checking, quantitative comparisons between slicers,
and evaluating the efficacy of mesh repair tools. We present a prototype
implementation of our algorithm in a tool, GlitchFinder, and evaluate it on 58
real-world CAD models. Our results show that GlitchFinder is particularly
effective in identifying slicing issues due to small features, can highlight
differences in how popular slicers (Cura and PrusaSlicer) slice the same model,
and can identify cases where mesh repair tools (MeshLab and Meshmixer)
introduce new errors during repair.

</details>


### [42] [Decision Procedure for A Theory of String Sequences](https://arxiv.org/abs/2509.00948)
*Denghang Hu,Taolue Chen,Philipp Rümmer,Fu Song,Zhilin Wu*

Main category: cs.PL

TL;DR: 本文提出了字符串序列理论，通过将字符串序列编码为字符串并限制为直线片段来恢复可判定性，实现了在OSTRICH框架中的决策过程。


<details>
  <summary>Details</summary>
Motivation: 现有SMT求解器仅支持通用序列理论，但缺乏对字符串操作（如正则表达式匹配、字符串分割和连接）的直接支持，而这些操作在字符串处理程序中经常使用。

Method: 将每个字符串序列编码为字符串，并将字符串序列操作映射为相应的字符串操作，通过自动机进行前像计算，集成到OSTRICH字符串约束求解框架中。

Result: 实现了工具ostrichseq，在真实JavaScript程序生成的基准约束、手工制作的模板和单元测试上进行了实验，验证了方法的有效性。

Conclusion: 虽然字符串序列理论在一般情况下是不可判定的，但通过限制为直线片段可以恢复可判定性，该方法能够有效处理字符串序列操作。

Abstract: The theory of sequences, supported by many SMT solvers, can model program
data types including bounded arrays and lists. Sequences are parameterized by
the element data type and provide operations such as accessing elements,
concatenation, forming sub-sequences and updating elements. Strings and
sequences are intimately related; many operations, e.g., matching a string
according to a regular expression, splitting strings, or joining strings in a
sequence, are frequently used in string-manipulating programs. Nevertheless,
these operations are typically not directly supported by existing SMT solvers,
which instead only consider the generic theory of sequences. In this paper, we
propose a theory of string sequences and study its satisfiability. We show
that, while it is undecidable in general, the decidability can be recovered by
restricting to the straight-line fragment. This is shown by encoding each
string sequence as a string, and each string sequence operation as a
corresponding string operation. We provide pre-image computation for the
resulting string operations with respect to automata, effectively casting it
into the generic OSTRICH string constraint solving framework. We implement the
new decision procedure as a tool $\ostrichseq$, and carry out experiments on
benchmark constraints generated from real-world JavaScript programs,
hand-crafted templates and unit tests. The experiments confirm the efficacy of
our approach.

</details>


### [43] [Type-Based Incorrectness Reasoning](https://arxiv.org/abs/2509.01511)
*Zhe Zhou,Benjamin Delaware,Suresh Jagannathan*

Main category: cs.PL

TL;DR: 本文探讨了覆盖类型与不正确性逻辑之间的关联，提出将不正确性推理系统集成到精化类型系统中的机制，为函数式程序员和程序分析工具提供新机会


<details>
  <summary>Details</summary>
Motivation: 覆盖类型能够支持must-style欠近似推理，这在基于属性的测试框架中特别有用，可以验证测试生成器的完整性和安全性。作者发现覆盖类型与最近提出的不正确性逻辑框架之间存在令人惊讶的联系

Method: 探索覆盖类型与不正确性逻辑之间的深层联系，识别机制来更系统地将不正确性推理集成到表达性精化类型系统中

Result: 建立了覆盖类型与不正确性逻辑的理论联系，提出了系统集成的方法

Conclusion: 这种集成为函数式程序员、程序验证器、程序分析器及相关工具提供了新的机会和可能性，推动了欠近似推理在程序分析中的应用

Abstract: A coverage type generalizes refinement types found in many functional
languages with support for must-style underapproximate reasoning.
Property-based testing frameworks are one particularly useful domain where such
capabilities are useful as they allow us to verify the completeness, as well as
safety, of test generators. There is a surprising connection between the kind
of underapproximate reasoning coverage types offer and the style of reasoning
enabled by recently proposed Incorrectness Logic frameworks. In our
presentation, we propose to explore this connection more deeply, identifying
mechanisms that more systematically integrate incorrectness reasoning within an
expressive refinement type system and the opportunities that such integration
offers to functional programmers, program verifiers, and program analyzers and
related tools.

</details>


### [44] [From Traces to Program Incorrectness: A Type-Theoretic Approach](https://arxiv.org/abs/2509.02428)
*Yongwei Yuan,Zhe Zhou,Julia Belyakova,Benjamin Delaware,Suresh Jagannathan*

Main category: cs.PL

TL;DR: 基于符号正则表达式的类型论框架，用于分析函数程序中与效果库API交互时的错误行为


<details>
  <summary>Details</summary>
Motivation: 需要一种系统方法来识别和规范功能程序中与不透明库API交互时可能出现的错误行为

Method: 使用符号正则表达式(SREs)表示API调用踪迹，提出新的类型推断算法，利用符号有限自动机(SFAs)进行组合性推理

Result: 算法成功时能够推断出类型，证明ADT实现可能会展现指定的错误行为子集

Conclusion: 这是首个系统性的跟踪基于错误性规范的下近估计方法，开启了踪迹导向的组合分析新方向

Abstract: We present a type-theoretic framework for reasoning about incorrectness in
functional programs that interact with effectful, opaque library APIs. Our
approach centers on traces -- temporally-ordered sequences of library API
invocations -- which naturally characterize both the preconditions of
individual APIs and their composite behavior. We represent these traces using
symbolic regular expressions (SREs), enabling formal specification of incorrect
abstract data type (ADT) behaviors across function boundaries. The core
contribution is a novel type inference algorithm that operates modulo specified
incorrectness properties and leverages the symbolic finite automata (SFAs)
representations of regexes for compositional reasoning of traces. When the
algorithm succeeds, the inferred types witness that an ADT implementation can
exhibit some subset of the specified incorrect behaviors. This represents the
first systematic approach to underapproximate reasoning against trace-based
incorrectness specifications, enabling a new form of trace-guided compositional
analysis.

</details>
