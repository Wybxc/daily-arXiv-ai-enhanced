<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 14]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Beyond Single LLMs: Enhanced Code Generation via Multi-Stage Performance-Guided LLM Orchestration](https://arxiv.org/abs/2510.01379)
*Huashan Chen,Zhenyu Qi,Haotang Li,Hong Chen,Jinfu Chen,Kebin Peng,In Kee Kim,Kyu Hyung Lee,Sen He*

Main category: cs.SE

TL;DR: 提出了一种多阶段性能引导的LLM编排框架PerfOrch，通过动态选择最适合的LLM来处理不同编程语言和开发阶段的代码生成任务，显著提升了代码正确性和运行时性能。


<details>
  <summary>Details</summary>
Motivation: 当前单一LLM方法忽视了不同模型在编程语言、算法领域和开发阶段上的异构计算优势，需要一种能够利用多模型优势的智能编排方法。

Method: 基于对17个最先进LLM在5种编程语言上的实证研究，开发了PerfOrch框架，采用生成-修复-优化的多阶段工作流，通过阶段验证和回滚机制动态选择最优LLM。

Result: 在HumanEval-X和EffiBench-X基准测试中分别达到96.22%和91.37%的平均正确率，显著超越GPT-4o的78.66%和49.11%，并在58.76%的问题上改善了执行时间，中位加速达17.67%-27.66%。

Conclusion: PerfOrch提供了一种可扩展的即插即用架构，能够适应快速发展的生成式AI环境，为生产级自动化软件工程提供了新范式。

Abstract: While Large Language Models (LLMs) have become the predominant paradigm for
automated code generation, current single-model approaches fundamentally ignore
the heterogeneous computational strengths that different models exhibit across
programming languages, algorithmic domains, and development stages. This paper
challenges the single-model convention by introducing a multi-stage,
performance-guided orchestration framework that dynamically routes coding tasks
to the most suitable LLMs within a structured generate-fix-refine workflow. Our
approach is grounded in a comprehensive empirical study of 17 state-of-the-art
LLMs across five programming languages (Python, Java, C++, Go, and Rust) using
HumanEval-X benchmark. The study, which evaluates both functional correctness
and runtime performance metrics (execution time, mean/max memory utilization,
and CPU efficiency), reveals pronounced performance heterogeneity by language,
development stage, and problem category. Guided by these empirical insights, we
present PerfOrch, an LLM agent that orchestrates top-performing LLMs for each
task context through stage-wise validation and rollback mechanisms. Without
requiring model fine-tuning, PerfOrch achieves substantial improvements over
strong single-model baselines: average correctness rates of 96.22% and 91.37%
on HumanEval-X and EffiBench-X respectively, surpassing GPT-4o's 78.66% and
49.11%. Beyond correctness gains, the framework delivers consistent performance
optimizations, improving execution time for 58.76% of problems with median
speedups ranging from 17.67% to 27.66% across languages on two benchmarks. The
framework's plug-and-play architecture ensures practical scalability, allowing
new LLMs to be profiled and integrated seamlessly, thereby offering a paradigm
for production-grade automated software engineering that adapts to the rapidly
evolving generative AI landscape.

</details>


### [2] [Deciphering WONTFIX: A Mixed-Method Study on Why GitHub Issues Get Rejected](https://arxiv.org/abs/2510.01514)
*J. Alexander Curtis,Sharadha Kasiviswanathan,Nasir Eisty*

Main category: cs.SE

TL;DR: 本研究分析了GitHub上wontfix标签的普遍性和使用原因，发现约30%的项目使用该标签，主要针对用户提交的bug报告和功能请求，并识别出8个常见的使用主题。


<details>
  <summary>Details</summary>
Motivation: wontfix标签在GitHub仓库中被广泛使用但理解有限，其对开源软件项目管理的影响尚不明确，需要系统研究其使用模式和影响。

Method: 采用混合方法，从3,132个最受欢迎的GitHub仓库收集数据，通过定量分析评估wontfix标签的普遍性，并通过开放编码和主题分析对使用原因进行定性分析。

Result: 约30%的GitHub项目使用wontfix标签，主要应用于用户提交的bug报告和功能请求，识别出8个常见使用主题，涵盖用户特定控制因素到维护者特定决策。

Conclusion: wontfix标签是GitHub项目中管理资源和引导贡献者努力的重要工具，但可能抑制社区参与并影响项目管理透明度。理解这些原因有助于项目管理者做出明智决策并促进开源社区的高效协作。

Abstract: Context: The ``wontfix'' label is a widely used yet narrowly understood tool
in GitHub repositories, indicating that an issue will not be pursued further.
Despite its prevalence, the impact of this label on project management and
community dynamics within open-source software development is not clearly
defined. Objective: This study examines the prevalence and reasons behind
issues being labeled as wontfix across various open-source repositories on
GitHub. Method: Employing a mixed-method approach, we analyze both quantitative
data to assess the prevalence of the wontfix label and qualitative data to
explore the reasoning that it was used. Data were collected from 3,132 of
GitHub's most-popular repositories. Later, we employ open coding and thematic
analysis to categorize the reasons behind wontfix labels, providing a
structured understanding of the issue management landscape. Results: Our
findings show that about 30% of projects on GitHub apply the wontfix label to
some issues. These issues most often occur on user-submitted issues for bug
reports and feature requests. The study identified eight common themes behind
labeling issues as wontfix, ranging from user-specific control factors to
maintainer-specific decisions. Conclusions: The wontfix label is a critical
tool for managing resources and guiding contributor efforts in GitHub projects.
However, it can also discourage community involvement and obscure the
transparency of project management. Understanding these reasons aids project
managers in making informed decisions and fostering efficient collaboration
within open-source communities.

</details>


### [3] [MIMIC: Integrating Diverse Personality Traits for Better Game Testing Using Large Language Model](https://arxiv.org/abs/2510.01635)
*Yifei Chen,Sarra Habchi,Lili Wei*

Main category: cs.SE

TL;DR: MIMIC是一个将多样化人格特征整合到游戏代理中的框架，使代理能在相似情境下采用不同的游戏策略，从而提高测试覆盖率和游戏交互多样性。


<details>
  <summary>Details</summary>
Motivation: 传统自动化测试算法难以应对现代视频游戏的复杂挑战，而现有的基于强化学习、模仿学习或大语言模型的游戏代理往往忽视人类玩家的多样化策略，导致在相似情境下产生重复的解决方案，无法触发多样化的游戏交互或发现边缘情况。

Method: 提出MIMIC框架，通过整合多样化人格特征到游戏代理中，使代理能够模仿不同的游戏风格和策略。

Result: MIMIC在不同游戏中实现了更高的测试覆盖率和更丰富的游戏交互，在Minecraft中超越了最先进的代理，获得了更高的任务完成率并提供了更多样化的解决方案。

Conclusion: MIMIC在游戏测试方面展现出显著潜力，能够有效提升游戏测试的质量和多样性。

Abstract: Modern video games pose significant challenges for traditional automated
testing algorithms, yet intensive testing is crucial to ensure game quality. To
address these challenges, researchers designed gaming agents using
Reinforcement Learning, Imitation Learning, or Large Language Models. However,
these agents often neglect the diverse strategies employed by human players due
to their different personalities, resulting in repetitive solutions in similar
situations. Without mimicking varied gaming strategies, these agents struggle
to trigger diverse in-game interactions or uncover edge cases.
  In this paper, we present MIMIC, a novel framework that integrates diverse
personality traits into gaming agents, enabling them to adopt different gaming
strategies for similar situations. By mimicking different playstyles, MIMIC can
achieve higher test coverage and richer in-game interactions across different
games. It also outperforms state-of-the-art agents in Minecraft by achieving a
higher task completion rate and providing more diverse solutions. These results
highlight MIMIC's significant potential for effective game testing.

</details>


### [4] [FOSS-chain: using blockchain for Open Source Software license compliance](https://arxiv.org/abs/2510.01740)
*Kypros Iacovou,Georgia M. Kapitsaki,Evangelia Vanezi*

Main category: cs.SE

TL;DR: FOSS-chain是一个基于区块链的开源软件许可证管理平台，旨在解决衍生作品中的许可证兼容性问题，通过自动化合规流程确保用户遵守OSS许可证条款。


<details>
  <summary>Details</summary>
Motivation: 开源软件许可证管理复杂，许可证间的不兼容性可能导致法律纠纷。区块链技术提供了透明性和不可篡改的记录机制，适合用于许可证管理。

Method: 设计并实现了FOSS-chain网络平台，集成区块链技术，自动化许可证合规流程，涵盖14种开源软件许可证。通过小规模用户研究对原型版本进行初步评估。

Result: 初步结果令人鼓舞，展示了该平台在实际软件系统中应用的潜力。

Conclusion: 区块链与许可证管理的结合为解决OSS许可证兼容性问题提供了有效方案，FOSS-chain平台显示出良好的应用前景。

Abstract: Open Source Software (OSS) is widely used and carries licenses that indicate
the terms under which the software is provided for use, also specifying
modification and distribution rules. Ensuring that users are respecting OSS
license terms when creating derivative works is a complex process. Compliance
issues arising from incompatibilities among licenses may lead to legal
disputes. At the same time, the blockchain technology with immutable entries
offers a mechanism to provide transparency when it comes to licensing and
ensure software changes are recorded. In this work, we are introducing an
integration of blockchain and license management when creating derivative
works, in order to tackle the issue of OSS license compatibility. We have
designed, implemented and performed a preliminary evaluation of FOSS-chain, a
web platform that uses blockchain and automates the license compliance process,
covering 14 OSS licenses. We have evaluated the initial prototype version of
the FOSS-chain platform via a small scale user study. Our preliminary results
are promising, demonstrating the potential of the platform for adaptation on
realistic software systems.

</details>


### [5] [ARENA: A tool for measuring and analysing the energy efficiency of Android apps](https://arxiv.org/abs/2510.01754)
*Hina Anwar*

Main category: cs.SE

TL;DR: ARENA是一个支持工具，让开发者和研究人员能够在IDE中直接连接物理测量设备，测量Android应用的能量消耗，并进行数据聚合、统计分析和可视化。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏开源工具来通过硬件设备进行可靠的能量测量，硬件测量过程耗时且不易适应或复现。

Method: 将ARENA实现为IntelliJ和Android Studio插件，通过在开发中的应用测试场景执行来计算Android智能手机的能量消耗。

Result: 开发了ARENA工具，支持在IDE中直接进行能量测量和数据分析。

Conclusion: ARENA简化了硬件能量测量过程，使开发者和研究人员能够更便捷地进行能量消耗分析和比较。

Abstract: To build energy-efficient apps, there is a need to estimate and analyze their
energy consumption in typical usage scenarios. The energy consumption of
Android apps could be estimated via software-based and hardware-based
approaches. Software-based approaches, while easier to implement, are not as
accurate as hardware-based approaches. The process of measuring the energy
consumption of an Android app via a hardware-based approach typically involves
1) setting up a measurement environment, 2) executing the app under test on a
mobile device, 3) recording current/voltage data via a hardware device to
measure energy consumption, and 4) cleaning and aggregating data for analyses,
reports, and visualizations. Specialized scripts are written for selected
hardware and software components to ensure reliable energy measurements. The
energy measurement process is repeated many times and aggregated to remove
noise. These steps make the hardware-based energy measurement process
time-consuming and not easy to adapt or reproduce. There is a lack of
open-source tools available for developers and researchers to take reliable
energy measurements via hardware devices. In this paper, we present and
demonstrate ARENA, a support tool that enables developers and researchers to
connect to a physical measurement device without leaving the comfort of their
IDE. Developers could use ARENA during development to compare energy
consumption between different apps or versions of the same app. ARENA
calculates energy consumption on an Android smartphone by executing a test
scenario on the app under development. Further, ARENA helps aggregate,
statistically analyze, report, and visualize the data, allowing developers and
researchers to dig into the data directly or visually. We implemented ARENA as
an IntelliJ and Android Studio plugin.

</details>


### [6] [Towards Speeding up Program Repair with Non-Autoregressive Model](https://arxiv.org/abs/2510.01825)
*Zhenyu Yang,Yue Pan,Zhen Yang,Zhongxing Yu*

Main category: cs.SE

TL;DR: NARRepair是首个为自动程序修复任务定制的非自回归代码生成模型，通过修复动作预测器、令牌间依赖提取器和两阶段解码器解决NAR方法在APR任务中的质量问题，在修复速度和准确性方面达到最先进的综合性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于机器学习的自动程序修复技术采用自回归方式，存在巨大的时间延迟问题，特别是参数较多的模型延迟更严重。非自回归方法可以并行输出目标代码，但直接应用于APR任务会导致补丁质量下降。

Method: 提出NARRepair模型，包含三个核心组件：1)修复动作预测器缓解过修正问题；2)令牌间依赖提取器缓解缺乏令牌间依赖信息问题；3)两阶段解码器缓解缺乏上下文信息问题。

Result: 在三个广泛使用的APR数据集上评估，NARRepair在有限修复时间内表现最佳，修复速度比AR-based APR技术提高1.4-6.4倍，在修复速度和准确性方面达到最先进的综合性能。

Conclusion: NARRepair成功将非自回归方法应用于自动程序修复任务，有效解决了传统AR方法的延迟问题，同时保持了高质量的修复效果，为APR领域提供了新的解决方案。

Abstract: Enlightened by the success of machine learning techniques in various
application areas, recent years have witnessed a surge of research efforts on
automatic program repair (APR) using machine learning techniques. Previous
machine learning-based APR techniques essentially modified bugs in the
autoregressive (AR) manner, which predicts future values based on past values.
Due to the manner of token-by-token generation, the AR-based APR technique has
a huge time delay. In particular, the delay of the APR model with a large
number of parameters is more serious. To address the issue, we aim to apply the
non-autoregressive (NAR) method to the APR task, which can output target code
in a parallel manner to avoid huge repair delays. However, the naive use of the
NAR manner for the APR task suffers from the issue of compromised patch
quality. To effectively adapt the NAR manner for the APR task, we in this paper
propose NARRepair, the first customized NAR code generation model for the APR
task. The NARRepair model features three major novelties, including 1) the
repair action predictor for alleviating the over-correction issue, 2) the
inter-token dependency extractor for alleviating the issue of lacking
inter-token dependency information, and 3) the two-stage decoder for
alleviating the issue of lacking contextual information. We evaluated NARRepair
on three widely used datasets in the APR community, and the results show that
1) compared to other APR techniques, the NARRepair model has the best
performance within the limited repair time, and 2) compared to AR-based APR
techniques, the repair speed of NARRepair has been increased by 1.4-6.4 times
in the GPU environment. Overall, the results show that NARRepair has achieved
state-of-the-art comprehensive performance in terms of repair speed and
accuracy.

</details>


### [7] [RefFilter: Improving Semantic Conflict Detection via Refactoring-Aware Static Analysis](https://arxiv.org/abs/2510.01960)
*Victor Lira,Paulo Borba,Rodrigo Bonifácio,Galileu Santos e Matheus barbosa*

Main category: cs.SE

TL;DR: RefFilter是一个重构感知的语义干扰检测工具，通过自动检测重构来减少误报，在标记数据集上减少近32%的误报。


<details>
  <summary>Details</summary>
Motivation: 协作软件开发中语义干扰检测面临高误报率问题，主要原因是现有技术无法有效区分行为保持的重构和影响行为的变更。

Method: 基于现有静态分析技术，结合自动重构检测来改进精度，从报告中排除行为保持的重构。

Result: 在标记数据集上减少近32%的误报，虽然轻微增加了漏报，但精度提升显著超过了召回率的微小损失。

Conclusion: 重构感知的干扰检测是改进现代开发工作流中合并支持的实用有效策略。

Abstract: Detecting semantic interference remains a challenge in collaborative software
development. Recent lightweight static analysis techniques improve efficiency
over SDG-based methods, but they still suffer from a high rate of false
positives. A key cause of these false positives is the presence of
behavior-preserving code refactorings, which current techniques cannot
effectively distinguish from changes that impact behavior and can interfere
with others. To handle this problem we present RefFilter, a refactoring-aware
tool for semantic interference detection. It builds on existing static
techniques by incorporating automated refactoring detection to improve
precision. RefFilter discards behavior-preserving refactorings from reports,
reducing false positives while preserving detection coverage. To evaluate
effectiveness and scalability, use two datasets: a labeled dataset with 99
scenarios and ground truth, and a novel dataset of 1,087 diverse merge
scenarios that we have built. Experimental results show that RefFilter reduces
false positives by nearly 32% on the labeled dataset. While this reduction
comes with a non significant increase in false negatives, the overall gain in
precision significantly outweighs the minor trade-off in recall. These findings
demonstrate that refactoring-aware interference detection is a practical and
effective strategy for improving merge support in modern development workflows.

</details>


### [8] [Clarifying Semantics of In-Context Examples for Unit Test Generation](https://arxiv.org/abs/2510.01994)
*Chen Yang,Lin Yang,Ziqi Wang,Dong Wang,Jianyi Zhou,Junjie Chen*

Main category: cs.SE

TL;DR: CLAST是一种通过系统化重构单元测试来提高语义清晰度的技术，能够显著提升基于上下文学习的单元测试生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的单元测试生成方法严重依赖上下文示例的质量，但现有测试往往结构复杂、语义不清，导致生成的测试效果不佳。

Method: CLAST通过程序分析和LLM重写相结合的方式，将复杂测试分解为逻辑更清晰的测试，并改进语义清晰度。

Result: 在4个开源项目和3个工业项目上的评估显示，CLAST在保持测试有效性和提升语义清晰度方面大幅优于现有最佳技术UTgen，且85.33%的用户更偏好CLAST重构的测试。

Conclusion: CLAST能够有效提升单元测试的语义清晰度，进而显著改善基于上下文学习的测试生成方法的效果，具有重要的实践应用价值。

Abstract: Recent advances in large language models (LLMs) have enabled promising
performance in unit test generation through in-context learning (ICL). However,
the quality of in-context examples significantly influences the effectiveness
of generated tests-poorly structured or semantically unclear test examples
often lead to suboptimal outputs. In this paper, we propose CLAST, a novel
technique that systematically refines unit tests to improve their semantic
clarity, thereby enhancing their utility as in-context examples. The approach
decomposes complex tests into logically clearer ones and improves semantic
clarity through a combination of program analysis and LLM-based rewriting. We
evaluated CLAST on four open-source and three industrial projects. The results
demonstrate that CLAST largely outperforms UTgen, the state-of-the-art
refinement technique, in both preserving test effectiveness and enhancing
semantic clarity. Specifically, CLAST fully retains the original effectiveness
of unit tests, while UTgen reduces compilation success rate (CSR), pass rate
(PR), test coverage (Cov), and mutation score (MS) by an average of 12.90%,
35.82%, 4.65%, and 5.07%, respectively. Over 85.33% of participants in our user
study preferred the semantic clarity of CLAST-refined tests. Notably,
incorporating CLAST-refined tests as examples effectively improves ICL-based
unit test generation approaches such as RAGGen and TELPA, resulting in an
average increase of 25.97% in CSR, 28.22% in PR, and 45.99% in Cov for
generated tests, compared to incorporating UTgen-refined tests. The insights
from the follow-up user study not only reinforce CLAST's potential impact in
software testing practice but also illuminate avenues for future research.

</details>


### [9] [Automatic Generation of Combinatorial Reoptimisation Problem Specifications: A Vision](https://arxiv.org/abs/2510.02002)
*Maximilian Kratz,Steffen Zschaler,Jens Kosiol,Gabriele Taentzer*

Main category: cs.SE

TL;DR: 该论文提出使用模型驱动工程（MDE）方法，通过声明式建模语言和模型转换来系统地从原始优化问题规范推导出重新优化问题规范，以解决上下文变化时的优化问题适应挑战。


<details>
  <summary>Details</summary>
Motivation: 当优化问题的上下文因素发生变化时，需要重新优化解决方案。重新优化问题与原始问题有三个关键区别：需要最小化对原始解决方案的更改、某些部分可能无法更改、需要生成从原始解决方案到新解决方案的变更脚本。

Method: 采用模型驱动工程方法，特别是使用声明式建模语言和模型转换来高层次地规范优化问题。基于GIPS工具开发了概念验证实现，并应用于助教分配的资源分配问题。

Result: 提出了组合重新优化问题的初步分类，以及推导相应重新优化规范的策略。通过助教分配案例验证了方法的可行性。

Conclusion: 模型驱动工程为从原始优化问题规范系统推导重新优化问题提供了新的机会，能够有效处理上下文变化时的优化问题适应需求。

Abstract: Once an optimisation problem has been solved, the solution may need
adaptation when contextual factors change. This challenge, also known as
reoptimisation, has been addressed in various problem domains, such as railway
crew rescheduling, nurse rerostering, or aircraft recovery. This requires a
modified problem to be solved again to ensure that the adapted solution is
optimal in the new context. However, the new optimisation problem differs
notably from the original problem: (i) we want to make only minimal changes to
the original solution to minimise the impact; (ii) we may be unable to change
some parts of the original solution (e.g., because they refer to past
allocations); and (iii) we need to derive a change script from the original
solution to the new solution. In this paper, we argue that Model-Driven
Engineering (MDE) - in particular, the use of declarative modelling languages
and model transformations for the high-level specification of optimisation
problems - offers new opportunities for the systematic derivation of
reoptimisation problems from the original optimisation problem specification.
We focus on combinatorial reoptimisation problems and provide an initial
categorisation of changing problems and strategies for deriving the
corresponding reoptimisation specifications. We introduce an initial
proof-of-concept implementation based on the GIPS (Graph-Based (Mixed) Integer
Linear Programming Problem Specification) tool and apply it to an example
resource-allocation problem: the allocation of teaching assistants to teaching
sessions.

</details>


### [10] [ACM SIGSOFT SEN Empirical Software Engineering: Introducing Our New Regular Column](https://arxiv.org/abs/2510.02007)
*Justus Bogner,Roberto Verdecchia*

Main category: cs.SE

TL;DR: 介绍一个新的ACM SIGSOFT SEN专栏(SEN-ESE)，旨在讨论经验软件工程研究的元方面，包括最佳实践、统计方法等，通过专家访谈、焦点小组等方式鼓励反思和改进ESE研究。


<details>
  <summary>Details</summary>
Motivation: 经验软件工程研究领域虽然成熟，但仍面临研究可重复性、外部有效性有限、评审主观性等问题，且许多方面缺乏明确文档，使新人难以掌握。

Method: 通过新的SEN专栏，采用专家访谈、焦点小组、调查和立场文章等方式，讨论ESE研究的元方面。

Result: 建立了一个定期讨论ESE研究隐含话题的平台，鼓励社区反馈和参与。

Conclusion: 该专栏将为ESE社区提供一个讨论挑战性、争议性或未充分探索话题的场所，旨在围绕社区兴趣塑造内容，促进ESE研究的改进。

Abstract: From its early foundations in the 1970s, empirical software engineering (ESE)
has evolved into a mature research discipline that embraces a plethora of
different topics, methodologies, and industrial practices. Despite its
remarkable progress, the ESE research field still needs to keep evolving, as
new impediments, shortcoming, and technologies emerge. Research
reproducibility, limited external validity, subjectivity of reviews, and
porting research results to industrial practices are just some examples of the
drivers for improvements to ESE research. Additionally, several facets of ESE
research are not documented very explicitly, which makes it difficult for
newcomers to pick them up. With this new regular ACM SIGSOFT SEN column
(SEN-ESE), we introduce a venue for discussing meta-aspects of ESE research,
ranging from general topics such as the nature and best practices for
replication packages, to more nuanced themes such as statistical methods,
interview transcription tools, and publishing interdisciplinary research. Our
aim for the column is to be a place where we can regularly spark conversations
on ESE topics that might not often be touched upon or are left implicit.
Contributions to this column will be grounded in expert interviews, focus
groups, surveys, and position pieces, with the goal of encouraging reflection
and improvement in how we conduct, communicate, teach, and ultimately improve
ESE research. Finally, we invite feedback from the ESE community on
challenging, controversial, or underexplored topics, as well as suggestions for
voices you would like to hear from. While we cannot promise to act on every
idea, we aim to shape this column around the community interests and are
grateful for all contributions.

</details>


### [11] [Towards fairer public transit: Real-time tensor-based multimodal fare evasion and fraud detection](https://arxiv.org/abs/2510.02165)
*Peter Wauyo,Dalia Bwiza,Alain Murara,Edwin Mugume,Eric Umuhoza*

Main category: cs.SE

TL;DR: 提出基于多模态融合的公共交通欺诈检测系统，结合视频和音频分析，使用ViViT和AST模型提取特征，通过张量融合网络实现跨模态交互，在检测欺诈行为方面达到89.5%准确率。


<details>
  <summary>Details</summary>
Motivation: 解决公共交通中的欺诈和逃票问题，减少收入损失，提高乘客安全和运营合规性。现有系统在检测复杂欺诈行为方面效果有限，需要更先进的跨模态分析方法。

Method: 使用Vision Transformer for Video (ViViT)提取视频特征，Audio Spectrogram Transformer (AST)分析音频，采用Tensor Fusion Network (TFN)架构通过2-fold笛卡尔积显式建模单模态和双模态交互。

Result: 在自定义数据集上达到89.5%准确率、87.2%精确率和84.0%召回率，显著优于早期融合基线，比最先进交通欺诈检测系统的75%召回率更高。消融研究显示张量融合方法比传统拼接方法在F1分数上提升7.0%，召回率提升8.8%。

Conclusion: 多模态张量融合方法能有效捕捉视觉行为（如尾随、未授权访问）和音频线索（如票价交易声音）之间的复杂跨模态动态，支持实时检测，为公共交通运营商提供有效的欺诈检测解决方案。

Abstract: This research introduces a multimodal system designed to detect fraud and
fare evasion in public transportation by analyzing closed circuit television
(CCTV) and audio data. The proposed solution uses the Vision Transformer for
Video (ViViT) model for video feature extraction and the Audio Spectrogram
Transformer (AST) for audio analysis. The system implements a Tensor Fusion
Network (TFN) architecture that explicitly models unimodal and bimodal
interactions through a 2-fold Cartesian product. This advanced fusion technique
captures complex cross-modal dynamics between visual behaviors (e.g.,
tailgating,unauthorized access) and audio cues (e.g., fare transaction sounds).
The system was trained and tested on a custom dataset, achieving an accuracy of
89.5%, precision of 87.2%, and recall of 84.0% in detecting fraudulent
activities, significantly outperforming early fusion baselines and exceeding
the 75% recall rates typically reported in state-of-the-art transportation
fraud detection systems. Our ablation studies demonstrate that the tensor
fusion approach provides a 7.0% improvement in the F1 score and an 8.8% boost
in recall compared to traditional concatenation methods. The solution supports
real-time detection, enabling public transport operators to reduce revenue
loss, improve passenger safety, and ensure operational compliance.

</details>


### [12] [SIEVE: Towards Verifiable Certification for Code-datasets](https://arxiv.org/abs/2510.02166)
*Fatou Ndiaye Mbodji,El-hacen Diallo,Jordan Samhi,Kui Liu,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.SE

TL;DR: SIEVE是一个社区驱动的框架，将代码数据集的质量检查转化为可验证的置信卡，提供统计保证，降低质量保证成本并增加信任。


<details>
  <summary>Details</summary>
Motivation: 现有代码数据集缺乏可验证的质量保证，静态数据集卡片不可审计且无统计保证，团队各自构建孤立的清理流程导致效率低下和成本增加。

Method: 提出SIEVE框架，将每个属性检查转化为置信卡——机器可读、可验证的证书，具有随时有效的统计边界。

Result: SIEVE框架能够将叙述性卡片替换为随时可验证的认证，提供统计保证。

Conclusion: SIEVE框架有望降低质量保证成本，增加对代码数据集的信任，推动社区驱动的数据集质量认证。

Abstract: Code agents and empirical software engineering rely on public code datasets,
yet these datasets lack verifiable quality guarantees. Static 'dataset cards'
inform, but they are neither auditable nor do they offer statistical
guarantees, making it difficult to attest to dataset quality. Teams build
isolated, ad-hoc cleaning pipelines. This fragments effort and raises cost. We
present SIEVE, a community-driven framework. It turns per-property checks into
Confidence Cards-machine-readable, verifiable certificates with anytime-valid
statistical bounds. We outline a research plan to bring SIEVE to maturity,
replacing narrative cards with anytime-verifiable certification. This shift is
expected to lower quality-assurance costs and increase trust in code-datasets.

</details>


### [13] [TAIBOM: Bringing Trustworthiness to AI-Enabled Systems](https://arxiv.org/abs/2510.02169)
*Vadim Safronov,Anthony McCaigue,Nicholas Allott,Andrew Martin*

Main category: cs.SE

TL;DR: 提出了Trusted AI Bill of Materials (TAIBOM)框架，将SBOM原则扩展到AI领域，解决AI系统特有的动态、数据驱动特性和松散耦合依赖问题。


<details>
  <summary>Details</summary>
Motivation: 开源软件和AI技术的融合给软件供应链带来了新的复杂性，现有依赖管理和系统保证方法无法有效处理AI系统的独特特性，包括动态数据驱动性质和跨数据集、模型、软件组件的松散耦合依赖。

Method: 开发了TAIBOM框架，包括：针对AI组件的结构化依赖模型、跨异构AI管道的完整性声明传播机制、以及验证组件来源的信任证明流程。

Result: TAIBOM在AI工作流中支持保证、安全和合规性，相比SPDX和CycloneDX等现有标准具有优势。

Conclusion: 这项工作通过结构化软件透明度为可信和可验证的AI系统奠定了基础。

Abstract: The growing integration of open-source software and AI-driven technologies
has introduced new layers of complexity into the software supply chain,
challenging existing methods for dependency management and system assurance.
While Software Bills of Materials (SBOMs) have become critical for enhancing
transparency and traceability, current frameworks fall short in capturing the
unique characteristics of AI systems -- namely, their dynamic, data-driven
nature and the loosely coupled dependencies across datasets, models, and
software components. These challenges are compounded by fragmented governance
structures and the lack of robust tools for ensuring integrity, trust, and
compliance in AI-enabled environments.
  In this paper, we introduce Trusted AI Bill of Materials (TAIBOM) -- a novel
framework extending SBOM principles to the AI domain. TAIBOM provides (i) a
structured dependency model tailored for AI components, (ii) mechanisms for
propagating integrity statements across heterogeneous AI pipelines, and (iii) a
trust attestation process for verifying component provenance. We demonstrate
how TAIBOM supports assurance, security, and compliance across AI workflows,
highlighting its advantages over existing standards such as SPDX and CycloneDX.
This work lays the foundation for trustworthy and verifiable AI systems through
structured software transparency.

</details>


### [14] [FalseCrashReducer: Mitigating False Positive Crashes in OSS-Fuzz-Gen Using Agentic AI](https://arxiv.org/abs/2510.02185)
*Paschal C. Amusuo,Dongge Liu,Ricardo Andres Calvo Mendez,Jonathan Metzman,Oliver Chang,James C. Davis*

Main category: cs.SE

TL;DR: 本文提出了两种AI驱动的策略来减少OSS-Fuzz-Gen中的误报崩溃：基于约束的模糊驱动程序生成和基于上下文的崩溃验证，在1500个基准函数上测试显示误报崩溃减少8%，报告崩溃减少一半以上。


<details>
  <summary>Details</summary>
Motivation: 现有自动生成的模糊驱动程序经常导致误报崩溃，特别是在需要高度结构化输入和复杂状态要求的函数中，这在工业级模糊驱动程序生成中严重影响系统可信度。

Method: 1. 基于约束的模糊驱动程序生成：主动对函数输入和状态施加约束来指导驱动程序创建；2. 基于上下文的崩溃验证：通过分析函数调用者来验证报告崩溃是否从程序入口点可行。

Result: 在1500个OSS-Fuzz基准函数上测试，误报崩溃减少达8%，报告崩溃减少超过一半，证明前沿LLM可以作为可靠的程序分析代理。

Conclusion: 研究结果突显了将AI集成到大规模模糊测试管道中的前景和挑战，展示了AI在提高模糊测试准确性方面的潜力。

Abstract: Fuzz testing has become a cornerstone technique for identifying software bugs
and security vulnerabilities, with broad adoption in both industry and
open-source communities. Directly fuzzing a function requires fuzz drivers,
which translate random fuzzer inputs into valid arguments for the target
function. Given the cost and expertise required to manually develop fuzz
drivers, methods exist that leverage program analysis and Large Language Models
to automatically generate these drivers. However, the generated fuzz drivers
frequently lead to false positive crashes, especially in functions highly
structured input and complex state requirements. This problem is especially
crucial in industry-scale fuzz driver generation efforts like OSS-Fuzz-en, as
reporting false positive crashes to maintainers impede trust in both the system
and the team.
  This paper presents two AI-driven strategies to reduce false positives in
OSS-Fuzz-Gen, a multi-agent system for automated fuzz driver generation. First,
constraint-based fuzz driver generation proactively enforces constraints on a
function's inputs and state to guide driver creation. Second, context-based
crash validation reactively analyzes function callers to determine whether
reported crashes are feasible from program entry points. Using 1,500 benchmark
functions from OSS-Fuzz, we show that these strategies reduce spurious crashes
by up to 8%, cut reported crashes by more than half, and demonstrate that
frontier LLMs can serve as reliable program analysis agents. Our results
highlight the promise and challenges of integrating AI into large-scale fuzzing
pipelines.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [15] [MightyPPL: Verification of MITL with Past and Pnueli Modalities](https://arxiv.org/abs/2510.01490)
*Hsi-Ming Ho,Shankara Narayanan Krishna,Khushraj Madnani,Rupak Majumdar,Paritosh Pandya*

Main category: cs.FL

TL;DR: MightyPPL是一个新工具，用于将度量区间时序逻辑（MITL）公式转换为时间自动机，支持更丰富的规范逻辑和性能优化。


<details>
  <summary>Details</summary>
Motivation: 现有的MITL验证方法要么只支持有限的逻辑片段，要么只能进行不完整的验证，存在明显缺陷。

Method: 通过新颖的符号化转换编码和对称性约简技术，将MITPPL公式转换为语言等价的时间自动机网络或单个时间自动机。

Result: MightyPPL能够生成与多个验证后端兼容的时间自动机，并在各种案例研究中表现出良好的性能。

Conclusion: MightyPPL为时序逻辑验证提供了更强大和高效的解决方案，支持更丰富的规范表达和优化的验证性能。

Abstract: Metric Interval Temporal Logic (MITL) is a popular formalism for specifying
properties of reactive systems with timing constraints. Existing approaches to
using MITL in verification tasks, however, have notable drawbacks: they either
support only limited fragments of the logic or allow for only incomplete
verification. This paper introduces MightyPPL, a new tool for translating
formulae in Metric Interval Temporal Logic with Past and Pnueli modalities
(MITPPL) over the pointwise semantics into timed automata. MightyPPL enables
satisfiability and model checking of a much more expressive specification logic
over both finite and infinite words and incorporates a number of performance
optimisations, including a novel symbolic encoding of transitions and a
symmetry reduction technique that leads to an exponential improvement in the
number of reachable discrete states. For a given MITPPL formula, MightyPPL can
generate either a network of timed automata or a single timed automaton that is
language-equivalent and compatible with multiple verification back-ends,
including Uppaal, TChecker, and LTSmin, which supports multi-core model
checking. We evaluate the performance of the toolchain across various case
studies and configuration options.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [16] [Sequent Calculi for Data-Aware Modal Logics](https://arxiv.org/abs/2510.01868)
*Carlos Areces,Valentin Cassano,Danae Dutto,Raul Fervari*

Main category: cs.LO

TL;DR: 为数据感知模态逻辑HXpathD开发了Gentzen风格的序列演算，证明其可靠性和完备性，所有规则可逆且满足切割消除。


<details>
  <summary>Details</summary>
Motivation: 以往对HXpathD的研究主要关注模型论性质，本文从证明论角度出发，旨在为数据感知模态逻辑建立证明论基础。

Method: 提出了HXpathD的Gentzen风格序列演算系统，证明其可靠性和完备性，并验证所有规则的可逆性和切割消除性质。

Result: 成功构建了HXpathD的序列演算系统，证明该系统是可靠且完备的，所有规则可逆，并满足切割消除。

Conclusion: 这项工作为数据感知模态逻辑奠定了证明论基础，为图结构数据查询语言的逻辑分析提供了新工具，并为扩展证明论技术到更广泛的模态系统奠定了基础。

Abstract: Data-aware modal logics offer a powerful formalism for reasoning about
semi-structured queries in languages such as DataGL, XPath, and GQL. In brief,
these logics can be viewed as modal systems capable of expressing both
reachability statements and data-aware properties, such as value comparisons.
One particularly expressive logic in this landscape is HXpathD, a hybrid modal
logic that captures not only the navigational core of XPath but also data
comparisons, node labels (keys), and key-based navigation operators. While
previous work on HXpathD has primarily focused on its model-theoretic
properties, in this paper we approach HXpathD from a proof-theoretic
perspective. Concretely, we present a sound and complete Gentzen-style sequent
calculus for HXpathD. Moreover, we show all rules in this calculus are
invertible, and that it enjoys cut elimination. Our work contributes to the
proof-theoretic foundations of data-aware modal logics, and enables a deeper
logical analysis of query languages over graph-structured data. Moreover, our
results lay the groundwork for extending proof-theoretic techniques to a
broader class of modal systems.

</details>
