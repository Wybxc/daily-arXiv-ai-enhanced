<div id=toc></div>

# Table of Contents

- [cs.LO](#cs.LO) [Total: 17]
- [cs.SE](#cs.SE) [Total: 22]
- [cs.FL](#cs.FL) [Total: 2]
- [cs.PL](#cs.PL) [Total: 6]


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [1] [Nested Sequents for Intuitionistic Multi-Modal Logics: Cut-Elimination and Lyndon Interpolation](https://arxiv.org/abs/2511.22174)
*Tim S. Lyon*

Main category: cs.LO

TL;DR: 本文为直觉主义语法逻辑(IGLs)引入了单结论嵌套相继式演算，统一处理模态公理，证明了切割可容许性，并给出了构造性的插值算法。


<details>
  <summary>Details</summary>
Motivation: 研究直觉主义语法逻辑(IGLs)的证明论性质，这类逻辑是经典语法逻辑的直觉主义对应物，包含标准直觉主义模态和时态逻辑。需要开发统一的证明系统来处理各种模态公理组合。

Method: 引入单结论嵌套相继式演算，提出新的结构规则(shift rule)统一处理模态框架条件，分析演算的基本可逆性和可容许性，构造基于嵌套相继式证明的插值算法。

Result: 证明了切割可容许性，获得了嵌套演算的完备性，为所有IGLs及其包含的直觉主义模态和时态逻辑构造性地证明了Lyndon插值性质(LIP)和Beth可定义性性质(BDP)。

Conclusion: 本文为直觉主义语法逻辑提供了统一的证明论框架，通过新的结构规则简化了证明，首次基于单结论嵌套相继式证明给出了构造性的插值结果。

Abstract: We introduce and study single-conclusioned nested sequent calculi for a broad class of intuitionistic multi-modal logics known as intuitionistic grammar logics (IGLs). These logics serve as the intuitionistic counterparts of classical grammar logics, and subsume standard intuitionistic modal and tense logics, including IK and IKt extended with combinations of the T, B, 4, 5, and D axioms. We analyze fundamental invertibility and admissibility properties of our calculi and introduce a novel structural rule, called the shift rule, which unifies standard structural rules arising from modal frame conditions into a single rule. This rule enables a purely syntactic proof of cut-admissibility that is uniform over all IGLs, and yields completeness of our nested calculi as a corollary. Finally, we define an interpolation algorithm that operates over single-conclusioned nested sequent proofs. This gives constructive proofs of both the Lyndon interpolation property (LIP) and Beth definability property (BDP) for all IGLs and for all intuitionistic modal and tense logics they subsume. To the best of the author's knowledge, this style of interpolation algorithm (that acts on single-conclusioned nested sequent proofs) and the resulting LIP and BDP results are new.

</details>


### [2] [Hyperintensional Intention](https://arxiv.org/abs/2511.22371)
*Daniil Khaitovich,Aybüke Özgün*

Main category: cs.LO

TL;DR: 本文提出了一种超内涵的意图逻辑，避免意图在逻辑等价下的封闭性，通过结合探究性语义和主题敏感理论来约束意图推理。


<details>
  <summary>Details</summary>
Motivation: 传统意图逻辑面临过度生成问题，特别是意图在逻辑等价下的封闭性会导致无法区分意图后果和意外副作用。需要避免不仅逻辑蕴含的封闭性，还要避免逻辑等价的封闭性。

Method: 开发超内涵意图逻辑，结合探究性语义和主题敏感理论，将意图约束在代理的决策问题中。提供该逻辑的完备公理化系统。

Result: 建立了避免逻辑等价封闭性的超内涵意图逻辑框架，证明了现有类似框架过度生成了某些逻辑等价实例的有效性，并提供了该逻辑的可靠且强完备的公理化。

Conclusion: 意图逻辑需要避免逻辑等价的封闭性，提出的超内涵框架通过结合探究性和主题敏感理论成功解决了这一问题，为意图推理提供了更精确的逻辑基础。

Abstract: Intentions are crucial for our practical reasoning. The rational intention obeys some simple logical principles, such as agglomeration and consistency, among others, motivating the search for a proper logic of intention. However, such a logic should be weak enough not to force the closure under entailment; otherwise, we cannot distinguish between intended consequences of agents' choices and their unintended side-effects.  In this paper we argue that we should avoid not only the closure under entailment, but the weaker closure under equivalence as well. To achieve this, we develop a hyperintensional logic of intention, where what an agent intends is constrained by the agent's decision problem. The proposed system combines some elements of inquisitive and topic-sensitive theories of  intensional modals. Along the way, we also show that the existing closest relatives of our framework overgenerate validities by validating some instances of closure under equivalence. Finally, we provide a sound and strongly complete axiomatization for this logic.

</details>


### [3] [Distributed Knowing How](https://arxiv.org/abs/2511.22374)
*Bin Liu,Yanjing Wang*

Main category: cs.LO

TL;DR: 本文提出了分布式知识-如何（distributed knowledge-how）的概念及其逻辑系统，将分布式知识理论从知识-什么扩展到知识-如何，并建立了相应的完备证明系统。


<details>
  <summary>Details</summary>
Motivation: 标准认知逻辑中的分布式知识（knowledge-that）已有成熟理论，但缺乏对应的分布式知识-如何（knowledge-how）概念。现有知识-如何逻辑框架分为个体多步方法和联盟单步方法，需要统一框架来研究群体如何通过分布式行动获得知识-如何。

Method: 提出分布式知识-如何的框架，结合个体多步和联盟单步两种传统方法。假设群体能完成比个体联合行动更多的事情，基于子群体能集体执行的分布式行动来构建多步策略，从而定义分布式知识-如何。

Result: 建立了分布式知识-如何逻辑的可靠且强完备的证明系统，该系统在公理和完备性证明方法上都与分布式知识-什么逻辑高度相似。

Conclusion: 成功将分布式知识理论从知识-什么扩展到知识-如何，为群体认知能力提供了新的逻辑分析工具，统一了现有知识-如何逻辑的不同传统。

Abstract: Distributed knowledge is a key concept in the standard epistemic logic of knowledge-that. In this paper, we propose a corresponding notion of distributed knowledge-how and study its logic. Our framework generalizes two existing traditions in the logic of know-how: the individual-based multi-step framework and the coalition-based single-step framework. In particular, we assume a group can accomplish more than what its individuals can jointly do. The distributed knowledge-how is based on the distributed knowledge-that of a group whose multi-step strategies derive from distributed actions that subgroups can collectively perform. As the main result, we obtain a sound and strongly complete proof system for our logic of distributed knowledge-how, which closely resembles the logic of distributed knowledge-that in both the axioms and the proof method of completeness.

</details>


### [4] [Conditionals Based on Selection Functions, Modal Operators and Probabilities](https://arxiv.org/abs/2511.22377)
*Tommaso Flaminio,Lluis Godo,Gluliano Rosella*

Main category: cs.LO

TL;DR: 该论文探讨概率更新方法（如贝叶斯条件化）与条件句之间的深层联系，旨在建立一般性理论框架来理解两者关系。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于特定类型的条件句或特定更新方法，缺乏对概率更新方法与条件连接词之间关系的一般性理论框架。本文旨在填补这一空白，从更广泛的视角探讨这两类建模工具的内在联系。

Method: 采用一般性视角，涵盖一大类条件句和广泛的更新方法。通过建立一般性结果来连接条件句及其概率，从而表征特定条件连接词的概率特性，并理解哪些更新过程可以用特定条件连接词表示。

Result: 建立了关于条件句与更新方法相互关系的一般性结果，能够表征某些条件连接词的概率特性，并确定可以用特定条件连接词表示的更新过程类别。

Conclusion: 通过建立一般性理论框架，深化了对概率更新方法与条件连接词之间关系的理解，为这一研究领域提供了新的理论视角和分析工具。

Abstract: Methods for probability updating, of which Bayesian conditionalization is the most well-known and widely used, are modeling tools that aim to represent the process of modifying an initial epistemic state, typically represented by a prior probability function P, which is adjusted in light of new information. Notably, updating methods and conditional sentences seem to intuitively share a deep connection, as is evident in the case of conditionalization. The present work contributes to this line of research and aims at shedding new light on the relationship between updating methods and conditional connectives. Departing from previous literature that often focused on a specific type of conditional or a particular updating method, our goal is to prove general results concerning the connection between conditionals and their probabilities. This will allow us to characterize the probabilities of certain conditional connectives and to understand what class of updating procedures can be represented using specific conditional connectives. Broadly, we adopt a general perspective that encompasses a large class of conditionals and a wide range of updating methods, enabling us to prove some general results concerning their interrelation.

</details>


### [5] [A programming language combining quantum and classical control](https://arxiv.org/abs/2511.22537)
*Kinnari Dave,Louis Lemonnier,Romain Péchoux,Vladimir Zamdzhiev*

Main category: cs.LO

TL;DR: 该论文提出了一种将量子控制和经典控制范式结合在同一系统中的方法，通过语法、操作和语义三个层面的创新实现。


<details>
  <summary>Details</summary>
Motivation: 传统上量子编程语言中的量子控制和经典控制两种范式被分开处理，无法很好地混合。量子控制专注于纯量子计算，基于叠加态；经典控制基于经典信息流，适合混合态量子计算。作者希望打破这种分离，让两种范式能够在同一系统中协同工作。

Method: 通过三个关键创新实现两种范式的结合：(1) 语法层面：引入模态将纯量子类型整合到混合态量子类型系统中；(2) 操作层面：改编量子λ演算中的"量子配置"概念，用量子原语替换量子数据；(3) 语义层面：使用希尔伯特空间的适当子范畴表示纯计算，用冯·诺依曼代数表示混合态计算（海森堡绘景）。

Result: 成功展示了量子控制和经典控制范式可以在同一系统中结合，为量子编程语言设计提供了新的理论框架。

Conclusion: 该工作打破了传统上量子控制和经典控制分离的局面，通过创新的语法、操作和语义设计，实现了两种范式的统一处理，为量子编程语言的发展提供了新的可能性。

Abstract: The two main notions of control in quantum programming languages are often referred to as "quantum" control and "classical" control. With the latter, the control flow is based on classical information, potentially resulting from a quantum measurement, and this paradigm is well-suited to mixed state quantum computation. Whereas with quantum control, we are primarily focused on pure quantum computation and there the "control" is based on superposition. The two paradigms have not mixed well traditionally and they are almost always treated separately. In this work, we show that the paradigms may be combined within the same system. The key ingredients for achieving this are: (1) syntactically: a modality for incorporating pure quantum types into a mixed state quantum type system; (2) operationally: an adaptation of the notion of "quantum configuration" from quantum lambda-calculi, where the quantum data is replaced with pure quantum primitives; (3) denotationally: suitable (sub)categories of Hilbert spaces, for pure computation and von Neumann algebras, for mixed state computation in the Heisenberg picture of quantum mechanics.

</details>


### [6] [Group Knowledge of Hypothetical Values](https://arxiv.org/abs/2511.22379)
*Alexandru Baltag,Sonja Smets*

Main category: cs.LO

TL;DR: 论文研究了带有变量知识算子的认知逻辑的动态扩展，通过添加数据交换事件模态、分布式知识、条件分布式知识和确定描述等算子，建立了完整的公理化系统和可判定性证明。


<details>
  <summary>Details</summary>
Motivation: 近年来，认知逻辑被扩展以处理变量知识算子K_ax，但缺乏对动态数据交换事件（如公开宣告、小组内数据共享、变量值改变）的系统处理。需要建立能够处理这些半公开数据交换事件的动态认知逻辑框架。

Method: 逐步扩展逻辑系统：首先添加分布式知识算子K_Ax，然后添加条件分布式知识K^P_Ax，接着添加确定描述x^P_A，最后引入条件公共分布式知识概念。通过这种方法处理数据交换事件，并提供完整的公理化系统和可判定性证明。

Result: 建立了包含变量知识、数据交换事件模态、分布式知识、条件分布式知识、确定描述和条件公共分布式知识的完整逻辑系统，提供了完整的公理化系统和可判定性证明。

Conclusion: 通过系统地扩展认知逻辑，成功构建了能够处理半公开数据交换事件的动态逻辑框架，为变量知识的动态推理提供了形式化工具，并证明了系统的完备性和可判定性。

Abstract: In recent years, epistemic logics have been extended with operators K_ax for knowledge of (the value of) a variable x (by an agent a). We study dynamic versions of these logics, enriched with modalities for semi-public data-exchange events (e.g., public announcements, data-sharing within a subgroup, or changing the value of a variable). To obtain a complete axiomatization of data-exchange events, in the presence of equality x = y and K_ax, one needs to extend the logic further: first, with an operator for distributed knowledge K_Ax of the value (by a group of agents A); next, with a conditional version of this: distributed knowledge K^P_A x (of the value by a group) given some hypothetical condition (expressed by some proposition P); then, with definite descriptions x^P_A , denoting the 'hypothetical' value of x according to A's (distributed) knowledge given condition P. In order to deal with common knowledge in the presence of semi-public data exchanges, we also need to add a novel conditional version of the recent concept of common distributed knowledge. We investigate the resulting logic, giving examples and presenting a complete axiomatization and a decidability proof.

</details>


### [7] [Graded Distributed Belief](https://arxiv.org/abs/2511.22381)
*Emiliano Lorini,Dmitry Rozplokhas*

Main category: cs.LO

TL;DR: 提出一种新的分级分布式信念逻辑，用于表达群体对某个事实的分布式信念强度至少为k，基于计算语义和信念基概念，提供公理化、可判定性和PSPACE完备性结果。


<details>
  <summary>Details</summary>
Motivation: 需要一种能够量化群体分布式信念强度的逻辑框架，以形式化表达群体对某个事实的集体信念程度，并处理认知分歧等概念。

Method: 基于信念基的计算语义，通过合并个体信念基来计算群体分布式信念强度，构建分级分布式信念逻辑系统。

Result: 提供了该逻辑的希尔伯特式公理系统（可靠且完备）、通过过滤法获得可判定性、基于表列法的决策程序，并证明该逻辑是PSPACE完备的。

Conclusion: 成功构建了一个计算基础的分级分布式信念逻辑框架，能够形式化表达群体信念强度，为分析认知分歧等概念提供了有力工具，并建立了完整的逻辑系统性质。

Abstract: We introduce a new logic of graded distributed belief that allows us to express the fact that a group of agents distributively believe that a certain fact holds with at least strength k. We interpret our logic by means of computationally grounded semantics relying on the concept of belief base. The strength of the group's distributed belief is directly computed from the group's belief base after having merged its members' individual belief bases. We illustrate our logic with an intuitive example, formalizing the notion of epistemic disagreement. We also provide a sound and complete Hilbert-style axiomatization, decidability result obtained via filtration, and a tableaux-based decision procedure that allows us to state PSPACE-completeness for our logic.

</details>


### [8] [Comparing State-Representations for DEL Model Checking](https://arxiv.org/abs/2511.22382)
*Gregor Behnke,Malvin Gattinger,Avijeet Ghosh,Haitian Wang*

Main category: cs.LO

TL;DR: 本文分析了符号结构（基于BDD）和简洁模型（基于心理程序）两种DEL模型检查表示方法的计算复杂性，证明两者都是PSPACE完全的，并建立了它们之间的转换关系。


<details>
  <summary>Details</summary>
Motivation: 传统Kripke模型在（动态）认知逻辑模型检查中存在可扩展性问题，虽然已开发出基于BDD的符号结构和基于心理程序的简洁模型，但符号结构的理论复杂性未知，而简洁模型虽有理论分析但缺乏实现。需要填补这一研究空白。

Method: 通过理论分析证明基于BDD的符号结构上DEL模型检查的复杂性，建立符号结构与心理程序之间的直接转换关系，分析转换的指数复杂度特性。

Result: 证明基于BDD的符号结构上DEL模型检查是PSPACE完全的，认知逻辑（无动态）在符号结构上也是PSPACE完全的。建立了两种表示之间的双向转换，但转换会产生指数级输出，且从心理程序到BDD的转换不存在小型转换。

Conclusion: 符号结构和简洁模型在计算复杂性上等价（都是PSPACE完全的），两者之间存在指数复杂度的转换关系，这填补了理论复杂性分析的空缺，为实际实现提供了理论基础。

Abstract: Model checking with the standard Kripke models used in (Dynamic) Epistemic Logic leads to scalability issues. Hence alternative representations have been developed, in particular symbolic structures based on Binary Decision Diagrams (BDDs) and succinct models based on mental programs. While symbolic structures have been shown to perform well in practice, their theoretical complexity was not known so far. On the other hand, for succinct models model checking is known to be PSPACE-complete, but no implementations are available.
  We close this gap and directly relate the two representations. We show that model checking DEL on symbolic structures encoded with BDDs is also PSPACE-complete. In fact, already model checking Epistemic Logic without dynamics is PSPACE-complete on symbolic structures. We also provide direct translations between BDDs and mental programs. Both translations yield exponential outputs. For the translation from mental programs to BDDs we show that no small translation exists. For the other direction we conjecture the same.

</details>


### [9] [Logic of (Common or Distributed) Knowledge](https://arxiv.org/abs/2511.22385)
*Chenwei Shi*

Main category: cs.LO

TL;DR: 将认知逻辑推广到可以推理"共同分布式知识"、"分布式共同知识"等组合形式，并研究任意读取事件下的动态更新逻辑


<details>
  <summary>Details</summary>
Motivation: 现有认知逻辑在处理共同知识和分布式知识的组合形式方面存在局限，需要一种能够系统推理这些复杂知识结构的逻辑框架

Method: 推广认知逻辑以支持共同知识和分布式知识的各种组合形式，并引入动态更新机制来处理任意读取事件

Result: 为这些逻辑提供了公理化系统，并证明了其可靠性和完备性

Conclusion: 成功构建了一个能够处理复杂知识组合形式的扩展认知逻辑框架，并建立了完整的动态更新逻辑理论

Abstract: In this paper, we generalize epistemic logic so that it can help reason about ways of combining common knowledge and distributed knowledge such as "common distributed knowledge", "distributed common knowledge", "distributed common distributed knowledge" and so on. Moreover, we study the logic of its dynamic update by arbitrary reading events. We axiomatize these logics and prove their soundness and completeness.

</details>


### [10] [Are Large Random Graphs Always Safe to Hide?](https://arxiv.org/abs/2511.22387)
*Sourav Chakraborty,Sujata Ghosh,Smiha Samanta*

Main category: cs.LO

TL;DR: 该论文研究了随机大图上警察与强盗游戏的变体，探讨了逻辑框架在分析游戏获胜条件中的应用，发现当获胜条件可用特定一阶逻辑公式表达时，该玩家几乎总是获胜。


<details>
  <summary>Details</summary>
Motivation: 研究随机大图上的警察与强盗游戏变体，将其作为网络查询和搜索问题的测试平台，探索逻辑框架在分析这类游戏结果中的应用，从零一律角度深入理解逻辑与游戏之间的联系。

Method: 使用逻辑框架分析随机图上的警察与强盗游戏，特别关注当玩家获胜条件可以表示为特定类型的一阶逻辑公式时的情况，从零一律视角研究逻辑与游戏的关系。

Result: 研究发现，当任一玩家的获胜条件可以表示为某种特定类型的一阶逻辑公式时，该玩家在随机大图上几乎总是获胜。这为逻辑与游戏之间的连接提供了新的洞察。

Conclusion: 通过逻辑框架分析随机图上的警察与强盗游戏，揭示了逻辑表达与游戏获胜概率之间的深刻联系，为零一律视角下的逻辑-游戏关系研究提供了新见解。

Abstract: We discuss winning possibilities of players in various variants of cops and robber game played on large random graphs, a testbed for various kinds of network queries, search problems in particular. We explore the use of logic frameworks to investigate such results; in particular, we show that whenever a winning condition for either player can be expressed as a certain kind of formula in first-order logic, that player almost always wins. In the process, we obtain more insight into the logic-game connection from the zero-one law perspective.

</details>


### [11] [Complexity of Łukasiewicz Modal Probabilistic Logics](https://arxiv.org/abs/2511.22389)
*Daniil Kozhemiachenko,Igor Sedlár*

Main category: cs.LO

TL;DR: 本文研究了基于模态Łukasiewicz多值逻辑的概率逻辑家族，证明了它们能表达上下概率等精细概念，并获得了局部推理问题的PSPACE完备性结果。


<details>
  <summary>Details</summary>
Motivation: 模态概率逻辑为在涉及知识、信念、时间和行动等模态语境中推理概率提供了框架。本文旨在研究基于模态Łukasiewicz多值逻辑的特定逻辑家族，探索其表达能力和计算复杂性。

Method: 扩展模态Łukasiewicz多值逻辑，构建能表达上下概率等精细概率概念的概率逻辑系统。通过形式化分析，研究这些逻辑的表达能力和推理问题的计算复杂性。

Result: 证明了这些逻辑能够表达上下概率等精细概率概念。主要贡献是获得了两个局部推理问题变体的PSPACE完备性结果，为这些逻辑提供了精确的计算复杂性刻画。

Conclusion: 基于模态Łukasiewicz多值逻辑的概率逻辑家族具有丰富的表达能力，能处理精细的概率概念，其局部推理问题在计算复杂性上属于PSPACE完备类，这为实际应用中的算法设计提供了理论依据。

Abstract: Modal probabilistic logics provide a framework for reasoning about probability in modal contexts, involving notions such as knowledge, belief, time, and action. In this paper, we study a particular family of these logics, extending the modal Łukasiewicz many-valued logic. These logics are shown to be capable of expressing nuanced probabilistic concepts, including upper and lower probabilities. Our main contribution is a PSPACE-completeness result for two variants of the local consequence problem, providing a precise computational characterisation.

</details>


### [12] [Modal Logic for Simulation, Refinement, and Mutual Ignorance](https://arxiv.org/abs/2511.22390)
*Hans van Ditmarsch,Tim French,Rustam Galimullin,Louwe B. Kuijer*

Main category: cs.LO

TL;DR: 本文探讨了模拟与精化这两种双模拟变体之间的关系，通过量化这些关系来刻画多智能体系统中的信息变化，并研究了包含模拟、精化及相互事实无知模态的逻辑系统。


<details>
  <summary>Details</summary>
Motivation: 在动态认知逻辑中，量化精化关系（agents变得更知情）已被充分研究，但量化模拟关系（agents变得更不知情）关注较少。本文旨在探索这两种关系之间的联系，并利用相互事实无知的概念来刻画agents尚未学习任何事实信息前的状态。

Method: 采用基于归约的公理化方法，为包含模拟模态、精化模态以及相互事实无知模态的多模态逻辑扩展提供公理系统。这些逻辑系统以模块化方式相互扩展构建。

Result: 为多个由此产生的逻辑系统提供了基于归约的公理化，这些逻辑系统以模块化方式相互扩展。特别关注了模拟与精化关系之间的理论联系。

Conclusion: 通过系统研究模拟与精化关系，以及引入相互事实无知的概念，本文为理解多智能体系统中信息变化的逻辑结构提供了新的理论框架和公理化工具。

Abstract: Simulation and refinement are variations of the bisimulation relation, where in the former we keep only atoms and forth, and in the latter only atoms and back. Quantifying over simulations and refinements captures the effects of information change in a multi-agent system. In the case of quantification over refinements, we are looking at all the ways the agents in a system can become more informed. Similarly, in the case of quantification over simulations, we are dealing with all the ways the agents can become less informed, or in other words, could have been less informed, as we are at liberty how to interpret time in dynamic epistemic logic. While quantification over refinements has been well explored in the literature, quantification over simulations has received considerably less attention. In this paper, we explore the relationship between refinements and simulations. To this end, we also employ the notion of mutual factual ignorance that allows us to capture the state of a model before agents have learnt any factual information. In particular, we consider the extensions of multi-modal logic with the simulation and refinement modalities, as well as modalities for mutual factual ignorance. We provide reduction-based axiomatizations for several of the resulting logics that are built extending one another in a modular fashion.

</details>


### [13] [Impure Simplicial Complex and Term-Modal Logic with Assignment Operators](https://arxiv.org/abs/2511.22391)
*Yuanzhe Yang*

Main category: cs.LO

TL;DR: 提出一种带有赋值算子的项模态语言，用于在非纯单纯复形上建模多智能体认知情境，避免涉及死亡智能体的可疑表达式，并建立语义等价性和完备公理化。


<details>
  <summary>Details</summary>
Motivation: 非纯单纯复形是建模多智能体认知情境（智能体可能死亡）的有力工具，但普通命题模态语言在这种模型上难以定义令人满意的语义，因为该语言可以表达许多涉及死亡智能体的概念上可疑的表达式。

Method: 引入带有赋值算子的项模态语言，在这种语言中概念上可疑的表达式被语法排除；为该语言定义单纯复形语义和一阶克里普克语义；通过互模拟概念刻画各自的表达能力；证明当考虑称为局部认知模型的一类特殊一阶克里普克模型时，两种语义等价；提供基于该语言的认知逻辑的完备公理化；展示该语言具有赋值范式概念。

Result: 成功定义了一种避免死亡智能体可疑表达式的模态语言；建立了单纯复形语义和一阶克里普克语义的等价性；提供了完备的公理系统；证明了赋值范式的存在；讨论了可以在该语言中自然表达的一种内涵分布式知识的行为。

Conclusion: 提出的带有赋值算子的项模态语言为在非纯单纯复形上建模多智能体认知情境提供了合适的逻辑框架，避免了普通模态语言在处理死亡智能体时的概念问题，并建立了完整的语义和证明论基础。

Abstract: Impure simplicial complexes are a powerful tool to model multi-agent epistemic situations where agents may die, but it is difficult to define a satisfactory semantics for the ordinary propositional modal language on such models, since many conceptually dubious expressions involving dead agents can be expressed in this language. In this paper, we introduce a term-modal language with assignment operators, in which such conceptually dubious expressions are syntactically excluded. We define both simplicial semantics and first-order Kripke semantics for this language, characterize their respective expressivity through notions of bisimulation, and show that the two semantics are equivalent when we consider a special class of first order Kripke models called local epistemic models. We also offer a complete axiomatization for the epistemic logic based on this language, and show that our language has a notion of assignment normal form. Finally, we discuss the behavior of a kind of intensional distributed knowledge that can be naturally expressed in our language.

</details>


### [14] [Muddy Waters](https://arxiv.org/abs/2511.22392)
*Hans van Ditmarsch*

Main category: cs.LO

TL;DR: 该论文将Woeginger提出的帽子问题中的特殊公告形式化，在公开公告逻辑及其带不动点的扩展中建模"虽然信息不足但能学习颜色"的公告。


<details>
  <summary>Details</summary>
Motivation: 研究2013年Woeginger提出的帽子问题中的特殊公告形式，该公告声明帽子颜色被选择使得参与者能够学习自己的颜色，尽管初始信息不足。这种公告在认知逻辑中具有理论意义。

Method: 使用公开公告逻辑(PAL)和带不动点的公开公告逻辑扩展来形式化Woeginger帽子问题中的特殊公告。通过逻辑框架建模"虽然信息不足但能学习颜色"的认知状态。

Result: 成功在公开公告逻辑及其不动点扩展中形式化了Woeginger帽子问题的特殊公告，为这类认知推理问题提供了逻辑建模框架。

Conclusion: Woeginger帽子问题的特殊公告可以在公开公告逻辑及其不动点扩展中得到形式化，这为分析类似认知推理问题提供了逻辑工具。

Abstract: In the 2013 Advent calender of the Berlin Mathematics Research Center MATH+, Gerhard Woeginger presents a novel hat problem with an uncommon initial announcement. Although the information given is insufficient for the hat bearers to learn their colour, they are informed that the colours have been chosen so that they can learn their colour. We formalize this announcement in public announcement logic and in an extension of public announcement logic with fixpoints.

</details>


### [15] [Formal Verification of Probabilistic Multi-Agent Systems for Ballistic Rocket Flight Using Probabilistic Alternating-Time Temporal Logic](https://arxiv.org/abs/2511.22572)
*Damian Kurpiewski,Jędrzej Michalczyk,Wojciech Jamroga,Jerzy Julian Michalski,Teofil Sidoruk*

Main category: cs.LO

TL;DR: 提出基于PATL的概率代理系统形式化验证框架，用于弹道火箭飞行轨迹安全分析，整合真实遥测数据，实现实时安全监控和自动干预


<details>
  <summary>Details</summary>
Motivation: 为用于科学实验的弹道火箭（特别是实现微重力条件的火箭）开发可靠的安全验证方法，解决环境随机性（特别是气象变化）带来的轨迹偏差风险，防止火箭降落在禁止或危险区域

Method: 使用概率交替时间时序逻辑（PATL）构建形式化验证框架，整合真实飞行遥测数据（速度向量、俯仰角、姿态参数、GPS坐标）建立概率状态转移系统，形式化关键安全属性，实现实时安全监控和自动干预机制

Result: 实验验证表明该方法在实际应用中有效可靠，能够在确保任务安全的同时保持科学任务目标，能够系统识别轨迹偏差状态并触发紧急干预措施

Conclusion: 提出的PATL验证框架为弹道火箭飞行轨迹安全分析提供了全面有效的解决方案，特别适用于需要微重力条件的科学实验任务，实现了安全性和任务目标的平衡

Abstract: This technical report presents a comprehensive formal verification approach for probabilistic agent systems modeling ballistic rocket flight trajectories using Probabilistic Alternating-Time Temporal Logic (PATL). We describe an innovative verification framework specifically designed for analyzing critical safety properties of ballistic rockets engineered to achieve microgravity conditions for scientific experimentation. Our model integrates authentic flight telemetry data encompassing velocity vectors, pitch angles, attitude parameters, and GPS coordinates to construct probabilistic state transition systems that rigorously account for environmental stochasticity, particularly meteorological variability. We formalize mission-critical safety properties through PATL specifications to systematically identify trajectory deviation states where the rocket risks landing in prohibited or hazardous zones. The verification framework facilitates real-time safety monitoring and enables automated intervention mechanisms, including emergency engine disengagement protocols, when predefined safety thresholds are exceeded. Experimental validation demonstrates the practical effectiveness and reliability of our approach in ensuring mission safety while maintaining scientific mission objectives.

</details>


### [16] [Denotational semantics for stabiliser quantum programs](https://arxiv.org/abs/2511.22734)
*Robert I. Booth,Cole Comfort*

Main category: cs.LO

TL;DR: 为量子稳定子操作开发了基于有限域上仿射关系的指称语义，作为标准算子代数语义的可计算替代方案


<details>
  <summary>Details</summary>
Motivation: 量子稳定子理论是量子纠错和容错编译的基础，但标准算子代数语义在状态空间增大时计算复杂度呈指数增长，需要更高效的可计算语义

Method: 将稳定子操作（包括测量、经典控制泡利算符和仿射经典操作）解释为有限域上的仿射关系，建立可靠、通用且完备的指称语义

Result: 开发了一个概念清晰且计算易处理的小型汇编语言原型，用于稳定子程序，具有完全抽象的指称语义

Conclusion: 基于有限域仿射关系的语义为量子稳定子程序提供了比标准算子代数语义更高效、更易处理的理论框架，特别适用于量子纠错码作为一等公民的场景

Abstract: The stabiliser fragment of quantum theory is a foundational building block for quantum error correction and the fault-tolerant compilation of quantum programs. In this article, we develop a sound, universal and complete denotational semantics for stabiliser operations which include measurement, classically-controlled Pauli operators, and affine classical operations, in which quantum error-correcting codes are first-class objects. The operations are interpreted as certain affine relations over finite fields. This offers a conceptually motivated and computationally-tractable alternative to the standard operator-algebraic semantics of quantum programs (whose time complexity grows exponentially as the state space increases in size). We demonstrate the power of the resulting semantics by describing a small, proof-of-concept assembly language for stabiliser programs with fully-abstract denotational semantics.

</details>


### [17] [Neuro-Symbolic Constrained Optimization for Cloud Application Deployment via Graph Neural Networks and Satisfiability Modulo Theory](https://arxiv.org/abs/2511.23109)
*Madalina Erascu*

Main category: cs.LO

TL;DR: 提出混合神经符号框架，用于云中组件化应用的最优可扩展部署，结合图神经网络预测与Z3 SMT求解器约束求解。


<details>
  <summary>Details</summary>
Motivation: 云环境中组件化应用部署面临复杂约束优化问题，传统精确求解器在NP-hard问题上面临可扩展性限制，需要结合机器学习提升求解效率。

Method: 将部署问题形式化为约束优化问题，训练图神经网络在小规模实例上学习模式，将GNN预测作为软约束集成到Z3 SMT求解器中，将问题重构为异构图上的边分类任务。

Result: 实验验证表明，结合GNN预测能显著提升求解器可扩展性，同时保持甚至改进成本最优性，在多个现实案例研究中表现良好。

Conclusion: 神经符号耦合为云基础设施规划带来实际效益，为一般NP-hard问题提供了可重用方法论，展示了机器学习增强约束求解的潜力。

Abstract: This paper proposes a novel hybrid neuro-symbolic framework for the optimal and scalable deployment of component-based applications in the Cloud. The challenge of efficiently mapping application components to virtual machines (VMs) across diverse VM Offers from Cloud Providers is formalized as a constrained optimization problem (COP), considering both general and application-specific constraints. Due to the NP-hard nature and scalability limitations of exact solvers, we introduce a machine learning-enhanced approach where graph neural networks (GNNs) are trained on small-scale deployment instances and their predictions are used as soft constraints within the Z3 SMT solver. The deployment problem is recast as a graph edge classification task over a heterogeneous graph, combining relational embeddings with constraint reasoning. Our framework is validated through several realistic case studies, each highlighting different constraint profiles. Experimental results confirm that incorporating GNN predictions improves solver scalability and often preserves or even improves cost-optimality. This work demonstrates the practical benefits of neuro-symbolic coupling for Cloud infrastructure planning and contributes a reusable methodology for general NP-hard problems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [18] [Technical knowledge and soft skills in software startups within the Colombian entrepreneurial ecosystem](https://arxiv.org/abs/2511.21769)
*Royer David Estrada-Esponda,Gerardo Matturro,Jose Reinaldo Sabogal-Pinilla*

Main category: cs.SE

TL;DR: 研究哥伦比亚软件创业生态系统中，创业团队在早期阶段最看重的技术知识和软技能，以及这些需求如何随创业公司成长而变化。


<details>
  <summary>Details</summary>
Motivation: 创业团队成员的技术知识和软技能对软件创业公司的早期阶段有重要影响，创业公司的成败很大程度上取决于创始团队的质量。然而，在特定创业生态系统中，哪些知识和技能最受重视，以及这些需求如何随公司发展而变化，需要实证研究。

Method: 在哥伦比亚创业生态系统中，对软件创业公司代表进行调查研究，分析创业团队最看重的技术知识和软技能，并探讨这些需求如何随创业公司成长而演变。

Result: 调查显示，最受重视的技术知识包括：需求工程、软件测试、项目规划与管理、敏捷方法、市场营销、商业模式定义和预算编制。最受重视的软技能通常是沟通能力、领导力和团队合作。

Conclusion: 该研究结果对软件创业者、孵化器和研究人员具有重要参考价值，揭示了在特定创业生态系统中创业团队最需要的核心能力，有助于指导创业教育和支持体系建设。

Abstract: The technical knowledge and soft skills of entrepreneurial team members significantly impact the early stages of software startups. It is widely recognized that the success or failure of a startup is determined by the quality of the individuals who constitute the founding team. This article presents the findings of a study conducted within the Colombian entrepreneurial ecosystem, focusing on which technical knowledge and soft skills are the most valued by founding teams of software startups, and how the needs for knowledge and skills evolve as the startup grows. A survey of software startup representatives revealed that the most valued knowledge includes requirements engineering, software testing, project planning and management, agile methodologies, marketing, business model definition, and budgeting. The most valued soft skills are typically communication, leadership, and teamwork. The outcomes of this work are relevant to software entrepreneurs, incubators, and researchers.

</details>


### [19] [Code Refactoring with LLM: A Comprehensive Evaluation With Few-Shot Settings](https://arxiv.org/abs/2511.21788)
*Md. Raihan Tapader,Md. Mostafizer Rahman,Ariful Islam Shiplu,Md Faizul Ibne Amin,Yutaka Watanobe*

Main category: cs.SE

TL;DR: 提出基于LLM的多语言代码重构框架，通过提示工程和指令微调提升重构效果，在Java、Python等语言上验证了重构质量改进。


<details>
  <summary>Details</summary>
Motivation: 现有重构方法依赖手动规则，难以跨语言通用。需要开发能跨多种编程语言进行准确高效代码重构的LLM框架。

Method: 使用基于提示工程的微调模型结合少样本学习，研究温度参数、不同样本算法等提示工程技术对重构效果的影响。

Result: Java在10-shot设置下正确率最高达99.99%，平均可编译性94.78%，保持高相似度(53-54%)；Python在所有样本设置下结构距离最低(277-294)，相似度适中(44-48%)。

Conclusion: 提出的LLM框架能有效进行多语言代码重构，Java在保持语义的同时实现结构改进，Python实现最小破坏性重构，证明了方法的有效性。

Abstract: In today's world, the focus of programmers has shifted from writing complex, error-prone code to prioritizing simple, clear, efficient, and sustainable code that makes programs easier to understand. Code refactoring plays a critical role in this transition by improving structural organization and optimizing performance. However, existing refactoring methods are limited in their ability to generalize across multiple programming languages and coding styles, as they often rely on manually crafted transformation rules. The objectives of this study are to (i) develop an Large Language Models (LLMs)-based framework capable of performing accurate and efficient code refactoring across multiple languages (C, C++, C#, Python, Java), (ii) investigate the impact of prompt engineering (Temperature, Different shot algorithm) and instruction fine-tuning on refactoring effectiveness, and (iii) evaluate the quality improvements (Compilability, Correctness, Distance, Similarity, Number of Lines, Token, Character, Cyclomatic Complexity) in refactored code through empirical metrics and human assessment. To accomplish these goals, we propose a fine-tuned prompt-engineering-based model combined with few-shot learning for multilingual code refactoring. Experimental results indicate that Java achieves the highest overall correctness up to 99.99% the 10-shot setting, records the highest average compilability of 94.78% compared to the original source code and maintains high similarity (Approx. 53-54%) and thus demonstrates a strong balance between structural modifications and semantic preservation. Python exhibits the lowest structural distance across all shots (Approx. 277-294) while achieving moderate similarity ( Approx. 44-48%) that indicates consistent and minimally disruptive refactoring.

</details>


### [20] [LLM-Empowered Event-Chain Driven Code Generation for ADAS in SDV systems](https://arxiv.org/abs/2511.21877)
*Nenad Petrovic,Norbert Kroth,Axel Torschmied,Yinglei Song,Fengjunjie Pan,Vahid Zolfaghari,Nils Purschke,Sven Kirchner,Chengdong Wu,Andre Schamschurko,Yi Zhang,Alois Knoll*

Main category: cs.SE

TL;DR: 提出基于事件链驱动、LLM增强的工作流，从自然语言需求生成经过验证的汽车代码，无需LLM重训练


<details>
  <summary>Details</summary>
Motivation: 解决从自然语言需求自动生成汽车代码时面临的挑战：减少LLM幻觉、确保架构正确性、保持行为一致性、满足实时可行性

Method: 1) RAG层从VSS目录检索相关信号作为上下文；2) 信号映射验证后转换为编码因果和时序约束的事件链；3) 事件链引导约束LLM代码合成

Result: 在紧急制动案例研究中，实现了有效的信号使用和一致的代码生成，无需LLM重训练

Conclusion: 提出的工作流能够从自然语言需求生成经过验证的汽车代码，确保架构正确性和行为一致性，为汽车软件开发提供了有效的自动化解决方案

Abstract: This paper presents an event-chain-driven, LLM-empowered workflow for generating validated, automotive code from natural-language requirements. A Retrieval-Augmented Generation (RAG) layer retrieves relevant signals from large and evolving Vehicle Signal Specification (VSS) catalogs as code generation prompt context, reducing hallucinations and ensuring architectural correctness. Retrieved signals are mapped and validated before being transformed into event chains that encode causal and timing constraints. These event chains guide and constrain LLM-based code synthesis, ensuring behavioral consistency and real-time feasibility. Based on our initial findings from the emergency braking case study, with the proposed approach, we managed to achieve valid signal usage and consistent code generation without LLM retraining.

</details>


### [21] [Advancing Automated In-Isolation Validation in Repository-Level Code Translation](https://arxiv.org/abs/2511.21878)
*Kaiyao Ke,Ali Reza Ibrahimzada,Rangeet Pan,Saurabh Sinha,Reyhaneh Jabbarvand*

Main category: cs.SE

TL;DR: TRAM提出了一种结合上下文感知类型解析和基于mock的隔离验证的仓库级代码翻译方法，在Java到Python翻译中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管仓库级代码翻译有所进展，但验证翻译结果仍然具有挑战性。现有方法要么使用代理进行验证成本高昂，要么依赖语言互操作性需要大量人工努力。

Method: TRAM结合RAG技术检索API文档和上下文代码信息，使用LLM解析跨语言类型映射，然后通过自定义序列化/反序列化工作流自动构建目标语言中的等效mock对象，实现方法片段的隔离验证。

Result: TRAM在Java到Python翻译中展示了最先进的性能，证明了其RAG类型解析与可靠隔离验证集成的有效性。

Conclusion: TRAM通过上下文感知类型解析和mock-based隔离验证，实现了高质量的跨语言代码翻译，为仓库级代码迁移提供了有效的解决方案。

Abstract: Repository-level code translation aims to migrate entire repositories across programming languages while preserving functionality automatically. Despite advancements in repository-level code translation, validating the translations remains challenging. This paper proposes TRAM, which combines context-aware type resolution with mock-based in-isolation validation to achieve high-quality translations between programming languages. Prior to translation, TRAM retrieves API documentation and contextual code information for each variable type in the source language. It then prompts a large language model (LLM) with retrieved contextual information to resolve type mappings across languages with precise semantic interpretations. Using the automatically constructed type mapping, TRAM employs a custom serialization/deserialization workflow that automatically constructs equivalent mock objects in the target language. This enables each method fragment to be validated in isolation, without the high cost of using agents for translation validation, or the heavy manual effort required by existing approaches that rely on language interoperability. TRAM demonstrates state-of-the-art performance in Java-to-Python translation, underscoring the effectiveness of its integration of RAG-based type resolution with reliable in-isolation validation.

</details>


### [22] [Toward Automated and Trustworthy Scientific Analysis and Visualization with LLM-Generated Code](https://arxiv.org/abs/2511.21920)
*Apu Kumar Chakroborti,Yi Ding,Lipeng Wan*

Main category: cs.SE

TL;DR: 该论文研究了开源大语言模型在科学数据分析和可视化中生成Python代码的可信度，发现当前模型在没有人工干预的情况下可靠性有限，并提出了三种改进策略来提升执行成功率和输出质量。


<details>
  <summary>Details</summary>
Motivation: 随着科学数据日益复杂，许多领域科学家缺乏编程专业知识来开发自定义数据分析工作流，这阻碍了及时有效的洞察。大语言模型通过自然语言描述生成可执行代码提供了有前景的解决方案，但其在科学数据分析和可视化中的可信度需要系统评估。

Method: 构建了反映真实研究任务的领域启发式提示基准套件，系统评估生成代码的可执行性和正确性。针对发现的问题，设计并评估了三种互补策略：数据感知提示消歧、检索增强提示改进和迭代错误修复。

Result: 研究发现，在没有人工干预的情况下，LLM生成代码的可靠性有限，经常因模糊提示和模型对领域特定上下文理解不足而失败。提出的三种策略显著提高了执行成功率和输出质量，但仍需进一步改进。

Conclusion: 这项工作突出了LLM驱动自动化在科学工作流中的前景和当前局限性，并引入了可操作的技术和可重复使用的基准，用于构建更具包容性、可访问性和可信度的AI辅助研究工具。

Abstract: As modern science becomes increasingly data-intensive, the ability to analyze and visualize large-scale, complex datasets is critical to accelerating discovery. However, many domain scientists lack the programming expertise required to develop custom data analysis workflows, creating barriers to timely and effective insight. Large language models (LLMs) offer a promising solution by generating executable code from natural language descriptions. In this paper, we investigate the trustworthiness of open-source LLMs in autonomously producing Python scripts for scientific data analysis and visualization. We construct a benchmark suite of domain-inspired prompts that reflect real-world research tasks and systematically evaluate the executability and correctness of the generated code. Our findings show that, without human intervention, the reliability of LLM-generated code is limited, with frequent failures caused by ambiguous prompts and the models' insufficient understanding of domain-specific contexts. To address these challenges, we design and assess three complementary strategies: data-aware prompt disambiguation, retrieval-augmented prompt enhancement, and iterative error repair. While these methods significantly improve execution success rates and output quality, further refinement is needed. This work highlights both the promise and current limitations of LLM-driven automation in scientific workflows and introduces actionable techniques and a reusable benchmark for building more inclusive, accessible, and trustworthy AI-assisted research tools.

</details>


### [23] [Beyond Like-for-Like: A User-centered Approach to Modernizing Legacy Applications](https://arxiv.org/abs/2511.21956)
*M. Polzin,M. Guzman*

Main category: cs.SE

TL;DR: 论文探讨了在现代化遗留应用时，不应简单进行工具和样式替换，而应利用现有应用作为参考，通过用户参与来创建更直观、支持用户任务效率的新应用。


<details>
  <summary>Details</summary>
Motivation: 现代化遗留应用时，开发团队往往倾向于创建功能对等的复制品，但这会延续原有应用的痛点、低效流程，最终导致用户不愿使用。需要探索如何利用现有应用的优势，同时解决其问题，创建真正满足用户需求的应用。

Method: 提出通过用户参与的方法来现代化遗留应用：1）将现有应用作为设计参考而非限制；2）让用户参与设计过程，平衡新手和专家用户需求；3）利用现有应用提供的洞察，识别需要改进的痛点和需要保留的高效流程。

Result: 通过用户参与的方法，可以在现代化过程中：1）弥合功能对等复制与全新GUI设计之间的差距；2）创建既满足新手用户直观性需求，又保留专家用户熟悉工作流程的应用；3）利用现有应用作为开发工具，而不是开发障碍。

Conclusion: 遗留应用现代化不应是简单的功能复制，而应将其视为设计资源，通过用户参与来创建更优的应用。现有应用提供了空白画布无法提供的宝贵洞察，合理利用这些洞察可以开发出既解决现有问题又满足不同用户需求的应用。

Abstract: When modernizing a legacy application, it is easy to fall back on a like-for-like replica with new tools and updated design stylings, but this is an opportunity to explore making a more intuitive application that supports user tasks and efficiency. Rather than having a blank canvas-unburdened by legacy tech debt-to create a new application, you are working with an existing application that is integral to accelerator operations and one that expert users are already familiar with. Due to this, you might assume people will prefer the like-for-like, but you could be carrying forward the pain points, processes that are inefficient, and ultimately wind up with an application that no one wants to use because it doesn't solve existing problems. Getting users involved can make all the difference in your approach to modernizing a legacy application that caters to both newer and expert users. It also can bridge the gap between like-for-like and introducing new GUI design. Having a legacy application doesn't have to make the modernized one difficult to develop, as the existing application is a tool in how you move forward with the new application. It provides insight into areas that an application with a clean slate doesn't give you.

</details>


### [24] [DRS-OSS: LLM-Driven Diff Risk Scoring Tool for PR Risk Prediction](https://arxiv.org/abs/2511.21964)
*Ali Sayedsalehi,Peter C. Rigby,Audris Mockus*

Main category: cs.SE

TL;DR: DRS-OSS是一个开源系统，用于评估代码变更引入缺陷的风险，帮助优先审查、测试规划和CI/CD门控，基于微调的Llama 3.1 8B模型，在ApacheJIT基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 大规模开源项目中每天有大量代码合并请求，每个都可能引入回归缺陷。需要一种方法来评估代码变更的风险，以便更好地进行审查优先级排序、测试规划和CI/CD门控。

Method: 使用微调的Llama 3.1 8B序列分类器，处理包含提交信息、结构化差异和变更指标的长上下文表示。通过参数高效适配、4位QLoRA和DeepSpeed ZeRO-3 CPU卸载技术，在单张20GB GPU上训练22k令牌的上下文。

Result: 在ApacheJIT基准测试中达到最先进性能（F1=0.64，ROC-AUC=0.89）。模拟显示，仅对风险最高的30%提交进行门控，可以防止高达86.4%的缺陷引入变更。

Conclusion: DRS-OSS是一个有效的开源风险评分系统，通过API网关、React仪表板和GitHub应用集成到开发者工作流中，能够显著减少缺陷引入，并提供了完整的复制包和部署方案。

Abstract: In large-scale open-source projects, hundreds of pull requests land daily, each a potential source of regressions. Diff Risk Scoring (DRS) estimates the likelihood that a diff will introduce a defect, enabling better review prioritization, test planning, and CI/CD gating. We present DRS-OSS, an open-source DRS system equipped with a public API, web UI, and GitHub plugin. DRS-OSS uses a fine-tuned Llama 3.1 8B sequence classifier trained on the ApacheJIT dataset, consuming long-context representations that combine commit messages, structured diffs, and change metrics. Through parameter-efficient adaptation, 4-bit QLoRA, and DeepSpeed ZeRO-3 CPU offloading, we train 22k-token contexts on a single 20 GB GPU. On the ApacheJIT benchmark, DRS-OSS achieves state-of-the-art performance (F1 = 0.64, ROC-AUC = 0.89). Simulations show that gating only the riskiest 30% of commits can prevent up to 86.4% of defect-inducing changes. The system integrates with developer workflows through an API gateway, a React dashboard, and a GitHub App that posts risk labels on pull requests. We release the full replication package, fine-tuning scripts, deployment artifacts, code, demo video, and public website.

</details>


### [25] [Statistical Independence Aware Caching for LLM Workflows](https://arxiv.org/abs/2511.22118)
*Yihan Dai,Dimitrios Stamatios Bouras,Haoxiang Jia,Sergey Mechtaev*

Main category: cs.SE

TL;DR: Mnimi是一个LLM缓存设计模式，通过封装统计约束在LLM引用类型中，确保组件级统计完整性，同时提高代码LLM应用的效率、可复现性和调试便利性。


<details>
  <summary>Details</summary>
Motivation: LLM推理成本高且延迟大，本地缓存可降低成本延迟，增强可复现性和实验灵活性。但简单重用缓存响应会破坏统计独立性，而统计独立性对代码LLM应用的性能指标（如Pass@k）、不确定性估计以及程序修复循环等算法至关重要。现有LLM缓存系统缺乏强制执行统计独立性约束的方法。

Method: 提出Mnimi缓存设计模式，核心创新是将统计约束封装在LLM引用类型中，允许用户根据算法范围和需求管理和转换这些类型。在Python中实现，结合装饰器和无限序列迭代器。

Result: 在SpecFix（自动化程序规范修复系统）的案例研究中，Mnimi在保持统计正确性的同时，提高了可复现性、调试便利性、时间和成本效率。

Conclusion: Mnimi为模块化LLM工作流提供了支持统计完整性的缓存解决方案，解决了现有系统缺乏统计独立性约束的问题，特别适用于需要统计正确性的代码LLM应用场景。

Abstract: Large language models (LLMs) inference is both expensive and slow. Local caching of responses offers a practical solution to reduce the cost and latency of LLM queries. In research contexts, caching also enhances reproducibility and provides flexibility for experimentation. However, naive reuse of cached responses compromises statistical independence, a critical property for probabilistic workflows. In applications of LLM for code, it underpins performance metrics such as Pass@k and uncertainty estimation, as well as algorithms like program repair loops and retries. Existing LLM caching systems lack ways to enforce statistical independence constraints. To address this, we introduce Mnimi, a cache design pattern that supports modular LLM workflows while ensuring statistical integrity at the component level. Its core innovation lies in encapsulating statistical constraints within the type of LLM references, allowing users to manage and transform these types according to the scope and requirements of their algorithm. We implemented this design pattern in Python using a combination of decorators and iterators over infinite sequences. A case study on SpecFix, an recent automated program specification repair system, highlights how Mnimi improves reproducibility, ease of debugging, time and cost efficiency while preserving statistical correctness.

</details>


### [26] [Exploring the SECURITY.md in the Dependency Chain: Preliminary Analysis of the PyPI Ecosystem](https://arxiv.org/abs/2511.22186)
*Chayanid Termphaiboon,Raula Gaikovina Kula,Youmei Fan,Morakot Choetkiertikul,Chaiyong Ragkhitwetsagul,Thanwadee Sunetnanta,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: 研究探讨了安全政策（SECURITY.md文件）对Python项目依赖管理的影响，发现拥有安全政策的项目依赖更多直接依赖包，且依赖更新更频繁。


<details>
  <summary>Details</summary>
Motivation: 尽管SECURITY.md文件在开源项目中越来越普遍，但尚不清楚这些安全政策如何影响软件依赖的结构和演变。软件依赖的互联性对功能和安全性都有重要影响，需要研究安全政策与依赖管理之间的关系。

Method: 通过分析PyPI项目中拥有和没有SECURITY.md文件的项目，检查它们的依赖树并跟踪依赖随时间的变化情况。

Result: 拥有安全政策的项目倾向于依赖更广泛的直接依赖集，而总体深度和传递依赖保持相似。在SECURITY.md引入后创建的项目，特别是后期采用者，显示出更频繁的依赖更新。

Conclusion: 安全政策与更模块化和功能丰富的项目相关，SECURITY.md在促进主动依赖管理和降低软件供应链风险方面发挥重要作用。

Abstract: Security policies, such as SECURITY.md files, are now common in open-source projects. They help guide responsible vulnerability reporting and build trust among users and contributors. Despite their growing use, it is still unclear how these policies influence the structure and evolution of software dependencies. Software dependencies are external packages or libraries that a project relies on, and their interconnected nature affects both functionality and security. This study explores the relationship between security policies and dependency management in PyPI projects. We analyzed projects with and without a SECURITY.md file by examining their dependency trees and tracking how dependencies change over time. The analysis shows that projects with a security policy tend to rely on a broader set of direct dependencies, while overall depth and transitive dependencies remain similar. Historically, projects created after the introduction of SECURITY.md, particularly later adopters, show more frequent dependency updates. These results suggest that security policies are linked to more modular and feature-rich projects, and highlight the role of SECURITY.md in promoting proactive dependency management and reducing risks in the software supply chain.

</details>


### [27] [UniBOM -- A Unified SBOM Analysis and Visualisation Tool for IoT Systems and Beyond](https://arxiv.org/abs/2511.22359)
*Vadim Safronov,Ionut Bostan,Nicholas Allott,Andrew Martin*

Main category: cs.SE

TL;DR: UniBOM是一个先进的SBOM生成、分析和可视化工具，专门针对二进制分析和非包管理语言（如C/C++）的依赖关系，提供细粒度漏洞检测和风险管理。


<details>
  <summary>Details</summary>
Motivation: 现代网络系统依赖复杂的软件栈，其中隐藏着由复杂依赖关系产生的漏洞。现有的SBOM解决方案在二进制分析和非包管理语言（如C/C++）方面缺乏精确性，无法有效识别依赖关系和缓解安全风险。

Method: UniBOM集成了二进制、文件系统和源代码分析，支持历史CPE跟踪、基于AI的漏洞分类（按严重程度和内存安全），以及非包管理的C/C++依赖关系分析。该工具提供端到端的统一分析和可视化解决方案。

Result: 通过对258个无线路由器固件二进制文件和四个流行物联网操作系统源代码的比较漏洞分析，证明了UniBOM相比其他广泛使用的SBOM生成和分析工具具有更优越的检测能力。

Conclusion: UniBOM作为开源工具，提供了先进的SBOM驱动安全管理解决方案，能够增强网络系统的安全可追溯性，并适用于更广泛的软件环境。

Abstract: Modern networked systems rely on complex software stacks, which often conceal vulnerabilities arising from intricate interdependencies. A Software Bill of Materials (SBOM) is effective for identifying dependencies and mitigating security risks. However, existing SBOM solutions lack precision, particularly in binary analysis and non-package-managed languages like C/C++.
  This paper introduces UniBOM, an advanced tool for SBOM generation, analysis, and visualisation, designed to enhance the security accountability of networked systems. UniBOM integrates binary, filesystem, and source code analysis, enabling fine-grained vulnerability detection and risk management. Key features include historical CPE tracking, AI-based vulnerability classification by severity and memory safety, and support for non-package-managed C/C++ dependencies.
  UniBOM's effectiveness is demonstrated through a comparative vulnerability analysis of 258 wireless router firmware binaries and the source code of four popular IoT operating systems, highlighting its superior detection capabilities compared to other widely used SBOM generation and analysis tools. Packaged for open-source distribution, UniBOM offers an end-to-end unified analysis and visualisation solution, advancing SBOM-driven security management for dependable networked systems and broader software.

</details>


### [28] [NOMAD: A Multi-Agent LLM System for UML Class Diagram Generation from Natural Language Requirements](https://arxiv.org/abs/2511.22409)
*Polydoros Giannouris,Sophia Ananiadou*

Main category: cs.SE

TL;DR: NOMAD是一个认知启发的模块化多智能体框架，通过角色专业化子任务分解来生成UML类图，在性能上超越基线方法，并首次系统分类了LLM生成UML图中的错误类型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在软件工程中的应用日益广泛，但其生成UML图等结构化制品的能力尚未得到充分探索。现有方法在生成结构化建模制品方面存在局限性，需要更可靠的语言到模型的工作流程。

Method: 提出NOMAD框架，采用认知启发的模块化多智能体设计，将UML生成分解为实体提取、关系分类、图合成等角色专业化子任务，模拟工程师的目标导向推理过程。通过混合评估设计：大型案例研究（Northwind）进行深度探测和错误分析，以及人工编写的UML练习进行广度和真实性评估。

Result: NOMAD在所有选定的基线方法中表现最佳，同时揭示了在细粒度属性提取方面的持续挑战。基于观察结果，首次提出了LLM生成UML图的系统错误分类法，包括结构、关系和语义/逻辑错误。验证作为设计探针显示出混合效果，自适应策略是有前景的方向。

Conclusion: NOMAD既是一个有效的UML类图生成框架，也为可靠的语言到模型工作流程的更广泛研究挑战提供了一个视角。该框架通过任务分解提高了可解释性，并支持有针对性的验证策略。

Abstract: Large Language Models (LLMs) are increasingly utilised in software engineering, yet their ability to generate structured artefacts such as UML diagrams remains underexplored. In this work we present NOMAD, a cognitively inspired, modular multi-agent framework that decomposes UML generation into a series of role-specialised subtasks. Each agent handles a distinct modelling activity, such as entity extraction, relationship classification, and diagram synthesis, mirroring the goal-directed reasoning processes of an engineer. This decomposition improves interpretability and allows for targeted verification strategies. We evaluate NOMAD through a mixed design: a large case study (Northwind) for in-depth probing and error analysis, and human-authored UML exercises for breadth and realism. NOMAD outperforms all selected baselines, while revealing persistent challenges in fine-grained attribute extraction. Building on these observations, we introduce the first systematic taxonomy of errors in LLM-generated UML diagrams, categorising structural, relationship, and semantic/logical. Finally, we examine verification as a design probe, showing its mixed effects and outlining adaptive strategies as promising directions. Together, these contributions position NOMAD as both an effective framework for UML class diagram generation and a lens onto the broader research challenges of reliable language-to-model workflows.

</details>


### [29] [Declarative Policy Control for Data Spaces: A DSL-Based Approach for Manufacturing-X](https://arxiv.org/abs/2511.22513)
*Jérôme Pfeiffer,Nicolai Maisch,Sebastian Friedl,Matthias Milan Strljic,Armin Lechler,Oliver Riedel,Andreas Wortmann*

Main category: cs.SE

TL;DR: 提出一种基于领域特定语言（DSL）的方法，让领域专家能够以声明式、人类可读的方式定义数据使用策略，用于工业4.0中的数据空间连接器，实现主权数据共享。


<details>
  <summary>Details</summary>
Motivation: 随着GAIA-X和国际数据空间（IDS）等联邦数据空间的采用，工业4.0中跨组织边界的安全主权数据共享成为可能。然而，现有技术框架（如AAS、EDC、ID-Link、OPC UA）面临一个主要挑战：如何让没有软件工程背景的领域专家实际描述和执行上下文相关的数据使用策略。

Method: 提出利用领域特定语言（DSL）的方法，实现声明式、人类可读且机器可执行的政策定义。DSL使领域专家能够指定细粒度的数据治理要求（如限制特定生产批次的数据访问、定义保留期后自动删除等），而无需编写命令式代码。

Result: 该方法通过DSL赋能领域专家，使他们能够直接定义复杂的数据治理策略，解决了现有数据空间技术中政策定义和执行的实际挑战，促进了制造业生态系统中的主权数据共享。

Conclusion: DSL方法为工业4.0数据空间中的政策定义和执行提供了实用解决方案，使领域专家能够有效参与数据治理，推动了跨工厂流程优化、预测性维护和供应商集成等用例的实现。

Abstract: The growing adoption of federated data spaces, such as in the GAIA-X and the International Data Spaces (IDS) initiative, promises secure and sovereign data sharing across organizational boundaries in Industry 4.0. In manufacturing ecosystems, this enables use cases, such as cross-factory process optimization, predictive maintenance, and supplier integration. Frameworks and standards, such as the Asset Administration Shell (AAS), Eclipse Dataspace Connector (EDC), ID-Link and Open Platform Communications Unified Architecture (OPC UA) provide a strong foundation to realize this ecosystem. However, a major open challenge is the practical description and enforcement of context-dependent data usage policies using these base technologies - especially by domain experts without software engineering backgrounds. Therefore, this article proposes a method for leveraging domain-specific languages (DSLs) to enable declarative, human-readable, and machine-executable policy definitions for sovereign data sharing via data space connectors. The DSL empowers domain experts to specify fine-grained data governance requirements - such as restricting access to data from specific production batches or enforcing automatic deletion after a defined retention period - without writing imperative code.

</details>


### [30] [The Repeat Offenders: Characterizing and Predicting Extremely Bug-Prone Source Methods](https://arxiv.org/abs/2511.22726)
*Ethan Friesen,Sasha Morton-Salmon,Md Nahidul Islam Opu,Shahidul Islam,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 极端Bug方法（多次涉及bug修复）仅占所有方法的极小部分，却造成了不成比例的大量bug，但早期预测仍不可靠，需要结合演化感知的代码表示和模式分析。


<details>
  <summary>Details</summary>
Motivation: 识别那些反复吸引bug的源代码子集对于减少长期维护成本至关重要。研究极端Bug方法（多次涉及bug修复）的普遍性、特征和可预测性，以帮助开发人员早期识别高风险代码。

Method: 使用98个开源Java项目的125万多个方法数据集，分析极端Bug方法的特征。评估五种机器学习模型的预测性能，并对265个极端Bug方法进行主题分析，识别视觉问题、上下文角色和常见缺陷模式。

Result: 极端Bug方法仅占所有方法的极小部分，但贡献了不成比例的bug数量。这些方法在创建时更大、更复杂、可读性和可维护性更差。然而，机器学习预测不可靠，主要受数据不平衡、项目异质性和演化引入bug的影响。主题分析揭示了控制流混乱、核心逻辑、错误处理脆弱等常见模式。

Conclusion: 需要更丰富的、演化感知的代码表示来改进预测，并为开发人员提供了可操作的见解，以便在开发生命周期早期优先处理高风险方法。

Abstract: Identifying the small subset of source code that repeatedly attracts bugs is critical for reducing long-term maintenance effort. We define ExtremelyBuggy methods as those involved in more than one bug fix and present the first large-scale study of their prevalence, characteristics, and predictability. Using a dataset of over 1.25 million methods from 98 open-source Java projects, we find that ExtremelyBuggy methods constitute only a tiny fraction of all methods, yet frequently account for a disproportionately large share of bugs. At their inception, these methods are significantly larger, more complex, less readable, and less maintainable than both singly-buggy and non-buggy methods. However, despite these measurable differences, a comprehensive evaluation of five machine learning models shows that early prediction of ExtremelyBuggy methods remains highly unreliable due to data imbalance, project heterogeneity, and the fact that many bugs emerge through subsequent evolution rather than initial implementation. To complement these quantitative findings, we conduct a thematic analysis of 265 ExtremelyBuggy methods, revealing recurring visual issues (e.g., confusing control flow, poor readability), contextual roles (e.g., core logic, data transformation, external resource handling), and common defect patterns (e.g., faulty conditionals, fragile error handling, misuse of variables). These results highlight the need for richer, evolution-aware representations of code and provide actionable insights for practitioners seeking to prioritize high-risk methods early in the development lifecycle.

</details>


### [31] [MBFL-DKMR: Improving Mutation-based Fault Localization through Denoising-based Kill Matrix Refinement](https://arxiv.org/abs/2511.22921)
*Hengyuan Liu,Xia Song,Yong Liu,Zheng Li*

Main category: cs.SE

TL;DR: 提出DKMR方法，通过信号处理技术对MBFL中的kill矩阵进行去噪，提高故障定位效果


<details>
  <summary>Details</summary>
Motivation: 基于变异的故障定位(MBFL)存在噪声问题，特别是变异体与测试之间的虚假kill关系，这会显著降低定位效果。现有方法主要修正最终定位结果，而没有直接解决底层噪声问题。

Method: 提出DKMR方法，将kill矩阵视为包含故障相关模式和噪声的信号，采用两阶段处理：1) 通过混合矩阵构建进行信号增强以提高信噪比；2) 通过频域滤波进行信号去噪，抑制噪声同时保留故障相关模式。基于此开发MBFL-DKMR框架，使用精炼后的模糊值矩阵进行可疑度计算。

Result: 在Defects4J v2.0.0上的评估显示，MBFL-DKMR有效缓解了噪声问题，优于最先进的MBFL技术。具体来说，MBFL-DKMR在Top-1定位了129个故障，而BLMu为85个，Delta4Ms为103个，且额外计算开销可忽略不计（0.11秒，占总时间的0.001%）。

Conclusion: 这项工作展示了信号处理技术在通过精炼kill矩阵来增强MBFL有效性方面的潜力，为解决MBFL中的噪声问题提供了新思路。

Abstract: Software debugging is a critical and time-consuming aspect of software development, with fault localization being a fundamental step that significantly impacts debugging efficiency. Mutation-Based Fault Localization (MBFL) has gained prominence due to its robust theoretical foundations and fine-grained analysis capabilities. However, recent studies have identified a critical challenge: noise phenomena, specifically the false kill relationships between mutants and tests, which significantly degrade localization effectiveness. While several approaches have been proposed to rectify the final localization results, they do not directly address the underlying noise. In this paper, we propose a novel approach to refine the kill matrix, a core data structure capturing mutant-test relationships in MBFL, by treating it as a signal that contains both meaningful fault-related patterns and high-frequency noise. Inspired by signal processing theory, we introduce DKMR (Denoising-based Kill Matrix Refinement), which employs two key stages: (1) signal enhancement through hybrid matrix construction to improve the signal-to-noise ratio for better denoising, and (2) signal denoising via frequency domain filtering to suppress noise while preserving fault-related patterns. Building on this foundation, we develop MBFL-DKMR, a fault localization framework that utilizes the refined matrix with fuzzy values for suspiciousness calculation. Our evaluation on Defects4J v2.0.0 demonstrates that MBFL-DKMR effectively mitigates the noise and outperforms the state-of-the-art MBFL techniques. Specifically, MBFL-DKMR achieves 129 faults localized at Top-1 compared to 85 for BLMu and 103 for Delta4Ms, with negligible additional computational overhead (0.11 seconds, 0.001\% of total time). This work highlights the potential of signal processing techniques to enhance the effectiveness of MBFL by refining the kill matrix.

</details>


### [32] [A transfer learning approach for automatic conflicts detection in software requirement sentence pairs based on dual encoders](https://arxiv.org/abs/2511.23007)
*Yizheng Wang,Tao Jiang,Jinyan Bai,Zhengbin Zou,Tiancheng Xue,Nan Zhang,Jie Luan*

Main category: cs.SE

TL;DR: 本文提出TSRCDF-SS框架，基于SBERT和SimCSE的双编码器架构，结合六元拼接策略和混合损失优化，用于软件需求冲突检测，在跨域场景中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 软件需求文档通常包含数万条需求，确保需求间一致性对项目成功至关重要。现有自动化检测方法面临三个主要挑战：1) 不平衡数据下检测准确率低；2) 单一编码器语义提取有限；3) 跨域迁移学习性能不佳。

Method: 提出TSRCDF-SS框架：1) 使用SBERT和SimCSE双独立编码器生成需求对句子嵌入，采用六元拼接策略；2) 通过两层全连接FFNN分类器，结合混合损失优化策略（Focal Loss变体、领域特定约束、置信度惩罚项）；3) 协同整合序列和跨域迁移学习。

Result: 实验结果表明：在域内设置中，宏F1和加权F1分数均提升10.4%；在跨域场景中，宏F1分数提升11.4%。

Conclusion: TSRCDF-SS框架有效解决了现有需求冲突检测方法的局限性，通过双编码器架构、混合损失优化和迁移学习策略，显著提升了检测性能，特别是在跨域场景中表现出色。

Abstract: Software Requirement Document (RD) typically contain tens of thousands of individual requirements, and ensuring consistency among these requirements is critical for the success of software engineering projects. Automated detection methods can significantly enhance efficiency and reduce costs; however, existing approaches still face several challenges, including low detection accuracy on imbalanced data, limited semantic extraction due to the use of a single encoder, and suboptimal performance in cross-domain transfer learning. To address these issues, this paper proposes a Transferable Software Requirement Conflict Detection Framework based on SBERT and SimCSE, termed TSRCDF-SS. First, the framework employs two independent encoders, Sentence-BERT (SBERT) and Simple Contrastive Sentence Embedding (SimCSE), to generate sentence embeddings for requirement pairs, followed by a six-element concatenation strategy. Furthermore, the classifier is enhanced by a two-layer fully connected feedforward neural network (FFNN) with a hybrid loss optimization strategy that integrates a variant of Focal Loss, domain-specific constraints, and a confidence-based penalty term. Finally, the framework synergistically integrates sequential and cross-domain transfer learning. Experimental results demonstrate that the proposed framework achieves a 10.4% improvement in both macro-F1 and weighted-F1 scores in in-domain settings, and an 11.4% increase in macro-F1 in cross-domain scenarios.

</details>


### [33] [APDT: A Digital Twin for Assessing Access Point Characteristics in a Network](https://arxiv.org/abs/2511.23009)
*D. Sree Yashaswinee,Gargie Tambe,Y. Raghu Reddy,Karthik Vaidhyanathan*

Main category: cs.SE

TL;DR: 提出用于计算机网络的接入点数字孪生（APDT），通过实时监控、仿真和预测建模来优化网络性能和服务质量


<details>
  <summary>Details</summary>
Motivation: 数字孪生技术在网络领域的应用尚未充分探索，当前网络面临客户端密度增加和流量拥塞等问题，需要更好的网络管理和性能优化方案

Method: 开发接入点数字孪生（APDT），通过Ruckus SmartZone API从三个接入点收集实时数据，结合NS-3仿真平台进行网络状态复制和预测建模，支持"假设分析"场景模拟

Result: 在大学网络环境中测试表明，APDT能够成功预测短期流量激增，实现主动客户端分流，从而改善服务质量并减少流量拥塞

Conclusion: APDT作为网络数字孪生解决方案，通过实时监控、仿真和预测能力，为网络管理和性能优化提供了有效工具，特别是在应对流量拥塞和提升服务质量方面具有潜力

Abstract: Digital twins (DT) have emerged as a transformative technology, enabling real-time monitoring, simulations, and predictive maintenance across various domains, though their Application in the networking domain remains underexplored. This paper focuses on issues such as increasing client density and traffic congestion by proposing a digital twin for computer networks. Our Digital Twin, named Access Point Digital Twin (APDT) is used for tracking user behavior and changing bandwidth demands, directly impacting network performance and Quality of Service (QoS) parameters like latency, jitter, etc. APDT captures the real-time state of networks with data from access points (APs), enabling simulation-based analyses and predictive modelling. APDT facilitates the simulation of various what-if scenarios thereby providing a better understanding of various aspects of the network characteristics. We tested APDT on our University network. APDT uses data collected from three access points via the Ruckus SmartZone API and incorporates NS-3 based simulations. The simulation replicates a real-time snapshot from a Ruckus access point and models metrics such as latency and inter-packet transfer time. Additionally, a forecasting model predicts traffic congestion and suggests proactive client offloading, enhancing network management and performance optimization. Preliminary results indicate that APDT can successfully predict short-term traffic surges, leading to improved QoS and reduced traffic congestion.

</details>


### [34] [Software for Studying CASCADE Error Correction Protocols in Quantum Communications](https://arxiv.org/abs/2511.23050)
*Nikita Repnkiov,Vladimir Faerman*

Main category: cs.SE

TL;DR: 该研究开发了基于CASCADE协议的量子通信密钥协商软件原型，采用基于actor模型的并行纠错算法，提高了效率并减少了数据交换量，同时指出了当前实现的局限性并提出了改进方向。


<details>
  <summary>Details</summary>
Motivation: 在量子计算威胁日益增长的背景下，量子通信方法的发展变得至关重要。该研究特别关注量子通信系统中的密钥协商问题，旨在开发一个用于研究和教育目的的软件原型。

Method: 研究聚焦于CASCADE协议，设计并实现了一个软件原型。采用基于actor模型的并行纠错算法，以提高密钥协商效率并减少交换数据量。通过实验评估原型性能。

Result: 实验证实了核心CASCADE算法的正确实现，但揭示了原型存在以下局限性：消息传递的计算成本高、错误处理复杂、迭代开发导致的代码冗余。并行算法确实提高了效率并减少了数据交换。

Conclusion: 研究提出了多项改进建议：重新设计系统架构、开发中间数据导出接口、将通信信道定义为独立组件、扩展系统验证工具以及比较分析盲密钥协商方法。这些改进将有助于未来量子通信系统的研究和教育应用。

Abstract: This article addresses the development of quantum communication methods in the context of emerging quantum computing threats and emphasizes the importance of key reconciliation in quantum communication systems. The study focuses on the CASCADE protocol and the design of a software prototype intended for research and educational purposes. A parallel error-correction algorithm based on the actor model was implemented, improving the efficiency of key reconciliation and reducing the amount of exchanged data. Evaluation of the prototype revealed limitations, including the computational cost of message passing, complexity of error handling, and code redundancy due to iterative development. Experimental results confirmed the correct implementation of the core CASCADE algorithms and informed the design of future improvements. Proposed enhancements include redesigning the system architecture, developing interfaces for exporting intermediate data, defining the communication channel as a separate component, and expanding tools for systematic verification and comparative analysis of blind key-reconciliation methods.

</details>


### [35] [Amplifiers or Equalizers? A Longitudinal Study of LLM Evolution in Software Engineering Project-Based Learning](https://arxiv.org/abs/2511.23157)
*Hana Kataoka,Jialong Li,Yutaka Matsuno*

Main category: cs.SE

TL;DR: 最新LLM在软件工程项目式学习中扮演双重角色：既是"均衡器"提升平均表现，又是"放大器"扩大绝对差距


<details>
  <summary>Details</summary>
Motivation: 随着LLM重塑软件开发，将其整合到软件工程教育中变得迫切。现有研究主要关注LLM在入门编程或孤立SE任务中的教育应用，但在更开放的项目式学习(PBL)中的影响尚未探索。

Method: 进行为期两年的纵向研究，比较2024年（使用早期免费LLM，n=48）和2025年（使用最新付费LLM，n=46）两个队列。研究在开放式的项目式学习环境中进行。

Result: 最新强大的LLM具有双重作用：1）作为"均衡器"，提升平均表现，即使编程能力较弱的学生也能受益，为更真实的SE实践提供机会；2）作为"放大器"，显著扩大绝对表现差距，为教育公平带来新的教学挑战。

Conclusion: LLM在软件工程项目式学习中具有复杂的双重影响，既带来教育机会也带来挑战，需要新的教学策略来应对教育不平等问题。

Abstract: As LLMs reshape software development, integrating LLM-augmented practices into SE education has become imperative. While existing studies explore LLMs' educational use in introductory programming or isolated SE tasks, their impact in more open-ended Project-Based Learning (PBL) remains unexplored. This paper introduces a two-year longitudinal study comparing a 2024 (using early free LLMs, $n$=48) and 2025 (using the latest paid LLMs, $n$=46) cohort. Our findings suggest the latest powerful LLMs' dual role: they act as "equalizers," boosting average performance even for programming-weak students, providing opportunities for more authentic SE practices; yet also as "amplifiers," dramatically widening absolute performance gaps, creating new pedagogical challenges for addressing educational inequities.

</details>


### [36] [AI for software engineering: from probable to provable](https://arxiv.org/abs/2511.23159)
*Bertrand Meyer*

Main category: cs.SE

TL;DR: 论文提出将AI编程的创造力与形式化规范和程序验证相结合，以解决AI编程中的目标指定困难和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: AI编程面临两大障碍：目标指定困难（提示工程本质上是需求工程，是软件工程中最困难的领域之一）和AI幻觉现象。程序只有在正确或接近正确时才有用。

Method: 将人工智能的创造力与形式化规范方法和形式化程序验证相结合，并借助现代证明工具的支持。

Result: 未在摘要中明确说明具体结果，但提出了一个解决方案框架。

Conclusion: 通过结合AI创造力、形式化规范和程序验证，可以克服AI编程中的关键障碍，确保生成程序的正确性。

Abstract: Vibe coding, the much-touted use of AI techniques for programming, faces two overwhelming obstacles: the difficulty of specifying goals ("prompt engineering" is a form of requirements engineering, one of the toughest disciplines of software engineering); and the hallucination phenomenon. Programs are only useful if they are correct or very close to correct.
  The solution? Combine the creativity of artificial intelligence with the rigor of formal specification methods and the power of formal program verification, supported by modern proof tools.

</details>


### [37] [GAPS: Guiding Dynamic Android Analysis with Static Path Synthesis](https://arxiv.org/abs/2511.23213)
*Samuele Doria,Eleonora Losiouk*

Main category: cs.SE

TL;DR: GAPS是一个结合静态和动态分析的Android方法可达性系统，通过图引导的路径合成显著提升目标方法可达率


<details>
  <summary>Details</summary>
Motivation: Android应用中动态解析方法可达性是一个关键且未解决的问题。现有工具在驱动执行到特定目标方法（特别是非图形组件中的方法）方面不足，而这对漏洞验证、调试和行为分析至关重要。

Method: GAPS整合静态方法引导的调用图分析与动态交互驱动执行。通过数据流分析引导的轻量级后向调用图遍历来重建到达目标方法的路径，然后将这些路径转换为指导运行时应用探索的指令。

Result: 在AndroTest基准测试中，GAPS静态识别到达88.24%目标方法的路径（每应用4.27秒），动态到达57.44%。相比之下，最先进的动态交互工具（APE 12.82%、GoalExplorer 9.69%、Guardian 17.12%）和静态分析工具（FlowDroid 58.81%、DroidReach 9.48%）表现显著较差。在50个最受欢迎的真实应用中，GAPS静态重建62.03%目标方法的路径（平均278.9秒），动态到达59.86%。

Conclusion: GAPS通过整合静态和动态分析，显著提升了Android应用中目标方法的可达性，在基准测试和真实应用中均表现出优越性能，为安全关键代码分析提供了实用工具。

Abstract: Dynamically resolving method reachability in Android applications remains a critical and largely unsolved problem. Despite notable advancements in GUI testing and static call graph construction, current tools are insufficient for reliably driving execution toward specific target methods, especially those not embedded in a graphical component (e.g., libraries' methods), a capability essential for tasks such as vulnerability validation, debugging, and behavioral analysis.
  We present GAPS (Graph-based Automated Path Synthesizer), the first system that integrates static, method-guided call graph analysis with dynamic, interaction-driven execution. GAPS performs a lightweight backward traversal of the call graph, guided by data-flow analysis, to reconstruct paths reaching the target methods. These paths are then translated into instructions that guide runtime app exploration.
  On the AndroTest benchmark, GAPS statically identifies paths to reach 88.24\% of the target methods in just 4.27 seconds per app and dynamically reaches 57.44\% of them. In contrast, state-of-the-art dynamic interaction tools show significantly lower reachability over three runs: APE, one of the best model-based GUI testers, achieves 12.82\%, while GoalExplorer, a hybrid analysis tool, reaches 9.69\%, and Guardian, an LLM-based UI automator, reaches 17.12\%. Static analysis tools also fall short: FlowDroid and DroidReach identify paths to reach 58.81\% and 9.48\% of the targets, requiring 35.06 seconds and 23.46 seconds per app, respectively.
  Finally, an evaluation on the 50 most downloaded real-world apps demonstrates GAPS's practical utility in analyzing security-critical code under a realistic scenario. With an average static analysis time of 278.9 seconds, GAPS statically reconstructs paths to 62.03\% of the target methods and dynamically reaches 59.86\% of them.

</details>


### [38] [FLIMs: Fault Localization Interference Mutants, Definition, Recognition and Mitigation](https://arxiv.org/abs/2511.23302)
*Hengyuan Liu,Zheng Li,Donghua Wang,Yankai Wu,Xiang Chen,Yong Liu*

Main category: cs.SE

TL;DR: 提出MBFL-FLIM框架，通过LLM语义分析识别和缓解干扰突变体，提升基于突变的故障定位效果


<details>
  <summary>Details</summary>
Motivation: 传统基于突变的故障定位(MBFL)面临干扰突变体的挑战，这些突变体来自非故障代码但能被失败测试杀死，模仿真实故障行为，削弱故障定位效果

Method: 1) 提出故障定位干扰突变体(FLIMs)概念并进行RIPR模型理论分析；2) 使用基于LLM的语义分析识别FLIMs，通过微调技术和置信度估计解决LLM输出不稳定问题；3) 通过精炼可疑度分数来缓解FLIMs影响；4) 集成到MBFL工作流中形成MBFL-FLIM框架

Result: 在Defects4J基准测试的395个程序版本上，使用8个LLM进行实验，MBFL-FLIM在Top-1指标上平均改进44个故障，优于传统SBFL/MBFL方法、动态特征方法和近期LLM-based故障定位技术，在多故障场景中表现稳健

Conclusion: MBFL-FLIM框架通过LLM语义分析有效识别和缓解干扰突变体，显著提升基于突变的故障定位效果，在单故障和多故障场景中都表现出优越性能

Abstract: Mutation-based Fault Localization (MBFL) has been widely explored for automated software debugging, leveraging artificial mutants to identify faulty code entities. However, MBFL faces significant challenges due to interference mutants generated from non-faulty code entities but can be killed by failing tests. These mutants mimic the test sensitivity behaviors of real faulty code entities and weaken the effectiveness of fault localization. To address this challenge, we introduce the concept of Fault Localization Interference Mutants (FLIMs) and conduct a theoretical analysis based on the Reachability, Infection, Propagation, and Revealability (RIPR) model, identifying four distinct interference causes. Building on this, we propose a novel approach to semantically recognize and mitigate FLIMs using LLM-based semantic analysis, enhanced by fine-tuning techniques and confidence estimation strategies to address LLM output instability. The recognized FLIMs are then mitigated by refining the suspiciousness scores calculated from MBFL techniques. We integrate FLIM recognition and mitigation into the MBFL workflow, developing MBFL-FLIM, a fault localization framework that enhances MBFL's effectiveness by reducing misleading interference while preserving real fault-revealing information. Our empirical experiments on the Defects4J benchmark with 395 program versions using eight LLMs demonstrate MBFL-FLIM's superiority over traditional SBFL and MBFL methods, advanced dynamic feature-based approaches, and recent LLM-based fault localization techniques. Specifically, MBFL-FLIM achieves an average improvement of 44 faults in the Top-1 metric, representing a significant enhancement over baseline methods. Further evaluation confirms MBFL-FLIM's robust performance in multi-fault scenarios, with ablation experiments validating the contributions of the fine-tuning and confidence estimation components.

</details>


### [39] [Chart2Code-MoLA: Efficient Multi-Modal Code Generation via Adaptive Expert Routing](https://arxiv.org/abs/2511.23321)
*Yifei Wang,Jacky Keung,Zhenyu Mao,Jingyu Zhang,Yuchen Cao*

Main category: cs.SE

TL;DR: C2C-MoLA是一个多模态图表到代码生成框架，结合了混合专家(MoE)和低秩适配(LoRA)，通过复杂度感知路由和参数高效调优，在精度、内存效率和收敛速度上显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有图表到代码生成方法在跨类型泛化、内存效率和模块化设计方面存在不足，需要一种能够动态适应不同图表复杂度并高效利用计算资源的解决方案。

Method: 提出C2C-MoLA框架：1) MoE组件采用复杂度感知路由机制，基于元素数量和图表复杂度等可学习结构指标动态分配输入；2) 使用LoRA进行参数高效更新；3) 定制训练策略对齐路由稳定性和语义准确性。

Result: 在Chart2Code-160k数据集上：生成精度提升高达17%，峰值GPU内存减少18%，收敛速度加快20%。消融研究验证了8个专家和rank-8 LoRA的最优设计，并确认了实际多模态代码生成的可扩展性。

Conclusion: C2C-MoLA通过MoE和LoRA的协同作用，有效解决了图表到代码生成中的跨类型泛化、内存效率和模块化设计挑战，为复杂图表的多模态代码生成提供了高效可扩展的解决方案。

Abstract: Chart-to-code generation is a critical task in automated data visualization, translating complex chart structures into executable programs. While recent Multi-modal Large Language Models (MLLMs) improve chart representation, existing approaches still struggle to achieve cross-type generalization, memory efficiency, and modular design. To address these challenges, this paper proposes C2C-MoLA, a multimodal framework that synergizes Mixture of Experts (MoE) with Low-Rank Adaptation (LoRA). The MoE component uses a complexity-aware routing mechanism with domain-specialized experts and load-balanced sparse gating, dynamically allocating inputs based on learnable structural metrics like element count and chart complexity. LoRA enables parameter-efficient updates for resource-conscious tuning, further supported by a tailored training strategy that aligns routing stability with semantic accuracy. Experiments on Chart2Code-160k show that the proposed model improves generation accuracy by up to 17%, reduces peak GPU memory by 18%, and accelerates convergence by 20%, when compared to standard fine-tuning and LoRA-only baselines, particularly on complex charts. Ablation studies validate optimal designs, such as 8 experts and rank-8 LoRA, and confirm scalability for real-world multimodal code generation.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [40] [Extended branching Rauzy induction](https://arxiv.org/abs/2511.22588)
*Francesco Dolce,Christian B. Hughes*

Main category: cs.FL

TL;DR: 本文扩展了分支Rauzy归纳法，使其适用于任意标准区间交换变换（包括非极小变换），并证明了任意IET语言中的返回词在Burrows-Wheeler意义下聚类。


<details>
  <summary>Details</summary>
Motivation: 传统分支Rauzy归纳法仅适用于正则区间交换变换，需要扩展该方法以处理任意标准IET，包括非极小变换，从而建立更一般的理论框架。

Method: 引入扩展的分支Rauzy归纳法，包含合并和分裂两个归纳步骤：合并步骤处理等长切割，分裂步骤处理不变分量，将方法推广到任意标准IET。

Result: 成功构建了适用于任意标准IET的扩展分支Rauzy归纳法，并通过逐步态射论证证明了任意IET语言中的返回词在Burrows-Wheeler意义下聚类。

Conclusion: 扩展的分支Rauzy归纳法为研究任意区间交换变换提供了新工具，Burrows-Wheeler聚类性质揭示了IET语言的结构特征，为符号动力学研究开辟了新方向。

Abstract: Branching Rauzy induction is a two-sided form of Rauzy induction that acts on regular interval exchange transformations (IETs). We introduce an extended form of branching Rauzy induction that applies to arbitrary standard IETs, including non-minimal ones. The procedure generalizes the branching Rauzy method with two induction steps, merging and splitting, to handle equal-length cuts and invariant components respectively. As an application, we show, via a stepwise morphic argument, that all return words in the language of an arbitrary IET cluster in the Burrows-Wheeler sense.

</details>


### [41] [The Target Discounted-Sum Problem](https://arxiv.org/abs/2511.22979)
*Udi Boker,Thomas A. Henzinger,Jan Otop*

Main category: cs.FL

TL;DR: 该论文研究了目标折扣和问题：给定有理折扣因子λ和有理值a,b,t，判断是否存在有限或无限序列w∈{a,b}*或w∈{a,b}ω，使得折扣和等于t。论文解决了有限版本，证明了无限版本的困难性，并提供了部分结果。


<details>
  <summary>Details</summary>
Motivation: 目标折扣和问题与数学和计算机科学的多个领域密切相关，包括β展开、折扣和自动机、分段仿射映射和康托集的推广等。该问题的可判定性问题非常困难，解决它有助于解决折扣和自动机中的一些开放问题。

Method: 论文采用理论分析方法，将目标折扣和问题与多个数学领域联系起来。对于有限版本，提供了完整的解决方案；对于无限版本，证明了其困难性，并给出了部分结果，包括限制在最终周期序列的情况，以及λ≥1/2或λ=1/n的情况。

Result: 1. 完全解决了有限版本的目标折扣和问题；2. 证明了无限版本的困难性，将其与多个开放问题联系起来；3. 提供了无限版本的部分结果：解决了最终周期序列的情况，以及λ≥1/2或λ=1/n的情况；4. 应用结果解决了折扣和自动机中的一些开放问题，包括功能自动机的精确值、通用性和包含问题。

Conclusion: 目标折扣和问题是一个连接多个数学和计算机科学领域的核心问题。虽然有限版本已完全解决，但无限版本仍然困难，与多个开放问题相关。论文的部分结果和与折扣和自动机的连接为未来研究提供了重要基础。

Abstract: The target discounted-sum problem is the following: Given a rational discount factor $0<λ<1$ and three rational values $a,b$, and $t$, does there exist a finite or an infinite sequence $w \in \{a,b\}^*$ or $w \in \{a,b\}^ω$, such that $\sum_{i=0}^{|w|} w(i) λ^i$ equals $t$?
  The problem turns out to relate to many fields of mathematics and computer science, and its decidability question is surprisingly hard to solve.
  We solve the finite version of the problem, and show the hardness of the infinite version, linking it to various areas and open problems in mathematics and computer science: $β$-expansions, discounted-sum automata, piecewise affine maps, and generalizations of the Cantor set. We provide some partial results to the infinite version, among which are solutions to its restriction to eventually-periodic sequences and to the cases that $λ\geq \frac{1}{2}$ or $λ=\frac{1}{n}$, for every $n\in \mathbb{N}$.
  We use our results for solving some open problems on discounted-sum automata, among which are the exact-value, universality and inclusion problems for functional automata.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [42] [Expanding Specification Capabilities of a Gradual Verifier with Pure Functions](https://arxiv.org/abs/2511.22075)
*Doruk Alp Mutlu*

Main category: cs.PL

TL;DR: 扩展Gradual C0验证工具，通过引入纯函数(pure functions)增强其规范表达能力，解决不精确规范下的纯函数公理化技术挑战。


<details>
  <summary>Details</summary>
Motivation: 当前Gradual C0的规范语言在复杂表达式方面能力有限，虽然支持递归堆数据结构，但缺乏许多静态验证工具支持的纯函数构造，这限制了规范表达能力和观察者方法的编码便利性。

Method: 扩展Gradual C0的设计，引入纯函数作为规范构造，解决不精确规范下纯函数公理化的技术挑战，既增强规范能力又提高观察者方法的编码便利性。

Result: 提出了Gradual C0的扩展设计，支持纯函数构造，增强了规范表达能力，并解决了不精确规范下纯函数公理化的技术问题。

Conclusion: 通过引入纯函数扩展Gradual C0，显著提升了其规范表达能力，使程序能够更灵活地进行部分规范，并逐步实现完整规范，同时提高了观察者方法的编码效率。

Abstract: Gradual verification soundly combines static checking and dynamic checking to provide an incremental approach for software verification. With gradual verification, programs can be partially specified first, and then the full specification of a program can be achieved in incremental steps. The first and only practicable gradual verifier based on symbolic execution, Gradual C0, supports recursive heap data structures. Despite recent efforts to improve the expressivity of Gradual C0's specification language, Gradual C0's specification language is still limited in its capabilities for complex expressions. This work explores an extension to Gradual C0's design with a common construct supported by many static verification tools, pure functions, which both extend the specification capabilities of Gradual C0 and increase the ease of encoding observer methods in Gradual C0. Our approach addresses the technical challenges related to the axiomatisation of pure functions with imprecise specifications.

</details>


### [43] [On Circuit Description Languages, Indexed Monads, and Resource Analysis](https://arxiv.org/abs/2511.22419)
*Ken Sakayori,Andrea Colledan,Ugo Dal Lago*

Main category: cs.PL

TL;DR: 为Proto-Quipper系列演算引入基于单子的指称模型，该模型能够分离计算值和产生的量子电路，支持丰富的类型系统并控制电路规模。


<details>
  <summary>Details</summary>
Motivation: Proto-Quipper是Quipper量子编程语言的理想化版本，需要建立指称语义模型来验证其类型系统，特别是要控制生成的量子电路规模。

Method: 采用基于单子的指称模型方法，将计算值与电路生成作为副作用分离，引入电路代数的新概念，支持效果类型系统来保证电路的定量性质。

Result: 提出的语义框架能够充分描述Proto-Quipper系列演算，即使在优化存在的情况下，也能通过效果类型保证生成电路的定量性质。

Conclusion: 基于单子的指称模型为Proto-Quipper提供了有效的语义基础，通过电路代数概念支持丰富的类型系统，能够控制量子电路的规模并保证优化后的性质。

Abstract: In this paper, a monad-based denotational model is introduced and shown adequate for the Proto-Quipper family of calculi, themselves being idealized versions of the Quipper programming language. The use of a monadic approach allows us to separate the value to which a term reduces from the circuit that the term itself produces as a side effect. In turn, this enables the denotational interpretation and validation of rich type systems in which the size of the produced circuit can be controlled. Notably, the proposed semantic framework, through the novel concept of circuit algebra, suggests forms of effect typing guaranteeing quantitative properties about the resulting circuit, even in presence of optimizations.

</details>


### [44] [A Synthetic Reconstruction of Multiparty Session Types (with Appendix)](https://arxiv.org/abs/2511.22692)
*David Castro-Perez,Francisco Ferreira,Sung-Shik Jongmans*

Main category: cs.PL

TL;DR: 提出一种新的多会话类型方法，通过直接验证进程与全局协议规范（表示为LTS）来同时实现表达性和组合性，无需中间本地类型和投影。


<details>
  <summary>Details</summary>
Motivation: 现有多会话类型方法面临两难：基于投影的经典方法具有组合性但表达性有限；而更现代的方法通过非组合的全系统模型检查实现更高表达性，但可扩展性差。需要一种既能保持组合性又能提供高表达性的新方法。

Method: 提出合成方法，核心创新是一个类型系统，直接验证每个进程与全局协议规范（通常表示为标记转移系统LTS，全局类型作为特例）。该方法独特地避免了中间本地类型和投影的需要。

Result: 该方法在概念更简单的同时，支持一系列具有挑战性的协议，这些协议之前超出了多会话类型文献中组合性技术的能力范围。类型系统可推广到验证进程与任何"行为良好"的LTS规范，支持标准全局类型语法无法表达的协议。

Conclusion: 该框架在Agda中形式化和机械化，并开发了VS Code扩展的原型实现，为多会话类型验证提供了一种同时具备表达性和组合性的新方法。

Abstract: Multiparty session types (MPST) provide a rigorous foundation for verifying the safety and liveness of concurrent systems. However, existing approaches often force a difficult trade-off: classical, projection-based techniques are compositional but limited in expressiveness, while more recent techniques achieve higher expressiveness by relying on non-compositional, whole-system model checking, which scales poorly.
  This paper introduces a new approach to MPST that delivers both expressiveness and compositionality, called the synthetic approach. Our key innovation is a type system that verifies each process directly against a global protocol specification, represented as a labelled transition system (LTS) in general, with global types as a special case. This approach uniquely avoids the need for intermediate local types and projection.
  We demonstrate that our approach, while conceptually simpler, supports a benchmark of challenging protocols that were previously beyond the reach of compositional techniques in the MPST literature. We generalise our type system, showing that it can validate processes against any specification that constitutes a "well-behaved" LTS, supporting protocols not expressible with the standard global type syntax. The entire framework, including all theorems and many examples, has been formalised and mechanised in Agda, and we have developed a prototype implementation as an extension to VS Code.

</details>


### [45] [All for One and One for All: Program Logics for Exploiting Internal Determinism in Parallel Programs](https://arxiv.org/abs/2511.23283)
*Alexandre Moine,Sam Westrick,Joseph Tassarotti*

Main category: cs.PL

TL;DR: 提出Musketeer分离逻辑用于证明程序的调度无关安全性，并基于此开发Angelic逻辑来简化内部确定性并行程序的验证。


<details>
  <summary>Details</summary>
Motivation: 内部确定性并行编程虽然简化了程序推理，但缺乏利用这一特性简化形式化验证的框架。现有方法无法充分利用内部确定性带来的简化优势。

Method: 定义了调度无关安全性属性，提出Musketeer分离逻辑来证明该属性，然后开发Angelic逻辑来验证满足该属性的程序。同时证明了MiniDet类型系统的正确性。

Result: 成功构建了完整的验证框架，能够简化内部确定性并行程序的验证。所有结果都在Rocq中使用Iris分离逻辑框架进行了形式化验证。

Conclusion: 通过调度无关安全性和Musketeer/Angelic逻辑，为内部确定性并行程序提供了系统化的验证方法，填补了该领域的形式化验证空白。

Abstract: Nondeterminism makes parallel programs challenging to write and reason about. To avoid these challenges, researchers have developed techniques for internally deterministic parallel programming, in which the steps of a parallel computation proceed in a deterministic way. Internal determinism is useful because it lets a programmer reason about a program as if it executed in a sequential order. However, no verification framework exists to exploit this property and simplify formal reasoning about internally deterministic programs.
  To capture the essence of why internally deterministic programs should be easier to reason about, this paper defines a property called schedule-independent safety. A program satisfies schedule-independent safety, if, to show that the program is safe across all orderings, it suffices to show that one terminating execution of the program is safe. We then present a separation logic called Musketeer for proving that a program satisfies schedule-independent safety. Once a parallel program has been shown to satisfy schedule-independent safety, we can verify it with a new logic called Angelic, which allows one to dynamically select and verify just one sequential ordering of the program.
  Using Musketeer, we prove the soundness of MiniDet, an affine type system for enforcing internal determinism. MiniDet supports several core algorithmic primitives for internally deterministic programming that have been identified in the research literature, including a deterministic version of a concurrent hash set. Because any syntactically well-typed MiniDet program satisfies schedule-independent safety, we can apply Angelic to verify such programs.
  All results in this paper have been verified in Rocq using the Iris separation logic framework.

</details>


### [46] [TypeDis: A Type System for Disentanglement](https://arxiv.org/abs/2511.23358)
*Alexandre Moine,Stephanie Balzer,Alex Xu,Sam Westrick*

Main category: cs.PL

TL;DR: TypeDis：一个基于时间戳的类型系统，用于自动验证并行程序的解耦性，无需手动证明


<details>
  <summary>Details</summary>
Motivation: 解耦性是并行程序的重要运行时属性，能实现快速自动内存管理，但现有验证方法（DisLog）需要专家知识和大量手动证明工作，对程序员负担重

Method: 提出TypeDis类型系统，受区域类型启发，每个类型都标注时间戳标识分配任务，支持等递归类型、类型和时间戳多态，通过子时间化等机制允许时间戳变化

Result: TypeDis能自动确保类型良好的程序具有解耦性，通过一系列示例展示其特性，并在Rocq证明助手中使用改进的DisLog2进行了机械化验证

Conclusion: TypeDis为解耦性验证提供了自动化的类型系统方法，显著减轻了程序员的证明负担，同时保持了形式化保证

Abstract: Disentanglement is a runtime property of parallel programs guaranteeing that parallel tasks remain oblivious to each other's allocations. As demonstrated in the MaPLe compiler and run-time system, disentanglement can be exploited for fast automatic memory management, especially task-local garbage collection with no synchronization between parallel tasks. However, as a low-level property, disentanglement can be difficult to reason about for programmers. The only means of statically verifying disentanglement so far has been DisLog, an Iris-fueled variant of separation logic, mechanized in the Rocq proof assistant. DisLog is a fully-featured program logic, allowing for proof of functional correctness as well as verification of disentanglement. Yet its employment requires significant expertise and per-program proof effort.
  This paper explores the route of automatic verification via a type system, ensuring that any well-typed program is disentangled and lifting the burden of carrying out manual proofs from the programmer. It contributes TypeDis, a type system inspired by region types, where each type is annotated with a timestamp, identifying the task that allocated it. TypeDis supports iso-recursive types as well as polymorphism over both types and timestamps. Crucially, timestamps are allowed to change during type-checking, at join points as well as via a form of subtyping, dubbed subtiming. The paper illustrates TypeDis and its features on a range of examples. The soundness of TypeDis and the examples are mechanized in the Rocq proof assistant, using an improved version of DisLog, dubbed DisLog2.

</details>


### [47] [RapunSL: Untangling Quantum Computing with Separation, Linear Combination and Mixing](https://arxiv.org/abs/2511.23472)
*Yusuke Matsushita,Kengo Hirata,Ryo Wakizaka,Emanuele D'Osualdo*

Main category: cs.PL

TL;DR: 提出RapunSL量子分离逻辑，通过引入线性组合和混合连接词，实现基态局部性和结果局部性，显著提升量子程序推理的可扩展性


<details>
  <summary>Details</summary>
Motivation: 现有量子分离逻辑(QSL)虽然通过分离作为解纠缠来提升量子程序推理的可扩展性，但缺乏处理量子领域特有的局部性概念，特别是叠加态和测量产生的混合态的推理

Method: 构建RapunSL量子分离逻辑，引入线性组合和混合两个新连接词，实现基态局部性（将叠加态推理简化为纯态推理）和结果局部性（将测量产生的混合态推理简化为纯态推理）

Result: RapunSL能够显著提升量子程序推理的可扩展性，在一系列具有挑战性的案例研究中得到验证

Conclusion: RapunSL通过引入量子领域特有的局部性概念和新连接词，为量子程序推理提供了更强大和可扩展的形式化验证框架

Abstract: Quantum Separation Logic (QSL) has been proposed as an effective tool to improve the scalability of deductive reasoning for quantum programs. In QSL, separation is interpreted as disentanglement, and the frame rule brings a notion of entanglement-local specification (one that only talks about the qubits entangled with those acted upon by the program). In this paper, we identify two notions of locality unique to the quantum domain, and we construct a novel quantum separation logic, RapunSL, which is able to soundly reduce reasoning about superposition states to reasoning about pure states (basis-locality), and reasoning about mixed states arising from measurement to reasoning about pure states (outcome-locality). To do so, we introduce two connectives, linear combination and mixing, which together with separation provide a dramatic improvement in the scalability of reasoning, as we demonstrate on a series of challenging case studies.

</details>
