<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.SE](#cs.SE) [Total: 24]
- [cs.FL](#cs.FL) [Total: 3]
- [cs.LO](#cs.LO) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [BALI: Branch-Aware Loop Invariant Inference with Large Language Models](https://arxiv.org/abs/2601.00882)
*Mingxiu Wang,Jiawei Wang,Xiao Cheng*

Main category: cs.PL

TL;DR: BALI：一个结合大语言模型和分支感知静态分析的框架，用于增强循环不变式的推理和验证


<details>
  <summary>Details</summary>
Motivation: 循环不变式对于推理迭代算法的正确性至关重要，但推导合适的不变式仍然是一个具有挑战性且通常是手动的任务，特别是对于复杂程序

Method: BALI框架结合大语言模型和分支感知静态程序分析，首先用SMT验证分支序列级（路径级）子句，然后将它们组合成程序级不变式

Result: 论文介绍了BALI的关键组件，展示了初步结果，并讨论了未来方向

Conclusion: BALI通过结合自动化推理和分支感知分析，提高了循环不变式推导的精确性和可扩展性，朝着完全自动化的不变式发现迈出了重要一步

Abstract: Loop invariants are fundamental for reasoning about the correctness of iterative algorithms. However, deriving suitable invariants remains a challenging and often manual task, particularly for complex programs. In this paper, we introduce BALI, a branch-aware framework that integrates large language models (LLMs) to enhance the inference and verification of loop invariants. Our approach combines automated reasoning with branch-aware static program analysis to improve both precision and scalability. Specifically, unlike prior LLM-only guess-and-check methods, BALI first verifies branch-sequence-level (path-level) clauses with SMT and then composes them into program-level invariants. We outline its key components, present preliminary results, and discuss future directions toward fully automated invariant discovery.

</details>


### [2] [The New Compiler Stack: A Survey on the Synergy of LLMs and Compilers](https://arxiv.org/abs/2601.02045)
*Shuoming Zhang,Jiacheng Zhao,Qiuchu Yu,Chunwei Xia,Zheng Wang,Xiaobing Feng,Huimin Cui*

Main category: cs.PL

TL;DR: 本文对LLM赋能编译领域进行了系统综述，提出了多维分类法，总结了三大优势，并指出了该领域的挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）技术的发展，其在编译器领域的应用逐渐兴起。本文旨在系统梳理LLM赋能编译这一新兴领域，回答关键研究问题，为研究者和从业者提供基础路线图。

Method: 提出一个全面的多维分类法，从设计哲学（选择器、翻译器、生成器）、LLM方法论、代码抽象层次和任务类型四个维度对现有工作进行分类。

Result: 识别了LLM赋能编译的三大主要优势：1）编译器开发的民主化；2）发现新颖的优化策略；3）拓宽编译器的传统范围。同时指出了确保正确性和实现可扩展性等关键挑战。

Conclusion: 混合系统的发展是最有前景的前进方向。本综述为研究者和从业者提供了基础路线图，指引新一代LLM驱动的智能、自适应、协同编译工具的发展方向。

Abstract: This survey has provided a systematic overview of the emerging field of LLM-enabled compilation by addressing several key research questions. We first answered how LLMs are being integrated by proposing a comprehensive, multi-dimensional taxonomy that categorizes works based on their Design Philosophy (Selector, Translator, Generator), LLM Methodology, their operational Level of Code Abstraction, and the specific Task Type they address. In answering what advancements these approaches offer, we identified three primary benefits: the democratization of compiler development, the discovery of novel optimization strategies, and the broadening of the compiler's traditional scope. Finally, in addressing the field's challenges and opportunities, we highlighted the critical hurdles of ensuring correctness and achieving scalability, while identifying the development of hybrid systems as the most promising path forward. By providing these answers, this survey serves as a foundational roadmap for researchers and practitioners, charting the course for a new generation of LLM-powered, intelligent, adaptive and synergistic compilation tools.

</details>


### [3] [Perish or Flourish? A Holistic Evaluation of Large Language Models for Code Generation in Functional Programming](https://arxiv.org/abs/2601.02060)
*Nguyet-Anh H. Lang,Eric Lang,Thanh Le-Cong,Bach Le,Quyet-Thang Huynh*

Main category: cs.PL

TL;DR: FPEval：基于FPBench（包含721个任务的基准）评估LLM在函数式编程语言（Haskell、OCaml、Scala）中代码生成能力的框架，发现LLM在纯函数式语言中错误率更高，且常生成非地道的命令式风格代码。


<details>
  <summary>Details</summary>
Motivation: 函数式编程虽能提高软件可靠性，但学习曲线陡峭。LLM在代码生成方面有潜力降低门槛，但现有评估主要关注命令式语言，对函数式编程语言的评估不足。

Method: 提出FPEval评估框架，基于FPBench（包含721个编程任务，分三个难度级别，覆盖Haskell、OCaml、Scala三种主流函数式语言）。框架包含测试验证和静态分析工具，评估功能正确性、代码风格和可维护性。评估了GPT-3.5、GPT-4o、GPT-5等LLM，并以Java作为命令式基线。

Result: 1. LLM在函数式编程中的性能随模型进步显著提升；2. 纯函数式语言（Haskell、OCaml）的错误率显著高于混合型（Scala）或命令式语言（Java）；3. LLM常生成遵循命令式模式的非地道函数式代码，影响代码风格和长期可维护性；4. LLM在提供静态分析反馈和手工指令时，能部分自我修复正确性和质量问题。

Conclusion: LLM在函数式编程代码生成方面仍有挑战，特别是在纯函数式语言中错误率高且代码风格不地道。但通过反馈机制和特定指令，LLM具备自我修复潜力，为降低函数式编程学习门槛提供了可能。

Abstract: Functional programming provides strong foundations for developing reliable and secure software systems, yet its adoption remains not widespread due to the steep learning curve. Recent advances in Large Language Models (LLMs) for code generation present new opportunities to lower these barriers. However, extensive evaluations of LLMs largely focus on imperative programming languages, and their capabilities in functional programming languages (FP) remain underexplored. To address this gap, we introduce FPEval, a holistic evaluation framework built on FPBench, a new benchmark of 721 programming tasks across three difficulty levels on three mainstream FP languages: Haskell, Ocaml and Scala. FPEval provides compehensive evaluation infrastructures with both test validations with comprehensive test suites and static analysis tools to assess both functional correctness and code style and maintainability. Using this framework, we evaluate state-of-the-art LLMs, including GPT-3.5, GPT-4o, and GPT-5, for code generation in functional programming languages and Java as an imperative baseline. Our results demonstrate that LLM performance in functional programming improves substantially with model advancement; however, error rates remain significantly higher in purely functional languages (Haskell and OCaml) than in hybrid (Scala) or imperative (Java) languages. Moreover, LLMs frequently generate non-idiomatic functional code that follows imperative patterns, raising concerns about code style and long-term maintainability. Finally, we show that LLMs can partially self-repair both correctness and quality issues when provided with static analysis feedback and hand-crafted instructions for common types of issues.

</details>


### [4] [MLIR-Smith: A Novel Random Program Generator for Evaluating Compiler Pipelines](https://arxiv.org/abs/2601.02218)
*Berke Ates,Filip Dobrosavljević,Theodoros Theodoridis,Zhendong Su*

Main category: cs.PL

TL;DR: MLIR-Smith是一个专门为MLIR框架设计的随机程序生成器，用于测试和评估基于MLIR的编译器优化，填补了编译器测试工具领域的空白。


<details>
  <summary>Details</summary>
Motivation: 编译器对软件性能和正确执行至关重要，但在MLIR这种可扩展的中间表示框架中，缺乏专门的测试和评估工具。现有的方法如Csmith无法适应MLIR的可扩展性特点。

Method: 开发了MLIR-Smith，这是一个专门为MLIR设计的随机程序生成器。通过生成随机的MLIR程序，对MLIR、LLVM、DaCe和DCIR等编译器流水线进行差分测试。

Result: 使用MLIR-Smith进行差分测试，在MLIR、LLVM、DaCe和DCIR等编译器流水线中发现了多个bug，证明了该工具的有效性。

Conclusion: MLIR-Smith填补了编译器测试领域的空白，强调了编译器系统全面测试的重要性。该工具不仅增强了评估和改进编译器的能力，还为未来测试工具的发展铺平了道路，可能影响软件测试和质量保证的更广泛领域。

Abstract: Compilers are essential for the performance and correct execution of software and hold universal relevance across various scientific disciplines. Despite this, there is a notable lack of tools for testing and evaluating them, especially within the adaptable Multi-Level Intermediate Representation (MLIR) context. This paper addresses the need for a tool that can accommodate MLIR's extensibility, a feature not provided by previous methods such as Csmith. Here we introduce MLIR-Smith, a novel random program generator specifically designed to test and evaluate MLIR-based compiler optimizations. We demonstrate the utility of MLIR-Smith by conducting differential testing on MLIR, LLVM, DaCe, and DCIR, which led to the discovery of multiple bugs in these compiler pipelines. The introduction of MLIR-Smith not only fills a void in the realm of compiler testing but also emphasizes the importance of comprehensive testing within these systems. By providing a tool that can generate random MLIR programs, this paper enhances our ability to evaluate and improve compilers and paves the way for future tools, potentially shaping the wider landscape of software testing and quality assurance.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [5] [SeRe: A Security-Related Code Review Dataset Aligned with Real-World Review Activities](https://arxiv.org/abs/2601.01042)
*Zixiao Zhao,Yanjie Jiang,Hui Liu,Kui Liu,Lu Zhang*

Main category: cs.SE

TL;DR: 构建了一个安全相关的代码审查数据集SeRe，采用主动学习集成分类方法从大量原始审查中提取安全相关评论，并评估了现有代码审查生成方法在安全反馈生成上的表现。


<details>
  <summary>Details</summary>
Motivation: 软件安全漏洞可能导致严重后果，需要早期检测。虽然代码审查是防止安全缺陷的关键机制，但由于审查者对安全问题关注不足或缺乏专业知识，相关反馈仍然稀缺。现有数据集和研究主要关注通用代码审查评论，要么缺乏安全特定标注，要么规模太小无法支持大规模研究。

Method: 采用基于主动学习的集成分类方法构建SeRe数据集。该方法通过人工标注迭代优化模型预测，实现高精度同时保持合理召回率。使用微调的集成分类器从373,824个原始审查实例中提取6,732个安全相关审查，确保跨多种编程语言的代表性。

Result: 统计分析表明SeRe总体上与真实世界安全相关审查分布一致。为了评估SeRe的实用性和现有代码审查评论生成方法的有效性，对最先进方法在安全相关反馈生成方面进行了基准测试。

Conclusion: 通过发布SeRe数据集和基准测试结果，旨在推进自动化安全导向代码审查的研究，为开发更有效的安全软件工程实践做出贡献。

Abstract: Software security vulnerabilities can lead to severe consequences, making early detection essential. Although code review serves as a critical defense mechanism against security flaws, relevant feedback remains scarce due to limited attention to security issues or a lack of expertise among reviewers. Existing datasets and studies primarily focus on general-purpose code review comments, either lacking security-specific annotations or being too limited in scale to support large-scale research. To bridge this gap, we introduce \textbf{SeRe}, a \textbf{security-related code review dataset}, constructed using an active learning-based ensemble classification approach. The proposed approach iteratively refines model predictions through human annotations, achieving high precision while maintaining reasonable recall. Using the fine-tuned ensemble classifier, we extracted 6,732 security-related reviews from 373,824 raw review instances, ensuring representativeness across multiple programming languages. Statistical analysis indicates that SeRe generally \textbf{aligns with real-world security-related review distribution}. To assess both the utility of SeRe and the effectiveness of existing code review comment generation approaches, we benchmark state-of-the-art approaches on security-related feedback generation. By releasing SeRe along with our benchmark results, we aim to advance research in automated security-focused code review and contribute to the development of more effective secure software engineering practices.

</details>


### [6] [RovoDev Code Reviewer: A Large-Scale Online Evaluation of LLM-based Code Review Automation at Atlassian](https://arxiv.org/abs/2601.01129)
*Kla Tantithamthavorn,Yaotian Zou,Andy Wong,Michael Gupta,Zhe Wang,Mike Buller,Ryan Jiang,Matthew Watson,Minwoo Jeong,Kun Chen,Ming Wu*

Main category: cs.SE

TL;DR: RovoDev Code Reviewer是一个企业级LLM代码审查自动化工具，无需微调即可生成基于审查指导、上下文感知和质量检查的代码审查评论，在Atlassian的Bitbucket中大规模部署，显著提高代码审查效率和质量。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM驱动的代码审查评论生成方法已有进展，但设计企业级代码审查自动化工具仍面临实际挑战。本文旨在解决一个实际问题：如何在不进行微调的情况下，设计出基于审查指导、上下文感知和质量检查的代码审查评论生成系统。

Method: 开发了RovoDev Code Reviewer，这是一个企业级LLM代码审查自动化工具，无缝集成到Atlassian的Bitbucket中。该方法采用无需微调的设计，通过离线、在线和用户反馈评估来验证系统效果。

Result: 经过一年期的评估，RovoDev Code Reviewer在38.70%的情况下生成了能导致代码解决的审查评论（即触发后续提交中代码更改的评论）。此外，该工具将PR周期时间减少了30.8%，人工编写的评论数量减少了35.6%，并能发现错误并提供可操作建议，从而改善整体软件质量。

Conclusion: RovoDev Code Reviewer是一个有效的企业级代码审查自动化工具，能够加速反馈周期、减轻审查者工作量并提高软件质量，展示了LLM在代码审查工作流程中的实际应用价值。

Abstract: Large Language Models (LLMs)-powered code review automation has the potential to transform code review workflows. Despite the advances of LLM-powered code review comment generation approaches, several practical challenges remain for designing enterprise-grade code review automation tools. In particular, this paper aims at answering the practical question: how can we design a review-guided, context-aware, quality-checked code review comment generation without fine-tuning?
  In this paper, we present RovoDev Code Reviewer, an enterprise-grade LLM-based code review automation tool designed and deployed at scale within Atlassian's development ecosystem with seamless integration into Atlassian's Bitbucket. Through the offline, online, user feedback evaluations over a one-year period, we conclude that RovoDev Code Reviewer is (1) effective in generating code review comments that could lead to code resolution for 38.70% (i.e., comments that triggered code changes in the subsequent commits); and (2) offers the promise of accelerating feedback cycles (i.e., decreasing the PR cycle time by 30.8%), alleviating reviewer workload (i.e., reducing the number of human-written comments by 35.6%), and improving overall software quality (i.e., finding errors with actionable suggestions).

</details>


### [7] [The Invisible Hand of AI Libraries Shaping Open Source Projects and Communities](https://arxiv.org/abs/2601.01944)
*Matteo Esposito,Andrea Janes,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: 该研究通过大规模分析157.7k个开源仓库，评估Python和Java项目中AI库的采用情况及其对开发活动、社区参与和代码复杂度的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在开源软件中的存在日益增加，但其在开源项目中的采用情况和影响尚未得到充分探索。需要了解AI库如何改变软件开发实践。

Method: 对157.7k个潜在开源仓库进行大规模分析，使用仓库指标和软件指标，比较采用AI库的项目与未采用的项目。

Result: 预计将发现采用AI库的开源项目与未采用的项目在开发活动、社区参与和代码复杂度方面存在可测量的差异。

Conclusion: 研究将为AI集成如何重塑软件开发实践提供基于证据的见解，帮助理解AI对开源生态系统的影响。

Abstract: In the early 1980s, Open Source Software emerged as a revolutionary concept amidst the dominance of proprietary software. What began as a revolutionary idea has now become the cornerstone of computer science. Amidst OSS projects, AI is increasing its presence and relevance. However, despite the growing popularity of AI, its adoption and impacts on OSS projects remain underexplored.
  We aim to assess the adoption of AI libraries in Python and Java OSS projects and examine how they shape development, including the technical ecosystem and community engagement. To this end, we will perform a large-scale analysis on 157.7k potential OSS repositories, employing repository metrics and software metrics to compare projects adopting AI libraries against those that do not. We expect to identify measurable differences in development activity, community engagement, and code complexity between OSS projects that adopt AI libraries and those that do not, offering evidence-based insights into how AI integration reshapes software development practices.

</details>


### [8] [Abductive Vibe Coding (Extended Abstract)](https://arxiv.org/abs/2601.01199)
*Logan Murphy,Aren A. Babikian,Marsha Chechik*

Main category: cs.SE

TL;DR: 提出从AI生成代码中提取可分析的半形式化理由框架，而非直接判断正确性


<details>
  <summary>Details</summary>
Motivation: 当AI生成代码时，人类工程师需要验证其正确性，但形式化证明在许多实际场景中不可行，特别是当需求难以形式化时

Method: 开发框架从AI生成的代码中提取可分析的半形式化理由，生成一组条件，在这些条件下生成的代码可以被认为是充分的

Result: 描述了当前框架实现工作的进展和预期的研究机会（这是正在进行的工作）

Conclusion: 提出了一种替代形式化证明的方法，通过提取半形式化理由来验证AI生成代码的充分性，为难以形式化的需求场景提供了实用解决方案

Abstract: When software artifacts are generated by AI models ("vibe coding"), human engineers assume responsibility for validating them. Ideally, this validation would be done through the creation of a formal proof of correctness. However, this is infeasible for many real-world vibe coding scenarios, especially when requirements for the AI-generated artifacts resist formalization. This extended abstract describes ongoing work towards the extraction of analyzable, semi-formal rationales for the adequacy of vibe-coded artifacts. Rather than deciding correctness directly, our framework produces a set of conditions under which the generated code can be considered adequate. We describe current efforts towards implementing our framework and anticipated research opportunities.

</details>


### [9] [Correctness isnt Efficiency: Runtime Memory Divergence in LLM-Generated Code](https://arxiv.org/abs/2601.01215)
*Prateek Rajput,Yewei Song,Abdoul Aziz Bonkoungou,Iyiola E. Olatunji,Abdoul Kader Kabore,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: 论文提出一个框架来评估LLM生成代码的运行稳定性，发现即使通过单元测试的正确解决方案，在内存使用和性能模式上存在显著差异，可能带来隐藏的操作风险。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型可以生成通过单元测试的程序，但通过测试并不能保证可靠的运行时行为。研究发现，同一任务的不同正确解决方案在内存和性能模式上存在很大差异，这可能带来隐藏的操作风险。

Method: 提出一个测量多个正确生成程序执行时间内存稳定性的框架。在解决方案层面，引入动态平均配对距离(DMPD)，使用动态时间规整比较内存使用轨迹的形状，通过转换为单调峰值轮廓(MPPs)来减少瞬态噪声。在模型层面，通过聚合DMPD得到模型不稳定性分数(MIS)。

Result: 在BigOBench和CodeContests上的实验显示，正确解决方案之间存在显著的运行时差异。不稳定性通常随着采样温度升高而增加，即使pass@1有所改善。还观察到稳定性度量与认知复杂度、圈复杂度等软件工程指标之间的相关性。

Conclusion: 研究结果支持在CI/CD中基于稳定性选择通过测试的候选方案，以降低操作风险而不牺牲正确性。稳定性度量与软件工程指标的相关性表明操作行为与可维护性之间存在联系。

Abstract: Large language models (LLMs) can generate programs that pass unit tests, but passing tests does not guarantee reliable runtime behavior. We find that different correct solutions to the same task can show very different memory and performance patterns, which can lead to hidden operational risks. We present a framework to measure execution-time memory stability across multiple correct generations. At the solution level, we introduce Dynamic Mean Pairwise Distance (DMPD), which uses Dynamic Time Warping to compare the shapes of memory-usage traces after converting them into Monotonic Peak Profiles (MPPs) to reduce transient noise. Aggregating DMPD across tasks yields a model-level Model Instability Score (MIS). Experiments on BigOBench and CodeContests show substantial runtime divergence among correct solutions. Instability often increases with higher sampling temperature even when pass@1 improves. We also observe correlations between our stability measures and software engineering indicators such as cognitive and cyclomatic complexity, suggesting links between operational behavior and maintainability. Our results support stability-aware selection among passing candidates in CI/CD to reduce operational risk without sacrificing correctness. Artifacts are available.

</details>


### [10] [HD-GEN: A High-Performance Software System for Human Mobility Data Generation Based on Patterns of Life](https://arxiv.org/abs/2601.01219)
*Hossein Amiri,Joon-Seok Kim,Hamdi Kavak,Andrew Crooks,Dieter Pfoser,Carola Wenk,Andreas Züfle*

Main category: cs.SE

TL;DR: 提出一个结合真实轨迹数据与模拟仿真的综合软件管道，用于生成大规模个体级人类移动数据集，包含生成、校准、处理和可视化四个组件。


<details>
  <summary>Details</summary>
Motivation: 真实轨迹数据存在稀疏性和参与者偏差问题，而合成数据缺乏真实性。需要结合两者的优势，创建既真实又可控的大规模人类移动数据集。

Method: 开发四组件系统：1) 基于OpenStreetMap的地理基础数据生成引擎；2) 遗传算法校准模块，使模拟参数与真实移动特征对齐；3) 数据处理套件，将原始日志转换为结构化格式；4) 可视化模块，通过视觉分析提取关键移动模式。

Result: 创建了一个综合软件管道，能够生成结合真实数据现实主义与模拟仿真可扩展性的大规模个体级人类移动数据集。

Conclusion: 该管道填补了真实轨迹数据与合成数据之间的差距，为下游应用提供了既真实又可控的大规模人类移动数据集，具有广泛的应用价值。

Abstract: Understanding individual-level human mobility is critical for a wide range of applications. Real-world trajectory datasets provide valuable insights into actual movement behaviors but are often constrained by data sparsity and participant bias. Synthetic data, by contrast, offer scalability and flexibility but frequently lack realism. To address this gap, we introduce a comprehensive software pipeline for calibrating, generating, processing, and visualizing large-scale individual-level human mobility datasets that combine the realism of empirical data with the control and extensibility of Patterns-of-Life simulations. Our system consists of four integrated components. (1) a data generation engine constructs geographically grounded simulations using OpenStreetMap data to produce diverse mobility logs. (2) a genetic algorithm-based calibration module fine-tunes simulation parameters to align with real-world mobility characteristics, such as daily trip counts and radius of gyration, enabling realistic behavioral modeling. (3) a data processing suite transforms raw simulation logs into structured formats suitable for downstream applications, including model training and benchmarking. (4) a visualization module extracts key mobility patterns and insights from the processed datasets and presents them through intuitive visual analytics for improved interpretability.

</details>


### [11] [Atomizer: An LLM-based Collaborative Multi-Agent Framework for Intent-Driven Commit Untangling](https://arxiv.org/abs/2601.01233)
*Kangchen Zhu,Zhiliang Tian,Shangwen Wang,Mingyue Leng,Xiaoguang Mao*

Main category: cs.SE

TL;DR: Atomizer：一个基于多智能体协作的复合提交解耦框架，通过意图导向的思维链和分组-评审迭代循环，显著提升代码变更解耦的准确性。


<details>
  <summary>Details</summary>
Motivation: 复合提交（包含多个无关变更的提交）普遍存在于软件开发中，严重阻碍程序理解和维护。现有自动解耦方法（特别是基于图聚类的方法）存在两个根本性限制：1）过度依赖结构信息，无法理解变更的语义意图；2）作为"单次通过"算法，缺乏类似人工评审的关键反思和优化机制。

Method: 提出Atomizer框架：1）采用意图导向的思维链策略，利用大语言模型根据代码结构和语义信息推断每个变更的意图；2）建立分组-评审协作优化循环，通过两个智能体（分组器和评审器）迭代优化分组，直到每个簇中的所有变更共享相同的底层语义意图。

Result: 在两个基准C#和Java数据集上的实验表明，Atomizer显著优于多个代表性基线方法。平均而言，在C#数据集上比最先进的基于图的方法高出6.0%，在Java数据集上高出5.5%。在复杂提交上优势更明显，性能优势扩大到超过16%。

Conclusion: Atomizer通过结合语义意图理解和迭代优化机制，有效解决了现有复合提交解耦方法的局限性，为代码变更的智能分析和维护提供了新的解决方案。

Abstract: Composite commits, which entangle multiple unrelated concerns, are prevalent in software development and significantly hinder program comprehension and maintenance. Existing automated untangling methods, particularly state-of-the-art graph clustering-based approaches, are fundamentally limited by two issues. (1) They over-rely on structural information, failing to grasp the crucial semantic intent behind changes, and (2) they operate as ``single-pass'' algorithms, lacking a mechanism for the critical reflection and refinement inherent in human review processes. To overcome these challenges, we introduce Atomizer, a novel collaborative multi-agent framework for composite commit untangling. To address the semantic deficit, Atomizer employs an Intent-Oriented Chain-of-Thought (IO-CoT) strategy, which prompts large language models (LLMs) to infer the intent of each code change according to both the structure and the semantic information of code. To overcome the limitations of ``single-pass'' grouping, we employ two agents to establish a grouper-reviewer collaborative refinement loop, which mirrors human review practices by iteratively refining groupings until all changes in a cluster share the same underlying semantic intent. Extensive experiments on two benchmark C# and Java datasets demonstrate that Atomizer significantly outperforms several representative baselines. On average, it surpasses the state-of-the-art graph-based methods by over 6.0% on the C# dataset and 5.5% on the Java dataset. This superiority is particularly pronounced on complex commits, where Atomizer's performance advantage widens to over 16%.

</details>


### [12] [CatchAll: Repository-Aware Exception Handling with Knowledge-Guided LLMs](https://arxiv.org/abs/2601.01271)
*Qingxiao Tao,Xiaodong Gu,Hao Zhong,Beijun Shen*

Main category: cs.SE

TL;DR: CatchAll是一个基于LLM的仓库感知异常处理方法，通过三层异常处理知识（API级异常知识、仓库级执行上下文、跨仓库处理模式）来提升代码生成中的异常处理能力。


<details>
  <summary>Details</summary>
Motivation: 异常处理是编程中的重要错误恢复机制，但LLM在仓库级别的异常处理上表现不佳，因为复杂的依赖关系和上下文约束。现有方法难以处理仓库级别的异常处理需求。

Method: CatchAll为LLM提供三层异常处理知识：1) 从真实代码库构建的API-异常映射；2) 通过建模目标代码周围调用跟踪捕获异常传播的仓库级执行上下文；3) 从跨项目历史代码中挖掘的可重用异常处理模式。这些知识被编码到结构化提示中指导LLM生成代码。

Result: 在两个新基准测试（RepoExEval和RepoExEval-Exec）上的实验表明，CatchAll在CodeBLEU（0.31 vs 0.27）、意图预测准确率（60.1% vs 48.0%）和Pass@1（29% vs 25%）上均优于现有最佳基线方法。

Conclusion: CatchAll通过整合多层异常处理知识，有效提升了LLM在真实仓库级别异常处理任务上的性能，为解决复杂依赖环境下的异常处理问题提供了新思路。

Abstract: Exception handling is a vital forward error-recovery mechanism in many programming languages, enabling developers to manage runtime anomalies through structured constructs (e.g., try-catch blocks). Improper or missing exception handling often leads to severe consequences, including system crashes and resource leaks. While large language models (LLMs) have demonstrated strong capabilities in code generation, they struggle with exception handling at the repository level, due to complex dependencies and contextual constraints. In this work, we propose CatchAll, a novel LLM-based approach for repository-aware exception handling. CatchAll equips LLMs with three complementary layers of exception-handling knowledge: (1) API-level exception knowledge, obtained from an empirically constructed API-exception mapping that characterizes the exception-throwing behaviors of APIs in real-world codebases; (2) repository-level execution context, which captures exception propagation by modeling contextual call traces around the target code; and (3) cross-repository handling knowledge, distilled from reusable exception-handling patterns mined from historical code across projects. The knowledge is encoded into structured prompts to guide the LLM in generating accurate and context-aware exception-handling code. To evaluate CatchAll, we construct two new benchmarks for repository-aware exception handling: a large-scale dataset RepoExEval and an executable subset RepoExEval-Exec. Experiments demonstrate that RepoExEval consistently outperforms state-of-the-art baselines, achieving a CodeBLEU score of 0.31 (vs. 0.27% for the best baseline), intent prediction accuracy of 60.1% (vs. 48.0%), and Pass@1 of 29% (vs. 25%). These results affirm RepoExEval's effectiveness in real-world repository-level exception handling.

</details>


### [13] [Adaptive Hierarchical Evaluation of LLMs and SAST tools for CWE Prediction in Python](https://arxiv.org/abs/2601.01320)
*Muntasir Adnan,Carlos C. N. Kuhn*

Main category: cs.SE

TL;DR: ALPHA是首个函数级Python漏洞检测基准，使用分层感知的CWE特定惩罚来评估LLM和SAST工具，发现LLM显著优于SAST但预测一致性差异很大。


<details>
  <summary>Details</summary>
Motivation: 现有代码漏洞检测基准采用二元分类，缺乏CWE级别的特异性，无法为迭代修正系统提供可操作的反馈。需要更精细的评估框架来反映实际诊断效用差异。

Method: 提出ALPHA基准，使用分层感知的CWE特定惩罚机制，区分过度泛化、过度规范和横向错误。评估了7个LLM和2个SAST工具。

Result: LLM在漏洞检测方面显著优于SAST工具，但SAST在检测发生时具有更高精度。不同模型间的预测一致性差异很大（8.26%-81.87%一致率）。

Conclusion: ALPHA为代码漏洞检测提供了更精细的评估框架，揭示了LLM和SAST工具的性能差异，并提出了将ALPHA惩罚机制融入监督微调的未来研究方向。

Abstract: Large Language Models have become integral to software development, yet they frequently generate vulnerable code. Existing code vulnerability detection benchmarks employ binary classification, lacking the CWE-level specificity required for actionable feedback in iterative correction systems. We present ALPHA (Adaptive Learning via Penalty in Hierarchical Assessment), the first function-level Python benchmark that evaluates both LLMs and SAST tools using hierarchically aware, CWE-specific penalties. ALPHA distinguishes between over-generalisation, over-specification, and lateral errors, reflecting practical differences in diagnostic utility. Evaluating seven LLMs and two SAST tools, we find LLMs substantially outperform SAST, though SAST demonstrates higher precision when detections occur. Critically, prediction consistency varies dramatically across models (8.26%-81.87% agreement), with significant implications for feedback-driven systems. We further outline a pathway for future work incorporating ALPHA penalties into supervised fine-tuning, which could provide principled hierarchy-aware vulnerability detection pending empirical validation.

</details>


### [14] [GlycoPy: An Equation-Oriented and Object-Oriented Software for Hierarchical Modeling, Optimization, and Control in Python](https://arxiv.org/abs/2601.01413)
*Yingjie Ma,Jing Guo,Richard D. Braatz*

Main category: cs.SE

TL;DR: GlycoPy是一个面向方程、面向对象的Python软件框架，用于过程建模、优化和非线性模型预测控制，旨在解决复杂系统NMPC应用中缺乏分层建模工具和高效实现的问题。


<details>
  <summary>Details</summary>
Motivation: 当前过程工业中大多数MPC应用采用线性模型，但实际（生物）化学过程通常是非线性的。线性模型限制了MPC在广泛操作条件下的性能和适用性。复杂系统NMPC应用面临缺乏分层建模工具和高效算法实现的挑战。

Method: 开发了GlycoPy软件框架，这是一个面向方程、面向对象的Python框架，支持分层建模。框架包含参数估计、动态优化和NMPC算法，允许用户自定义仿真、优化和控制算法。

Result: 通过三个案例研究验证了GlycoPy的建模、优化和NMPC能力，案例范围从简单的微分代数方程系统到多尺度生物过程模型。

Conclusion: GlycoPy有潜力弥合先进NMPC算法与实际（生物）化学过程应用之间的差距，使NMPC在分层系统中更加实用。

Abstract: Most existing model predictive control (MPC) applications in process industries employ lin-ear models, although real-world (bio)chemical processes are typically nonlinear. The use of linear models limits the performance and applicability of MPC for processes that span a wide range of operating conditions. A challenge in employing nonlinear models in MPC for com-plex systems is the lack of tools that facilitate hierarchical model development, as well as lack of efficient implementations of the corresponding nonlinear MPC (NMPC) algorithms. As a step towards making NMPC more practical for hierarchical systems, we introduce Gly-coPy, an equation-oriented, object-oriented software framework for process modeling, opti-mization, and NMPC in Python. GlycoPy enables users to focus on writing equations for modeling while supporting hierarchical modeling. GlycoPy includes algorithms for parame-ter estimation, dynamic optimization, and NMPC, and allows users to customize the simula-tion, optimization, and control algorithms. Three case studies, ranging from a simple differ-ential algebraic equation system to a multiscale bioprocess model, validate the modeling, optimization, and NMPC capabilities of GlycoPy. GlycoPy has the potential to bridge the gap between advanced NMPC algorithms and their practical application in real-world (bio)chemical processes.

</details>


### [15] [SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving](https://arxiv.org/abs/2601.01426)
*Chaofan Tao,Jierun Chen,Yuxin Jiang,Kaiqi Kou,Shaowei Wang,Ruoyu Wang,Xiaohui Li,Sidi Yang,Yiming Du,Jianbo Dai,Zhiming Mao,Xinyu Wang,Lifeng Shang,Haoli Bai*

Main category: cs.SE

TL;DR: SWE-Lego是一个用于软件工程问题解决的监督微调方法，仅通过轻量级SFT就能达到最先进性能，包含高质量数据集、改进的SFT流程和测试时扩展技术。


<details>
  <summary>Details</summary>
Motivation: 当前软件工程问题解决方法通常依赖复杂的训练范式（如中期训练、SFT、强化学习等组合），本文探索如何通过轻量级的纯SFT方法在SWE任务上达到极限性能。

Method: 1) 构建SWE-Lego数据集：包含32k高质量任务实例和18k验证轨迹，结合真实和合成数据；2) 改进的SFT流程：包含错误掩码和基于难度的课程学习；3) 测试时扩展：基于训练良好的验证器提升模型性能。

Result: SWE-Lego-Qwen3-8B在SWE-bench Verified上达到42.2%，32B版本达到52.6%。通过测试时扩展（TTS@16），8B模型提升至49.6%，32B模型提升至58.8%，在同类开源模型中达到最先进水平。

Conclusion: 仅通过轻量级的监督微调方法，结合高质量数据集、改进的训练流程和测试时扩展，就能在软件工程问题解决任务上达到最先进的性能，证明了简单方法的有效性。

Abstract: We present SWE-Lego, a supervised fine-tuning (SFT) recipe designed to achieve state-ofthe-art performance in software engineering (SWE) issue resolving. In contrast to prevalent methods that rely on complex training paradigms (e.g., mid-training, SFT, reinforcement learning, and their combinations), we explore how to push the limits of a lightweight SFT-only approach for SWE tasks. SWE-Lego comprises three core building blocks, with key findings summarized as follows: 1) the SWE-Lego dataset, a collection of 32k highquality task instances and 18k validated trajectories, combining real and synthetic data to complement each other in both quality and quantity; 2) a refined SFT procedure with error masking and a difficulty-based curriculum, which demonstrably improves action quality and overall performance. Empirical results show that with these two building bricks alone,the SFT can push SWE-Lego models to state-of-the-art performance among open-source models of comparable size on SWE-bench Verified: SWE-Lego-Qwen3-8B reaches 42.2%, and SWE-Lego-Qwen3-32B attains 52.6%. 3) We further evaluate and improve test-time scaling (TTS) built upon the SFT foundation. Based on a well-trained verifier, SWE-Lego models can be significantly boosted--for example, 42.2% to 49.6% and 52.6% to 58.8% under TTS@16 for the 8B and 32B models, respectively.

</details>


### [16] [Group versus Individual Review Requests: Tradeoffs in Speed and Quality at Mozilla Firefox](https://arxiv.org/abs/2601.01514)
*Matej Kucera,Marco Castelluccio,Daniel Feitosa,Ayushi Rastogi*

Main category: cs.SE

TL;DR: 研究探讨代码审查中"群组审查请求"与"个人审查请求"对审查速度和质量的影响，发现在Mozilla Firefox项目中，群组审查与更高质量的审查（更少的回归问题）相关，但对审查速度影响不大。


<details>
  <summary>Details</summary>
Motivation: 代码审查速度是衡量软件开发效率和开发者满意度的重要指标。虽然已有研究探讨影响审查速度的因素，但审查分配过程中"群组审查请求"的作用尚不明确。本研究旨在探究群组审查请求与个人审查请求对审查速度和质量的影响差异。

Method: 研究分析了Mozilla Firefox项目中约66,000个代码修订，结合统计建模和从业者焦点小组讨论。通过比较群组审查请求（将代码变更分配给一个审查组，任何成员都可审查）和个人审查请求（分配给特定审查者）的效果，并借鉴管理学中的共享任务队列理论。

Result: 研究发现群组审查与更高质量的审查相关，表现为更少的回归问题，但对审查速度的影响可以忽略不计。从业者还认为群组审查有助于平衡工作分配和为新手审查者提供培训机会。

Conclusion: 群组审查请求在提高代码审查质量方面具有优势，特别是在减少回归问题方面，而对审查速度影响有限。这种审查方式还能带来工作平衡和培训新审查者等额外好处，值得在软件开发实践中考虑采用。

Abstract: The speed at which code changes are integrated into the software codebase, also referred to as code review velocity, is a prevalent industry metric for improved throughput and developer satisfaction. While prior studies have explored factors influencing review velocity, the role of the review assignment process, particularly the `group review request', is unclear. In group review requests, available on platforms like Phabricator, GitHub, and Bitbucket, a code change is assigned to a reviewer group, allowing any member to review it, unlike individual review assignments to specific reviewers. Drawing parallels with shared task queues in Management Sciences, this study examines the effects of group versus individual review requests on velocity and quality. We investigate approximately 66,000 revisions in the Mozilla Firefox project, combining statistical modeling with practitioner views from a focus group discussion. Our study associates group reviews with improved review quality, characterized by fewer regressions, while having a negligible association with review velocity. Additional perceived benefits include balanced work distribution and training opportunities for new reviewers.

</details>


### [17] [MTS-1: A Lightweight Delta-Encoded Telemetry Format optimised for Low-Resource Environments and Offline-First System Health Monitoring](https://arxiv.org/abs/2601.01602)
*Henry Ndou*

Main category: cs.SE

TL;DR: MTS-1是一种新型的二进制遥测格式，针对带宽受限环境设计，相比JSON等传统格式可减少74.7%的数据量，适用于离线优先监控和物联网传输。


<details>
  <summary>Details</summary>
Motivation: 现有遥测编码格式（如JSON、CBOR、Protocol Buffers）是为高带宽、始终在线的环境设计的，在撒哈拉以南非洲、农村企业部署和不稳定局域网等带宽受限网络中会产生显著开销。

Method: 提出了MTS-1（Magenta Telemetry Standard v1），一种基于差分编码的二进制遥测格式，专为离线优先系统监控、局域网辅助代理传输和节能的物联网到服务器传输而设计。

Result: 与JSON、JSON Lines、CBOR、MessagePack和Protocol Buffers相比，MTS-1在有效载荷大小、编码成本、网络效率和成本延迟性能方面表现优异。合成基准测试显示，相比JSON可减少74.7%的数据量，相比MessagePack可减少5.4%，且在不同数据集规模下具有线性扩展特性。

Conclusion: MTS-1为带宽受限环境提供了一种高效的遥测编码解决方案，特别适合离线优先监控和物联网应用，在数据压缩和传输效率方面显著优于现有格式。

Abstract: System-level telemetry is fundamental to modern remote monitoring, predictive maintenance, and AI-driven infrastructure optimisation. Existing telemetry encodings such as JSON, JSON Lines, CBOR, and Protocol Buffers were designed for high-bandwidth, always-online environments. They impose significant overhead when deployed in bandwidth-constrained networks common across Sub-Saharan Africa, rural enterprise deployments, and unstable LAN environments. This paper introduces MTS-1 (Magenta Telemetry Standard v1), a novel delta-encoded binary telemetry format designed for offline-first system monitoring, LAN-assisted proxy delivery, and energy-efficient IoT-to-server transmission. We compare MTS-1 against JSON, JSON Lines, CBOR, MessagePack, and Protocol Buffers across payload size, encoding cost, network efficiency, and cost-latency performance. Synthetic benchmarking demonstrates preliminary compression improvements of up to 74.7% versus JSON and 5.4% versus MessagePack, with linear scaling characteristics across dataset sizes.

</details>


### [18] [LIA: Supervised Fine-Tuning of Large Language Models for Automatic Issue Assignment](https://arxiv.org/abs/2601.01780)
*Arsham Khosravani,Alireza Hosseinpour,Arshia Akhavan,Mehdi Keshani,Abbas Heydarnoori*

Main category: cs.SE

TL;DR: 提出LIA方法，使用监督微调LLM进行自动化问题分配，相比基础模型和现有方法在Hit@1指标上提升显著。


<details>
  <summary>Details</summary>
Motivation: 软件维护中的问题分配过程至关重要，但手动分配不一致且容易出错。现有自动化方法依赖大量项目特定训练数据或稀疏嘈杂的关系信息，效果有限。

Method: 提出LIA方法，使用DeepSeek-R1-Distill-Llama-8B进行监督微调，利用LLM对自然语言和软件相关文本的语义理解，直接从问题标题和描述生成开发者推荐排名。

Result: 相比基础预训练模型，Hit@1提升高达+187.8%；相比四种领先的问题分配方法，Hit@1提升高达+211.2%。

Conclusion: 领域适应的LLM在软件维护任务中效果显著，LIA为问题分配提供了实用高效解决方案。

Abstract: Issue assignment is a critical process in software maintenance, where new issue reports are validated and assigned to suitable developers. However, manual issue assignment is often inconsistent and error-prone, especially in large open-source projects where thousands of new issues are reported monthly. Existing automated approaches have shown promise, but many rely heavily on large volumes of project-specific training data or relational information that is often sparse and noisy, which limits their effectiveness. To address these challenges, we propose LIA (LLM-based Issue Assignment), which employs supervised fine-tuning to adapt an LLM, DeepSeek-R1-Distill-Llama-8B in this work, for automatic issue assignment. By leveraging the LLM's pretrained semantic understanding of natural language and software-related text, LIA learns to generate ranked developer recommendations directly from issue titles and descriptions. The ranking is based on the model's learned understanding of historical issue-to-developer assignments, using patterns from past tasks to infer which developers are most likely to handle new issues. Through comprehensive evaluation, we show that LIA delivers substantial improvements over both its base pretrained model and state-of-the-art baselines. It achieves up to +187.8% higher Hit@1 compared to the DeepSeek-R1-Distill-Llama-8B pretrained base model, and outperforms four leading issue assignment methods by as much as +211.2% in Hit@1 score. These results highlight the effectiveness of domain-adapted LLMs for software maintenance tasks and establish LIA as a practical, high-performing solution for issue assignment.

</details>


### [19] [The Machine Learning Canvas: Empirical Findings on Why Strategy Matters More Than AI Code Generation](https://arxiv.org/abs/2601.01839)
*Martin Prause*

Main category: cs.SE

TL;DR: 研究创建并测试了机器学习画布框架，识别出影响ML项目成功的四个关键因素：战略、流程、生态系统和支持，发现这些因素相互关联，而AI编码助手虽能加速编码但不能保证项目成功。


<details>
  <summary>Details</summary>
Motivation: 尽管AI编码助手日益流行，但超过80%的机器学习项目未能交付实际商业价值。研究旨在通过结合业务战略、软件工程和数据科学的实用框架，确定导致ML项目成功的关键因素。

Method: 创建并测试了机器学习画布框架，调查了150名数据科学家，并使用统计建模分析他们的回答，识别影响项目成功的关键因素。

Result: 识别出四个关键成功因素：战略（清晰目标和规划）、流程（工作执行方式）、生态系统（工具和基础设施）和支持（组织支持和资源）。这些因素相互关联，每个因素影响下一个。研究发现AI助手虽能加速编码，但不能保证项目成功。

Conclusion: ML项目成功需要战略、流程、生态系统和支持四个相互关联的因素协同作用。AI编码助手只能解决"如何"编码的问题，无法替代战略思考中的"为什么"和"什么"，因此不能单独保证项目成功。

Abstract: Despite the growing popularity of AI coding assistants, over 80% of machine learning (ML) projects fail to deliver real business value. This study creates and tests a Machine Learning Canvas, a practical framework that combines business strategy, software engineering, and data science in order to determine the factors that lead to the success of ML projects. We surveyed 150 data scientists and analyzed their responses using statistical modeling. We identified four key success factors: Strategy (clear goals and planning), Process (how work gets done), Ecosystem (tools and infrastructure), and Support (organizational backing and resources). Our results show that these factors are interconnected - each one affects the next. For instance, strong organizational support results in a clearer strategy (β= 0.432, p < 0.001), which improves work processes (β= 0.428, p < 0.001) and builds better infrastructure (β= 0.547, p < 0.001). Together, these elements determine whether a project succeeds. The surprising finding? Although AI assistants make coding faster, they don't guarantee project success. AI assists with the "how" of coding but cannot replace the "why" and "what" of strategic thinking.

</details>


### [20] [A Defect is Being Born: How Close Are We? A Time Sensitive Forecasting Approach](https://arxiv.org/abs/2601.01921)
*Mikel Robredo,Matteo Esposito,Fabio Palomba,Rafael Peñaloza,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: 该研究探索时间敏感的缺陷预测技术，旨在通过时间序列方法预测软件项目的未来缺陷密度，并识别缺陷发生前的早期征兆。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统持续演化，需要能够预测缺陷发生的时间敏感方法。现有研究已成功实现即时缺陷预测，但需要更早的预测能力来预防缺陷发生。

Method: 训练多种时间敏感的预测技术来预测软件项目的未来缺陷密度，并识别缺陷发生前的早期症状。

Result: 预期结果为早期缺陷倾向性评估方法的有效性提供实证证据。

Conclusion: 该研究将探索时间敏感技术在缺陷预测中的应用，为软件工程领域提供更早的缺陷预警能力。

Abstract: Background. Defect prediction has been a highly active topic among researchers in the Empirical Software Engineering field. Previous literature has successfully achieved the most accurate prediction of an incoming fault and identified the features and anomalies that precede it through just-in-time prediction. As software systems evolve continuously, there is a growing need for time-sensitive methods capable of forecasting defects before they manifest.
  Aim. Our study seeks to explore the effectiveness of time-sensitive techniques for defect forecasting. Moreover, we aim to investigate the early indicators that precede the occurrence of a defect.
  Method. We will train multiple time-sensitive forecasting techniques to forecast the future bug density of a software project, as well as identify the early symptoms preceding the occurrence of a defect.
  Expected results. Our expected results are translated into empirical evidence on the effectiveness of our approach for early estimation of bug proneness.

</details>


### [21] [Context-Adaptive Requirements Defect Prediction through Human-LLM Collaboration](https://arxiv.org/abs/2601.01952)
*Max Unterbusch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: 提出基于人类-大语言模型协作的适应性缺陷预测方法，通过反馈循环和思维链推理，利用少量验证示例实现快速性能提升


<details>
  <summary>Details</summary>
Motivation: 传统需求评估方法依赖通用模式作为缺陷代理，但缺陷定义具有上下文依赖性，因项目、领域和利益相关者解释而异。需要超越"一刀切"模型的适应性方法

Method: 提出人类-大语言模型协作方法，将缺陷预测视为适应性过程而非静态分类任务。利用LLM思维链推理构建反馈循环：用户验证预测及其解释，验证示例通过少样本学习自适应指导未来预测

Result: 在包含1,266个梅赛德斯-奔驰需求的QuRE基准测试中，HLC能有效适应验证示例，仅需20个验证示例即可实现快速性能提升。结合验证解释（不仅是标签）使HLC显著优于标准少样本提示和微调BERT模型，同时保持高召回率

Conclusion: LLM的上下文学习和思维链能力支持适应性分类方法，超越通用模型，为从利益相关者反馈中持续学习的工具创造机会

Abstract: Automated requirements assessment traditionally relies on universal patterns as proxies for defectiveness, implemented through rule-based heuristics or machine learning classifiers trained on large annotated datasets. However, what constitutes a "defect" is inherently context-dependent and varies across projects, domains, and stakeholder interpretations. In this paper, we propose a Human-LLM Collaboration (HLC) approach that treats defect prediction as an adaptive process rather than a static classification task. HLC leverages LLM Chain-of-Thought reasoning in a feedback loop: users validate predictions alongside their explanations, and these validated examples adaptively guide future predictions through few-shot learning. We evaluate this approach using the weak word smell on the QuRE benchmark of 1,266 annotated Mercedes-Benz requirements. Our results show that HLC effectively adapts to the provision of validated examples, with rapid performance gains from as few as 20 validated examples. Incorporating validated explanations, not just labels, enables HLC to substantially outperform both standard few-shot prompting and fine-tuned BERT models while maintaining high recall. These results highlight how the in-context and Chain-of-Thought learning capabilities of LLMs enable adaptive classification approaches that move beyond one-size-fits-all models, creating opportunities for tools that learn continuously from stakeholder feedback.

</details>


### [22] [Reporting LLM Prompting in Automated Software Engineering: A Guideline Based on Current Practices and Expectations](https://arxiv.org/abs/2601.01954)
*Alexander Korn,Lea Zaruchas,Chetan Arora,Andreas Metzger,Sven Smolka,Fanyu Wang,Andreas Vogelsang*

Main category: cs.SE

TL;DR: 该研究分析了SE领域LLM研究中提示工程报告的现状，通过文献分析和PC成员调查，发现当前实践与审稿人期望存在显著差距，并提出结构化报告指南以提升透明度和可复现性。


<details>
  <summary>Details</summary>
Motivation: LLM在软件工程任务中应用日益广泛，但提示工程相关的决策很少被系统或透明地记录，这阻碍了研究的可复现性和可比性。需要填补这一报告空白。

Method: 采用两阶段实证研究：1) 分析2022年以来顶级SE会议近300篇论文的提示设计、测试和优化报告情况；2) 调查105名程序委员会成员对LLM驱动研究中提示报告的期望。

Result: 发现当前实践与审稿人期望存在显著不一致，特别是在版本披露、提示理由和有效性威胁方面。基于发现提出了区分必要、期望和优秀报告元素的结构化指南。

Conclusion: 提出的报告指南有助于提高LLM基础SE研究的透明度、可复现性和方法严谨性，是改进该领域研究实践的重要一步。

Abstract: Large Language Models, particularly decoder-only generative models such as GPT, are increasingly used to automate Software Engineering tasks. These models are primarily guided through natural language prompts, making prompt engineering a critical factor in system performance and behavior. Despite their growing role in SE research, prompt-related decisions are rarely documented in a systematic or transparent manner, hindering reproducibility and comparability across studies. To address this gap, we conducted a two-phase empirical study. First, we analyzed nearly 300 papers published at the top-3 SE conferences since 2022 to assess how prompt design, testing, and optimization are currently reported. Second, we surveyed 105 program committee members from these conferences to capture their expectations for prompt reporting in LLM-driven research. Based on the findings, we derived a structured guideline that distinguishes essential, desirable, and exceptional reporting elements. Our results reveal significant misalignment between current practices and reviewer expectations, particularly regarding version disclosure, prompt justification, and threats to validity. We present our guideline as a step toward improving transparency, reproducibility, and methodological rigor in LLM-based SE research.

</details>


### [23] [The State of Open Science in Software Engineering Research: A Case Study of ICSE Artifacts](https://arxiv.org/abs/2601.02066)
*Al Muttakin,Saikat Mondal,Chanchal Roy*

Main category: cs.SE

TL;DR: 评估ICSE十年间100个复制包的可用性：仅40%可执行，其中仅35%能复现原始结果，揭示软件工程研究中复制包可用性与可复现性之间的显著差距。


<details>
  <summary>Details</summary>
Motivation: 软件工程研究中复制包对于透明度、验证和重用至关重要，但目前缺乏对其实际可用性、可执行性和可复现性的全面研究。虽然顶级会议如ICSE已要求共享研究制品，但这些复制包的实用性仍未被充分探索。

Method: 评估2015-2024年间ICSE会议发表的100个复制包，从四个方面进行评估：(1)可执行性，(2)执行所需努力和修改，(3)导致执行失败的挑战，(4)原始结果的可复现性。总共花费约650人时执行制品和复现研究结果。

Result: 仅40%的复制包可执行，其中仅32.5%无需修改即可运行。在执行努力方面，17.5%需要低努力，82.5%需要中等到高努力。识别出5种常见修改类型和13种导致执行失败的挑战。在可执行制品中，仅35%能复现原始结果。

Conclusion: 研究揭示了制品可用性、可执行性和可复现性之间的显著差距。提出了三条可操作的指南，以改进研究制品的准备、文档和评审，从而加强软件工程研究中开放科学实践的严谨性和可持续性。

Abstract: Replication packages are crucial for enabling transparency, validation, and reuse in software engineering (SE) research. While artifact sharing is now a standard practice and even expected at premier SE venues such as ICSE, the practical usability of these replication packages remains underexplored. In particular, there is a marked lack of studies that comprehensively examine the executability and reproducibility of replication packages in SE research. In this paper, we aim to fill this gap by evaluating 100 replication packages published as part of ICSE proceedings over the past decade (2015--2024). We assess the (1) executability of the replication packages, (2) efforts and modifications required to execute them, (3) challenges that prevent executability, and (4) reproducibility of the original findings. We spent approximately 650 person-hours in total executing the artifacts and reproducing the study findings. Our findings reveal that only 40\% of the 100 evaluated artifacts were executable, of which 32.5\% (13 out of 40) ran without any modification. Regarding effort levels, 17.5\% (7 out of 40) required low effort, while 82.5\% (33 out of 40) required moderate to high effort to execute successfully. We identified five common types of modifications and 13 challenges leading to execution failure, spanning environmental, documentation, and structural issues. Among the executable artifacts, only 35\% (14 out of 40) reproduced the original results. These findings highlight a notable gap between artifact availability, executability, and reproducibility. Our study proposes three actionable guidelines to improve the preparation, documentation, and review of research artifacts, thereby strengthening the rigor and sustainability of open science practices in SE research.

</details>


### [24] [Code for Machines, Not Just Humans: Quantifying AI-Friendliness with Code Health Metrics](https://arxiv.org/abs/2601.02200)
*Markus Borg,Nadim Hagatulah,Adam Tornhill,Emma Söderberg*

Main category: cs.SE

TL;DR: 研究发现人类友好的代码（CodeHealth评分高）在AI重构时语义保持更好，表明人类可读性高的代码也更容易被AI工具处理


<details>
  <summary>Details</summary>
Motivation: 随着人类开发者与AI编码代理在相同代码库中协作的时代到来，需要确保不同能力的LLM能够可靠地编辑代码。传统上代码优化主要针对人类理解，现在需要研究AI友好的代码特性

Method: 使用LLM对5,000个Python竞争编程文件进行重构，分析CodeHealth（人类理解质量指标）与AI重构后语义保持之间的关联

Result: 发现CodeHealth与AI重构后的语义保持存在有意义的关联，人类友好的代码也更容易与AI工具兼容

Conclusion: 组织可以使用CodeHealth指标来指导AI干预的风险评估，投资代码可维护性不仅帮助人类开发者，也为大规模AI采用做好准备

Abstract: We are entering a hybrid era in which human developers and AI coding agents work in the same codebases. While industry practice has long optimized code for human comprehension, it is increasingly important to ensure that LLMs with different capabilities can edit code reliably. In this study, we investigate the concept of ``AI-friendly code'' via LLM-based refactoring on a dataset of 5,000 Python files from competitive programming. We find a meaningful association between CodeHealth, a quality metric calibrated for human comprehension, and semantic preservation after AI refactoring. Our findings confirm that human-friendly code is also more compatible with AI tooling. These results suggest that organizations can use CodeHealth to guide where AI interventions are lower risk and where additional human oversight is warranted. Investing in maintainability not only helps humans; it also prepares for large-scale AI adoption.

</details>


### [25] [LLM-Empowered Functional Safety and Security by Design in Automotive Systems](https://arxiv.org/abs/2601.02215)
*Nenad Petrovic,Vahid Zolfaghari,Fengjunjie Pan,Alois Knoll*

Main category: cs.SE

TL;DR: 提出一个LLM赋能的软件定义汽车开发工作流，涵盖安全感知系统拓扑设计和事件驱动代码分析


<details>
  <summary>Details</summary>
Motivation: 为软件定义汽车开发提供系统化的安全验证方法，解决功能安全和语义有效性问题

Method: 采用事件链模型进行代码分析，结合MDE方法和OCL规则进行安全拓扑分析，支持本地部署和专有解决方案

Result: 在ADAS相关场景中进行评估，验证了工作流的有效性

Conclusion: LLM赋能的工作流能够有效支持SDV软件开发，特别是在安全感知系统设计和事件驱动代码验证方面

Abstract: This paper presents LLM-empowered workflow to support Software Defined Vehicle (SDV) software development, covering the aspects of security-aware system topology design, as well as event-driven decision-making code analysis. For code analysis we adopt event chains model which provides formal foundations to systematic validation of functional safety, taking into account the semantic validity of messages exchanged between key components, including both CAN and Vehicle Signal Specification (VSS). Analysis of security aspects for topology relies on synergy with Model-Driven Engineering (MDE) approach and Object Constraint Language (OCL) rules. Both locally deployable and proprietary solution are taken into account for evaluation within Advanced Driver-Assistance Systems (ADAS)-related scenarios.

</details>


### [26] [NQC2: A Non-Intrusive QEMU Code Coverage Plugin](https://arxiv.org/abs/2601.02238)
*Nils Bosbach,Alwalid Salama,Lukas Jünger,Mark Burton,Niko Zurstraßen,Rebecca Pelke,Rainer Leupers*

Main category: cs.SE

TL;DR: NQC2是一个QEMU插件，用于在嵌入式系统裸机程序中提取代码覆盖率信息，无需目标软件插桩，性能比Xilinx方案快8.5倍。


<details>
  <summary>Details</summary>
Motivation: 嵌入式系统裸机程序缺乏操作系统和文件系统支持，传统代码覆盖率测量方法（需要插桩和运行时数据收集）无法适用，需要新的解决方案。

Method: 开发NQC2作为QEMU插件，在模拟器运行时提取覆盖率信息并存储到主机文件中，兼容修改版QEMU且无需目标软件插桩。

Result: NQC2性能显著优于Xilinx的类似方案，速度提升最高达8.5倍。

Conclusion: NQC2为嵌入式系统裸机程序提供了一种有效的代码覆盖率测量方案，克服了传统方法的限制，具有更好的兼容性和性能。

Abstract: Code coverage analysis has become a standard approach in software development, facilitating the assessment of test suite effectiveness, the identification of under-tested code segments, and the discovery of performance bottlenecks. When code coverage of software for embedded systems needs to be measured, conventional approaches quickly meet their limits. A commonly used approach involves instrumenting the source files with added code that collects and dumps coverage information during runtime. This inserted code usually relies on the existence of an operating and a file system to dump the collected data. These features are not available for bare-metal programs that are executed on embedded systems.
  To overcome this issue, we present NQC2, a plugin for QEMU.NQC2 extracts coverage information from QEMU during runtime and stores them into a file on the host machine. This approach is even compatible with modified QEMU versions and does not require target-software instrumentation. NQC2 outperforms a comparable approach from Xilinx by up to 8.5 x.

</details>


### [27] [Automatic Assertion Mining in Assertion-Based Verification: Techniques, Challenges, and Future Directions](https://arxiv.org/abs/2601.02248)
*Mohammad Reza Heidari Iman,Giorgio Di Natale,Katell Morin-Allory*

Main category: cs.SE

TL;DR: 本文综述了最新的高级断言挖掘工具，比较了它们的方法论，旨在为研究人员和验证从业者提供现有工具的见解，并指出未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着基于断言的验证（ABV）在硬件设计验证中变得越来越重要，自动断言挖掘工具成为关键。然而，需要对这些工具进行系统性的比较分析，以了解它们的能力和局限性，从而指导未来更强大的断言挖掘工具的开发。

Method: 本文采用文献综述和比较分析方法，回顾了最新、最先进且广泛采用的断言挖掘工具，对它们的方法论进行了系统性比较。

Result: 通过对现有断言挖掘工具的全面分析，本文识别了各种工具的优缺点，为研究人员和验证从业者提供了关于这些工具能力和局限性的深入见解。

Conclusion: 本文不仅总结了当前断言挖掘工具的状态，还指出了现有工具的不足之处，为开发更强大、更先进的未来断言挖掘工具指明了方向。

Abstract: Functional verification increasingly relies on Assertion-Based Verification (ABV), which has become a key approach for verifying hardware designs due to its efficiency and effectiveness. Central to ABV are automatic assertion miners, which apply different techniques to generate assertions automatically. This paper reviews the most recent, advanced, and widely adopted assertion miners, offering a comparative analysis of their methodologies. The goal is to provide researchers and verification practitioners with insights into the capabilities and limitations of existing miners. By identifying their shortcomings, this work also points toward directions for developing more powerful and advanced assertion miners in the future.

</details>


### [28] [Question Answering for Multi-Release Systems: A Case Study at Ciena](https://arxiv.org/abs/2601.02345)
*Parham Khamsepour,Mark Cole,Ish Ashraf,Sandeep Puri,Mehrdad Sabetzadeh,Shiva Nejati*

Main category: cs.SE

TL;DR: QAMR是一个针对多版本系统文档的问答聊天机器人，通过增强RAG架构和双分块策略，在相似但不同的多版本文档中提高问答准确性。


<details>
  <summary>Details</summary>
Motivation: 多版本系统中不同版本的软件文档高度相似但有差异，现有问答技术在处理这类文档时准确性不足，需要专门解决方案。

Method: 提出QAMR系统，通过预处理、查询重写和上下文选择增强传统RAG，采用双分块策略分别优化检索和答案生成的分块大小。

Result: QAMR在公共基准和真实工业文档上评估，平均答案正确率达88.5%，检索准确率90%，比基线分别提升16.5%和12%，响应时间减少8%。

Conclusion: QAMR有效解决了多版本系统文档的问答挑战，其机制显著提升准确性，评估指标与专家评估高度相关，验证了方法的可靠性。

Abstract: Companies regularly have to contend with multi-release systems, where several versions of the same software are in operation simultaneously. Question answering over documents from multi-release systems poses challenges because different releases have distinct yet overlapping documentation. Motivated by the observed inaccuracy of state-of-the-art question-answering techniques on multi-release system documents, we propose QAMR, a chatbot designed to answer questions across multi-release system documentation. QAMR enhances traditional retrieval-augmented generation (RAG) to ensure accuracy in the face of highly similar yet distinct documentation for different releases. It achieves this through a novel combination of pre-processing, query rewriting, and context selection. In addition, QAMR employs a dual-chunking strategy to enable separately tuned chunk sizes for retrieval and answer generation, improving overall question-answering accuracy. We evaluate QAMR using a public software-engineering benchmark as well as a collection of real-world, multi-release system documents from our industry partner, Ciena. Our evaluation yields five main findings: (1) QAMR outperforms a baseline RAG-based chatbot, achieving an average answer correctness of 88.5% and an average retrieval accuracy of 90%, which correspond to improvements of 16.5% and 12%, respectively. (2) An ablation study shows that QAMR's mechanisms for handling multi-release documents directly improve answer accuracy. (3) Compared to its component-ablated variants, QAMR achieves a 19.6% average gain in answer correctness and a 14.0% average gain in retrieval accuracy over the best ablation. (4) QAMR reduces response time by 8% on average relative to the baseline. (5) The automatically computed accuracy metrics used in our evaluation strongly correlate with expert human assessments, validating the reliability of our methodology.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [29] [The asymptotic size of finite irreducible semigroups of rational matrices](https://arxiv.org/abs/2601.01236)
*Stefan Kiefer,Andrew Ryzhikov*

Main category: cs.FL

TL;DR: 本文研究了有理数矩阵有限半群的最大规模，改进了不可约半群的上界至3^{n^2}，并证明该界在某种意义下是紧的。


<details>
  <summary>Details</summary>
Motivation: 有理数矩阵有限半群是确定有限自动机转移幺半群的推广，研究其最大规模有助于理解这类矩阵半群的结构。不可约矩阵半群作为一般矩阵半群的基本构件，在数学和计算机科学中扮演重要角色。

Method: 采用与Schützenberger完全不同的技术，通过新的分析方法改进了不可约有理数矩阵有限半群规模的上界。

Result: 将不可约有理数矩阵有限半群规模的上界从2^{O(n^2 log n)}改进至3^{n^2}，并证明存在规模为3^{⌊n^2/4⌋}的有限不可约半群，表明该界在某种意义下是紧的。这一结果还改进了Almeida和Steinberg关于死亡率阈值(mortality threshold)的界。

Conclusion: 本文显著改进了有理数矩阵有限半群规模的上界，为理解这类半群的结构提供了新的视角，并在自动机理论中具有应用价值，特别是对死亡率阈值问题的改进。

Abstract: We study finite semigroups of $n \times n$ matrices with rational entries. Such semigroups provide a rich generalization of transition monoids of unambiguous (and, in particular, deterministic) finite automata. In this paper we determine the maximum size of finite semigroups of rational $n \times n$ matrices, with the goal of shedding more light on the structure of such matrix semigroups.
  While in general such semigroups can be arbitrarily large in terms of $n$, a classical result of Schützenberger from 1962 implies an upper bound of $2^{O(n^2 \log n)}$ for irreducible semigroups, i.e., the only subspaces of $Q^n$ that are invariant for all matrices in the semigroup are $Q^n$ and the subspace consisting only of the zero vector. Irreducible matrix semigroups can be viewed as the building blocks of general matrix semigroups, and as such play an important role in mathematics and computer science. From the point of view of automata theory, they generalize strongly connected automata.
  Using a very different technique from that of Schützenberger, we improve the upper bound on the cardinality to $3^{n^2}$. This is the main result of the paper. The bound is in some sense tight, as we show that there exists, for every $n$, a finite irreducible semigroup with $3^{\lfloor n^2/4 \rfloor}$ rational matrices. Our main result also leads to an improvement of a bound, due to Almeida and Steinberg, on the mortality threshold. The mortality threshold is a number $\ell$ such that if the zero matrix is in the semigroup, then the zero matrix can be written as a product of at most $\ell$ matrices from any subset that generates the semigroup.

</details>


### [30] [From Historical Puzzles to Grammatical Constraints: Circular Partitions, Generalized Run-Length Encodings, and Polynomial-Time Decidability](https://arxiv.org/abs/2601.01375)
*Omid Khormali,Ghaya Mtimet,Nuh Aydin*

Main category: cs.FL

TL;DR: 该论文研究圆形分区算法，结合历史约瑟夫问题和形式语言理论，提出确定性有限自动机中的分区问题，并证明其多项式时间可解性。


<details>
  <summary>Details</summary>
Motivation: 受历史组合问题（类似约瑟夫问题）启发，研究圆形分区算法，探索确定性有限自动机中的实际问题。历史问题涉及将个体排列在圆圈上并按规则淘汰，直到保留所需群体。

Method: 分析圆形分区的移除和非移除方法，建立平衡分区条件并提供明确算法。引入分区字母表上的广义游程编码来捕捉交替字母模式，使用第二类斯特林数计算其基数。将组合结构与形式语言理论连接，提出存在性问题：给定字典上的上下文无关文法和字母块模式约束，是否存在有效句子？

Result: 证明块语言是正则的，通过标准解析技术证明存在性问题在多项式时间内可判定。提供完整算法和复杂度分析，并在历史和合成实例上验证实现。

Conclusion: 成功将历史组合问题与形式语言理论相结合，建立了圆形分区算法的理论框架，并提供了多项式时间可解的算法解决方案，为实际问题提供了有效的计算方法。

Abstract: Motivated by a historical combinatorial problem that resembles the well-known Josephus problem, we investigate circular partition algorithms and formulate problems in deterministic finite automata with practical algorithms. The historical problem involves arranging individuals on a circle and eliminating every k-th person until a desired group remains. We analyze both removal and non-removal approaches to circular partitioning, establishing conditions for balanced partitions and providing explicit algorithms. We introduce generalized run-length encodings over partitioned alphabets to capture alternating letter patterns, computing their cardinalities using Stirling numbers of the second kind. Connecting these combinatorial structures to formal language theory, we formulate an existence problem: given a context-free grammar over a dictionary and block-pattern constraints on letters, does a valid sentence exist? We prove decidability in polynomial time by showing block languages are regular and applying standard parsing techniques. Complete algorithms with complexity analysis are provided and validated through implementation on both historical and synthetic instances.

</details>


### [31] [Deciding Serializability in Network Systems](https://arxiv.org/abs/2601.02251)
*Guy Amir,Mark Barbone,Nicolas Amat,Jules Jacobs*

Main category: cs.FL

TL;DR: SER语言用于自动验证并发程序的串行化，通过受限程序使问题可判定，将串行化验证转化为Petri网可达性查询，并优化搜索空间。


<details>
  <summary>Details</summary>
Motivation: 验证并发程序的串行化（即每个并发执行是否等价于某个串行执行）是重要但困难的问题，需要自动化工具来证明或反驳串行化。

Method: 提出SER建模语言，限制程序使问题可判定但保持无界并发线程；将SER程序编译为网络系统抽象，将串行化验证转化为Petri网可达性查询；采用Petri网切片、半线性集压缩和Presburger公式操作等优化技术。

Result: 实现了首个自动化端到端决策程序，能生成可检查的证书证明串行化，或生成反例轨迹反驳串行化；框架成功处理了包括有状态防火墙、BGP路由器在内的真实世界程序模型。

Conclusion: 尽管串行化验证在理论上困难，但通过SER语言和Petri网可达性查询的转化，结合多种优化技术，能够有效处理实际程序的串行化验证问题。

Abstract: We present the SER modeling language for automatically verifying serializability of concurrent programs, i.e., whether every concurrent execution of the program is equivalent to some serial execution.
  SER programs are suitably restricted to make this problem decidable, while still allowing for an unbounded number of concurrent threads of execution, each potentially running for an unbounded number of steps.
  Building on prior theoretical results, we give the first automated end-to-end decision procedure that either proves serializability by producing a checkable certificate, or refutes it by producing a counterexample trace.
  We also present a network-system abstraction to which SER programs compile. Our decision procedure then reduces serializability in this setting to a Petri net reachability query.
  Furthermore, in order to scale, we curtail the search space via multiple optimizations, including Petri net slicing, semilinear-set compression, and Presburger-formula manipulation.
  We extensively evaluate our framework and show that, despite the theoretical hardness of the problem, it can successfully handle various models of real-world programs, including stateful firewalls, BGP routers, and more.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [32] [Tapes as Stochastic Matrices of String Diagrams](https://arxiv.org/abs/2601.01472)
*Filippo Bonchi,Cipriano Junior Cioffo*

Main category: cs.LO

TL;DR: 论文展示了子分布单子下，tape图与字符串图的子分布随机矩阵同构，并利用此结果完全公理化概率布尔电路。


<details>
  <summary>Details</summary>
Motivation: Tape图已被推广到处理任意幺半单子的Kleisli范畴，但针对具体单子（如子分布单子）的应用和特性尚未充分探索。本文旨在研究子分布单子下tape图的特性，并应用于概率布尔电路的完全公理化。

Method: 首先证明对于子分布单子，tape图与字符串图的子分布随机矩阵同构。然后利用这一同构关系，为概率布尔电路提供完整的公理化体系。

Result: 建立了子分布单子下tape图与字符串图的子分布随机矩阵之间的同构关系。基于此，成功为概率布尔电路提供了完整的公理化系统。

Conclusion: 本文通过建立子分布单子下tape图与随机矩阵的同构关系，为概率布尔电路提供了坚实的理论基础和完整的公理化框架，扩展了tape图在概率计算领域的应用。

Abstract: Tape diagrams provide a graphical notation for categories equipped with two monoidal products, $\otimes$ and $\oplus$, where $\oplus$ is a biproduct. Recently, they have been generalised to handle Kleisli categories of arbitrary monoidal monads. In this work, we show that for the subdistribution monad, tapes are isomorphic to stochastic matrices of subdistributions of string diagrams. We then exploit this result to provide a complete axiomatisation of probabilistic Boolean circuits.

</details>


### [33] [Interaction Improvement](https://arxiv.org/abs/2601.01638)
*Adrienne Lancelot,Giulio Manzonetto,Guy McCusker,Gabriele Vanoni*

Main category: cs.LO

TL;DR: 使用检查器演算为关系语义提供定量上下文解释，证明关系语义细化了约束相关项与上下文交互次数的上下文预序


<details>
  <summary>Details</summary>
Motivation: 线性逻辑的关系语义是定义λ演算资源感知模型的强大框架，但其定量方面未反映在模型诱导的预序和等式理论中。这些理论本质上是定性的，基于Böhm树在扩展性下的（不）等式。

Method: 采用最近引入的检查器演算（checkers calculus），为关系语义相关的预序提供定量和上下文的解释。

Result: 证明关系语义细化了上下文预序，约束了相关项与上下文之间的交互次数，从而为关系语义提供了定量维度。

Conclusion: 检查器演算能够为线性逻辑关系语义提供定量上下文解释，揭示关系语义不仅捕捉定性等价关系，还能约束项与上下文交互的资源消耗。

Abstract: The relational semantics of linear logic is a powerful framework for defining resource-aware models of the $λ$-calculus. However, its quantitative aspects are not reflected in the preorders and equational theories induced by these models. Indeed, they can be characterized in terms of (in)equalities between Böhm trees up to extensionality, which are qualitative in nature. We employ the recently introduced checkers calculus to provide a quantitative and contextual interpretation of the preorder associated to the relational semantics. This way, we show that the relational semantics refines the contextual preorder constraining the number of interactions between the related terms and the context.

</details>
