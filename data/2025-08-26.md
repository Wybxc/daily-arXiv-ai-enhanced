<div id=toc></div>

# Table of Contents

- [cs.FL](#cs.FL) [Total: 2]
- [cs.LO](#cs.LO) [Total: 8]
- [cs.SE](#cs.SE) [Total: 26]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [1] [On The Space Complexity of Partial Derivatives of Regular Expressions with Shuffle](https://arxiv.org/abs/2508.17451)
*Davide Ancona,Angelo Ferrando*

Main category: cs.FL

TL;DR: 该论文研究正则表达式偏导数的空间复杂度，特别关注运行时验证中带shuffle算子的正则表达式，证明了偏导数的高度最多增加1，大小是正则表达式大小的二次方。


<details>
  <summary>Details</summary>
Motivation: 在运行时验证中，使用正则表达式描述属性时，生成的NFA可能状态数爆炸，特别是当允许独立事件交错时。通过引入shuffle算子使规范更紧凑，但需要研究偏导数的空间复杂度。

Method: 利用Antimirov的偏导数方法，采用基于重写的方法进行运行时验证，每一步只存储一个偏导数，避免构建大型自动机。研究偏导数相对于高度和大小两个度量的复杂度。

Result: 证明偏导数的高度最多增加1，大小是正则表达式大小的二次方。令人惊讶的是，这些结果在包含shuffle算子时也成立。

Conclusion: 该研究为运行时验证中使用带shuffle算子的正则表达式提供了理论基础，证明了偏导数方法的空间复杂度是可接受的，即使对于复杂的交错模式也是如此。

Abstract: Partial derivatives of regular expressions, introduced by Antimirov, define
an elegant algorithm for generating equivalent non-deterministic finite
automata (NFA) with a limited number of states.
  Here we focus on runtime verification (RV) of simple properties expressible
with regular expressions. In this case, words are finite traces of monitorable
events forming the language's alphabet, and the generated NFA may have an
intractable number of states.
  This typically occurs when sub-traces of mutually independent events are
allowed to interleave.
  To address this issue, regular expressions used for RV are extended with the
shuffle operator to make specifications more compact and easier to read.
  Exploiting partial derivatives enables a rewriting-based approach to RV,
where only one derivative is stored at each step, avoiding the construction of
an intractably large automaton.
  This raises the question of the space complexity of the largest generated
partial derivative. While the total number of generated partial derivatives is
known to be linear in the size of the initial regular expression, no results
can be found in the literature regarding the size of the largest partial
derivative.
  We study this problem w.r.t. two metrics (height and size of regular
expressions), and show that the former increases by at most one, while the
latter is quadratic in the size of the regular expression. Surprisingly, these
results also hold with shuffle.

</details>


### [2] [Weighing Obese Timed Languages](https://arxiv.org/abs/2508.18133)
*Eugene Asarin,Aldric Degorre,Catalin Dima,Bernardo Jacobo Inclán*

Main category: cs.FL

TL;DR: 计算肥胖定时自动机的带宽，表示为≈α/ε，通过将问题转化为在加权定时图中计算最佳奖励时间比


<details>
  <summary>Details</summary>
Motivation: 定时语言的带宽表征了在有限观测精度ε下单位时间内的信息量，肥胖定时自动机具有无界事件频率并以最大可能速率产生信息

Method: 将问题转化为在从给定定时自动机构建的加权定时图中计算最佳奖励时间比，权重对应于辅助有限自动机的熵

Result: 得到了定时自动机带宽的近似表达式≈α/ε

Conclusion: 该方法成功地将定时自动机的带宽计算问题转化为图论中的奖励时间比优化问题

Abstract: The bandwidth of a timed language characterizes the quantity of information
per time unit (with a finite observation precision $\varepsilon$). Obese timed
automata have an unbounded frequency of events and produce information at the
maximal possible rate. In this article, we compute the bandwidth of any such
automaton in the form $\approx\alpha/\varepsilon$. Our approach reduces the
problem to computing the best reward-to-time ratio in a weighted timed graph
constructed from the given timed automaton, with weights corresponding to the
entropy of auxiliary finite automata.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [3] [On systematic construction of correct logic programs](https://arxiv.org/abs/2508.16782)
*Włodzimierz Drabent*

Main category: cs.LO

TL;DR: 本文提出了一种系统化构建可证明正确和半完备逻辑程序的方法，基于Kunen的三值完成语义和良基语义，适用于日常编程实践。


<details>
  <summary>Details</summary>
Motivation: 逻辑编程中的正确性和完备性概念需要形式化方法来实现，传统方法往往过于复杂或不够系统化，需要一种简单实用的方法来构建符合规范的程序。

Method: 使用Kunen的三值完成语义（有限失败否定）和良基语义（可能无限失败否定），采用声明式方法，抽象化操作语义细节，系统化构建逻辑程序。

Result: 开发出一种能够系统化构建可证明正确和半完备逻辑程序的方法，该方法简单实用，适用于实际日常编程。

Conclusion: 提出的方法为逻辑编程提供了一种简单而系统的途径来确保程序的正确性和半完备性，具有实际应用价值。

Abstract: Partial correctness of imperative or functional programming divides in logic
programming into two notions. Correctness means that all answers of the program
are compatible with the specification. Completeness means that the program
produces all the answers required by the specifications. We also consider
semi-completeness -- completeness for those queries for which the program does
not diverge. This paper presents an approach to systematically construct
provably correct and semi-complete logic programs, for a given specification.
Normal programs are considered, under Kunen's 3-valued completion semantics (of
negation as finite failure) and the well-founded semantics (of negation as
possibly infinite failure). The approach is declarative, it abstracts from
details of operational semantics, like e.g.\ the form of the selected literals
(``procedure calls'') during the computation. The proposed method is simple,
and can be used (maybe informally) in actual everyday programming.

</details>


### [4] [Paraconsistent Constructive Modal Logic](https://arxiv.org/abs/2508.17758)
*Han Gao,Daniil Kozhemiachenko,Nicola Olivetti*

Main category: cs.LO

TL;DR: 提出了CK构造模态逻辑的次协调对应逻辑族，用于形式化处理矛盾但非平凡的命题态度（如信念或义务），基于直觉主义框架和双赋值语义，建立了希尔伯特式公理化和可判定的模块化割消序列演算。


<details>
  <summary>Details</summary>
Motivation: 旨在形式化处理矛盾但非平凡的命题态度推理，如信念或义务中的不一致性，避免爆炸原理，提供次协调的逻辑框架。

Method: 基于直觉主义框架，使用两个独立支持真值和假值的赋值，通过尼尔森逻辑的强否定连接，定义Kripke风格语义；根据模态算子使用相同或不同可达关系定义正负支持，构建逻辑系统族；提出希尔伯特式公理化和模块化割消序列演算。

Result: 定义了次协调模态逻辑的语义框架，提出了完整的公理系统和可判定的序列演算，为处理矛盾命题态度提供了形式化工具。

Conclusion: 成功构建了CK构造模态逻辑的次协调对应逻辑族，提供了处理矛盾但非平凡命题态度的有效形式化方法，并通过割消序列演算证明了系统的可判定性。

Abstract: We present a family of paraconsistent counterparts of the constructive modal
logic CK. These logics aim to formalise reasoning about contradictory but
non-trivial propositional attitudes like beliefs or obligations. We define
their Kripke-style semantics based on intuitionistic frames with two valuations
which provide independent support for truth and falsity; they are connected by
strong negation as defined in Nelson's logic. A family of systems is obtained
depending on whether both modal operators are defined using the same or by
different accessibility relations for their positive and negative support. We
propose Hilbert-style axiomatisations for all logics determined by this
semantic framework. We also propose a~family of modular cut-free sequent
calculi that we use to establish decidability.

</details>


### [5] [Certificates and Witnesses for Multi-objective ω-regular Queries in Markov Decision Processes](https://arxiv.org/abs/2508.17859)
*Christel Baier,Calvin Chau,Volodymyr Drobitko,Simon Jantsch,Sascha Klüppelholz*

Main category: cs.LO

TL;DR: 本文提出了多目标概率模型检查的可独立验证证书和见证子系统方法，改进了现有技术并提供了更高效的实现


<details>
  <summary>Details</summary>
Motivation: 增强随机系统多目标验证的可信度和可解释性，为模型检查工具提供可验证的证据支持

Method: 扩展最大末端组件分解和可达性属性证书，推导混合整数线性规划寻找最小见证子系统，针对马尔可夫链和LTL属性使用无歧义Büchi自动机

Result: 实现了单指数空间复杂度的算法，相比基于确定性自动机的双指数空间方法有显著改进，实验验证了技术的有效性

Conclusion: 提出的证书和见证子系统方法为多目标ω-正则查询提供了可验证且高效的解决方案，提升了模型检查的可信度和实用性

Abstract: Multi-objective probabilistic model checking is a powerful technique for
verifying stochastic systems against multiple (potentially conflicting)
properties. To enhance the trustworthiness and explainability of model checking
tools, we present independently checkable certificates and witnesses for
multi-objective {\omega}-regular queries in Markov decision processes. For the
certification, we extend and improve existing certificates for the
decomposition of maximal end components and reachability properties. We then
derive mixed-integer linear programs (MILPs) for finding minimal witnessing
subsystems. For the special case of Markov chains and LTL properties, we use
unambiguous B\"uchi automata to find witnesses, resulting in an algorithm that
requires single-exponential space. Existing approaches based on deterministic
automata require doubly-exponential space in the worst case. Finally, we
consider the practical computation of our certificates and witnesses and
provide an implementation of the developed techniques, along with an
experimental evaluation, demonstrating the efficacy of our techniques.

</details>


### [6] [Compositional Verification in Concurrent Separation Logic with Permissions Regions](https://arxiv.org/abs/2508.18115)
*Quang Loc Le*

Main category: cs.LO

TL;DR: 本文提出了一个组合式验证系统CoSl，用于并发分离逻辑CSL-Perm，通过新颖的逻辑原则和蕴含过程解决自动化与组合性问题


<details>
  <summary>Details</summary>
Motivation: CSL-Perm虽然为细粒度并发程序验证提供了坚实基础，但缺乏自动化和组合性支持，限制了其实际应用

Method: 引入了新颖的逻辑原则和蕴含过程，能够推断帧规则中的剩余堆，支持具有显式算术约束的内存堆分离性片段

Result: 实现了原型工具CoSl，测试了10个具有挑战性的并发程序，包括超越现有技术水平的情况，验证了方法的优势

Conclusion: 该方法成功解决了CSL-Perm的组合性验证问题，为并发内存操作程序提供了有效的自动化验证方案

Abstract: Concurrent separation logic with fractional permissions (CSLPerm) provides a
promising reasoning system to verify most complex sequential and concurrent
fine-grained programs. The logic with strong and weak separating conjunctions
offers a solid foundation for producing concise and precise proofs. However, it
lacks automation and compositionality support. This paper addresses this
limitation by introducing a compositional verification system for concurrent
programs that manipulate regions of shared memory. The centre of our system is
novel logical principles and an entailment procedure that can infer the
residual heaps in the frame rule for a fragment of CSL-Perm with explicit
arithmetical constraints for memory heaps' disjointness. This procedure enables
the compositional reasoning for concurrent threads and function calls. We have
implemented the proposal in a prototype tool called CoSl, tested it with 10
challenging concurrent programs, including those beyond the state-of-the-art,
and confirmed the advantage of our approach.

</details>


### [7] [Model-Based Testing of an Intermediate Verifier Using Executable Operational Semantics](https://arxiv.org/abs/2508.17895)
*Lidia Losavio,Marco Paganoni,Carlo A. Furia*

Main category: cs.LO

TL;DR: BCC是一种基于模型的随机测试技术，用于测试Boogie验证器，通过生成随机程序并比较PLT Redex语义执行与Boogie验证结果来发现实现错误


<details>
  <summary>Details</summary>
Motivation: 轻量级验证技术（如随机测试）可以作为完全形式化验证的实用替代方案，即使对于已形式化验证的工具，也能测试超出形式模型范围的复杂系统部分

Method: 结合Boogie语言确定性子集的形式化与PLT Redex框架的生成能力，生成随机Boogie程序，分别通过PLT Redex操作语义执行和Boogie验证器运行，比较结果一致性

Result: 生成300万个Boogie程序进行实验，发现2%的情况表明Boogie工具链存在完整性失败（即虚假验证失败）

Conclusion: 轻量级分析工具（如基于模型的随机测试）对于测试和验证形式化验证工具（如Boogie）同样具有实用价值

Abstract: Lightweight validation technique, such as those based on random testing, are
sometimes practical alternatives to full formal verification -- providing
valuable benefits, such as finding bugs, without requiring a disproportionate
effort. In fact, they can be useful even for fully formally verified tools, by
exercising the parts of a complex system that go beyond the reach of formal
models.
  In this context, this paper introduces BCC: a model-based testing technique
for the Boogie intermediate verifier. BCC combines the formalization of a
small, deterministic subset of the Boogie language with the generative
capabilities of the PLT Redex language engineering framework. Basically, BCC
uses PLT Redex to generate random Boogie programs, and to execute them
according to a formal operational semantics; then, it runs the same programs
through the Boogie verifier. Any inconsistency between the two executions (in
PLT Redex and with Boogie) may indicate a potential bug in Boogie's
implementation.
  To understand whether BCC can be useful in practice, we used it to generate
three million Boogie programs. These experiments found 2% of cases indicative
of completeness failures (i.e., spurious verification failures) in Boogie's
toolchain. These results indicate that lightweight analysis tools, such as
those for model-based random testing, are also useful to test and validate
formal verification tools such as Boogie.

</details>


### [8] [To bind or not to bind? Discovering Stable Relationships in Object-centric Processes (Extended Version)](https://arxiv.org/abs/2508.18231)
*Anjo Seidel,Sarah Winkler,Alessandro Gianola,Marco Montali,Mathias Weske*

Main category: cs.LO

TL;DR: 本文提出了一种将对象中心Petri网(OCPN)映射到具有标识符的对象中心Petri网(OPID)的方法，通过显式数据模型捕获对象间稳定的多对一关系，实现相关对象的正确同步。


<details>
  <summary>Details</summary>
Motivation: 现有的对象中心Petri网无法充分表示对象间的关系和同步行为，无法识别违规执行。虽然OPID能够正确同步对象关系，但其发现方法尚未被研究。

Method: 结合OCPN与显式稳定的多对一关系，建立从OCPN到OPID的严格映射，明确捕获预期的稳定关系和相关对象的同步。

Result: 证明了原始OCPN与生成的OPID在满足预期关系的执行中是一致的，并提供了从OCPN到OPID映射的实现。

Conclusion: 该方法通过显式数据模型填补了OCPN与形式化OPID之间的空白，能够正确表示对象关系并实现同步，为对象中心流程挖掘提供了更精确的建模能力。

Abstract: Object-centric process mining investigates the intertwined behavior of
multiple objects in business processes. From object-centric event logs,
object-centric Petri nets (OCPN) can be discovered to replay the behavior of
processes accessing different object types. Although they indicate how objects
flow through the process and co-occur in events, OCPNs remain underspecified
about the relationships of objects. Hence, they are not able to represent
synchronization, i.e. executing objects only according to their intended
relationships, and fail to identify violating executions. Existing formal
modeling approaches, such as object-centric Petri nets with identifiers (OPID),
represent object identities and relationships to synchronize them correctly.
However, OPID discovery has not yet been studied. This paper uses explicit data
models to bridge the gap between OCPNs and formal OPIDs. We identify the
implicit assumptions of stable many-to-one relationships in object-centric
event logs, which implies synchronization of related objects. To formally
underpin this observation, we combine OCPNs with explicit stable many-to-one
relationships in a rigorous mapping from OCPNs to OPIDs explicitly capturing
the intended stable relationships and the synchronization of related objects.
We prove that the original OCPNs and the resulting OPIDs coincide for those
executions that satisfy the intended relationships. Moreover, we provide an
implementation of the mapping from OCPN to OPID under stable relationships.

</details>


### [9] [First-Order LTLf Synthesis with Lookback (Extended Version)](https://arxiv.org/abs/2508.18149)
*Sarah Winkler*

Main category: cs.LO

TL;DR: 提出了支持跨时刻变量比较的LTLfMT反应式合成方法，解决了现有方法在表达能力和适用性上的限制


<details>
  <summary>Details</summary>
Motivation: 现有LTLfMT反应式合成方法严重限制或排除了跨时刻变量比较的可能性，这严重限制了表达能力和适用性。为了满足从AI到业务流程管理等应用需求，需要支持lookback功能的完整LTLfMT合成方法

Method: 开发了一个支持lookback功能的LTLfMT反应式合成过程，允许建模跨时刻变量比较。该方法适用于完整的LTLfMT，包含先前研究的可实现性片段

Result: 证明了方法的正确性，并表明如果存在策略长度界限，则方法是完备的。同时证明了该方法构成多个相关LTLfMT片段的决策过程，重新证明了已知的可判定性结果并识别了新的可判定类

Conclusion: 虽然跨时刻比较设置本质上具有高复杂性（可实现性即使在可判定背景理论下也是不可判定的），但提出的方法为完整LTLfMT提供了实用的反应式合成解决方案，扩展了表达能力和应用范围

Abstract: Reactive synthesis addresses the problem of generating a controller for a
temporal specification in an adversarial environment; it was typically studied
for LTL. Driven by applications ranging from AI to business process management,
LTL modulo first order-theories over finite traces (LTLfMT) has recently gained
traction, where propositional variables in properties are replaced by
first-order constraints. Though reactive synthesis for LTLf with some
first-order features has been addressed, existing work in this direction
strongly restricts or excludes the possibility to compare variables across
instants, a limitation that severely restricts expressiveness and
applicability.
  In this work we present a reactive synthesis procedure for LTLfMT, where
properties support "lookback" to model cross-instant comparison of variables.
Our procedure works for full LTLfMT with lookback, subsuming the fragments of
LTLfMT for which realizability was studied earlier. However, the setting with
cross-instant comparison is inherently highly complex, as realizability is
undecidable even over decidable background theories. Hence termination of our
approach is in general not guaranteed. Nevertheless, we prove its soundness,
and show that it is complete if a bound on the strategy length exists. Finally,
we show that our approach constitutes a decision procedure for several relevant
fragments of LTLfMT, at once re-proving known decidability results and
identifying new decidable classes.

</details>


### [10] [The Computational Complexity of Satisfiability in State Space Models](https://arxiv.org/abs/2508.18162)
*Eric Alsmann,Martin Lange*

Main category: cs.LO

TL;DR: 本文分析了状态空间模型(SSM)的可满足性问题ssmSAT的复杂性，发现该问题在一般情况下是不可判定的，但在有界上下文长度和量化SSM两种限制条件下变得可判定，并建立了相应的复杂度界限。


<details>
  <summary>Details</summary>
Motivation: 研究SSM的可满足性问题复杂性，为SSM形式化验证建立理论基础，揭示其计算能力和验证的局限性。

Method: 通过理论分析证明ssmSAT在一般情况下的不可判定性，并针对两种实际限制条件（有界上下文长度和量化SSM）建立复杂度界限。

Result: ssmSAT一般不可判定；有界上下文SSM的复杂度为NP完全(NEXPTIME/PSPACE-hard)；量化SSM的复杂度为PSPACE完全(EXPSPACE)；同时建立了时不变SSM的复杂度界限。

Conclusion: 建立了SSM形式化推理的首个复杂度图谱，揭示了SSM验证的基本限制和机遇，为基于SSM的语言模型验证提供了理论基础。

Abstract: We analyse the complexity of the satisfiability problem ssmSAT for State
Space Models (SSM), which asks whether an input sequence can lead the model to
an accepting configuration. We find that ssmSAT is undecidable in general,
reflecting the computational power of SSM. Motivated by practical settings, we
identify two natural restrictions under which ssmSAT becomes decidable and
establish corresponding complexity bounds. First, for SSM with bounded context
length, ssmSAT is NP-complete when the input length is given in unary and in
NEXPTIME (and PSPACE-hard) when the input length is given in binary. Second,
for quantised SSM operating over fixed-width arithmetic, ssmSAT is
PSPACE-complete resp. in EXPSPACE depending on the bit-width encoding. While
these results hold for diagonal gated SSM we also establish complexity bounds
for time-invariant SSM. Our results establish a first complexity landscape for
formal reasoning in SSM and highlight fundamental limits and opportunities for
the verification of SSM-based language models.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [11] [Reflective Paper-to-Code Reproduction Enabled by Fine-Grained Verification](https://arxiv.org/abs/2508.16671)
*Mingyang Zhou,Quanming Yao,Lun Du,Lanning Wei,Da Zheng*

Main category: cs.SE

TL;DR: RePro是一个反思性论文到代码复现框架，通过提取论文指纹作为监督信号，在迭代验证循环中检测差异并针对性修订，显著提升机器学习论文复现效果


<details>
  <summary>Details</summary>
Motivation: 现有基于智能体的方法难以准确复现数学公式和算法逻辑等实现细节，而人类使用系统检查表调试复杂代码的方式启发了本方法

Method: 首先自动提取论文指纹（准确原子标准作为监督信号），生成初始代码，然后在迭代验证和精炼循环中利用指纹系统检测差异并针对性修订

Result: 在PaperBench Code-Dev基准测试中，RePro相比基线方法性能提升13.0%，在复杂逻辑和数学标准的正确修订方面效果显著

Conclusion: RePro框架通过论文指纹提取和迭代验证机制，有效解决了机器学习论文复现中实现细节准确性的挑战，为自动化论文复现提供了新思路

Abstract: Reproducing machine learning papers is essential for scientific progress but
remains challenging for both humans and automated agents. Existing agent-based
methods often struggle to fully and accurately reproduce implementation details
such as mathematical formulas and algorithmic logic. Previous studies show that
reflection with explicit feedback improves agent performance. However, current
paper reproduction methods fail to effectively adopt this strategy. This gap
mainly arises from the diverse paper patterns, complex method modules, and
varied configurations encountered in research papers. Motivated by how humans
use systematic checklists to efficiently debug complex code, we propose
\textbf{RePro}, a \textbf{Re}flective Paper-to-Code \textbf{Repro}duction
framework that automatically extracts a paper's fingerprint, referring to a
comprehensive set of accurate and atomic criteria serving as high-quality
supervisory signals. The framework first generates code based on the extracted
information, and then leverages the fingerprint within iterative verification
and refinement loop. This approach systematically detects discrepancies and
produces targeted revisions to align generated code with the paper's
implementation details. Extensive experiments on the PaperBench Code-Dev
benchmark have been conducted, RePro achieves 13.0\% performance gap over
baselines, and it correctly revises complex logical and mathematical criteria
in reflecting, on which the effectiveness is obvious.

</details>


### [12] [Cognitive Agents Powered by Large Language Models for Agile Software Project Management](https://arxiv.org/abs/2508.16678)
*Konrad Cinkusz,Jarosław A. Chudziak,Ewa Niewiadomska-Szynkiewicz*

Main category: cs.SE

TL;DR: 本研究探讨了将基于大语言模型(LLM)的认知智能体集成到规模化敏捷框架(SAFe)中，通过智能自动化优化软件项目管理效果。


<details>
  <summary>Details</summary>
Motivation: 探索认知智能体在IT项目开发中承担基础角色的潜力，通过智能自动化优化项目成果，特别关注这些智能体对敏捷方法的适应性及其对决策制定、问题解决和协作动态的变革性影响。

Method: 使用CogniSim生态系统平台模拟真实世界的软件工程挑战，通过迭代模拟部署虚拟智能体，利用自然语言处理促进有意义的对话，模拟人类角色并提高敏捷实践的效率和精确度。

Result: 认知智能体在任务委派、智能体间通信和项目生命周期管理方面展现出先进能力，在任务完成时间、交付质量、沟通连贯性等多个指标上实现了可衡量的改进，表现出可扩展性和适应性。

Conclusion: 将LLM驱动的认知智能体集成到敏捷项目管理框架中具有巨大潜力，不仅能完善项目管理任务的执行，还为团队协作和应对新兴挑战的方式带来了范式转变。

Abstract: This paper investigates the integration of cognitive agents powered by Large
Language Models (LLMs) within the Scaled Agile Framework (SAFe) to reinforce
software project management. By deploying virtual agents in simulated software
environments, this study explores their potential to fulfill fundamental roles
in IT project development, thereby optimizing project outcomes through
intelligent automation. Particular emphasis is placed on the adaptability of
these agents to Agile methodologies and their transformative impact on
decision-making, problem-solving, and collaboration dynamics. The research
leverages the CogniSim ecosystem, a platform designed to simulate real-world
software engineering challenges, such as aligning technical capabilities with
business objectives, managing interdependencies, and maintaining project
agility. Through iterative simulations, cognitive agents demonstrate advanced
capabilities in task delegation, inter-agent communication, and project
lifecycle management. By employing natural language processing to facilitate
meaningful dialogues, these agents emulate human roles and improve the
efficiency and precision of Agile practices. Key findings from this
investigation highlight the ability of LLM-powered cognitive agents to deliver
measurable improvements in various metrics, including task completion times,
quality of deliverables, and communication coherence. These agents exhibit
scalability and adaptability, ensuring their applicability across diverse and
complex project environments. This study underscores the potential of
integrating LLM-powered agents into Agile project management frameworks as a
means of advancing software engineering practices. This integration not only
refines the execution of project management tasks but also sets the stage for a
paradigm shift in how teams collaborate and address emerging challenges.

</details>


### [13] [Democratizing AI Development: Local LLM Deployment for India's Developer Ecosystem in the Era of Tokenized APIs](https://arxiv.org/abs/2508.16684)
*Vikranth Udandarao,Nipun Misra*

Main category: cs.SE

TL;DR: 印度开发者使用Ollama本地部署LLM替代商业API，成本降低33%，实验迭代次数翻倍，显著提升AI学习效果和创新能力


<details>
  <summary>Details</summary>
Motivation: 印度开发者社区面临商业LLM API的经济和基础设施障碍，需要寻找更经济可行的替代方案来支持持续实验和学习

Method: 采用混合方法分析，对180名印度开发者、学生和AI爱好者进行实证评估，使用Ollama进行本地LLM部署

Result: 本地部署使实验迭代次数增加一倍以上，成本降低33%，开发者对高级AI架构的理解更深

Conclusion: 本地部署是促进包容性和可访问性AI开发的关键推动因素，在资源受限环境中通过技术可访问性提升学习成果和创新能力

Abstract: India's developer community faces significant barriers to sustained
experimentation and learning with commercial Large Language Model (LLM) APIs,
primarily due to economic and infrastructural constraints. This study
empirically evaluates local LLM deployment using Ollama as an alternative to
commercial cloud-based services for developer-focused applications. Through a
mixed-methods analysis involving 180 Indian developers, students, and AI
enthusiasts, we find that local deployment enables substantially greater
hands-on development and experimentation, while reducing costs by 33% compared
to commercial solutions. Developers using local LLMs completed over twice as
many experimental iterations and reported deeper understanding of advanced AI
architectures. Our results highlight local deployment as a critical enabler for
inclusive and accessible AI development, demonstrating how technological
accessibility can enhance learning outcomes and innovation capacity in
resource-constrained environments.

</details>


### [14] [Cybernaut: Towards Reliable Web Automation](https://arxiv.org/abs/2508.16688)
*Ankur Tomar,Hengyue Liang,Indranil Bhattacharya,Natalia Larios,Francesco Carbone*

Main category: cs.SE

TL;DR: Cybernaut是一个针对企业级Web自动化的框架，解决了LLM驱动的Web自动化在复杂内部Web界面中的执行一致性、元素识别精度和规模化自动化等核心挑战，相比现有方案将任务执行成功率从72%提升到88.68%。


<details>
  <summary>Details</summary>
Motivation: 现有的Web自动化解决方案主要针对设计良好的消费级网站，无法有效处理设计不佳的内部Web界面的复杂性，存在执行一致性差、元素识别不准确、缺乏基准测试数据等问题。

Method: 提出了Cybernaut框架，包含三个核心组件：(1) SOP生成器将用户演示转换为可靠的自动化指令；(2) 高精度HTML DOM元素识别系统；(3) 执行一致性量化评估指标。

Result: 在内部基准测试中，任务执行成功率从72%提升到88.68%（提升23.2%），执行模式识别准确率达到84.7%，能够实现可靠的置信度评估和自适应指导。

Conclusion: Cybernaut在企业级Web自动化中表现出色，为Web自动化的未来发展奠定了基础，特别适用于复杂的企业内部Web应用环境。

Abstract: The emergence of AI-driven web automation through Large Language Models
(LLMs) offers unprecedented opportunities for optimizing digital workflows.
However, deploying such systems within industry's real-world environments
presents four core challenges: (1) ensuring consistent execution, (2)
accurately identifying critical HTML elements, (3) meeting human-like accuracy
in order to automate operations at scale and (4) the lack of comprehensive
benchmarking data on internal web applications. Existing solutions are
primarily tailored for well-designed, consumer-facing websites (e.g.,
Amazon.com, Apple.com) and fall short in addressing the complexity of
poorly-designed internal web interfaces. To address these limitations, we
present Cybernaut, a novel framework to ensure high execution consistency in
web automation agents designed for robust enterprise use. Our contributions are
threefold: (1) a Standard Operating Procedure (SOP) generator that converts
user demonstrations into reliable automation instructions for linear browsing
tasks, (2) a high-precision HTML DOM element recognition system tailored for
the challenge of complex web interfaces, and (3) a quantitative metric to
assess execution consistency. The empirical evaluation on our internal
benchmark demonstrates that using our framework enables a 23.2% improvement
(from 72% to 88.68%) in task execution success rate over the browser_use.
Cybernaut identifies consistent execution patterns with 84.7% accuracy,
enabling reliable confidence assessment and adaptive guidance during task
execution in real-world systems. These results highlight Cybernaut's
effectiveness in enterprise-scale web automation and lay a foundation for
future advancements in web automation.

</details>


### [15] [A Scalable Framework for the Management of STPA Requirements: a Case Study on eVTOL Operations](https://arxiv.org/abs/2508.16708)
*Shufeng Chen,Halima El Badaoui,Mariat James Elizebeth,Takuya Nakashima,Siddartha Khastgir,Paul Jennings*

Main category: cs.SE

TL;DR: 这篇论文提出了一种可扩展框架，用于优先级排序STPA法得到的安全要求，通过专家评估和蒙特卡洛模拟减少主观性，并在eVTOL实际案例中验证了框架的效果。


<details>
  <summary>Details</summary>
Motivation: STPA方法能够识别传统方法忽略的数千个安全要求，但缺乏结构化的优先级排序框架，特别是在快速发展环境中面临挑战。

Method: 提出了一种可扩展框架，整合STPA每个步骤的输出，并统筹专家根据四个关键因素（实施时间、成本、要求类型、法规覆盖范围）进行评估。使用蒙特卡洛模拟来计算和稳定要求排名，并通过自动化工具链支持动态可视化。

Result: 通过与英国民航局合作的eVTOL运营实际案例验证，该框架直接贡献了CAP3141民航公告，为监管机构、运营商和垂直机场识别系统性运营风险和安全减少措施。优先级排序过程支持利益相关方高效识别和管理高影响要求。

Conclusion: 这项工作提供了管理STPA输出的实用解决方案，帮助填补了要求优先级排序方面的空白，支持新兴技术中安全关键系统的发展。

Abstract: System-Theoretic Process Analysis (STPA) is a recommended method for
analysing complex systems, capable of identifying thousands of safety
requirements often missed by traditional techniques such as Failure Mode and
Effects Analysis (FMEA) and Fault Tree Analysis (FTA). However, the absence of
a structured framework for managing and prioritising these requirements
presents challenges, particularly in fast-paced development environments. This
paper introduces a scalable framework for prioritising STPA-derived
requirements. The framework integrates outputs from each STPA step and
incorporates expert evaluations based on four key factors: implementation time,
cost, requirement type, and regulatory coverage. To reduce subjectivity,
Monte-Carlo Simulation (MCS) is employed to calculate and stabilise requirement
rankings. An automation toolchain supports the framework, enabling dynamic
mapping of prioritised requirements in a scaling matrix. This visualisation
aids decision-making and ensures traceability across development phases. The
framework is applicable from early conceptualisation to more advanced stages,
enhancing its utility in iterative system development. The framework was
validated through a real-world case study focused on Electric Vertical Take-off
and Landing (eVTOL) operations, conducted in collaboration with the UK Civil
Aviation Authority. The findings contributed directly to CAP3141, a Civil
Aviation Publication that identifies systemic operational risks and safety
mitigations for regulators, operators, and vertiports. The prioritisation
process supported decision-making by helping stakeholders identify and manage
high-impact requirements efficiently. This work contributes a practical
solution for managing STPA outputs, bridging gaps in requirement prioritisation
and supporting safety-critical development in emerging technologies.

</details>


### [16] [CelloAI: Leveraging Large Language Models for HPC Software Development in High Energy Physics](https://arxiv.org/abs/2508.16713)
*Mohammad Atif,Kriti Chopra,Ozgur Kilic,Tianle Wang,Zhihua Dong,Charles Leggett,Meifeng Lin,Paolo Calafiura,Salman Habib*

Main category: cs.SE

TL;DR: CelloAI是一个本地部署的AI编程助手，利用大语言模型和检索增强生成技术，帮助高能物理实验解决遗留代码迁移和文档化问题，支持代码文档生成和代码生成两大功能。


<details>
  <summary>Details</summary>
Motivation: 下一代高能物理实验将产生前所未有的数据量，需要高性能计算与传统高吞吐量计算集成。但HEP领域采用HPC的障碍在于将遗留软件移植到异构架构的挑战，以及这些复杂科学代码库的文档稀疏问题。

Method: 开发了本地托管的CelloAI编码助手，采用检索增强生成(RAG)技术的大语言模型。系统包含代码文档化组件（Doxygen风格注释生成、文件级摘要、交互式聊天机器人）和代码生成组件（语法感知分块策略、调用图知识集成、性能优化建议）。

Result: 使用ATLAS、CMS和DUNE实验的真实HEP应用程序进行评估，比较了不同嵌入模型的代码检索效果。结果表明AI助手能够增强代码理解并支持可靠的代码生成。

Conclusion: CelloAI在保持科学计算环境所需的透明度和安全要求的同时，证明了其提升代码理解和支持可靠代码生成的能力，为高能物理领域的HPC集成提供了有效解决方案。

Abstract: Next-generation High Energy Physics (HEP) experiments will generate
unprecedented data volumes, necessitating High Performance Computing (HPC)
integration alongside traditional high-throughput computing. However, HPC
adoption in HEP is hindered by the challenge of porting legacy software to
heterogeneous architectures and the sparse documentation of these complex
scientific codebases. We present CelloAI, a locally hosted coding assistant
that leverages Large Language Models (LLMs) with retrieval-augmented generation
(RAG) to support HEP code documentation and generation. This local deployment
ensures data privacy, eliminates recurring costs and provides access to large
context windows without external dependencies. CelloAI addresses two primary
use cases, code documentation and code generation, through specialized
components. For code documentation, the assistant provides: (a) Doxygen style
comment generation for all functions and classes by retrieving relevant
information from RAG sources (papers, posters, presentations), (b) file-level
summary generation, and (c) an interactive chatbot for code comprehension
queries. For code generation, CelloAI employs syntax-aware chunking strategies
that preserve syntactic boundaries during embedding, improving retrieval
accuracy in large codebases. The system integrates callgraph knowledge to
maintain dependency awareness during code modifications and provides
AI-generated suggestions for performance optimization and accurate refactoring.
We evaluate CelloAI using real-world HEP applications from ATLAS, CMS, and DUNE
experiments, comparing different embedding models for code retrieval
effectiveness. Our results demonstrate the AI assistant's capability to enhance
code understanding and support reliable code generation while maintaining the
transparency and safety requirements essential for scientific computing
environments.

</details>


### [17] [Who Wins the Race? (R Vs Python) - An Exploratory Study on Energy Consumption of Machine Learning Algorithms](https://arxiv.org/abs/2508.17344)
*Rajrupa Chattaraj,Sridhar Chimalakonda,Vibhu Saujanya Sharma,Vikrant Kaulgud*

Main category: cs.SE

TL;DR: 这篇论文通过实证研究比较了Python和R语言在机器学习任务中的能耗表现，发现编程语言选择对能源效率有显著影响


<details>
  <summary>Details</summary>
Motivation: 机器学习应用能耗巨大但缺乏环境影响研究，特别是编程语言选择对ML任务能耗影响的比较研究空白

Method: 采用实证研究方法，测量并比较Python和R语言在5个回归和5个分类任务中的能量消耗和运行性能

Result: 在95%的情况下两种语言存在显著统计差异，编程语言选择可影响能源效率达99.16%（训练）和99.8%（推理）

Conclusion: 编程语言选择对ML系统能耗有重要影响，应考虑环境友好性作为软件开发的关键因素

Abstract: The utilization of Machine Learning (ML) in contemporary software systems is
extensive and continually expanding. However, its usage is energy-intensive,
contributing to increased carbon emissions and demanding significant resources.
While numerous studies examine the performance and accuracy of ML, only a
limited few focus on its environmental aspects, particularly energy
consumption. In addition, despite emerging efforts to compare energy
consumption across various programming languages for specific algorithms and
tasks, there remains a gap specifically in comparing these languages for
ML-based tasks. This paper aims to raise awareness of the energy costs
associated with employing different programming languages for ML model training
and inference. Through this empirical study, we measure and compare the energy
consumption along with run-time performance of five regression and five
classification tasks implemented in Python and R, the two most popular
programming languages in this context. Our study results reveal a statistically
significant difference in costs between the two languages in 95% of the cases
examined. Furthermore, our analysis demonstrates that the choice of programming
language can influence energy efficiency significantly, up to 99.16% during
model training and up to 99.8% during inferences, for a given ML task.

</details>


### [18] [EyeMulator: Improving Code Language Models by Mimicking Human Visual Attention](https://arxiv.org/abs/2508.16771)
*Yifan Zhang,Chen Huang,Yueke Zhang,Jiahao Zhang,Toby Jia-Jun Li,Collin McMillan,Kevin Leach,Yu Huang*

Main category: cs.SE

TL;DR: EyeMulator是一种训练代码语言模型的技术，通过模仿人类视觉注意力来提升模型在代码翻译、补全和摘要等任务上的性能


<details>
  <summary>Details</summary>
Motivation: 现有代码语言模型的注意力机制仅基于输入token的重要性，而人类开发者具有直觉性的注意力模式。人类视觉注意力数据可以为模型训练提供更好的指导

Method: 在LLM微调过程中，为每个输入token添加特殊权重到损失函数中，这些权重来源于公开的眼动追踪数据集，模仿人类视觉注意力模式

Result: EyeMulator在多个软件工程任务（代码翻译、补全、摘要）上优于强基线模型，消融研究表明改进确实源于模型学会了模仿人类注意力

Conclusion: 利用人类视觉注意力数据可以有效地指导代码语言模型的训练，提升模型性能，且推理时无需眼动追踪数据

Abstract: Code language models (so-called CodeLLMs) are now commonplace in software
development. As a general rule, CodeLLMs are trained by dividing training
examples into input tokens and then learn importance of those tokens in a
process called machine attention. Machine attention is based solely on input
token salience to output token examples during training. Human software
developers are different, as humans intuitively know that some tokens are more
salient than others. While intuition itself is ineffable and a subject of
philosophy, clues about salience are present in human visual attention, since
people tend to look at more salient words more often. In this paper, we present
EyeMulator, a technique for training CodeLLMs to mimic human visual attention
while training for various software development tasks. We add special weights
for each token in each input example to the loss function used during LLM
fine-tuning. We draw these weights from observations of human visual attention
derived from a previously-collected publicly-available dataset of eye-tracking
experiments in software engineering tasks. These new weights ultimately induce
changes in the attention of the subject LLM during training, resulting in a
model that does not need eye-tracking data during inference. Our evaluation
shows that EyeMulator outperforms strong LLM baselines on several tasks such as
code translation, completion and summarization. We further show an ablation
study that demonstrates the improvement is due to subject models learning to
mimic human attention.

</details>


### [19] [DevLicOps: A Framework for Mitigating Licensing Risks in AI-Generated Code](https://arxiv.org/abs/2508.16853)
*Pratyush Nidhi Sharma,Lauren Wright,Anne Herfurth,Munsif Sokiyna,Pratyaksh Nidhi Sharma,Sethu Das,Mikko Siponen*

Main category: cs.SE

TL;DR: DevLicOps框架帮助IT领导者管理AI编程助手相关的开源许可证合规风险，通过治理、事件响应和权衡决策来应对GPL等限制性许可证带来的法律风险。


<details>
  <summary>Details</summary>
Motivation: 生成式AI编程助手广泛采用但存在严重法律和合规风险，可能生成受限制性开源许可证（如GPL）约束的代码，使公司面临诉讼或被强制开源的风险。

Method: 提出DevLicOps实践框架，包含治理机制、事件响应流程和明智的权衡决策，帮助管理AI编程助手相关的许可证合规风险。

Result: 提供了一个实用的风险管理框架，帮助组织在AI编程助手时代实现负责任的、风险感知的软件开发。

Conclusion: 随着AI编程助手采用率增长和法律框架演变，主动的许可证合规管理对于AI时代的软件开发至关重要。

Abstract: Generative AI coding assistants (ACAs) are widely adopted yet pose serious
legal and compliance risks. ACAs can generate code governed by restrictive
open-source licenses (e.g., GPL), potentially exposing companies to litigation
or forced open-sourcing. Few developers are trained in these risks, and legal
standards vary globally, especially with outsourcing. Our article introduces
DevLicOps, a practical framework that helps IT leaders manage ACA-related
licensing risks through governance, incident response, and informed tradeoffs.
As ACA adoption grows and legal frameworks evolve, proactive license compliance
is essential for responsible, risk-aware software development in the AI era.

</details>


### [20] [TriagerX: Dual Transformers for Bug Triaging Tasks with Content and Interaction Based Rankings](https://arxiv.org/abs/2508.16860)
*Md Afif Al Mamun,Gias Uddin,Lan Xia,Longyu Zhang*

Main category: cs.SE

TL;DR: TriagerX是一个用于bug分配的双transformer架构，通过结合内容排名和开发者历史交互排名，显著提升了开发人员推荐准确率，在工业部署中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的预训练语言模型在bug分配任务中可能关注不相关的token，且未考虑开发者历史交互信息，导致推荐效果不理想。

Method: 采用双transformer架构收集两个transformer的推荐结果，结合最后三层输出生成稳健的内容排名，再通过新颖的基于交互的排名方法考虑开发者历史修复相似bug的记录来优化排名。

Result: 在五个数据集上超越所有九个transformer基线方法，Top-1和Top-3准确率提升超过10%。工业部署中组件推荐提升10%，开发人员推荐提升54%。

Conclusion: TriagerX通过双transformer架构和历史交互信息整合，显著提升了bug分配任务的性能，在实际工业环境中验证了其有效性。

Abstract: Pretrained Language Models or PLMs are transformer-based architectures that
can be used in bug triaging tasks. PLMs can better capture token semantics than
traditional Machine Learning (ML) models that rely on statistical features
(e.g., TF-IDF, bag of words). However, PLMs may still attend to less relevant
tokens in a bug report, which can impact their effectiveness. In addition, the
model can be sub-optimal with its recommendations when the interaction history
of developers around similar bugs is not taken into account. We designed
TriagerX to address these limitations. First, to assess token semantics more
reliably, we leverage a dual-transformer architecture. Unlike current
state-of-the-art (SOTA) baselines that employ a single transformer
architecture, TriagerX collects recommendations from two transformers with each
offering recommendations via its last three layers. This setup generates a
robust content-based ranking of candidate developers. TriagerX then refines
this ranking by employing a novel interaction-based ranking methodology, which
considers developers' historical interactions with similar fixed bugs. Across
five datasets, TriagerX surpasses all nine transformer-based methods, including
SOTA baselines, often improving Top-1 and Top-3 developer recommendation
accuracy by over 10%. We worked with our large industry partner to successfully
deploy TriagerX in their development environment. The partner required both
developer and component recommendations, with components acting as proxies for
team assignments-particularly useful in cases of developer turnover or team
changes. We trained TriagerX on the partner's dataset for both tasks, and it
outperformed SOTA baselines by up to 10% for component recommendations and 54%
for developer recommendations.

</details>


### [21] [Mind the Gap: A Decade-Scale Empirical Study of Multi-Stakeholder Dynamics in VR Ecosystem](https://arxiv.org/abs/2508.16903)
*Yijun Lu,Hironori Washizaki,Naoyasu Ubayashi,Nobukazu Yoshioka,Chenhao Wu,Masanari Kondo,Yuyin Ma,Jiong Dong,Jianjin Zhao,Dongqi Han*

Main category: cs.SE

TL;DR: 本研究提出了一种多视角实证框架，通过对用户评论和开发者讨论的系统分析，揭示了VR生态系统中用户期望与开发者行动之间的差距，特别是在包容性和社区安全方面的潜在问题。


<details>
  <summary>Details</summary>
Motivation: 识别用户期望与开发者行动之间的差距，指导更有效的质量保证和用户为中心的创新。之前的研究通常单独分析用户评论或开发者讨论，无法反映具体用户关注点如何被技术活动处理。

Method: 采用多视角实证框架，对944,320条用户评论和389,477条开发者帖子进行主题建模和定量影响分析，系统比较和对齐利益相关者观点。

Result: 识别了关注点的重合区域（如性能、输入方法）以及明显的差距领域（如包容性、社区安全）。发现用户多次提出的问题（如LGBTQ+代表性、儿童友好内容）在开发者论坛中很少被讨论。

Conclusion: 该研究为缩小VR生态系统中用户-开发者差距提供了数据驱动的建议，对平台治理和下一代VR系统设计具有实践意义。

Abstract: In the development and evolution of VR ecosystem, platform stakeholders
continuously adapt their products in response to user and technical feedback,
often reflected in subtle shifts in discussion topics or system updates. A
comprehensive understanding of these changes is essential for identifying gaps
between user expectations and developer actions, which can guide more effective
quality assurance and user-centered innovation. While previous studies have
analyzed either user reviews or developer discussions in isolation, such
approaches typically fail to reveal how specific user concerns are (or are not)
addressed by corresponding technical activities. To address this limitation,
our study introduces a multi-view empirical framework that systematically
compares and aligns stakeholder perspectives. By applying topic modeling and
quantitative impact analysis to 944,320 user reviews and 389,477 developer
posts, we identify not only the overlap in concerns (e.g., performance, input
methods), but also clear gaps in areas like inclusivity and community safety
(e.g., LGBTQ+ representation, child-friendly content). Our findings show that
while users repeatedly raise such issues, they are rarely discussed in
developer forums. These insights enable data-driven recommendations for closing
the user-developer gap in VR ecosystems, offering practical implications for
platform governance and the design of next-generation VR systems.

</details>


### [22] [What Developers Ask to ChatGPT in GitHub Pull Requests? an Exploratory Study](https://arxiv.org/abs/2508.17161)
*Julyanara R. Silva,Carlos Eduardo C. Dantas,Marcelo A. Maia*

Main category: cs.SE

TL;DR: 本文通过分析155个ChatGPT分享链接，研究了开发者如何通过与ChatGPT的交互（提示）来贡献代码库，识别出14种请求类型并发现代码生成提示需要更多交互。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型如ChatGPT已成为软件开发的重要工具，但开发者与ChatGPT之间的交互如何促成代码库贡献的理解仍然有限。

Method: 对从139个已合并Pull Requests中提取的155个有效ChatGPT分享链接进行手动评估，分析开发者和审阅者与ChatGPT的交互过程。

Result: 识别出14种ChatGPT请求类型，分为四大类：代码审查、基于特定任务的代码实现、技术解释澄清、网页文本精炼。发现代码生成请求通常需要更多交互才能获得满意答案。

Conclusion: 研究揭示了开发者使用ChatGPT的具体模式，为理解AI辅助编程工具的实际应用提供了实证基础，特别是代码生成任务需要更复杂的交互过程。

Abstract: The emergence of Large Language Models (LLMs), such as ChatGPT, has
introduced a new set of tools to support software developers in solving pro-
gramming tasks. However, our understanding of the interactions (i.e., prompts)
between developers and ChatGPT that result in contributions to the codebase
remains limited. To explore this limitation, we conducted a manual evaluation
of 155 valid ChatGPT share links extracted from 139 merged Pull Requests (PRs),
revealing the interactions between developers and reviewers with ChatGPT that
led to merges into the main codebase. Our results produced a catalog of 14
types of ChatGPT requests categorized into four main groups. We found a
significant number of requests involving code review and the implementation of
code snippets based on specific tasks. Developers also sought to clarify doubts
by requesting technical explanations or by asking for text refinements for
their web pages. Furthermore, we verified that prompts involving code
generation generally required more interactions to produce the desired answer
compared to prompts requesting text review or technical information.

</details>


### [23] [Agentic AI for Software: thoughts from Software Engineering community](https://arxiv.org/abs/2508.17343)
*Abhik Roychoudhury*

Main category: cs.SE

TL;DR: 本文探讨了AI代理在软件工程中的广泛应用，超越了代码生成，涵盖了测试、修复、架构设计等多个层面，强调了意图推断和AI验证的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前AI在软件工程中的应用主要集中在代码生成，但软件工程包含更多复杂任务。作者希望探索AI代理如何在这些更广泛的软件工程任务中发挥作用，实现真正的AI软件工程师愿景。

Method: 提出通过AI代理自主处理软件工程中的微观决策，结合程序分析工具，重点关注意图推断（specification inference）技术，并引入AI驱动的验证验证（V&V）机制。

Result: 构建了AI软件工程师的概念框架，其中AI代理可以作为开发团队成员，处理从代码级到设计级的各种软件任务，并通过AI-based V&V应对自动化代码爆炸问题。

Conclusion: 成功的AI代理软件工程工作流需要解决意图推断这一核心挑战，未来的发展方向是集成AI驱动的验证验证机制，以建立可信的AI软件工程自动化体系。

Abstract: AI agents have recently shown significant promise in software engineering.
Much public attention has been transfixed on the topic of code generation from
Large Language Models (LLMs) via a prompt. However, software engineering is
much more than programming, and AI agents go far beyond instructions given by a
prompt.
  At the code level, common software tasks include code generation, testing,
and program repair. Design level software tasks may include architecture
exploration, requirements understanding, and requirements enforcement at the
code level. Each of these software tasks involves micro-decisions which can be
taken autonomously by an AI agent, aided by program analysis tools. This
creates the vision of an AI software engineer, where the AI agent can be seen
as a member of a development team.
  Conceptually, the key to successfully developing trustworthy agentic AI-based
software workflows will be to resolve the core difficulty in software
engineering - the deciphering and clarification of developer intent.
Specification inference, or deciphering the intent, thus lies at the heart of
many software tasks, including software maintenance and program repair. A
successful deployment of agentic technology into software engineering would
involve making conceptual progress in such intent inference via agents.
  Trusting the AI agent becomes a key aspect, as software engineering becomes
more automated. Higher automation also leads to higher volume of code being
automatically generated, and then integrated into code-bases. Thus to deal with
this explosion, an emerging direction is AI-based verification and validation
(V & V) of AI generated code. We posit that agentic software workflows in
future will include such AIbased V&V.

</details>


### [24] [Code Difference Guided Fuzzing for FPGA Logic Synthesis Compilers via Bayesian Optimization](https://arxiv.org/abs/2508.17713)
*Zhihao Xu,Shikai Guo,Guilin Zhao,Peiyu Zou,Siwen Wang,Qian Ma,Hui Li,Furui Zhan*

Main category: cs.SE

TL;DR: LSC-Fuzz是一个基于贝叶斯优化的引导变异策略，用于检测FPGA逻辑综合编译器中的bug，通过生成多样化HDL代码和等效检查发现了16个bug。


<details>
  <summary>Details</summary>
Motivation: FPGA在安全关键环境中广泛应用，但逻辑综合编译器中的bug可能导致安全风险，现有方法的简单盲目变异策略效果有限。

Method: 提出基于贝叶斯优化的引导变异策略LSC-Fuzz，包含测试程序生成、贝叶斯多样性选择和等效检查三个组件。

Result: 在三个月内发现了16个bug，其中12个已被官方技术支持确认。

Conclusion: LSC-Fuzz能有效生成多样化复杂HDL代码，全面测试FPGA逻辑综合编译器，成功检测出多个bug。

Abstract: Field Programmable Gate Arrays (FPGAs) play a crucial role in Electronic
Design Automation (EDA) applications, which have been widely used in
safety-critical environments, including aerospace, chip manufacturing, and
medical devices. A critical step in FPGA development is logic synthesis, which
enables developers to translate their software designs into hardware net lists,
which facilitates the physical implementation of the chip, detailed timing and
power analysis, gate-level simulation, test vector generation, and optimization
and consistency checking. However, bugs or incorrect implementations in FPGA
logic synthesis compilers may lead to unexpected behaviors in target
wapplications, posing security risks. Therefore, it is crucial to eliminate
such bugs in FPGA logic synthesis compilers. The effectiveness of existing
works is still limited by its simple, blind mutation strategy. To address this
challenge, we propose a guided mutation strategy based on Bayesian optimization
called LSC-Fuzz to detect bugs in FPGA logic synthesis compilers. Specifically,
LSC-Fuzz consists of three components: the test-program generation component,
the Bayesian diversity selection component, and the equivalent check component.
By performing test-program generation and Bayesian diversity selection,
LSC-Fuzz generates diverse and complex HDL code, thoroughly testing the FPGA
logic synthesis compilers using equivalent check to detect bugs. Through three
months, LSC-Fuzz has found 16 bugs, 12 of these has been confirmed by official
technical support.

</details>


### [25] [DocFetch - Towards Generating Software Documentation from Multiple Software Artifacts](https://arxiv.org/abs/2508.17719)
*Akhila Sri Manasa Venigalla,Sridhar Chimalakonda*

Main category: cs.SE

TL;DR: DocFetch是一个基于多层级提示LLM的系统，能够从多个软件工件自动生成不同类型的文档，显著减少文档维护工作量。


<details>
  <summary>Details</summary>
Motivation: 开源软件的广泛采用导致项目快速发展，但相关文档维护困难。现有自动化文档生成方法主要关注源代码，而有用信息分散在多个共进化的工作中。

Method: 采用基于多层提示的大语言模型(LLM)，从DocMine数据集中的多个软件工件生成结构化文档，对应不同的文档类型。

Result: 评估显示，从五个文档源生成API相关和文件相关信息时获得最高BLEU-4分数43.24%和ROUGE-L分数0.39，其他文档类型的BLEU-4分数接近30%。

Conclusion: DocFetch可用于半自动生成文档，帮助理解项目，同时最小化文档维护工作量。

Abstract: Software Documentation plays a major role in the usage and development of a
project. Widespread adoption of open source software projects contributes to
larger and faster development of the projects, making it difficult to maintain
the associated documentation. Existing automated approaches to generate
documentation largely focus on source code. However, information useful for
documentation is observed to be scattered across various artifacts that
co-evolve with the source code. Leveraging this information across multiple
artifacts can reduce the effort involved in maintaining documentation. Hence,
we propose DocFetch, to generate different types of documentation from multiple
software artifacts. We employ a multi-layer prompt based LLM and generate
structured documentation corresponding to different documentation types for the
data consolidated in DocMine dataset. We evaluate the performance of DocFetch
using a manually curated groundtruth dataset by analysing the artifacts in
DocMine. The evaluation yields a highest BLEU-4 score of 43.24% and ROUGE-L
score of 0.39 for generation of api-related and file-related information from
five documentation sources. The generation of other documentation type related
information also reported BLEU-4 scores close to 30% indicating good
performance of the approach. Thus,DocFetch can be employed to
semi-automatically generate documentation, and helps in comprehending the
projects with minimal effort in maintaining the documentation.

</details>


### [26] [RepoTransAgent: Multi-Agent LLM Framework for Repository-Aware Code Translation](https://arxiv.org/abs/2508.17720)
*Ziqi Guan,Xin Yin,Zhiyuan Peng,Chao Ni*

Main category: cs.SE

TL;DR: RepoTransAgent是一个多智能体LLM框架，通过分解代码翻译为上下文检索、动态提示构建和迭代代码优化等子任务，显著提升了Java-C#代码翻译的编译通过率和测试通过率。


<details>
  <summary>Details</summary>
Motivation: 现有代码翻译方法在实践场景中存在上下文理解不足、提示设计不灵活和错误纠正机制不完善等问题，阻碍了复杂代码库的准确高效翻译。

Method: 提出多智能体框架，采用检索增强生成(RAG)获取上下文信息，使用自适应提示针对不同仓库场景，并引入基于反思的系统性错误纠正机制。

Result: 在6个开源项目的数百个Java-C#翻译对上，RepoTransAgent达到55.34%的编译通过率和45.84%的测试通过率，显著优于现有最优方法。

Conclusion: RepoTransAgent展示了强大的鲁棒性和泛化能力，为实际仓库感知的代码翻译任务提供了有效解决方案。

Abstract: Repository-aware code translation is critical for modernizing legacy systems,
enhancing maintainability, and enabling interoperability across diverse
programming languages. While recent advances in large language models (LLMs)
have improved code translation quality, existing approaches face significant
challenges in practical scenarios: insufficient contextual understanding,
inflexible prompt designs, and inadequate error correction mechanisms. These
limitations severely hinder accurate and efficient translation of complex,
real-world code repositories. To address these challenges, we propose
RepoTransAgent, a novel multi-agent LLM framework for repository-aware code
translation. RepoTransAgent systematically decomposes the translation process
into specialized subtasks-context retrieval, dynamic prompt construction, and
iterative code refinement-each handled by dedicated agents. Our approach
leverages retrieval-augmented generation (RAG) for contextual information
gathering, employs adaptive prompts tailored to varying repository scenarios,
and introduces a reflection-based mechanism for systematic error correction. We
evaluate RepoTransAgent on hundreds of Java-C# translation pairs from six
popular open-source projects. Experimental results demonstrate that
RepoTransAgent significantly outperforms state-of-the-art baselines in both
compile and pass rates. Specifically, RepoTransAgent achieves up to 55.34%
compile rate and 45.84% pass rate. Comprehensive analysis confirms the
robustness and generalizability of RepoTransAgent across different LLMs,
establishing its effectiveness for real-world repository-aware code
translation.

</details>


### [27] [Logging Requirement for Continuous Auditing of Responsible Machine Learning-based Applications](https://arxiv.org/abs/2508.17851)
*Patrick Loic Foalem,Leuson Da Silva,Foutse Khomh,Heng Li,Ettore Merlo*

Main category: cs.SE

TL;DR: 该论文探讨了机器学习应用中通过日志监控来实现审计、透明度和合规性的方法，强调了当前日志实践的不足并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在各行业的广泛应用，由于缺乏透明度、公平性和问责制，引发了伦理和法律合规性的担忧。传统软件中的日志监控实践为审计机器学习应用提供了潜在途径。

Method: 研究通过分析现有日志实践，提出需要增强日志记录方法和工具，系统性地整合负责任AI指标，以支持可审计、透明和符合伦理的机器学习系统开发。

Result: 研究发现当前日志实践存在特定缺陷，但同时也识别了改进机会，为从业者和工具开发者提供了可操作的指导。

Conclusion: 增强的日志实践和工具对于满足日益增长的监管要求和社会期望至关重要，能够加强机器学习应用的可问责性和可信度。

Abstract: Machine learning (ML) is increasingly applied across industries to automate
decision-making, but concerns about ethical and legal compliance remain due to
limited transparency, fairness, and accountability. Monitoring through logging
a long-standing practice in traditional software offers a potential means for
auditing ML applications, as logs provide traceable records of system behavior
useful for debugging, performance analysis, and continuous auditing.
systematically auditing models for compliance or accountability. The findings
underscore the need for enhanced logging practices and tooling that
systematically integrate responsible AI metrics. Such practices would support
the development of auditable, transparent, and ethically responsible ML
systems, aligning with growing regulatory requirements and societal
expectations. By highlighting specific deficiencies and opportunities, this
work provides actionable guidance for both practitioners and tool developers
seeking to strengthen the accountability and trustworthiness of ML
applications.

</details>


### [28] [modelSolver: A Symbolic Model-Driven Solver for Power Network Simulation and Monitoring](https://arxiv.org/abs/2508.17882)
*Izudin Dzafic,Rabih A. Jabr*

Main category: cs.SE

TL;DR: modelSolver是一个基于符号数学建模的软件框架，让电力系统领域专家无需编程技能即可通过数学表达式定义模型，简化了潮流计算和状态估计等分析任务。


<details>
  <summary>Details</summary>
Motivation: 现有电力系统分析工具需要编程技能来修改内置模型，这对缺乏编码能力的领域专家构成了障碍。需要一种更直观的建模方式让专家专注于电力系统建模本身。

Method: 提出基于符号数学建模的新框架，用户可以通过直观的数学表达式定义模型，无需使用传统编程结构如数组、循环和稀疏矩阵计算。采用开放式方法支持实数和复数变量，并提供与MATPOWER兼容的转换器。

Result: 能够表示各种高级功能，包括带电压调节器和负载分接开关的潮流计算、连续潮流计算、以及带等式约束的高斯-牛顿状态估计。确保了与现有工具的兼容性。

Conclusion: 该框架优先考虑模型驱动开发，消除了编程障碍，使电力系统计算对学生、科学家和从业者更加易于使用，让领域专家能够专注于电力系统建模而非编程技术。

Abstract: The development of advanced software tools for power system analysis requires
extensive programming expertise. Even when using open-source tools, programming
skills are essential to modify built-in models. This can be particularly
challenging for domain experts who lack coding proficiency. This paper
introduces modelSolver, a software solution with a new framework centered
around symbolic mathematical modeling. The proposed paradigm facilitates
defining models through intuitive mathematical expressions, thus eliminating
the need for traditional programming constructs such as arrays, loops, and
sparse matrix computations. The modelSolver focuses on power flow and state
estimation using an open-box approach, which allows users to specify custom
models using either real or complex variables. Unlike existing tools that rely
on hard-coded models, modelSolver enables the representation of a wide range of
advanced functionalities, including power flow with voltage regulators and load
tap changers, continuation power flow, and Gauss-Newton state estimation with
equality constraints. Compatibility with MATPOWER is ensured via a converter
that automates importing data files. The framework prioritizes model-driven
development and empowers domain experts to focus on power system modeling
without programming barriers. It aims to simplify power system computations,
making them more accessible to students, scientists, and practitioners.

</details>


### [29] [A Defect Classification Framework for AI-Based Software Systems (AI-ODC)](https://arxiv.org/abs/2508.17900)
*Mohammed O. Alannsary*

Main category: cs.SE

TL;DR: 本文提出了一种专门用于人工智能系统的缺陷分析框架AIODC，通过修改正交缺陷分类(ODC)来适应AI系统的数据、学习和思维特性。


<details>
  <summary>Details</summary>
Motivation: 当前缺陷分析模型无法抓取AI系统的独特属性，需要专门的框架来进行质量保证。

Method: 修改ODC框架，新增了数据、学习和思维三个分类维度，添加新的严重级别，并将影响区域替换为AI相关特性。

Result: 在机器学习错误数据集上的案例研究显示，学习阶段的缺陷最为普遍且与高严重级别显著相关，思维阶段的缺陷则对可信过和准确性有不成比例影响。

Conclusion: AIODC框架能够识别高风险缺陷类别，为有针对性的质量保证措施提供信息支持。

Abstract: Artificial Intelligence has gained a lot of attention recently, it has been
utilized in several fields ranging from daily life activities, such as
responding to emails and scheduling appointments, to manufacturing and
automating work activities. Artificial Intelligence systems are mainly
implemented as software solutions, and it is essential to discover and remove
software defects to assure its quality using defect analysis which is one of
the major activities that contribute to software quality. Despite the
proliferation of AI-based systems, current defect analysis models fail to
capture their unique attributes. This paper proposes a framework inspired by
the Orthogonal Defect Classification (ODC) paradigm and enables defect analysis
of Artificial Intelligence systems while recognizing its special attributes and
characteristics. This study demonstrated the feasibility of modifying ODC for
AI systems to classify its defects. The ODC was adjusted to accommodate the
Data, Learning, and Thinking aspects of AI systems which are newly introduced
classification dimensions. This adjustment involved the introduction of an
additional attribute to the ODC attributes, the incorporation of a new severity
level, and the substitution of impact areas with characteristics pertinent to
AI systems. The framework was showcased by applying it to a publicly available
Machine Learning bug dataset, with results analyzed through one-way and two-way
analysis. The case study indicated that defects occurring during the Learning
phase were the most prevalent and were significantly linked to high-severity
classifications. In contrast, defects identified in the Thinking phase had a
disproportionate effect on trustworthiness and accuracy. These findings
illustrate AIODC's capability to identify high-risk defect categories and
inform focused quality assurance measures.

</details>


### [30] [Evaluating Citizen Satisfaction with Saudi Arabia's E-Government Services: A Standards-Based, Theory-Informed Approach](https://arxiv.org/abs/2508.17912)
*Mohammed O. Alannsary*

Main category: cs.SE

TL;DR: 该研究基于ISO/IEC标准框架和UTAUT理论，调查沙特阿拉伯电子政务服务的公民满意度，发现用户对可用性和信任度满意度高，但在服务清晰度、系统响应性和情感参与方面存在挑战。


<details>
  <summary>Details</summary>
Motivation: 随着数字政务平台成为公共服务交付的核心，了解公民评估对于提升可用性、信任度和包容性至关重要，特别是在沙特阿拉伯这样电子政务发展全球领先的国家。

Method: 采用基于ISO/IEC 25010和25022标准的质量使用框架，结合UTAUT理论，通过结构化问卷调查500名公民，获得276份有效回复，从整体满意度、功能满意度、信任和情感参与四个维度评估满意度。

Result: 研究发现公民对电子政务服务的可用性和信任度满意度很高，这与沙特在全球电子政务排名中的领先地位一致，但服务清晰度和系统响应性方面仍存在持续挑战，情感参与度有限。

Conclusion: 研究为政策制定者提供了宝贵见解，并将基于标准的行为采纳模型理论整合到公民身份背景中，表明用户主要将这些服务视为功能性工具而非参与性数字体验。

Abstract: As digital government platforms become central to public service delivery,
understanding citizen assessment is crucial for enhancing usability, trust, and
inclusivity. This study investigates citizen satisfaction with the e-government
services in Saudi Arabia through a quality-in-use framework based on ISO/IEC
25010 and ISO/IEC 25022 standards, interpreted through the lens of the Unified
Theory of Acceptance and Use of Technology (UTAUT). A structured questionnaire
was administered to 500 citizens, yielding 276 valid responses. Satisfaction
was evaluated across four dimensions: overall satisfaction, feature
satisfaction, trust, and emotional engagement (pleasure). The findings
demonstrate consistently high levels of satisfaction regarding usability and
trust, aligning with Saudi Arabia's top-tier global ranking in e-government
development. However, the results also highlight persistent challenges related
to service clarity and system responsiveness. Emotional engagement was limited,
indicating that users perceive these services primarily as functional tools
rather than as engaging digital experiences. The study offers valuable insights
for policymakers and contributes to the theoretical integration of
standards-based and behavioral adoption models in the context of citizenship.

</details>


### [31] [DesCartes Builder: A Tool to Develop Machine-Learning Based Digital Twins](https://arxiv.org/abs/2508.17988)
*Eduardo de Conto,Blaise Genest,Arvind Easwaran,Nicholas Ng,Shweta Menon*

Main category: cs.SE

TL;DR: DesCartes Builder是一个开源工具，用于系统化地构建基于机器学习的数字孪生管道，通过可视化数据流范式促进ML模型的规范、组合和重用。


<details>
  <summary>Details</summary>
Motivation: 数字孪生需要多个任务和领域相关的机器学习模型，但当前ML在数字孪生工程中的应用仍较为零散，缺乏系统化的设计方法。

Method: 开发了DesCartes Builder工具，采用开放灵活的可视化数据流范式，集成了参数化的核心操作和针对数字孪生设计的ML算法库。

Result: 通过土木工程用例（预测结构塑性应变的实时数字孪生原型）展示了该工具的有效性和可用性。

Conclusion: DesCartes Builder为数字孪生的机器学习管道工程提供了系统化的解决方案，解决了当前ad hoc方法的问题。

Abstract: Digital twins (DTs) are increasingly utilized to monitor, manage, and
optimize complex systems across various domains, including civil engineering. A
core requirement for an effective DT is to act as a fast, accurate, and
maintainable surrogate of its physical counterpart, the physical twin (PT). To
this end, machine learning (ML) is frequently employed to (i) construct
real-time DT prototypes using efficient reduced-order models (ROMs) derived
from high-fidelity simulations of the PT's nominal behavior, and (ii)
specialize these prototypes into DT instances by leveraging historical sensor
data from the target PT. Despite the broad applicability of ML, its use in DT
engineering remains largely ad hoc. Indeed, while conventional ML pipelines
often train a single model for a specific task, DTs typically require multiple,
task- and domain-dependent models. Thus, a more structured approach is required
to design DTs.
  In this paper, we introduce DesCartes Builder, an open-source tool to enable
the systematic engineering of ML-based pipelines for real-time DT prototypes
and DT instances. The tool leverages an open and flexible visual data flow
paradigm to facilitate the specification, composition, and reuse of ML models.
It also integrates a library of parameterizable core operations and ML
algorithms tailored for DT design. We demonstrate the effectiveness and
usability of DesCartes Builder through a civil engineering use case involving
the design of a real-time DT prototype to predict the plastic strain of a
structure.

</details>


### [32] [Previously on... Automating Code Review](https://arxiv.org/abs/2508.18003)
*Robert Heumüller,Frank Ortmeier*

Main category: cs.SE

TL;DR: 对现代代码审查自动化研究的首次全面分析，系统调查了691篇文献，识别出24项相关研究，揭示了该领域在任务定义、数据集和评估方法上的显著差异和标准化需求。


<details>
  <summary>Details</summary>
Motivation: 现代代码审查需要大量时间和资源投入，虽然机器学习方法被广泛探索用于自动化核心审查任务，但存在任务定义、数据集和评估程序的高度不一致性，需要系统性的领域分析来指导未来研究。

Method: 系统性地调查了2015年5月至2024年4月期间的691篇出版物，识别出24项相关研究，从任务、模型、指标、基线、结果、有效性问题和工件可用性等方面进行分析。

Result: 发现了48种任务指标组合，其中22种是其原始论文独有的，数据集重用有限，识别出时间偏差威胁等很少被解决的方法学挑战。

Conclusion: 该研究为领域提供了清晰的概览，支持新研究的框架构建，帮助避免陷阱，并促进评估实践的更大标准化，提出了具体的改进建议。

Abstract: Modern Code Review (MCR) is a standard practice in software engineering, yet
it demands substantial time and resource investments. Recent research has
increasingly explored automating core review tasks using machine learning (ML)
and deep learning (DL). As a result, there is substantial variability in task
definitions, datasets, and evaluation procedures. This study provides the first
comprehensive analysis of MCR automation research, aiming to characterize the
field's evolution, formalize learning tasks, highlight methodological
challenges, and offer actionable recommendations to guide future research.
Focusing on the primary code review tasks, we systematically surveyed 691
publications and identified 24 relevant studies published between May 2015 and
April 2024. Each study was analyzed in terms of tasks, models, metrics,
baselines, results, validity concerns, and artifact availability. In
particular, our analysis reveals significant potential for standardization,
including 48 task metric combinations, 22 of which were unique to their
original paper, and limited dataset reuse. We highlight challenges and derive
concrete recommendations for examples such as the temporal bias threat, which
are rarely addressed so far. Our work contributes to a clearer overview of the
field, supports the framing of new research, helps to avoid pitfalls, and
promotes greater standardization in evaluation practices.

</details>


### [33] [A Large-Scale Study on Developer Engagement and Expertise in Configurable Software System Projects](https://arxiv.org/abs/2508.18070)
*Karolina M. Milano,Wesley K. G. Assunção,Bruno B. P. Cafeo*

Main category: cs.SE

TL;DR: 这篇论文研究了可配置软件系统中变量代码的开发者分布情况，发现59%开发者从未修改变量代码，而17%开发者负责83%的变量代码维护工作。传统专业知识指标在识别变量代码开发者方面效果差。


<details>
  <summary>Details</summary>
Motivation: 可配置软件系统中的变量代码通过预处理器指令实现，使维护变得复杂且错误风险高。但少有研究了解变量代码在开发者中的分布情况，以及传统专业知识指标是否能够准确描述变量代码的专业能力。

Method: 研究挖掘了25个可配置软件系统的代码仓库，分析了9,678名开发者的450,255个提交记录，研究开发者对变量代码和必选代码的参与情况、变量代码工作负荷的集中程度，以及传统专业知识指标的有效性。

Result: 结果显示59%开发者从未修改变量代码，而约17%的开发者负责了83%的变量代码开发和维护工作，表明变量代码专业知识高度集中在少数开发者中。传统专业知识指标表现差强，在识别变量代码开发者方面仅达到约55%的精确度和50%的召回率。

Conclusion: 研究发现可配置软件系统中存在变量代码责任分配不均表现，应重新评估专业知识指标以更好地支持任务分配，从而促进更公平的工作负荷分配。

Abstract: Modern systems operate in multiple contexts making variability a fundamental
aspect of Configurable Software Systems (CSSs). Variability, implemented via
pre-processor directives (e.g., #ifdef blocks) interleaved with other code and
spread across files, complicates maintenance and increases error risk. Despite
its importance, little is known about how variable code is distributed among
developers or whether conventional expertise metrics adequately capture
variable code proficiency. This study investigates developers' engagement with
variable versus mandatory code, the concentration of variable code workload,
and the effectiveness of expertise metrics in CSS projects. We mined
repositories of 25 CSS projects, analyzing 450,255 commits from 9,678
developers. Results show that 59% of developers never modified variable code,
while about 17% were responsible for developing and maintaining 83% of it. This
indicates a high concentration of variable code expertise among a few
developers, suggesting that task assignments should prioritize these
specialists. Moreover, conventional expertise metrics performed
poorly--achieving only around 55% precision and 50% recall in identifying
developers engaged with variable code. Our findings highlight an unbalanced
distribution of variable code responsibilities and underscore the need to
refine expertise metrics to better support task assignments in CSS projects,
thereby promoting a more equitable workload distribution.

</details>


### [34] [Debian in the Research Software Ecosystem: A Bibliometric Analysis](https://arxiv.org/abs/2508.18073)
*Joenio Marques da Costa,Christina von Flach*

Main category: cs.SE

TL;DR: 这是一份关于Debian系统在学术文献中影响力的文献计量分析研究，通过Scopus数据库搜索和分析相关文献，描绘了Debian在学术研究中的地图咈趋势。


<details>
  <summary>Details</summary>
Motivation: 研究Debian系统在学术文献中的影响力，分类文章、映射研究领域、识别趋势咈找发展机会，以了解其在科学研究软件生态中的贡献。

Method: 采用文献计量分析方法，从Scopus数据库中搜索包含"Debian"的标题、摘要或关键词的学术文献，进行共引用、共同作者咈关键词共现分析。

Result: 研究包含了多个知识领域的文章，提供了Debian学术文献的地图，报告了人口统计咈文献计量趋势，包括最高引用文章、活跃国家、研究人员咈热门会议。

Conclusion: 通过文献计量咈人口统计分析，揭示了Debian学术研究的知识结构，为研究人员提供了现有文献趋势的概览，并指出了需要更多关注的研究领域。

Abstract: Context: The Debian system has historically participated in academic works
and scientific projects, with well-known examples including NeuroDebian, Debian
Med, Debsources, Debian Science, and Debian GIS, where the scientific relevance
of Debian and its contribution to the Research Software ecosystem are evident.
  Objective: The objective of this study is to investigate the Debian system
through academic publications, with the aim of classifying articles, mapping
research, identifying trends, and finding opportunities.
  Method: The study is based on a bibliometric analysis starting with an
initial search for the term "Debian" in the titles, abstracts, or keywords of
academic publications, using the Scopus database. This analysis calculates
metrics of co-citation, co-authorship, and word co-occurrence, and is guided by
a set of research questions and criteria for inclusion and exclusion to conduct
the bibliometric analysis.
  Results: The study includes a set of articles published across various fields
of knowledge, providing a map of the academic publication space about Debian.
The study's data will be available in a public repository, reporting
demographic and bibliometric trends, including the most cited articles, active
countries, researchers, and popular conferences.
  Conclusion: Results includes a bibliometric and demographic analysis
identified in publications about Debian, shedding light on the intellectual
structure of academic research. The results of the analyses can help
researchers gain an overview of existing trends in publications about Debian
and identify areas that require more attention from the scientific community.

</details>


### [35] [LLM-Guided Genetic Improvement: Envisioning Semantic Aware Automated Software Evolution](https://arxiv.org/abs/2508.18089)
*Karine Even-Mendoza,Alexander Brownlee,Alina Geiger,Carol Hanna,Justyna Petke,Federica Sarro,Dominik Sobania*

Main category: cs.SE

TL;DR: 本文提出PatchCat方法，将遗传改进(GI)与LLM编辑的自动聚类相结合，实现了语义感知的软件补丁分类，能够识别18种补丁类型并提前检测NoOp编辑，提高GI效率。


<details>
  <summary>Details</summary>
Motivation: 传统基于搜索的GI主要在语法层面操作，缺乏语义感知；而LLM虽然能提供语义感知的编辑，但缺乏目标导向的反馈和控制。需要结合两者的优势。

Method: 提出PatchCat方法，通过自动聚类LLM生成的编辑来增强GI。使用小型本地LLM对软件补丁进行聚类分析。

Result: PatchCat成功识别了18种不同的软件补丁类型，对新建议的补丁分类准确率高，能够提前检测NoOp编辑，节省测试套件执行资源。

Conclusion: 该方法为构建可解释、高效和绿色的GI迈出了有希望的一步，未来需要建立对LLM驱动突变的原理性理解，用语义信号指导GI搜索过程。

Abstract: Genetic Improvement (GI) of software automatically creates alternative
software versions that are improved according to certain properties of
interests (e.g., running-time). Search-based GI excels at navigating large
program spaces, but operates primarily at the syntactic level. In contrast,
Large Language Models (LLMs) offer semantic-aware edits, yet lack goal-directed
feedback and control (which is instead a strength of GI). As such, we propose
the investigation of a new research line on AI-powered GI aimed at
incorporating semantic aware search. We take a first step at it by augmenting
GI with the use of automated clustering of LLM edits. We provide initial
empirical evidence that our proposal, dubbed PatchCat, allows us to
automatically and effectively categorize LLM-suggested patches. PatchCat
identified 18 different types of software patches and categorized newly
suggested patches with high accuracy. It also enabled detecting NoOp edits in
advance and, prospectively, to skip test suite execution to save resources in
many cases. These results, coupled with the fact that PatchCat works with
small, local LLMs, are a promising step toward interpretable, efficient, and
green GI. We outline a rich agenda of future work and call for the community to
join our vision of building a principled understanding of LLM-driven mutations,
guiding the GI search process with semantic signals.

</details>


### [36] [A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Code](https://arxiv.org/abs/2508.18106)
*Keke Lian,Bin Wang,Lei Zhang,Libo Chen,Junjie Wang,Ziming Zhao,Yujiu Yang,Haotong Duan,Haoran Zhao,Shuang Liao,Mingda Guo,Jiazheng Quan,Yilu Zhong,Chenhao He,Zichuan Chen,Jie Wu,Haoling Li,Zhaoxuan Li,Jiongchi Yu,Hui Li,Dong Zhang*

Main category: cs.SE

TL;DR: 提出了A.S.E基准测试，用于评估LLM在仓库级别的安全代码生成能力，基于真实CVE漏洞构建任务，提供可复现的容器化评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在不足：关注孤立代码片段、评估方法不稳定不可复现、未能关联输入上下文质量与输出安全性。

Method: 从有CVE记录的真实仓库构建任务，保留完整仓库上下文（构建系统、跨文件依赖），使用容器化评估框架和专家定义规则进行稳定可审计的安全评估。

Result: 评估发现：Claude-3.7-Sonnet整体表现最佳；开源与专有模型安全差距很小，Qwen3-235B-A22B-Instruct获得最高安全分；简洁的"快速思考"解码策略在安全修补中优于复杂的"慢速思考"推理。

Conclusion: A.S.E基准测试填补了现有评估方法的空白，为LLM代码生成安全性提供了更全面、可复现的评估标准，揭示了模型性能和解码策略的重要发现。

Abstract: The increasing adoption of large language models (LLMs) in software
engineering necessitates rigorous security evaluation of their generated code.
However, existing benchmarks are inadequate, as they focus on isolated code
snippets, employ unstable evaluation methods that lack reproducibility, and
fail to connect the quality of input context with the security of the output.
To address these gaps, we introduce A.S.E (AI Code Generation Security
Evaluation), a benchmark for repository-level secure code generation. A.S.E
constructs tasks from real-world repositories with documented CVEs, preserving
full repository context like build systems and cross-file dependencies. Its
reproducible, containerized evaluation framework uses expert-defined rules to
provide stable, auditable assessments of security, build quality, and
generation stability. Our evaluation of leading LLMs on A.S.E reveals three key
findings: (1) Claude-3.7-Sonnet achieves the best overall performance. (2) The
security gap between proprietary and open-source models is narrow;
Qwen3-235B-A22B-Instruct attains the top security score. (3) Concise,
``fast-thinking'' decoding strategies consistently outperform complex,
``slow-thinking'' reasoning for security patching.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [37] [SafeTree: Expressive Tree Policies for Microservices](https://arxiv.org/abs/2508.16746)
*Karuna Grewal,P. Brighten Godfrey,Justin Hsu*

Main category: cs.PL

TL;DR: 基于可见下推自动机的微服务树策略动态执行机制，通过服务网格实现低延迟监控


<details>
  <summary>Details</summary>
Motivation: 当前微服务部署工具仅支持限制性的单跳策略，忽略了微服务调用的丰富树状结构，导致策略过于允许

Method: 设计表达式性策略语言来指定服务树结构，开发基于VPA的动态执行机制，在Istio服务网格上实现分布式监控器

Result: 监控器能够执行丰富的安全属性，仅添加毫秒级别的延迟负荷

Conclusion: 该方法提供了一种非侵入式的精细化通信控制方案，有效利用服务网格技术实现微服务树策略的动态执行

Abstract: A microservice-based application is composed of multiple self-contained
components called microservices, and controlling inter-service communication is
important for enforcing safety properties. Presently, inter-service
communication is configured using microservice deployment tools. However, such
tools only support a limited class of single-hop policies, which can be overly
permissive because they ignore the rich service tree structure of microservice
calls. Policies that can express the service tree structure can offer
development and security teams more fine-grained control over communication
patterns.
  To this end, we design an expressive policy language to specify service tree
structures, and we develop a visibly pushdown automata-based dynamic
enforcement mechanism to enforce service tree policies. Our technique is
non-invasive: it does not require any changes to service implementations, and
does not require access to microservice code. To realize our method, we build a
runtime monitor on top of a service mesh, an emerging network infrastructure
layer that can control inter-service communication during deployment. In
particular, we employ the programmable network traffic filtering capabilities
of Istio, a popular service mesh implementation, to implement an online and
distributed monitor. Our experiments show that our monitor can enforce rich
safety properties while adding minimal latency overhead on the order of
milliseconds.

</details>


### [38] [Syntactic Completions with Material Obligations](https://arxiv.org/abs/2508.16848)
*David Moon,Andrew Blinn,Thomas J. Porter,Cyrus Omar*

Main category: cs.PL

TL;DR: 本文介绍了tylr，一种解析器和编辑器生成器，通过插入义务（obligations）来补全任意格式错误的代码，解决了现有语法错误恢复技术过于粗糙或导致补全选项爆炸的问题。


<details>
  <summary>Details</summary>
Motivation: 现有代码编辑器的语法错误恢复技术（如panic模式和多重修复选项）存在局限性：要么删除大量代码过于粗糙，要么产生过多的补全可能性。需要一种更好的方法来处理语法错误。

Method: 基于瓦片解析理论，扩展了运算符优先级解析：1）用语法行走替代传统token优先级比较来生成义务；2）基于语法拉链的模制系统通过义务最小化标准来消除歧义。tylr能够插入覆盖缺失操作数、运算符、混合关键字和类型转换的义务。

Result: 开发了tylr系统，不仅作为新颖的错误纠正方法，还实现了可视化显示义务的编辑器，成为文本编辑器和结构编辑器之间的混合体。通过人类受试者研究评估了其可用性和实用性。

Conclusion: tylr提供了一种处理语法错误的新方法，通过可视化义务在编辑器中实现了文本和结构编辑的混合。研究发现既有积极方面，也为未来工作提供了有趣的新方向。

Abstract: Code editors provide essential services that help developers understand,
navigate, and modify programs. However, these services often fail in the
presence of syntax errors. Existing syntax error recovery techniques, like
panic mode and multi-option repairs, are either too coarse, e.g. in deleting
large swathes of code, or lead to a proliferation of possible completions. This
paper introduces $\texttt{tylr}$, a parser and editor generator that completes
arbitrarily malformed code by inserting obligations, which generalize holes to
cover missing operands, operators, mixfix keywords, and sort transitions.
$\texttt{tylr}$ is backed by a novel theory of tile-based parsing, which
extends operator-precedence parsing in two ways. First, traditional token
precedence comparisons are replaced by a notion of grammar walks, which form
the basis for generating obligations. Second, a distinct "molding" system based
on grammar zippers expand grammar expressivity by allowing the system to
disambiguate between possible parses and completions based on an obligation
minimization criterion. In addition to serving as a novel approach to error
correction, $\texttt{tylr}$'s design enables the development of an editor that
visually materializes obligations to the human user, serving as a novel hybrid
between a text editor and a structure editor. We introduce $\texttt{tylr}$ by
example, then formalize its key ideas. Finally, we conduct a human subjects
study to evaluate the extent to which an editor like $\texttt{tylr}$ that
materializes syntactic obligations might be usable and useful, finding both
points of positivity and interesting new avenues for future work.

</details>
