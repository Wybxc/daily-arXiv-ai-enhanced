<div id=toc></div>

# Table of Contents

- [cs.LO](#cs.LO) [Total: 2]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.SE](#cs.SE) [Total: 12]


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [1] [ProofBridge: Auto-Formalization of Natural Language Proofs in Lean via Joint Embeddings](https://arxiv.org/abs/2510.15681)
*Prithwish Jana,Kaan Kale,Ahmet Ege Tanriverdi,Cruise Song,Sriram Vishwanath,Vijay Ganesh*

Main category: cs.LO

TL;DR: ProofBridge是一个统一的框架，用于将自然语言数学定理和证明自动翻译成Lean 4形式化语言，通过联合嵌入模型在共享语义空间中对齐自然语言和形式化语言的定理-证明对，实现跨模态检索引导翻译。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法将定理翻译和证明生成分开处理的问题，这种两阶段方法在AlphaProof的IMO表现中显示出局限性，需要手动翻译问题陈述。

Method: 使用联合嵌入模型在共享语义空间中对齐NL-FL定理证明对，结合检索增强微调和迭代证明修复，利用Lean的类型检查器和语义等价反馈确保语法正确性和语义保真度。

Result: 在miniF2F-Test-PF数据集上显著优于强基线模型，跨模态检索质量提升3.28倍Recall@1，语义正确性提升31.14%，类型正确性提升1.64%。

Conclusion: ProofBridge通过统一的端到端框架显著提升了自动形式化证明的性能，解决了现有两阶段方法的局限性。

Abstract: Translating human-written mathematical theorems and proofs from natural
language (NL) into formal languages (FLs) like Lean 4 has long been a
significant challenge for AI. Most state-of-the-art methods address this
separately, first translating theorems and then generating proofs, creating a
fundamental disconnect vis-a-vis true proof auto-formalization. This two-step
process and its limitations were evident even in AlphaProof's silver-medal
performance at the 2024 IMO, where problem statements needed manual translation
before automated proof synthesis.
  We present ProofBridge, a unified framework for automatically translating
entire NL theorems and proofs into Lean 4. At its core is a joint embedding
model that aligns NL and FL (NL-FL) theorem-proof pairs in a shared semantic
space, enabling cross-modal retrieval of semantically relevant FL examples to
guide translation. Our training ensures that NL-FL theorems (and their proofs)
are mapped close together in this space if and only if the NL-FL pairs are
semantically equivalent. ProofBridge integrates retrieval-augmented fine-tuning
with iterative proof repair, leveraging Lean's type checker and semantic
equivalence feedback to ensure both syntactic correctness and semantic
fidelity. Experiments show substantial improvements in proof auto-formalization
over strong baselines (including GPT-5, Gemini-2.5, Kimina-Prover,
DeepSeek-Prover), with our retrieval-augmented approach yielding significant
gains in semantic correctness (SC, via proving bi-directional equivalence) and
type correctness (TC, via type-checking theorem+proof) across pass@k metrics on
miniF2F-Test-PF, a dataset we curated. In particular, ProofBridge improves
cross-modal retrieval quality by up to 3.28x Recall@1 over all-MiniLM-L6-v2,
and achieves +31.14% SC and +1.64% TC (pass@32) compared to the baseline
Kimina-Prover-RL-1.7B.

</details>


### [2] [Weakening Goals in Logical Specifications](https://arxiv.org/abs/2510.15718)
*Ben M. Andrew*

Main category: cs.LO

TL;DR: 提出了一种反例引导的属性弱化技术，用于处理系统退化或环境变化时传统验证方法失效的情况。


<details>
  <summary>Details</summary>
Motivation: 在复杂机器人系统中，系统退化或环境变化可能导致逻辑属性不再成立，传统验证方法会失败，但较弱的属性版本仍然有用。

Method: 采用反例引导的迭代弱化技术，应用于命题逻辑规范，并计划扩展到基于状态的表示。

Result: 开发了一种能够自动弱化逻辑属性的方法，使系统在不确定条件下仍能保持有用的行为理解。

Conclusion: 该方法有助于在系统变化时理解系统行为，并支持组合验证。

Abstract: Logical specifications are widely used to represent software systems and
their desired properties. Under system degradation or environmental changes,
commonly seen in complex real-world robotic systems, these properties may no
longer hold and so traditional verification methods will simply fail to
construct a proof. However, weaker versions of these properties do still hold
and can be useful for understanding the system's behaviour in uncertain
conditions, as well as aiding compositional verification. We present a
counterexample-guided technique for iteratively weakening properties, apply it
to propositional logic specifications, and discuss planned extensions to
state-based representations.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [3] [Grassroots Logic Programs: A Secure, Multiagent, Concurrent, Logic Programming Language](https://arxiv.org/abs/2510.15747)
*Ehud Shapiro*

Main category: cs.PL

TL;DR: GLP是一种安全的多智能体并发逻辑编程语言，用于实现去中心化的草根平台，通过加密签名消息提供安全通信通道，支持身份和代码完整性验证。


<details>
  <summary>Details</summary>
Motivation: 草根平台面临恶意参与者的挑战，缺乏安全编程支持使得正确参与者无法可靠识别彼此、建立安全通信或验证代码完整性。

Method: 扩展逻辑程序，添加配对单读单写逻辑变量，通过加密签名和认证消息提供安全通信通道，逐步构建并发、多智能体GLP并增强密码学安全性。

Result: 证明了GLP计算是演绎、SRSW保持性、无环性和单调性等安全属性，多智能体GLP具有草根特性，GLP流实现区块链安全属性，并展示了安全的草根社交网络应用。

Conclusion: GLP为草根平台提供了安全可靠的实现基础，能够建立认证的点对点连接并支持安全的草根社交网络应用。

Abstract: Grassroots platforms are distributed applications run by\linebreak
cryptographically-identified people on their networked personal devices, where
multiple disjoint platform instances emerge independently and coalesce when
they interoperate. Their foundation is the grassroots social graph, upon which
grassroots social networks, grassroots cryptocurrencies, and grassroots
democratic federations can be built.
  Grassroots platforms have yet to be implemented, the key challenge being
faulty and malicious participants: without secure programming support, correct
participants cannot reliably identify each other, establish secure
communication, or verify each other's code integrity.
  We present Grassroots Logic Programs (GLP), a secure, multiagent, concurrent,
logic programming language for implementing grassroots platforms. GLP extends
logic programs with paired single-reader/single-writer (SRSW) logic variables,
providing secure communication channels among cryptographically-identified
people through encrypted, signed and attested messages, which enable identity
and code integrity verification. We present GLP progressively: logic programs,
concurrent GLP, multiagent GLP, augmenting it with cryptographic security, and
providing smartphone implementation-ready specifications. We prove safety
properties including that GLP computations are deductions, SRSW preservation,
acyclicity, and monotonicity. We prove multiagent GLP is grassroots and that
GLP streams achieve blockchain security properties. We present a grassroots
social graph protocol establishing authenticated peer-to-peer connections and
demonstrate secure grassroots social networking applications.

</details>


### [4] [Visualizing miniKanren Search with a Fine-Grained Small-Step Semantics](https://arxiv.org/abs/2510.15178)
*Brysen Pfingsten,Jason Hemann*

Main category: cs.PL

TL;DR: 为miniKanren开发了确定性小步操作语义，显式表示执行过程中的搜索树演化，并基于此实现交互式可视化工具


<details>
  <summary>Details</summary>
Motivation: 帮助用户理解miniKanren的公平搜索行为和操作效果，解释令人惊讶的答案顺序

Method: 构建显式表示搜索树演化的操作语义模型，实现交互式可视化器，通过基于属性的测试进行验证

Result: 开发了能够精确可视化每个评估步骤（目标激活、挂起、恢复和成功）的教学工具

Conclusion: 该语义模型和工具为理解miniKanren的搜索行为提供了有效的教学概念机器

Abstract: We present a deterministic small-step operational semantics for miniKanren
that explicitly represents the evolving search tree during execution. This
semantics models interleaving and goal scheduling at fine granularity, allowing
each evaluation step-goal activation, suspension, resumption, and success -- to
be visualized precisely. Building on this model, we implement an interactive
visualizer that renders the search tree as it develops and lets users step
through execution. The tool acts as a pedagogical notional machine for
reasoning about miniKanren's fair search behavior, helping users understand
surprising answer orders and operational effects. Our semantics and tool are
validated through property-based testing and illustrated with several examples.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [5] [Automated Snippet-Alignment Data Augmentation for Code Translation](https://arxiv.org/abs/2510.15004)
*Zhiming Zhang,Qingfu Zhu,Xianzhen Luo,Yixuan Wang,Bohan Li,Wanxiang Che*

Main category: cs.SE

TL;DR: 提出了一种利用大语言模型自动生成代码片段对齐数据的数据增强方法，并结合两阶段训练策略提升代码翻译性能。


<details>
  <summary>Details</summary>
Motivation: 代码翻译在软件开发中有广泛应用，但平行语料库有限。现有的程序对齐数据虽然上下文完整但长度过长，片段对齐数据更精细但数量不足。

Method: 使用LLM自动生成片段对齐数据，并采用两阶段训练策略：先在程序对齐数据上预训练，再在片段对齐数据上微调。

Result: 在TransCoder-test上的实验表明，该方法相比仅在程序对齐数据上微调的基线有显著提升，最大增益达3.78% on pass@k。

Conclusion: 结合程序对齐数据和片段对齐数据的优势，通过数据增强和两阶段训练能有效提升代码翻译模型的性能。

Abstract: Code translation aims to translate the code from its source language to the
target language and is used in various software development scenarios. Recent
developments in Large Language Models (LLMs) have showcased their capabilities
in code translation, and parallel corpora play a crucial role in training
models for code translation. Parallel corpora can be categorized into
program-alignment (PA) and snippet-alignment (SA) data. Although PA data has
complete context and is suitable for semantic alignment learning, it may not
provide adequate fine-grained training signals due to its extended length,
while the brevity of SA data enables more fine-grained alignment learning. Due
to limited parallel corpora, researchers explore several augmentation methods
for code translation. Previous studies mainly focus on augmenting PA data. In
this paper, we propose a data augmentation method that leverages LLMs to
generate SA data automatically. To fully leverage both PA data and SA data, we
explore a simple yet effective two-stage training strategy, which consistently
enhances model performance compared to fine-tuning solely on PA data.
Experiments on TransCoder-test demonstrate that our augmented SA data combined
with the two-stage training approach yields consistent improvements over the
baseline, achieving a maximum gain of 3.78% on pass@k.

</details>


### [6] [Assessing Coherency and Consistency of Code Execution Reasoning by Large Language Models](https://arxiv.org/abs/2510.15079)
*Changshu Liu,Yang Chen,Reyhaneh Jabbarvand*

Main category: cs.SE

TL;DR: CES是一个评估LLM在程序执行模拟和编程任务中推理能力的新任务，通过测量执行模拟的正确性和连贯性来排除推理捷径、幻觉或数据泄露导致的虚假正确预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注变量预测的正确性，但无法区分真正的执行推理与虚假的成功。CES引入连贯性概念来评估LLM是否遵循常识执行逻辑，即使预测值不正确。

Method: 提出CES任务，引入连贯性评估执行模拟是否符合常识逻辑，并设计新指标测量不同测试覆盖度下的推理一致性（强、弱、随机）。评估16个LLM在HumanEval数据集上的表现。

Result: 在HumanEval上，81.42%的执行模拟是连贯的，其中46.92%预测正确，53.08%预测错误。前沿LLM如GPT-4和DeepSeek-R1的推理最不连贯，主要由于自然语言捷径。LLM的推理一致性大多为随机(48.87%)或弱(45.37%)。

Conclusion: LLM在bug相关任务中很少融入执行推理，成功主要依赖模式匹配或自然语言捷径。缺乏推理能力威胁LLM处理未见bug或不同上下文模式的泛化能力。CES可系统性地验证LLM在这些任务中的可疑成功。

Abstract: This paper proposes CES, a task to evaluate the abilities of LLMs in
simulating program execution and using that reasoning in programming tasks.
Besides measuring the correctness of variable predictions during execution
simulation, CES introduces the notion of coherence to determine whether the
simulation complies with commonsense execution logic, even if the predicted
values along the simulations are incorrect. This enables CES to rule out
suspiciously correct output predictions due to reasoning shortcuts,
hallucinations, or potential data leakage. CES also introduces a novel metric
to measure reasoning consistency across tests with the same or different prime
path coverage in a spectrum: strong, weak, and random. Evaluating 16 LLMs
(including three reasoning LLMs) using CES indicates 81.42% coherent execution
simulation on HumanEval, 46.92% and 53.08% of which result in correct and
incorrect output predictions. Frontier LLMs such as GPT-4 and DeepSeek-R1 have
the most incoherent execution reasoning, mostly due to natural language
shortcuts. Despite relatively coherent execution simulation, LLMs' reasoning
performance across different tests is inconsistent, mostly random (48.87%) or
weak (45.37%), potentially explaining their weakness in programming tasks that
require path-sensitive program analysis to succeed. We also compare CES with
bug prediction/localization/repair, which intuitively requires control- and
data-flow awareness. We observe that LLMs barely incorporate execution
reasoning into their analysis for bug-related tasks, and their success is
primarily due to inherent abilities in pattern matching or natural language
shortcuts, if not data leakage. Without reasoning, there is a threat to the
generalizability of LLMs in dealing with unseen bugs or patterns in different
contexts. CES can be used to vet the suspicious success of LLMs in these tasks
systematically.

</details>


### [7] [Leveraging Test Driven Development with Large Language Models for Reliable and Verifiable Spreadsheet Code Generation: A Research Framework](https://arxiv.org/abs/2510.15585)
*Dr Simon Thorne,Dr Advait Sarkar*

Main category: cs.SE

TL;DR: 该论文提出将测试驱动开发(TDD)与大型语言模型(LLM)生成相结合的研究框架，通过"测试优先"方法提高生成代码的正确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: LLM在生成代码时经常出现幻觉、逻辑不一致和语法错误等问题，在金融建模和科学计算等高风险领域尤其危险，需要提高生成结果的准确性和可靠性。

Method: 提出结构化研究框架，将TDD实践与LLM驱动生成相结合，采用"测试优先"方法，为LLM输出提供技术约束和认知支持。

Result: 框架适用于多种编程环境，包括电子表格公式生成、Python脚本和Rust等强类型语言，并包含明确的实验设计、评估指标和TDD提示示例。

Conclusion: 通过强调测试驱动思维，旨在提高计算思维、提示工程技能和用户参与度，最终目标是在教育和专业开发实践中建立负责任和可靠的LLM集成。

Abstract: Large Language Models (LLMs), such as ChatGPT, are increasingly leveraged for
generating both traditional software code and spreadsheet logic. Despite their
impressive generative capabilities, these models frequently exhibit critical
issues such as hallucinations, subtle logical inconsistencies, and syntactic
errors, risks particularly acute in high stakes domains like financial
modelling and scientific computations, where accuracy and reliability are
paramount. This position paper proposes a structured research framework that
integrates the proven software engineering practice of Test-Driven Development
(TDD) with Large Language Model (LLM) driven generation to enhance the
correctness of, reliability of, and user confidence in generated outputs. We
hypothesise that a "test first" methodology provides both technical constraints
and cognitive scaffolding, guiding LLM outputs towards more accurate,
verifiable, and comprehensible solutions. Our framework, applicable across
diverse programming contexts, from spreadsheet formula generation to scripting
languages such as Python and strongly typed languages like Rust, includes an
explicitly outlined experimental design with clearly defined participant
groups, evaluation metrics, and illustrative TDD based prompting examples. By
emphasising test driven thinking, we aim to improve computational thinking,
prompt engineering skills, and user engagement, particularly benefiting
spreadsheet users who often lack formal programming training yet face serious
consequences from logical errors. We invite collaboration to refine and
empirically evaluate this approach, ultimately aiming to establish responsible
and reliable LLM integration in both educational and professional development
practices.

</details>


### [8] [Community Engagement and the Lifespan of Open-Source Software Projects](https://arxiv.org/abs/2510.15408)
*Mohit,Kuljit Kaur Chahal*

Main category: cs.SE

TL;DR: 该研究分析了开源软件项目中社区参与度对项目动态和寿命的影响，通过33,946个GitHub仓库数据发现社区参与度与项目动态显著相关，且对项目寿命有复杂影响模式。


<details>
  <summary>Details</summary>
Motivation: 开源软件项目依赖社区参与度来维持长期发展，但其对项目动态和寿命的可量化影响尚未得到充分探索。

Method: 分析33,946个GitHub仓库，使用经过验证的月度指标（问题、评论、关注者、星标）定义和操作化社区参与度，通过非参数检验和相关分析评估与项目动态和寿命的关系。

Result: 社区参与度指标与项目动态显著相关，在高度参与的项目中相关性更强。对于项目寿命，呈现复杂模式：月度参与率在年轻项目中最高，随项目年龄下降，但部分长寿项目保持异常高活跃度。初始参与爆发对项目建立至关重要，持续高参与度驱动极端长寿。

Conclusion: 社区参与度动态驱动开源软件项目的寿命和开发。研究建立了经过验证的社区参与度指标，并深入揭示了不同社区活动模式如何促进项目长寿。

Abstract: Open-source software (OSS) projects depend on community engagement (CE) for
longevity. However, CE's quantifiable impact on project dynamics and lifespan
is underexplored. Objectives: This study defines CE in OSS, identifies key
metrics, and evaluates their influence on project dynamics (releases, commits,
branches) and lifespan. Methods: We analyzed 33,946 GitHub repositories,
defining and operationalizing CE with validated per-month metrics (issues,
comments, watchers, stargazers). Non-parametric tests and correlations assessed
relationships with project dynamics and lifespan across quartiles. Results: CE
metrics significantly associate with project dynamics, with stronger
correlations in highly engaged projects. For lifespan, a complex pattern
emerged: per-month CE rates are highest in younger projects, declining with
age. Yet, a subset of long-lived projects maintains exceptionally high
activity. Initial CE bursts appear crucial for establishment, while sustained
high engagement drives extreme longevity. Active issue engagement's influence
intensifies with age, but passive attention's declines. Conclusion: CE
dynamically drives OSS project longevity and development. Our findings
establish validated CE metrics and offer deeper insights into how diverse
community activity patterns contribute to project longevity.

</details>


### [9] [Selecting and Combining Large Language Models for Scalable Code Clone Detection](https://arxiv.org/abs/2510.15480)
*Muslim Chochlov,Gul Aftab Ahmed,James Vincent Patten,Yuanhua Han,Guoxian Lu,David Gregg,Jim Buckley*

Main category: cs.SE

TL;DR: 该论文研究了LLM在代码克隆检测中的应用，通过筛选76个LLM模型并评估其在工业数据集上的表现，发现没有单一最佳模型，但CodeT5+110M、CuBERT和SPTCode表现较好。同时探索了LLM集成方法，在大型商业数据集上集成模型精度达到46.91%。


<details>
  <summary>Details</summary>
Motivation: 源代码克隆存在知识产权侵权和安全漏洞风险，但有效的可扩展克隆检测（特别是针对变异克隆）仍然具有挑战性。虽然LLM已被应用于克隆检测，但模型快速涌现带来了最佳模型选择和集成效果的问题。

Method: 1. 识别76个LLM并筛选适合大规模克隆检测的候选模型；2. 在两个公共工业数据集和一个商业大规模数据集上评估候选模型；3. 探索LLM集成方法，包括分数归一化和不同集成策略（最大值、求和、平均）。

Result: 没有发现统一的'最佳LLM'，但CodeT5+110M、CuBERT和SPTCode表现最佳。在商业大规模数据集上，CodeT5+110M达到39.71%精度，是之前CodeBERT的两倍。集成方法在大型数据集上表现更好，最佳集成模型精度达到46.91%。分析表明较小的嵌入尺寸、较小的分词器词汇量和定制数据集具有优势。

Conclusion: LLM在代码克隆检测中表现出色，但没有单一最佳模型。集成方法（特别是最大值或求和策略）能显著提高检测精度，尤其在大型数据集上效果更明显。较小的模型配置和定制训练数据有助于提升性能。

Abstract: Source code clones pose risks ranging from intellectual property violations
to unintended vulnerabilities. Effective and efficient scalable clone
detection, especially for diverged clones, remains challenging. Large language
models (LLMs) have recently been applied to clone detection tasks. However, the
rapid emergence of LLMs raises questions about optimal model selection and
potential LLM-ensemble efficacy.
  This paper addresses the first question by identifying 76 LLMs and filtering
them down to suitable candidates for large-scale clone detection. The
candidates were evaluated on two public industrial datasets, BigCloneBench, and
a commercial large-scale dataset. No uniformly 'best-LLM' emerged, though
CodeT5+110M, CuBERT and SPTCode were top-performers. Analysis of LLM-candidates
suggested that smaller embedding sizes, smaller tokenizer vocabularies and
tailored datasets are advantageous. On commercial large-scale dataset a
top-performing CodeT5+110M achieved 39.71\% precision: twice the precision of
previously used CodeBERT.
  To address the second question, this paper explores ensembling of the
selected LLMs: effort-effective approach to improving effectiveness. Results
suggest the importance of score normalization and favoring ensembling methods
like maximum or sum over averaging. Also, findings indicate that ensembling
approach can be statistically significant and effective on larger datasets: the
best-performing ensemble achieved even higher precision of 46.91\% over
individual LLM on the commercial large-scale code.

</details>


### [10] [An Experimental Study of Real-Life LLM-Proposed Performance Improvements](https://arxiv.org/abs/2510.15494)
*Lirong Yi,Gregory Gay,Philipp Leitner*

Main category: cs.SE

TL;DR: LLM能生成代码，但在生成高性能代码方面表现有限。在65个真实Java任务中，LLM生成的代码大多能提升性能，但仍显著落后于人工优化方案。


<details>
  <summary>Details</summary>
Motivation: 研究LLM是否能生成高性能代码，而不仅仅是功能正确的代码。

Method: 从开源Java程序中选取65个开发者实现显著加速的任务，使用两种领先LLM和四种提示变体自动生成补丁，并与基准和人工方案进行严格基准测试。

Result: LLM生成的代码在大多数情况下确实提升了性能，但人工开发者提出的补丁在统计上显著优于LLM修复方案。约三分之二的LLM解决方案与开发者优化思路语义相同或相似，其余三分之一提出更原创的想法但很少带来实质性性能提升。

Conclusion: LLM在生成高性能代码方面仍有局限，虽然能产生性能改进，但往往无法找到真正最优的解决方案。

Abstract: Large Language Models (LLMs) can generate code, but can they generate fast
code? In this paper, we study this question using a dataset of 65 real-world
tasks mined from open-source Java programs. We specifically select tasks where
developers achieved significant speedups, and employ an automated pipeline to
generate patches for these issues using two leading LLMs under four prompt
variations. By rigorously benchmarking the results against the baseline and
human-authored solutions, we demonstrate that LLM-generated code indeed
improves performance over the baseline in most cases. However, patches proposed
by human developers outperform LLM fixes by a statistically significant margin,
indicating that LLMs often fall short of finding truly optimal solutions. We
further find that LLM solutions are semantically identical or similar to the
developer optimization idea in approximately two-thirds of cases, whereas they
propose a more original idea in the remaining one-third. However, these
original ideas only occasionally yield substantial performance gains.

</details>


### [11] [Enhancing Code Review through Fuzzing and Likely Invariants](https://arxiv.org/abs/2510.15512)
*Wachiraphan Charoenwet,Patanamon Thongtanunam,Van-Thuan Pham,Christoph Treude*

Main category: cs.SE

TL;DR: FuzzSight是一个利用模糊测试和不变式分析来增强代码审查的框架，通过检测程序行为变化来识别潜在缺陷。


<details>
  <summary>Details</summary>
Motivation: 传统代码审查主要依赖静态检查，难以发现程序动态行为问题。模糊测试能生成多样化输入但产生的丰富数据难以被审查者利用。

Method: 将程序行为表示为可能不变式，通过非崩溃模糊测试输入捕获运行时行为变化，突出显示跨版本的行为差异。

Result: 在评估中，FuzzSight标记了75%的回归缺陷和高达80%的漏洞，检测率比SAST高10倍且误报更少。

Conclusion: FuzzSight展示了将模糊测试和不变式分析用于早期代码审查的潜力，连接了静态检查与动态行为洞察。

Abstract: Many software projects employ manual code review to gatekeep defects and
vulnerabilities in the code before integration. However, reviewers often work
under time pressure and rely primarily on static inspection, leaving the
dynamic aspects of the program unexplored. Dynamic analyses could reveal such
behaviors, but they are rarely integrated into reviews. Among them, fuzzing is
typically applied later to uncover crashing bugs. Yet its ability to exercise
code with diverse inputs makes it promising for exposing non-crashing, but
unexpected, behaviors earlier. Still, without suitable mechanisms to analyze
program behaviors, the rich data produced during fuzzing remains inaccessible
to reviewers, limiting its practical value in this context.
  We hypothesize that unexpected variations in program behaviors could signify
potential bugs. The impact of code changes can be automatically captured at
runtime. Representing program behavior as likely invariants, dynamic properties
consistently observed at specific program points, can provide practical signals
of behavioral changes. Such signals offer a way to distinguish between intended
changes and unexpected behavioral shifts from code changes.
  We present FuzzSight, a framework that leverages likely invariants from
non-crashing fuzzing inputs to highlight behavioral differences across program
versions. By surfacing such differences, it provides insights into which code
blocks may need closer attention. In our evaluation, FuzzSight flagged 75% of
regression bugs and up to 80% of vulnerabilities uncovered by 24-hour fuzzing.
It also outperformed SAST in identifying buggy code blocks, achieving ten times
higher detection rates with fewer false alarms. In summary, FuzzSight
demonstrates the potential and value of leveraging fuzzing and invariant
analysis for early-stage code review, bridging static inspection with dynamic
behavioral insights.

</details>


### [12] [Colepp: uma ferramenta multiplataforma para coleta de dados de dispositivos vestiveis](https://arxiv.org/abs/2510.15565)
*Vinicius Moraes de Jesus,Andre Georghton Cardoso Pacheco*

Main category: cs.SE

TL;DR: Colepp是一个开源跨平台工具，用于从多个可穿戴设备收集和同步心率和运动数据，通过智能手机作为中心枢纽，生成适合人类活动识别和心率估计的定制化数据集。


<details>
  <summary>Details</summary>
Motivation: 可穿戴设备广泛使用但面临高质量公共数据集有限和数据收集条件控制不足的挑战，阻碍了稳健算法的发展。

Method: 开发了Colepp系统，集成智能手机作为中心枢纽，接收来自Polar H10胸带和Wear OS智能手表的数据，使用自定义同步协议和用户友好界面导出同步的CSV格式数据集。

Result: 通过实际用例展示了该工具能够产生一致且同步的信号，有效支持人类活动识别和心率估计等应用。

Conclusion: Colepp提供了一个实用的解决方案，能够促进可穿戴设备数据收集和算法开发，特别是在真实世界场景中生成高质量同步数据集。

Abstract: The widespread adoption of wearable devices such as smartwatches and fitness
trackers has fueled the demand for reliable physiological and movement data
collection tools. However, challenges such as limited access to large,
high-quality public datasets and a lack of control over data collection
conditions hinder the development of robust algorithms. This work presents
Colepp, an open-source, cross-platform tool designed to collect and synchronize
data from multiple wearable devices, including heart rate (via ECG and PPG) and
motion signals (accelerometer and gyroscope). The system integrates a
smartphone as a central hub, receiving data from a Polar H10 chest strap and a
Wear OS smartwatch, and exporting synchronized datasets in CSV format. Through
a custom synchronization protocol and user-friendly interface, Colepp
facilitates the generation of customizable, real-world datasets suitable for
applications such as human activity recognition and heart rate estimation. A
use case shows the effectiveness of the tool in producing consistent and
synchronized signals.

</details>


### [13] [Interact and React: Exploring Gender Patterns in Development and the Impact on Innovation and Robustness of a User Interface Tool](https://arxiv.org/abs/2510.15642)
*Sian Brooke*

Main category: cs.SE

TL;DR: 该研究分析了React项目中性别多样性对软件开发的影响，发现女性在功能增强和依赖管理方面贡献显著，性别排斥对软件质量有害。


<details>
  <summary>Details</summary>
Motivation: 开源软件设计中，女性参与常被简单提及，但很少关注性别多样性如何从根本上改变开发模式。本研究旨在理解性别包容对软件开发模式的潜在影响。

Method: 研究调查了React这个广泛使用的JavaScript库，分析了11年间性别差异在稳健性和创新指标上的表现，以及主要版本发布前的贡献模式变化。

Result: 结果显示女性在功能增强和依赖管理方面贡献显著更多，排除女性对软件质量有害。

Conclusion: 通过探索性别如何影响React开发中的创新和稳健性，该研究为增加性别多样性如何带来更包容、创新和稳健的软件提供了关键见解。

Abstract: In open-source software design, the inclusion of women is often highlighted
simply to remind programmers that women exist. Yet, little attention is given
to how greater gender diversity, specifically women's participation, could
fundamentally alter development patterns. To understand the potential impact of
gender inclusion, this study investigates React, a widely used JavaScript
library for building user interfaces with an active contributor community. I
examine gender differences in metrics of robustness and innovation, as well as
shifts in contribution patterns leading up to major version releases over 11
years of the React project. My results show that the exclusion of women is
detrimental to software as women contribute significantly more to feature
enhancement and dependency management. By exploring how gender influences
innovation and robustness in the development of React, the study offers
critical insights into how increasing gender diversity could lead to more
inclusive, innovative, and robust software.

</details>


### [14] [MirrorFuzz: Leveraging LLM and Shared Bugs for Deep Learning Framework APIs Fuzzing](https://arxiv.org/abs/2510.15690)
*Shiwen Ou,Yuwei Li,Lu Yu,Chengkun Wei,Tingke Wen,Qiangpu Chen,Yu Chen,Haizhi Tang,Zulie Pan*

Main category: cs.SE

TL;DR: MirrorFuzz是一个自动化API模糊测试解决方案，用于发现深度学习框架中的共享bug。它通过收集历史bug数据、匹配相似API以及使用LLM合成测试代码，在四个流行DL框架中发现了315个bug，其中262个是新发现的。


<details>
  <summary>Details</summary>
Motivation: 深度学习框架中的bug会级联影响上层应用，但现有研究对跨框架API模式及其潜在风险探索有限。许多DL框架暴露相似的API，使它们容易受到共享bug的影响。

Method: MirrorFuzz采用三阶段方法：1)收集每个API的历史bug数据识别潜在buggy API；2)在框架内和跨框架匹配相似API；3)使用LLM基于相似API的历史bug数据合成测试代码来触发类似bug。

Result: 在TensorFlow、PyTorch、OneFlow和Jittor四个框架上的评估显示，MirrorFuzz相比最先进方法在TensorFlow和PyTorch上分别提高了39.92%和98.20%的代码覆盖率。发现了315个bug，其中262个是新发现的，80个已修复，52个获得CNVD ID。

Conclusion: MirrorFuzz通过利用跨框架API相似性和历史bug数据，有效发现了深度学习框架中的共享bug，证明了其在提高代码覆盖率和bug发现能力方面的有效性。

Abstract: Deep learning (DL) frameworks serve as the backbone for a wide range of
artificial intelligence applications. However, bugs within DL frameworks can
cascade into critical issues in higher-level applications, jeopardizing
reliability and security. While numerous techniques have been proposed to
detect bugs in DL frameworks, research exploring common API patterns across
frameworks and the potential risks they entail remains limited. Notably, many
DL frameworks expose similar APIs with overlapping input parameters and
functionalities, rendering them vulnerable to shared bugs, where a flaw in one
API may extend to analogous APIs in other frameworks. To address this
challenge, we propose MirrorFuzz, an automated API fuzzing solution to discover
shared bugs in DL frameworks. MirrorFuzz operates in three stages: First,
MirrorFuzz collects historical bug data for each API within a DL framework to
identify potentially buggy APIs. Second, it matches each buggy API in a
specific framework with similar APIs within and across other DL frameworks.
Third, it employs large language models (LLMs) to synthesize code for the API
under test, leveraging the historical bug data of similar APIs to trigger
analogous bugs across APIs. We implement MirrorFuzz and evaluate it on four
popular DL frameworks (TensorFlow, PyTorch, OneFlow, and Jittor). Extensive
evaluation demonstrates that MirrorFuzz improves code coverage by 39.92\% and
98.20\% compared to state-of-the-art methods on TensorFlow and PyTorch,
respectively. Moreover, MirrorFuzz discovers 315 bugs, 262 of which are newly
found, and 80 bugs are fixed, with 52 of these bugs assigned CNVD IDs.

</details>


### [15] [EASELAN: An Open-Source Framework for Multimodal Biosignal Annotation and Data Management](https://arxiv.org/abs/2510.15767)
*Rathi Adarshi Rammohan,Moritz Meier,Dennis Küster,Tanja Schultz*

Main category: cs.SE

TL;DR: EASELAN是一个基于ELAN的多模态生物信号标注框架，通过集成GitHub版本控制和简化工作流程，支持复杂生物信号数据集的标注处理。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习和自适应认知系统的发展，对大规模多模态标注数据的需求日益增长，特别是融合多种生物信号的复杂数据集需要更高效的标注工具。

Method: 基于ELAN工具构建，添加了新的组件来支持标注流程的各个阶段：从准备标注文件到设置额外通道、集成GitHub版本控制以及简化后处理。

Result: 成功应用于DFG资助的EASE项目中的人类日常活动（餐桌布置）高维生物信号收集计划，并发布了完全标注的Table Setting Database。

Conclusion: EASELAN提供了无缝的工作流程，能够有效整合生物信号并促进丰富标注，为生物信号收集、标注和处理研究提供了实用工具。

Abstract: Recent advancements in machine learning and adaptive cognitive systems are
driving a growing demand for large and richly annotated multimodal data. A
prominent example of this trend are fusion models, which increasingly
incorporate multiple biosignals in addition to traditional audiovisual
channels. This paper introduces the EASELAN annotation framework to improve
annotation workflows designed to address the resulting rising complexity of
multimodal and biosignals datasets. It builds on the robust ELAN tool by adding
new components tailored to support all stages of the annotation pipeline: From
streamlining the preparation of annotation files to setting up additional
channels, integrated version control with GitHub, and simplified
post-processing. EASELAN delivers a seamless workflow designed to integrate
biosignals and facilitate rich annotations to be readily exported for further
analyses and machine learning-supported model training. The EASELAN framework
is successfully applied to a high-dimensional biosignals collection initiative
on human everyday activities (here, table setting) for cognitive robots within
the DFG-funded Collaborative Research Center 1320 Everyday Activity Science and
Engineering (EASE). In this paper we discuss the opportunities, limitations,
and lessons learned when using EASELAN for this initiative. To foster research
on biosignal collection, annotation, and processing, the code of EASELAN is
publicly available(https://github.com/cognitive-systems-lab/easelan), along
with the EASELAN-supported fully annotated Table Setting Database.

</details>


### [16] [Towards Supporting Open Source Library Maintainers with Community-Based Analytics](https://arxiv.org/abs/2510.15794)
*Rachna Raj,Diego Elias Costa*

Main category: cs.SE

TL;DR: 该论文提出使用社区分析来了解开源库在其依赖生态系统中的使用情况，发现平均只有16%的API方法被实际使用，且仅74%的被使用API方法在库测试套件中得到覆盖。


<details>
  <summary>Details</summary>
Motivation: 开源软件维护者缺乏对其库在实际项目中如何使用的持续反馈，这些见解可以帮助维护者改进测试策略、理解变更影响并更有效地指导库的演进。

Method: 对10个流行Java库及其各自50个依赖项目进行实证研究，分析API使用情况和测试覆盖情况，并提出了两个评估测试套件的指标。

Result: 研究发现库开发者提供了广泛的API方法，但平均只有16%被生态系统实际使用，且仅74%的被使用API方法在测试套件中得到部分或完全覆盖。

Conclusion: 社区分析可以为开源库维护者提供有价值的见解，帮助其基于实际使用情况做出更好的维护决策，如优化测试策略和指导库的演进。

Abstract: Open-source software (OSS) is a pillar of modern software development. Its
success depends on the dedication of maintainers who work constantly to keep
their libraries stable, adapt to changing needs, and support a growing
community. Yet, they receive little to no continuous feedback on how the
projects that rely on their libraries actually use their APIs. We believe that
gaining these insights can help maintainers make better decisions, such as
refining testing strategies, understanding the impact of changes, and guiding
the evolution of their libraries more effectively. We propose the use of
community-based analytics to analyze how an OSS library is used across its
dependent ecosystem. We conduct an empirical study of 10 popular Java libraries
and each with their respective dependent ecosystem of 50 projects. Our results
reveal that while library developers offer a wide range of API methods, only
16% on average are actively used by their dependent ecosystem. Moreover, only
74% of the used API methods are partially or fully covered by their library
test suite. We propose two metrics to help developers evaluate their test suite
according to the APIs used by their community, and we conduct a survey on
open-source practitioners to assess the practical value of these insights in
guiding maintenance decisions.

</details>
