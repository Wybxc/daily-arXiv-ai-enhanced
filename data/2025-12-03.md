<div id=toc></div>

# Table of Contents

- [cs.FL](#cs.FL) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.SE](#cs.SE) [Total: 10]


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [1] [Symbolic ω-automata with obligations](https://arxiv.org/abs/2512.02873)
*Luca Di Stefano*

Main category: cs.FL

TL;DR: 提出基于义务（obligations）的ω-自动机扩展，通过附加在转移上的赋值式构造处理无限字母表，比寄存器自动机更简单实用


<details>
  <summary>Details</summary>
Motivation: 现有处理无限字母表的ω-自动机扩展方法（如符号守卫和寄存器）存在缺陷：符号转移难以处理历史信息，寄存器自动机语义复杂且可处理性差

Method: 提出义务自动机，在转移上附加义务构造（类似赋值），义务对当前符号求值后产生对下一个符号的约束；支持存在和全称分支以及Emerson-Lei接受条件

Result: 义务自动机识别严格超集于ω-正则语言的语言类；开发了机器可读格式表达义务自动机，并实现了包含自动机乘积和空性检查等操作的工具

Conclusion: 义务自动机为处理无限字母表提供了一种比寄存器自动机更简单实用的替代方案，具有更强的表达能力和更好的可操作性

Abstract: Extensions of ω-automata to infinite alphabets typically rely on symbolic guards to keep the transition relation finite, and on registers or memory cells to preserve information from past symbols. Symbolic transitions alone are ill-suited to act on this information, and register automata have intricate formal semantics and issues with tractability. We propose a slightly different approach based on obligations, i.e., assignment-like constructs attached to transitions. Whenever a transition with an obligation is taken, the obligation is evaluated against the current symbol and yields a constraint on the next symbol that the automaton will read. We formalize obligation automata with existential and universal branching and Emerson-Lei acceptance conditions, which subsume classic families such as Büchi, Rabin, Strett, and parity automata. We show that these automata recognise a strict superset of ω-regular languages. To illustrate the practicality of our proposal, we also introduce a machine-readable format to express obligation automata and describe a tool implementing several operations over them, including automata product and emptiness checking.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [2] [The role of counting quantifiers in laminar set systems](https://arxiv.org/abs/2512.02617)
*Rutger Campbell,Noleen Köhler*

Main category: cs.LO

TL;DR: 该论文证明了层状集合系统可以通过MSO转换获得对应的层状树，解决了Courcelle提出的开放问题，并展示了如何用MSO而非CMSO获得多种图分解结构。


<details>
  <summary>Details</summary>
Motivation: 层状集合系统是许多图分解（如模块分解、分裂分解、双连接分解）的核心。Courcelle提出了一个开放问题：是否可以通过MSO转换从层状集合系统获得对应的层状树。由于MSO是集合系统的自然逻辑且足以定义"层状"属性，因此这是一个令人满意的解决方案。

Method: 使用MSO（一阶二阶逻辑）转换从层状集合系统获得对应的层状树。利用Campbell等人的研究成果，将原本需要CMSO（计数MSO）的图分解转换为仅使用MSO即可实现。

Result: 成功证明了层状集合系统可以通过MSO转换获得对应的层状树，解决了Courcelle的开放问题。现在可以使用MSO而非CMSO获得模块分解、共树、分裂分解和双连接分解的转换。进一步获得了关于计数量词表达能力的见解。

Conclusion: 该研究解决了层状集合系统与MSO转换之间的基本关系问题，为图分解理论提供了更简洁的逻辑表达方式，并深入探讨了计数量词在MSO中的可模拟性问题。

Abstract: Laminar set systems consist of non-crossing subsets of a universe with set inclusion essentially corresponding to the descendant relationship of a tree, the so-called laminar tree. Laminar set systems lie at the core of many graph decompositions such as modular decompositions, split decompositions, and bi-join decompositions. We show that from a laminar set system we can obtain the corresponding laminar tree by means of a monadic second order logic (MSO) transduction. This resolves an open question originally asked by Courcelle and is a satisfying resolution as MSO is the natural logic for set systems and is sufficient to define the property ``laminar''. Using results from Campbell et al. [STACS 2025], we can now obtain transductions for obtaining modular decompositions, co-trees, split decompositions and bi-join decompositions using MSO instead of CMSO. We further gain some insight into the expressive power of counting quantifiers and provide some results towards determining when counting quantifiers can be simulated in MSO in laminar set systems and when they cannot.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [3] [Pushing Tensor Accelerators Beyond MatMul in a User-Schedulable Language](https://arxiv.org/abs/2512.02371)
*Yihong Zhang,Derek Gerstmann,Andrew Adams,Maaz Bin Safeer Ahmad*

Main category: cs.PL

TL;DR: 本文提出一种编译器技术，利用Halide语言和基于等式饱和的指令选择器，使张量加速器能应用于图像处理等传统ML之外的领域，实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 张量加速器在现代CPU/GPU中占比增加，但编程困难，开发者只能使用厂商提供的核库，限制了其在传统ML和科学计算之外的应用。本文旨在展示张量加速器能提升图像处理等更广泛应用的性能。

Method: 使用Halide用户可调度语言简洁表达算法，实现基于等式饱和的灵活张量指令选择器，支持CPU和GPU附带的张量加速器，并与现有调度操作（如生产者-消费者融合）协同工作。

Result: 系统成功将张量加速器应用于图像处理管道（滤波、重采样、去噪等），相比非加速器基线实现显著加速。例如，在Nvidia RTX 4070 GPU上，下采样例程通过Tensor Cores实现6.1倍加速。

Conclusion: 通过编译器技术，张量加速器可以超越传统ML领域，应用于更广泛的线性变换应用，如图像处理，开发者只需几十行代码即可编写利用加速器的多样化应用。

Abstract: Tensor accelerators now represent a growing share of compute resources in modern CPUs and GPUs. However, they are hard to program, leading developers to use vendor-provided kernel libraries that support tensor accelerators. As a result, the usage of tensor accelerators is limited to the provided interface, mainly designed for traditional ML and scientific computing workloads.
  In this paper, we show that tensor accelerators can improve the performance of applications beyond simple variants of MatMul. For example, many image processing pipelines are linear transformations over matrices in disguise and can therefore utilize such specialized hardware. This is nonetheless hindered by the difficulties in programming tensor accelerators. We tackle this problem with compiler-based techniques. We use the Halide user-schedulable language and express operations as Halide algorithms succinctly. To this end, we implement a flexible tensor instruction selector based on equality saturation. The tensor instruction selector supports both CPU- and GPU-attached tensor accelerators and works with existing scheduling operations (e.g., producer-consumer fusion). Together, this enables developers to write diverse accelerator-leveraging applications in a few dozen lines.
  Using our system, we demonstrate the potential of tensor accelerators beyond their traditional domains. We implement several image processing pipelines (e.g., filtering, resampling, and denoising) in our system and evaluate them against non-accelerator-leveraging baselines. We show that these pipelines can achieve significant speedups. For example, a downsampling routine is sped up by $6.1\times$ by utilizing Tensor Cores on an Nvidia RTX 4070 GPU.

</details>


### [4] [Probabilistic energy profiler for statically typed JVM-based programming languages](https://arxiv.org/abs/2512.02738)
*Joel Nyholm,Wojciech Mostowski,Christoph Reichenbach*

Main category: cs.PL

TL;DR: 提出基于贝叶斯统计的字节码模式能耗预测方法，用于静态类型JVM语言（Java/Scala），考虑数据大小、类型、操作和设备四个因素，相比传统CPU点估计方法更全面。


<details>
  <summary>Details</summary>
Motivation: 现有能耗分析方法主要关注CPU能耗的点估计，忽略了其他硬件影响，且无法进行统计推理和可解释性分析。需要更精细的源代码级别能耗预测方法。

Method: 测量字节码模式的能耗，使用贝叶斯统计构建统计模型，考虑数据大小、数据类型、操作和设备四个因素，预测能耗分布而非点估计。

Result: 所有四个因素都对能耗有显著影响，同型号设备能耗存在差异，操作和数据类型导致能耗差异。模型对未见程序的能耗预测与实际能耗高度一致。

Conclusion: 提出了一种构建能耗模型的方法论，可用于未来验证工具等应用的能耗估计，为静态类型JVM语言提供了更全面的能耗分析框架。

Abstract: Energy consumption is a growing concern in several fields, from mobile devices to large data centers. Developers need detailed data on the energy consumption of their software to mitigate consumption issues. Previous approaches have a broader focus, such as on specific functions or programs, rather than source code statements. They primarily focus on estimating the CPU's energy consumption using point estimates, thereby disregarding other hardware effects and limiting their use for statistical reasoning and explainability. We developed a novel methodology to address the limitations of measuring only the CPU's consumption and using point estimates, focusing on predicting the energy usage of statically typed JVM-based programming languages, such as Java and Scala. We measure the energy consumption of Bytecode patterns, the translation from the programming language's source code statement to their Java Bytecode representation. With the energy measurements, we construct a statistical model using Bayesian statistics, which allows us to predict the energy consumption through statistical distributions and analyze individual factors. The model includes three factors we obtain statically from the code: data size, data type, operation, and one factor about the hardware platform the code executes on: device. To validate our methodology, we implemented it for Java and evaluated its energy predictions on unseen programs. We observe that all four factors are influential, notably that two devices of the same model may differ in energy consumption and that the operations and data types cause consumption differences. The experiments also show that the energy prediction of programs closely follows the program's real energy consumption, validating our approach. Our work presents a methodology for constructing an energy model that future work, such as verification tools, can use for their energy estimates.

</details>


### [5] [Lumos: Let there be Language Model System Certification](https://arxiv.org/abs/2512.02966)
*Isha Chaudhary,Vedaant Jain,Avaljot Singh,Kavya Sachdeva,Sayan Ranu,Gagandeep Singh*

Main category: cs.PL

TL;DR: Lumos是一个用于规范和形式化认证语言模型系统行为的概率编程框架，通过图结构处理提示分布，支持复杂关系和时间规范，并首次为自动驾驶场景中的视觉语言模型开发了安全规范。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型系统在关键应用中的部署增加，需要系统化的方法来规范和认证其行为。现有方法缺乏统一的框架来指定复杂的行为规范，特别是在安全关键场景中，如自动驾驶。

Method: Lumos是一个基于图的概率编程领域特定语言，提供混合（操作和指称）语义。它通过图结构表示提示分布，从采样子图生成随机提示，并与统计认证器集成以支持任意提示分布的认证。

Result: Lumos能够编码现有的语言模型规范，并开发了首个自动驾驶场景中视觉语言模型的安全规范。实验显示，最先进的VLM Qwen-VL在雨天右转场景中至少90%的概率产生错误和不安全的响应，揭示了重大安全风险。

Conclusion: Lumos是首个系统化、可扩展的基于语言的框架，用于规范和认证语言模型系统行为，为更广泛采用语言模型认证铺平了道路。其模块化结构便于规范修改，使认证能够跟上快速演变的威胁环境。

Abstract: We introduce the first principled framework, Lumos, for specifying and formally certifying Language Model System (LMS) behaviors. Lumos is an imperative probabilistic programming DSL over graphs, with constructs to generate independent and identically distributed prompts for LMS. It offers a structured view of prompt distributions via graphs, forming random prompts from sampled subgraphs. Lumos supports certifying LMS for arbitrary prompt distributions via integration with statistical certifiers. We provide hybrid (operational and denotational) semantics for Lumos, providing a rigorous way to interpret the specifications. Using only a small set of composable constructs, Lumos can encode existing LMS specifications, including complex relational and temporal specifications. It also facilitates specifying new properties - we present the first safety specifications for vision-language models (VLMs) in autonomous driving scenarios developed with Lumos. Using these, we show that the state-of-the-art VLM Qwen-VL exhibits critical safety failures, producing incorrect and unsafe responses with at least 90% probability in right-turn scenarios under rainy driving conditions, revealing substantial safety risks. Lumos's modular structure allows easy modification of the specifications, enabling LMS certification to stay abreast with the rapidly evolving threat landscape. We further demonstrate that specification programs written in Lumos enable finding specific failure cases exhibited by state-of-the-art LMS. Lumos is the first systematic and extensible language-based framework for specifying and certifying LMS behaviors, paving the way for a wider adoption of LMS certification.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [6] [Bin2Vec: Interpretable and Auditable Multi-View Binary Analysis for Code Plagiarism Detection](https://arxiv.org/abs/2512.02197)
*Moussa Moussaoui,Tarik Houichime,Abdelalim Sadiq*

Main category: cs.SE

TL;DR: Bin2Vec是一个结合程序静态特征和动态行为的可解释软件相似性分析框架，通过多视图表示和可视化图表提供可靠且人类可理解的相似性判断。


<details>
  <summary>Details</summary>
Motivation: 现有软件相似性分析方法通常只关注单一类型信息（如静态特征或动态行为），缺乏全面性和可解释性，需要一种能结合多源信息并提供人类可理解解释的框架。

Method: Bin2Vec框架结合程序的静态特征（内置函数、导入导出）和动态行为（指令执行、内存使用），将不同信息表示为可单独检查的视图，通过可视化图表展示，最终整合为总体相似性分数，作为二进制表示和机器学习技术之间的桥梁。

Result: 在PuTTY和7-Zip多个版本上的测试表明，该方法能计算最优且可视化友好的软件表示，PuTTY版本显示更复杂的行为和内存活动，而7-Zip版本更关注性能相关模式，验证了方法的有效性。

Conclusion: Bin2Vec提供可靠且人类可解释的软件相似性决策，其模块化和可扩展性使其适用于审计、软件来源验证、网络安全和逆向工程中的大规模程序筛查等任务。

Abstract: We introduce Bin2Vec, a new framework that helps compare software programs in a clear and explainable way. Instead of focusing only on one type of information, Bin2Vec combines what a program looks like (its built-in functions, imports, and exports) with how it behaves when it runs (its instructions and memory usage). This gives a more complete picture when deciding whether two programs are similar or not. Bin2Vec represents these different types of information as views that can be inspected separately using easy-to-read charts, and then brings them together into an overall similarity score. Bin2Vec acts as a bridge between binary representations and machine learning techniques by generating feature representations that can be efficiently processed by machine-learning models. We tested Bin2Vec on multiple versions of two well-known Windows programs, PuTTY and 7-Zip. The primary results strongly confirmed that our method compute an optimal and visualization-friendly representation of the analyzed software. For example, PuTTY versions showed more complex behavior and memory activity, while 7-Zip versions focused more on performance-related patterns. Overall, Bin2Vec provides decisions that are both reliable and explainable to humans. Because it is modular and easy to extend, it can be applied to tasks like auditing, verifying software origins, or quickly screening large numbers of programs in cybersecurity and reverse-engineering work.

</details>


### [7] [Towards autonomous normative multi-agent systems for Human-AI software engineering teams](https://arxiv.org/abs/2512.02329)
*Hoa Khanh Dam,Geeta Mahala,Rashina Hoda,Xi Zheng,Cristina Conati*

Main category: cs.SE

TL;DR: 论文提出由AI驱动的完全自主软件工程代理新范式，利用大语言模型实现类人推理，通过规范协调人机协作，建立可扩展、透明、可信的未来软件工程框架。


<details>
  <summary>Details</summary>
Motivation: 当前软件工程过程在速度、可靠性和适应性方面存在局限，需要变革性范式将AI作为软件开发核心活动的主要驱动力，实现超越现有流程的软件开发。

Method: 引入基于大语言模型的新型软件工程代理，配备信念、欲望、意图和记忆以实现类人推理；采用道义模态（承诺、义务、禁止、许可）作为规范来协调代理间协作并确保合规性。

Result: 建立了一个可扩展、透明、可信赖的框架，使AI代理能够与人类和其他代理协作，在设计、实现、测试和部署软件系统方面达到远超当前流程的速度、可靠性和适应性。

Conclusion: AI驱动的自主软件工程代理代表了软件工程的变革性范式，通过规范协调的人机协作框架，为未来软件工程团队提供了可扩展、透明和可信赖的解决方案。

Abstract: This paper envisions a transformative paradigm in software engineering, where Artificial Intelligence, embodied in fully autonomous agents, becomes the primary driver of the core software development activities. We introduce a new class of software engineering agents, empowered by Large Language Models and equipped with beliefs, desires, intentions, and memory to enable human-like reasoning. These agents collaborate with humans and other agents to design, implement, test, and deploy software systems with a level of speed, reliability, and adaptability far beyond the current software development processes. Their coordination and collaboration are governed by norms expressed as deontic modalities - commitments, obligations, prohibitions and permissions - that regulate interactions and ensure regulatory compliance. These innovations establish a scalable, transparent and trustworthy framework for future Human-AI software engineering teams.

</details>


### [8] [Process-Centric Analysis of Agentic Software Systems](https://arxiv.org/abs/2512.02393)
*Shuyang Liu,Yang Chen,Rahul Krishna,Saurabh Sinha,Jatin Ganhotra,Reyhan Jabbarvand*

Main category: cs.SE

TL;DR: Graphectory：一种用于编码智能体系统轨迹的图表示方法，支持过程中心化评估，超越传统结果导向评估


<details>
  <summary>Details</summary>
Motivation: 当前智能体系统评估过于结果导向，仅关注最终成功与否，忽视了智能体推理、规划、行动和策略演变的详细过程。需要一种系统化方法来分析智能体工作流的执行轨迹。

Method: 提出Graphectory方法，将智能体系统的执行轨迹编码为图结构，捕获时间和语义关系。使用该方法分析了4000个轨迹，涉及SWE-agent和OpenHands两种智能体编程工作流，结合4种LLM骨干模型，解决SWE-bench Verified问题。

Result: 1. 使用更丰富提示或更强LLM的智能体表现出更复杂的Graphectory，反映更深探索、更广上下文收集和更彻底验证；2. 智能体策略随问题难度和底层LLM变化，已解决问题遵循定位-补丁-验证的连贯步骤，未解决问题则呈现混乱、重复或回溯行为；3. 即使成功，智能体编程系统也常显示低效过程，导致不必要延长轨迹。

Conclusion: Graphectory提供了一种系统化方法来分析智能体系统的执行过程，支持过程中心化评估，揭示了智能体工作流的质量特征和效率问题，超越了传统结果导向评估的局限性。

Abstract: Agentic systems are modern software systems: they consist of orchestrated modules, expose interfaces, and are deployed in software pipelines. Unlike conventional programs, their execution (i.e., trajectories) is inherently stochastic and adaptive to the problem they are solving. Evaluation of such systems is often outcome-centric, judging their performance based on success or failure at the final step. This narrow focus overlooks detailed insights about such systems, failing to explain how agents reason, plan, act, or change their strategies over time. Inspired by the structured representation of conventional software systems as graphs, we introduce Graphectory to systematically encode the temporal and semantic relations in such software systems. Graphectory facilitates the design of process-centric metrics and analyses to assess the quality of agentic workflows independent of final success.
  Using Graphectory, we analyze 4000 trajectories of two dominant agentic programming workflows, namely SWE-agent and OpenHands, with a combination of four backbone Large Language Models (LLMs), attempting to resolve SWE-bench Verified issues. Our fully automated analyses reveal that: (1) agents using richer prompts or stronger LLMs exhibit more complex Graphectory, reflecting deeper exploration, broader context gathering, and more thorough validation before patch submission; (2) agents' problem-solving strategies vary with both problem difficulty and the underlying LLM -- for resolved issues, the strategies often follow coherent localization-patching-validation steps, while unresolved ones exhibit chaotic, repetitive, or backtracking behaviors; (3) even when successful, agentic programming systems often display inefficient processes, leading to unnecessarily prolonged trajectories.

</details>


### [9] [Model-Based Diagnosis with Multiple Observations: A Unified Approach for C Software and Boolean Circuits](https://arxiv.org/abs/2512.02898)
*Pedro Orvalho,Marta Kwiatkowska,Mikoláš Janota,Vasco Manquinho*

Main category: cs.SE

TL;DR: CFaults是一个基于模型诊断的多故障定位工具，通过将所有失败测试用例整合到统一的MaxSAT公式中，保证跨观察的一致性并生成子集最小化诊断。


<details>
  <summary>Details</summary>
Motivation: 现有基于公式的故障定位方法存在两个主要问题：1) 无法保证在所有失败测试中都能找到诊断集合；2) 可能产生非子集最小化的冗余诊断，特别是在多故障场景下。这些问题在软件开发和电路设计中尤为突出，因为调试是耗时且昂贵的过程。

Method: CFaults采用基于模型诊断方法，将所有失败测试用例聚合到一个统一的最大可满足性公式中。这种方法确保了跨观察的一致性，并简化了故障定位过程。工具支持C软件和布尔电路的多故障定位。

Result: 在TCAS、C-Pack-IPAs和ISCAS85三个基准测试集上的实验表明：1) 对于C软件，CFaults比BugAssist、SNIPER和HSD等其他FBFL方法更快；2) 在ISCAS85电路基准上，CFaults通常比HSD慢，但能在仅少6%的电路中定位故障，仍具竞争力；3) CFaults仅生成子集最小化诊断，而其他方法倾向于枚举冗余诊断。

Conclusion: CFaults通过整合多观察的MBD方法和MaxSAT公式，有效解决了现有故障定位方法的局限性，特别是在多故障场景下保证了诊断的一致性和最小化，在软件和电路故障定位领域表现出色。

Abstract: Debugging is one of the most time-consuming and expensive tasks in software development and circuit design. Several formula-based fault localisation (FBFL) methods have been proposed, but they fail to guarantee a set of diagnoses across all failing tests or may produce redundant diagnoses that are not subset-minimal, particularly for programs/circuits with multiple faults.
  This paper introduces CFaults, a novel fault localisation tool for C software and Boolean circuits with multiple faults. CFaults leverages Model-Based Diagnosis (MBD) with multiple observations and aggregates all failing test cases into a unified Maximum Satisfiability (MaxSAT) formula. Consequently, our method guarantees consistency across observations and simplifies the fault localisation procedure. Experimental results on three benchmark sets, two of C programs, TCAS and C-Pack-IPAs, and one of Boolean circuits, ISCAS85, show that CFaults is faster at localising faults in C software than other FBFL approaches such as BugAssist, SNIPER, and HSD. On the ISCAS85 benchmark, CFaults is generally slower than HSD; however, it localises faults in only 6% fewer circuits, demonstrating that it remains competitive in this domain. Furthermore, CFaults produces only subset-minimal diagnoses of faulty statements, whereas the other approaches tend to enumerate redundant diagnoses (e.g., BugAssist and SNIPER).

</details>


### [10] [Feedback Loops and Code Perturbations in LLM-based Software Engineering: A Case Study on a C-to-Rust Translation System](https://arxiv.org/abs/2512.02567)
*Martin Weiss,Jesko Hecking-Harbusch,Jochen Quante,Matthias Woehrle*

Main category: cs.SE

TL;DR: 研究自动C到Rust代码翻译系统中反馈循环、LLM选择和代码扰动对翻译质量的影响，发现反馈循环能显著减小不同LLM间的性能差异，且代码扰动带来的多样性甚至能提升系统性能。


<details>
  <summary>Details</summary>
Motivation: 虽然生成式AI在软件工程任务中应用广泛，但自动化方法需要更高的可靠性才能用于工业实践。本文关注影响代码翻译质量的三个关键因素：自动反馈循环的效果、大语言模型的选择，以及行为保持代码变更的影响。

Method: 基于生成-检查模式构建C到Rust自动翻译系统。LLM生成Rust代码后，系统自动检查编译性和与原始C代码的行为等价性。对于负面检查结果，通过反馈循环重新提示LLM修复输出。通过改变三个变量（反馈循环、LLM选择、代码扰动）来评估系统成功率。

Result: 无反馈循环时，LLM选择对翻译成功率影响很大；但当系统使用反馈循环时，不同模型间的差异显著减小。这一现象在系统平均性能和代码扰动下的鲁棒性中均观察到。此外，代码扰动提供的多样性甚至能提升系统性能。

Conclusion: 反馈循环是提升自动代码翻译系统可靠性的关键机制，能有效减小不同LLM间的性能差异。代码扰动带来的多样性对系统性能有积极影响，为工业实践中构建更可靠的AI驱动代码翻译系统提供了重要见解。

Abstract: The advent of strong generative AI has a considerable impact on various software engineering tasks such as code repair, test generation, or language translation. While tools like GitHub Copilot are already in widespread use in interactive settings, automated approaches require a higher level of reliability before being usable in industrial practice. In this paper, we focus on three aspects that directly influence the quality of the results: a) the effect of automated feedback loops, b) the choice of Large Language Model (LLM), and c) the influence of behavior-preserving code changes.
  We study the effect of these three variables on an automated C-to-Rust translation system. Code translation from C to Rust is an attractive use case in industry due to Rust's safety guarantees. The translation system is based on a generate-and-check pattern, in which Rust code generated by the LLM is automatically checked for compilability and behavioral equivalence with the original C code. For negative checking results, the LLM is re-prompted in a feedback loop to repair its output. These checks also allow us to evaluate and compare the respective success rates of the translation system when varying the three variables.
  Our results show that without feedback loops LLM selection has a large effect on translation success. However, when the translation system uses feedback loops the differences across models diminish. We observe this for the average performance of the system as well as its robustness under code perturbations. Finally, we also identify that diversity provided by code perturbations can even result in improved system performance.

</details>


### [11] [Empirical Assessment of the Perception of Software Product Line Engineering by an SME before Migrating its Code Base](https://arxiv.org/abs/2512.02707)
*Thomas Georges,Marianne Huchard,Mélanie König,Clémentine Nebut,Chouki Tibermacine*

Main category: cs.SE

TL;DR: 该研究通过与中小企业的合作，评估了将现有代码库迁移到软件产品线(SPL)的过程，分析了利益相关者对迁移的认知、预期收益和风险，并提出了有效的风险缓解策略。


<details>
  <summary>Details</summary>
Motivation: 软件产品线(SPL)工程虽然能带来显著效益，但迁移过程成本高昂且具有挑战性，会对公司的开发流程和开发者实践产生重大影响。本研究通过与一家决定迁移到SPL的中小企业合作，旨在深入评估现有开发流程、实践以及迁移的预期收益和风险。

Method: 采用定性研究方法，对参与软件开发的关键利益相关者进行深入访谈。研究设计了专门的访谈方案，收集利益相关者对迁移的认知、潜在抵制态度以及他们对迁移过程的看法。

Result: 研究发现，无论参与者在开发过程中的角色如何，所有参与者都识别出了与自身活动相关的迁移收益。有效的风险缓解策略包括：在整个过程中保持利益相关者的知情和参与、尽可能保留良好实践、积极让他们参与迁移过程以确保平稳过渡并最小化潜在挑战。

Conclusion: 成功实施SPL迁移需要充分考虑组织因素和人员因素。通过让利益相关者参与、保持良好实践并确保透明沟通，可以显著降低迁移风险并提高成功率。这项研究为其他组织进行类似迁移提供了有价值的实践指导。

Abstract: Migrating a set of software variants into a software product line (SPL) is an expensive and potentially challenging endeavor. Indeed, SPL engineering can significantly impact a company's development process and often requires changes to established developer practices. The work presented in this paper stems from a collaboration with a Small and Medium-sized Enterprise (SME) that decided to migrate its existing code base into an SPL. In this study, we conducted an in-depth evaluation of the company's current development processes and practices, as well as the anticipated benefits and risks associated with the migration. Key stakeholders involved in software development participated in this evaluation to provide insight into their perceptions of the migration and their potential resistance to change. This paper describes the design of the interviews conducted with these stakeholders and presents an analysis of the results. Among the qualitative findings, we observed that all participants, regardless of their role in the development process, identified benefits of the migration relevant to their own activities. Furthermore, our results suggest that an effective risk mitigation strategy involves keeping stakeholders informed and engaged throughout the process, preserving as many good practices as possible, and actively involving them in the migration to ensure a smooth transition and minimize potential challenges.

</details>


### [12] [Integrative Analysis of Risk Management Methodologies in Data Science Projects](https://arxiv.org/abs/2512.02728)
*Sabrina Delmondes da Costa Feitosa*

Main category: cs.SE

TL;DR: 该研究对数据科学项目的主要风险管理方法进行对比分析，发现传统方法对新兴风险覆盖有限，而现代模型能整合伦理监督、治理和持续监控，为开发平衡技术效率、组织对齐和负责任数据实践的混合框架提供理论支持。


<details>
  <summary>Details</summary>
Motivation: 数据科学项目失败率高，主要由于技术限制、组织局限和风险管理不足。现有挑战包括数据成熟度低、缺乏治理、技术与业务团队错位，以及缺乏应对伦理和社会技术风险的结构化机制。

Method: 采用整合性文献综述方法，使用索引数据库和结构化协议进行文献筛选和内容分析。比较了ISO 31000、PMBOK风险管理、NIST RMF等传统标准，以及CRISP-DM和新兴的DS EthiCo RMF等数据科学专用框架。

Result: 研究发现传统风险管理方法对新兴风险覆盖有限，而当代模型提出了能够整合伦理监督、治理和持续监控的多维结构。DS EthiCo RMF等框架将伦理和社会技术维度纳入项目生命周期。

Conclusion: 本研究为开发平衡技术效率、组织对齐和负责任数据实践的混合框架提供了理论支持，同时指出了研究空白，可指导未来研究。强调了需要结合传统风险管理与数据科学特定需求的方法。

Abstract: Data science initiatives frequently exhibit high failure rates, driven by technical constraints, organizational limitations and insufficient risk management practices. Challenges such as low data maturity, lack of governance, misalignment between technical and business teams, and the absence of structured mechanisms to address ethical and sociotechnical risks have been widely identified in the literature. In this context, the purpose of this study is to conduct a comparative analysis of the main risk management methodologies applied to data science projects, aiming to identify, classify, and synthesize their similarities, differences and existing gaps. An integrative literature review was performed using indexed databases and a structured protocol for selection and content analysis. The study examines widely adopted risk management standards ISO 31000, PMBOK Risk Management and NIST RMF, as well as frameworks specific to data science workflows, such as CRISP DM and the recently proposed DS EthiCo RMF, which incorporates ethical and sociotechnical dimensions into the project life cycle. The findings reveal that traditional approaches provide limited coverage of emerging risks, whereas contemporary models propose multidimensional structures capable of integrating ethical oversight, governance and continuous monitoring. As a contribution, this work offers theoretical support for the development of hybrid frameworks that balance technical efficiency, organizational alignment and responsible data practices, while highlighting research gaps that can guide future investigations.

</details>


### [13] ["Can you feel the vibes?": An exploration of novice programmer engagement with vibe coding](https://arxiv.org/abs/2512.02750)
*Kiev Gama,Filipe Calegario,Victoria Jackson,Alexander Nolte,Luiz Augusto Morais,Vinicius Garcia*

Main category: cs.SE

TL;DR: 论文研究"氛围编程"在教育中的应用，通过巴西大学的黑客松活动分析新手程序员如何通过自然语言提示而非直接编码来开发软件，发现其能促进快速原型设计和跨学科合作，但也存在创意收敛过早、代码质量不均等问题。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI和AI辅助编程的兴起，"氛围编程"（通过自然语言提示而非直接编写代码来创建软件）逐渐流行。这种方法有望使软件开发民主化，但其教育影响尚未得到充分探索。本研究旨在调查新手程序员和混合经验团队如何参与氛围编程。

Method: 在巴西一所公立大学组织为期一天的包容性黑客松活动，有31名来自计算和非计算学科的本科生参与，分为9个团队。通过观察、退出调查和半结构化访谈，研究创意过程、工具使用模式、协作动态和学习成果。

Result: 研究发现氛围编程能够实现快速原型设计和跨学科协作，参与者在时间限制内开发了提示工程技能并交付了功能演示。但也观察到创意过早收敛、代码质量不均需要返工、对核心软件工程实践参与有限等问题。团队采用了结合多种AI工具的复杂工作流程，人类判断在关键改进中仍然至关重要。9小时的短格式对新手的信心建立有效，同时适应了时间有限的参与者。

Conclusion: 氛围编程黑客松可以作为有价值的低风险学习环境，但需要结合明确的支架来支持发散思维、批判性评估AI输出，以及对生产质量的现实期望。

Abstract: Emerging alongside generative AI and the broader trend of AI-assisted coding, the term "vibe coding" refers to creating software via natural language prompts rather than direct code authorship. This approach promises to democratize software development, but its educational implications remain underexplored. This paper reports on a one-day educational hackathon investigating how novice programmers and mixed-experience teams engage with vibe coding. We organized an inclusive event at a Brazilian public university with 31 undergraduate participants from computing and non-computing disciplines, divided into nine teams. Through observations, an exit survey, and semi-structured interviews, we examined creative processes, tool usage patterns, collaboration dynamics, and learning outcomes. Findings reveal that vibe coding enabled rapid prototyping and cross-disciplinary collaboration, with participants developing prompt engineering skills and delivering functional demonstrations within time constraints. However, we observed premature convergence in ideation, uneven code quality requiring rework, and limited engagement with core software engineering practices. Teams adopted sophisticated workflows combining multiple AI tools in pipeline configurations, with human judgment remaining essential for critical refinement. The short format (9 hours) proved effective for confidence-building among newcomers while accommodating participants with limited availability. We conclude that vibe coding hackathons can serve as valuable low-stakes learning environments when coupled with explicit scaffolds for divergent thinking, critical evaluation of AI outputs, and realistic expectations about production quality.

</details>


### [14] [Towards Observation Lakehouses: Living, Interactive Archives of Software Behavior](https://arxiv.org/abs/2512.02795)
*Marcus Kessel*

Main category: cs.SE

TL;DR: 论文提出观察数据湖仓（observation lakehouses）架构，用于持续收集和分析代码执行行为数据，为代码生成LLMs提供动态行为地面真值。


<details>
  <summary>Details</summary>
Motivation: 当前代码生成LLMs主要基于静态代码训练，缺乏运行时行为数据，容易学习到有bug或错误标注的代码。由于语义属性通常不可判定，获取真实功能性的唯一实用方法是通过动态执行观察。

Method: 提出观察数据湖仓架构，基于Apache Parquet + Iceberg + DuckDB构建，包含：1）持续SRCs（刺激-响应立方体）；2）高维追加式观察表存储所有执行记录；3）按需生成SRC切片的SQL查询。从控制管道（LASSO）和CI管道（如单元测试执行）中摄取数据。

Result: 在509个问题的基准测试中，摄取了约860万行观察数据（<51MiB），在笔记本电脑上能在<100ms内重建SRM/SRC视图和聚类，证明持续行为挖掘无需分布式集群即可实现。

Conclusion: 观察数据湖仓使行为地面真值成为与其它运行时数据同等重要的一类数据，为行为感知的评估和训练提供了基础设施路径。项目已在GitHub开源。

Abstract: Code-generating LLMs are trained largely on static artifacts (source, comments, specifications) and rarely on materializations of run-time behavior. As a result, they readily internalize buggy or mislabeled code. Since non-trivial semantic properties are undecidable in general, the only practical way to obtain ground-truth functionality is by dynamic observation of executions. In prior work, we addressed representation with Sequence Sheets, Stimulus-Response Matrices (SRMs), and Stimulus-Response Cubes (SRCs) to capture and compare behavior across tests, implementations, and contexts. These structures make observation data analyzable offline and reusable, but they do not by themselves provide persistence, evolution, or interactive analytics at scale. In this paper, therefore, we introduce observation lakehouses that operationalize continual SRCs: a tall, append-only observations table storing every actuation (stimulus, response, context) and SQL queries that materialize SRC slices on demand. Built on Apache Parquet + Iceberg + DuckDB, the lakehouse ingests data from controlled pipelines (LASSO) and CI pipelines (e.g., unit test executions), enabling n-version assessment, behavioral clustering, and consensus oracles without re-execution. On a 509-problem benchmark, we ingest $\approx$8.6M observation rows ($<$51MiB) and reconstruct SRM/SRC views and clusters in $<$100ms on a laptop, demonstrating that continual behavior mining is practical without a distributed cluster of machines. This makes behavioral ground truth first-class alongside other run-time data and provides an infrastructure path toward behavior-aware evaluation and training. The Observation Lakehouse, together with the accompanying dataset, is publicly available as an open-source project on GitHub: https://github.com/SoftwareObservatorium/observation-lakehouse

</details>


### [15] [The Evolutionary Ecology of Software: Constraints, Innovation, and the AI Disruption](https://arxiv.org/abs/2512.02953)
*Sergi Valverde,Blai Vidiella,Salva Duran-Nebreda*

Main category: cs.SE

TL;DR: 该研究从进化生态学视角分析软件的演化，探讨软件与创新的共生关系，以及AI工具如何影响软件生态系统多样性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解软件作为社会技术系统的复杂演化轨迹，特别是约束、修补和频率依赖选择之间的相互作用如何驱动软件进化。作者关注编程语言演化如何影响开发者实践，以及AI驱动开发工具引入的新进化压力。

Method: 采用基于主体的建模和案例研究相结合的方法，运用复杂网络分析和进化理论，探索软件在创新生成和模仿竞争力量下的演化过程。

Result: 研究发现技术制品与社会规范、文化动态和人类互动共同演化。AI工具虽然提供前所未有的信息访问，但其广泛采用可能引入导致文化停滞的新进化压力，类似于过去软件生态系统多样性的衰退。

Conclusion: 理解AI介导的软件生产引入的进化压力对于预测文化变革、技术适应和软件创新未来的更广泛模式至关重要。需要从生态学视角分析软件演化，以维持创新多样性。

Abstract: This chapter investigates the evolutionary ecology of software, focusing on the symbiotic relationship between software and innovation. An interplay between constraints, tinkering, and frequency-dependent selection drives the complex evolutionary trajectories of these socio-technological systems. Our approach integrates agent-based modeling and case studies, drawing on complex network analysis and evolutionary theory to explore how software evolves under the competing forces of novelty generation and imitation. By examining the evolution of programming languages and their impact on developer practices, we illustrate how technological artifacts co-evolve with and shape societal norms, cultural dynamics, and human interactions. This ecological perspective also informs our analysis of the emerging role of AI-driven development tools in software evolution. While large language models (LLMs) provide unprecedented access to information, their widespread adoption introduces new evolutionary pressures that may contribute to cultural stagnation, much like the decline of diversity in past software ecosystems. Understanding the evolutionary pressures introduced by AI-mediated software production is critical for anticipating broader patterns of cultural change, technological adaptation, and the future of software innovation.

</details>
