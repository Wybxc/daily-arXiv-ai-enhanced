<div id=toc></div>

# Table of Contents

- [cs.FL](#cs.FL) [Total: 1]
- [cs.PL](#cs.PL) [Total: 6]
- [cs.LO](#cs.LO) [Total: 8]
- [cs.SE](#cs.SE) [Total: 17]


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [1] [A Variety of Request-Response Specifications](https://arxiv.org/abs/2509.13078)
*Daichi Aiba,Masaki Waga,Hiroya Fujinami,Koko Muroya,Shutaro Ouchi,Naoki Ueda,Yosuke Yokoyama,Yuta Wada,Ichiro Hasuo*

Main category: cs.FL

TL;DR: 本文对请求-响应规范的六种变体进行了分类和形式化，并提供了监控工具调查


<details>
  <summary>Details</summary>
Motivation: 基于实际应用需求，发现请求-响应规范存在多种需要区分的变体

Method: 1) 将变体分类为六种类型并用决策树引导用户选择；2) 用时序逻辑、语法和自动机等形式化方法对六种类型进行形式化；3) 调查相关监控工具

Result: 建立了完整的请求-响应规范分类体系，其中两种类型是非正则规范，需要扩展的形式化方法

Conclusion: 提出了系统化的请求-响应规范分类和形式化框架，为实践者提供了实用的指导工具

Abstract: We find, motivated by real-world applications, that the well-known
request-response specification comes with multiple variations, and that these
variations should be distinguished. As the first main contribution, we
introduce a classification of those variations into six types, and present it
as a decision tree, where a user is led to the type that is suited for their
application by answering a couple of questions. Our second main contribution is
the formalization of those six types in various formalisms such as temporal
logics, grammars, and automata; here, two types out of the six are non-regular
specifications and their formalization requires extended formalisms. We also
survey tools for monitoring these specifications to cater for practitioners'
needs.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [2] [Converting IEC 61131-3 LD into SFC Using Large Language Model: Dataset and Testing](https://arxiv.org/abs/2509.12593)
*Yimin Zhang,Mario de Sousa*

Main category: cs.PL

TL;DR: 本文研究了使用大型语言模型(LLM)将梯形图(LD)转换为顺序功能图(SFC)的可行性，通过构建符合IEC 61131-3标准的数据集并进行微调实验，取得了最高91%的准确率。


<details>
  <summary>Details</summary>
Motivation: PLC编程领域中LD到SFC的转换存在挑战，传统方法缺乏领域知识且面临状态爆炸问题，而AI特别是LLM的发展为此提供了新的解决方案机会。

Method: 构建了多个符合IEC 61131-3标准的SFC和LD程序文本表示配对数据集，并基于这些数据集对LLM进行微调，探索自动化LD-SFC转换的可行性。

Result: 实验结果显示，经过微调的LLM模型在特定数据集上达到最高91%的准确率，最低准确率为79%，表明LLM能够有效支持LD-SFC转换。

Conclusion: 研究表明，通过适当的训练和表示方法，LLM在LD-SFC转换任务中具有可行性和未来潜力，为PLC编程自动化提供了新的技术路径。

Abstract: In the domain of Programmable Logic Controller (PLC) programming, converting
a Ladder Diagram (LD) into a Sequential Function Chart (SFC) is an inherently
challenging problem, primarily due to the lack of domain-specific knowledge and
the issue of state explosion in existing algorithms. However, the rapid
development of Artificial Intelligence (AI) - especially Large Language Model
(LLM) - offers a promising new approach.
  Despite this potential, data-driven approaches in this field have been
hindered by a lack of suitable datasets. To address this gap, we constructed
several datasets consisting of paired textual representations of SFC and LD
programs that conform to the IEC 61131-3 standard.
  Based on these datasets, we explored the feasibility of automating the LD-SFC
conversion using LLM. Our preliminary experiments show that a fine-tuned LLM
model achieves up to 91% accuracy on certain dataset, with the lowest observed
accuracy being 79%, suggesting that with proper training and representation,
LLMs can effectively support LD-SFC conversion. These early results highlight
the viability and future potential of this approach.

</details>


### [3] [Efficient Compilation of Algorithms into Compact Linear Programs](https://arxiv.org/abs/2509.13006)
*Shermin Khosravi,David Bremner*

Main category: cs.PL

TL;DR: 提出了一种新颖的分层线性流水线技术，通过将嵌套程序结构分解为同步区域来生成更小的线性规划(LP)公式，相比传统方法可减少25倍的LP规模并显著提升求解器性能。


<details>
  <summary>Details</summary>
Motivation: 现有LP编译器生成的线性规划虽然多项式规模但往往极其庞大，给现有求解器带来挑战。需要为具有指数扩展复杂度的问题建立最小尺寸的紧凑LP公式。

Method: 引入分层线性流水线技术，将嵌套程序结构分解为具有明确定义执行转换的同步区域，使LP约束和变量在每个区域内局部化，从而显著减少LP规模。

Result: 在两个基准问题(最大完工时间问题和加权最小生成树问题)上实现了高达25倍的LP规模缩减，并在商业和非商业LP求解器上都获得了显著的性能提升。

Conclusion: 该方法能够系统地为具有多项式时间分离预言机的指数规模整数规划问题生成紧凑公式，为算法到LP的编译提供了更高效的解决方案。

Abstract: Linear Programming (LP) is widely applied in industry and is a key component
of various other mathematical problem-solving techniques. Recent work
introduced an LP compiler translating polynomial-time, polynomial-space
algorithms into polynomial-size LPs using intuitive high-level programming
languages, offering a promising alternative to manually specifying each set of
constraints through Algebraic Modeling Languages (AMLs). However, the resulting
LPs, while polynomial in size, are often extremely large, posing challenges for
existing LP solvers. In this paper, we propose a novel approach for generating
substantially smaller LPs from algorithms. Our goal is to establish
minimum-size compact LP formulations for problems in P having natural
formulations with exponential extension complexities. Our broader vision is to
enable the systematic generation of Compact Integer Programming (CIP)
formulations for problems with exponential-size IPs having polynomial-time
separation oracles. To this end, we introduce a hierarchical linear pipelining
technique that decomposes nested program structures into synchronized regions
with well-defined execution transitions -- functions of compile-time
parameters. This decomposition allows us to localize LP constraints and
variables within each region, significantly reducing LP size without the loss
of generality, ensuring the resulting LP remains valid for all inputs of size
$n$. We demonstrate the effectiveness of our method on two benchmark problems
-- the makespan problem, which has exponential extension complexity, and the
weighted minimum spanning tree problem -- both of which have exponential-size
natural LPs. Our results show up to a $25$-fold reduction in LP size and
substantial improvements in solver performance across both commercial and
non-commercial LP solvers.

</details>


### [4] [Pleasant Imperative Program Proofs with GallinaC](https://arxiv.org/abs/2509.13019)
*Frédéric Fort,David Nowak,Vlad Rusu*

Main category: cs.PL

TL;DR: GallinaC是一个在Rocq证明助手的Gallina函数式语言中浅嵌入的图灵完备命令式语言，旨在为底层系统软件提供机器验证的正确性证明


<details>
  <summary>Details</summary>
Motivation: 当前命令式编程语言语义过于宽松，使得为可信计算基础（如操作系统内核）提供数学正确性证明变得繁琐且容易出错，需要一种既支持命令式编程范式又具有良好语义的证明导向语言

Method: 在Gallina函数式语言中浅嵌入一个图灵完备的命令式语言，包含真正通用且无界的while循环，利用函数式核心使得程序证明可以使用与纯函数式程序相同的策略

Result: 原型实现已证明GallinaC的可行性，成功完成了未知大小链表反转程序的正确性证明，目前正专注于GallinaC中间表示与CompCert后端入口语言Cminor的前向模拟

Conclusion: GallinaC为底层系统软件提供了一种在证明助手中开发机器验证正确性证明的有效方法，虽然工作仍在进行中，但初步结果显示了其潜力

Abstract: Even with the increase of popularity of functional programming, imperative
programming remains a key programming paradigm, especially for programs
operating at lower levels of abstraction. When such software offers key
components of a Trusted Computing Base (TCB), e.g. an operating system kernel,
it becomes desirable to provide mathematical correctness proofs.
  However, current real-world imperative programming languages possess
"expressive", i.e. overly permissive, semantics. Thus, producing correctness
proofs of such programs becomes tedious and error-prone, requiring to take care
of numerous "administrative" details. Ideally, a proof-oriented imperative
language should feature well-behaved semantics while allowing imperative
idioms.
  To obtain a high-degree of confidence in the correctness of such a language,
its tools should be developed inside a proof-assistant such that program proofs
are machine checked.
  We present GallinaC, a shallow embedding of a Turing-complete imperative
language directly inside the functional programming language of the Rocq proof
assistant, Gallina. In particular, it features a truly generic and unbounded
while loop. Having a functional core means proofs about GallinaC programs may
use the same tactics as proofs about pure functional ones.
  Work on GallinaC is still under progress, but we present first promising
results. A prototype implementation has shown the viability of GallinaC with
the correctness proof of a list reversal procedure for linked-lists of unknown
size. We currently focus on the forward simulation between the GallinaC
intermediate representation (IR) and Cminor, the entry language of the CompCert
back-end.

</details>


### [5] [Navigating the Python Type Jungle](https://arxiv.org/abs/2509.13022)
*Andrei Nacu,Dorel Lucanu*

Main category: cs.PL

TL;DR: 本文提出对Python碎片化类型系统的形式化理论基础，使用类型理论概念来优雅描述Python类型系统，为未来类型推断工具开发奠定基础


<details>
  <summary>Details</summary>
Motivation: Python的类型系统在实践中演化得强大但理论碎片化，缺乏统一的形式化规范，需要建立理论基础来支持未来发展

Method: 采用类型理论的概念和方法，构建形式化基础来系统描述Python的类型系统

Result: 成功建立了Python类型系统的形式化理论基础，证明了可以用优雅的方式描述Python的类型系统

Conclusion: 这项工作为Python类型系统的未来发展提供了关键的形式化基础，是开发类型推断工具的重要第一步

Abstract: Python's typing system has evolved pragmatically into a powerful but
theoretically fragmented system, with scattered specifications. This paper
proposes a formalization to address this fragmentation. The central
contribution is a formal foundation that uses concepts from type theory to
demonstrate that Python's type system can be elegantly described. This work
aims to serve as a crucial first step toward the future development of type
inference tools.

</details>


### [6] [Try-Mopsa: Relational Static Analysis in Your Pocket](https://arxiv.org/abs/2509.13128)
*Raphaël Monat*

Main category: cs.PL

TL;DR: Try-Mopsa是一个基于JavaScript的简化版Mopsa静态分析平台，可在浏览器中运行，无需安装，便于学习和采用。


<details>
  <summary>Details</summary>
Motivation: 静态分析器依赖复杂、安装困难，阻碍了采用和学习。需要一种无需安装、易于访问的解决方案来降低使用门槛。

Method: 将Mopsa静态分析平台的核心组件编译为JavaScript，创建纯客户端Web应用，支持桌面和移动设备，并保留关系数值域等核心功能。

Result: 成功开发了Try-Mopsa，提供了响应式界面，完整支持Mopsa的核心功能，特别是关系数值域分析。

Conclusion: Try-Mopsa作为便捷的Web平台，有效解决了静态分析器安装困难的问题，特别适合入门教学和快速上手使用。

Abstract: Static analyzers are complex pieces of software with large dependencies. They
can be difficult to install, which hinders adoption and creates barriers for
students learning static analysis. This work introduces Try-Mopsa: a
scaled-down version of the Mopsa static analysis platform, compiled into
JavaScript to run purely as a client-side application in web browsers.
Try-Mopsa provides a responsive interface that works on both desktop and mobile
devices. Try-Mopsa features all the core components of Mopsa. In particular, it
supports relational numerical domains. We present the interface, changes and
adaptations required to have a pure JavaScript version of Mopsa. We envision
Try-Mopsa as a convenient platform for onboarding or teaching purposes.

</details>


### [7] [Rebound: Efficient, Expressive, and Well-Scoped Binding](https://arxiv.org/abs/2509.13261)
*Noé De Santo,Stephanie Weirich*

Main category: cs.PL

TL;DR: Rebound是一个Haskell库，通过静态追踪作用域和一等环境来支持良好作用域的项表示，自动定义替换、alpha等价等绑定结构操作，提供性能优化和正确性保证。


<details>
  <summary>Details</summary>
Motivation: 处理绑定结构时，de Bruijn索引等技术的微妙不变量难以正确维护，需要自动化工具来减少错误并提高开发效率。

Method: 使用一等环境映射变量到表达式，静态追踪作用域，优化替换应用，同时提供对环境数据结构的显式访问。

Result: 库具有表达性，能实现复杂的语言特性，在pi-forall依赖类型语言类型检查器等示例中表现良好，性能优于竞争库。

Conclusion: Rebound库通过作用域追踪和环境管理，为Haskell中的绑定结构操作提供了正确、高效且表达性强的解决方案。

Abstract: We introduce the Rebound library that supports well-scoped term
representations in Haskell and automates the definition of substitution,
alpha-equivalence, and other operations that work with binding structures. The
key idea of our design is the use of first-class environments that map
variables to expressions in some new scope. By statically tracking scopes,
users of this library gain confidence that they have correctly maintained the
subtle invariants that stem from using de Bruijn indices. Behind the scenes,
Rebound uses environments to optimize the application of substitutions, while
providing explicit access to these data structures when desired. We demonstrate
that this library is expressive by using it to implement a wide range of
language features with sophisticated uses of binding and several different
operations that use this abstract syntax. Our examples include pi-forall, a
tutorial implementation of a type checker for a dependently-typed programming
language. Finally, we benchmark Rebound to understand its performance
characteristics and find that it produces faster code than competing libraries.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [8] [Determination of the fifth Busy Beaver value](https://arxiv.org/abs/2509.12337)
*The bbchallenge Collaboration,Justin Blanchard,Daniel Briggs,Konrad Deka,Nathan Fenner,Yannick Forster,Georgi Georgiev,Matthew L. House,Rachel Hunter,Iijil,Maja Kądziołka,Pavel Kropitz,Shawn Ligocki,mxdys,Mateusz Naściszewski,savask,Tristan Stérin,Chris Xu,Jason Yuen,Théo Zimmermann*

Main category: cs.LO

TL;DR: 使用Coq证明助手验证了S(5)=47,176,870，这是40多年来首次确定新的Busy Beaver值，也是首个经过形式化验证的Busy Beaver值


<details>
  <summary>Details</summary>
Motivation: Busy Beaver函数S(n)是图灵机理论中最简单的不可计算函数之一，自1962年提出以来，S(5)的确切值一直是未解问题

Method: 枚举了181,385,789个5状态图灵机，对每个机器判断其是否停机，使用Coq证明助手进行形式化验证

Result: 成功确定了S(5)=47,176,870，这是首个经过形式化验证的Busy Beaver值

Conclusion: 这项工作证明了大规模在线协作研究（bbchallenge.org）的有效性，为不可计算函数的研究提供了新的验证方法

Abstract: We prove that $S(5) = 47,176,870$ using the Coq proof assistant. The Busy
Beaver value $S(n)$ is the maximum number of steps that an $n$-state 2-symbol
Turing machine can perform from the all-zero tape before halting, and $S$ was
historically introduced by Tibor Rad\'o in 1962 as one of the simplest examples
of an uncomputable function. The proof enumerates $181,385,789$ Turing machines
with 5 states and, for each machine, decides whether it halts or not. Our
result marks the first determination of a new Busy Beaver value in over 40
years and the first Busy Beaver value ever to be formally verified, attesting
to the effectiveness of massively collaborative online research
(bbchallenge$.$org).

</details>


### [9] [Probabilistic Model Checking: Applications and Trends](https://arxiv.org/abs/2509.12968)
*Marta Kwiatkowska,Gethin Norman,David Parker*

Main category: cs.LO

TL;DR: 概率模型检验在随机系统形式化建模与分析中的应用综述，涵盖25年来的主要应用领域、理论技术发展和实际效益


<details>
  <summary>Details</summary>
Motivation: 总结概率模型检验在过去25年中的发展历程，识别其主要应用领域和价值，为潜在用户提供技术指导并推动该领域未来发展

Method: 通过文献综述和案例分析的方法，识别概率模型检验的主要应用领域，总结关键理论和技术发展，并通过具体示例说明其实际效益

Result: 系统梳理了概率模型检验在不同应用领域的成功案例，展示了该技术在随机系统形式化分析中的广泛适用性和实际价值

Conclusion: 概率模型检验已成为随机系统分析的重要工具，其应用范围不断扩大，未来发展需要继续扩展形式化方法和技术以应对更复杂的实际问题

Abstract: Probabilistic model checking is an approach to the formal modelling and
analysis of stochastic systems. Over the past twenty five years, the number of
different formalisms and techniques developed in this field has grown
considerably, as has the range of problems to which it has been applied. In
this paper, we identify the main application domains in which probabilistic
model checking has proved valuable and discuss how these have evolved over
time. We summarise the key strands of the underlying theory and technologies
that have contributed to these advances, and highlight examples which
illustrate the benefits that probabilistic model checking can bring. The aim is
to inform potential users of these techniques and to guide future developments
in the field.

</details>


### [10] [On a Dependently Typed Encoding of Matching Logic](https://arxiv.org/abs/2509.13018)
*Ádám Kurucz,Péter Bereczky,Dániel Horpácsi*

Main category: cs.LO

TL;DR: 本文提出了第一个依赖类型化的匹配μ逻辑定义，通过类型索引中的排序上下文确保良好排序性，使不良排序的语法元素无法表示，并保证良好排序元素的语义在其关联排序的域内。


<details>
  <summary>Details</summary>
Motivation: 匹配逻辑是一个通用的形式框架，特别适用于编程语言语义推理。为了进行元理论推理，需要将逻辑表达在基础理论中，选择依赖类型理论可以使对象理论中的良好排序性直接对应于宿主理论中的良好类型性。

Method: 使用依赖类型定义匹配μ逻辑，通过类型索引中的排序上下文来编码排序信息，确保语法元素的良好排序性。

Result: 实现了第一个依赖类型化的匹配μ逻辑定义，使得不良排序的语法元素无法表示，同时保证良好排序元素的语义在其关联排序的域内。

Conclusion: 依赖类型化的方法为匹配μ逻辑提供了严格的排序保证，为形式化语义推理提供了更可靠的基础。

Abstract: Matching logic is a general formal framework for reasoning about a wide range
of theories, with particular emphasis on programming language semantics.
Notably, the intermediate language of the K semantics framework is an extension
of matching $\mu$-logic, a sorted, polyadic variant of the logic. Metatheoretic
reasoning requires the logic to be expressed within a foundational theory;
opting for a dependently typed one enables well-sortedness in the object theory
to correspond directly to well-typedness in the host theory. In this paper, we
present the first dependently typed definition of matching $\mu$-logic,
ensuring well-sortedness via sorted contexts encoded in type indices. As a
result, ill-sorted syntax elements are unrepresentable, and the semantics of
well-sorted elements are guaranteed to lie within the domain of their
associated sort.

</details>


### [11] [The Hidden Strength of Costrong Functors](https://arxiv.org/abs/2509.13026)
*Adriana Balan,Silviu-George Pantelimon*

Main category: cs.LO

TL;DR: 本文探讨了costrong函子的概念，这是strong函子的对偶性质，旨在研究函子与幺半结构交互的新方式及其在计算语义中的应用


<details>
  <summary>Details</summary>
Motivation: strong函子和monad在计算机科学中无处不在，comonad最近在结构化上下文依赖计算概念中展示了其用途，但"being strong"性质的对偶化至今未被充分研究

Method: 探索costrong函子及其自然性质，分析函子如何与幺半结构交互

Result: 提出了costrong函子的概念框架，为理解函子与幺半结构的交互提供了新的视角

Conclusion: costrong函子为计算语义提供了不同的理解方式，这项工作仍在进行中，旨在进一步探索这些概念

Abstract: Strong functors and monads are ubiquitous in Computer Science. More recently,
comonads have demonstrated their use in structuring context-dependent notions
of computation. However, the dualisation of ``being strong'' property passed
somehow unobserved so far. We argue that ``being costrong'' gives a different
understanding of how functors can interact with monoidal structures. This work
in progress aims to explore costrong functors and their natural properties,
with an eye towards the semantics of computations.

</details>


### [12] [Łukasiewicz Logic with Actions for Neural Networks training](https://arxiv.org/abs/2509.13020)
*Ioana Leuştean,Bogdan Macovei*

Main category: cs.LO

TL;DR: 将多层感知机的训练过程建模为三排序混合模态逻辑，其中训练动作作为模态算子，训练过程是逻辑推导序列，并使用Lean 4进行算法实现的逻辑证明认证


<details>
  <summary>Details</summary>
Motivation: 基于多层感知机与带有理系数的Lukasiewicz逻辑之间的已知联系，进一步分析其训练过程的逻辑基础

Method: 使用三排序混合模态逻辑框架：将多层感知机视为逻辑公式，训练动作作为模态算子，训练过程建模为逻辑推导序列，并利用Lean 4证明助手进行算法实现的逻辑认证

Result: 建立了多层感知机训练过程的逻辑形式化模型，并通过Lean 4实现了算法正确性的逻辑证明

Conclusion: 该方法为神经网络训练过程提供了严格的逻辑基础，通过形式化验证确保了训练算法的正确性，为可信AI系统开发提供了新途径

Abstract: Based on the already known connection between multilayer perceptrons and
Lukasiewicz logic with rational coefficients, we take a step forward in
analyzing its training process using a three-sorted hybrid modal logic: a
multilayer perceptron is a logical formula; the actions of the training process
are modal operators; the training process is a sequence of logical deductions.
Using the proof assistant and the programming language Lean 4, the algorithmic
implementation of the training process is certified by logical proofs.

</details>


### [13] [Intuitionistic modal logics: epistemic reasoning with distributed knowledge](https://arxiv.org/abs/2509.13038)
*Philippe Balbiani*

Main category: cs.LO

TL;DR: 本文在Artemov和Protopopescu提出的直觉主义信念逻辑和直觉主义认知逻辑的参数化盒式命题语言中添加了菱形算子，证明了扩展后的直觉主义信念逻辑和具有分布式知识的直觉主义认知逻辑相对于其关系语义的完备性。


<details>
  <summary>Details</summary>
Motivation: 扩展直觉主义信念逻辑和直觉主义认知逻辑的表达能力，通过添加菱形算子来增强原有逻辑系统的表达能力，并建立完备的语义理论。

Method: 在原有的参数化盒式命题语言基础上添加菱形算子，构建扩展的逻辑系统，并为其设计适当的关系语义模型，通过形式化证明方法建立语法和语义之间的对应关系。

Result: 成功证明了扩展后的直觉主义信念逻辑和具有分布式知识的直觉主义认知逻辑相对于其关系语义的完备性，即所有在语义模型中有效的公式都在逻辑系统中可证，反之亦然。

Conclusion: 通过添加菱形算子扩展了直觉主义信念和认知逻辑的表达能力，并建立了完备的语义理论基础，为后续相关逻辑系统的研究提供了重要支撑。

Abstract: In this article, we add a diamond to the parametrized box-based propositional
language of intuitionistic doxastic logic and intuitionistic epistemic logic
introduced by Artemov and Protopopescu. The main results of this article are
the proofs of completeness with respect to their appropriate relational
semantics of the resulting intuitionistic doxastic logic and intuitionistic
epistemic logic with distributed knowledge.

</details>


### [14] [Reducts of fuzzy contexts: Formal concept analysis vs. rough set theory](https://arxiv.org/abs/2509.13059)
*Yuxu Chen,Jing Liu,Lili Shen,Xiaoye Tang*

Main category: cs.LO

TL;DR: 本文基于形式概念分析和粗糙集理论，提出了模糊上下文的约简概念，并证明了在完全剩余格L中，形式概念分析和粗糙集理论的L-上下文约简通过否定可相互定义当且仅当L满足双重否定律。


<details>
  <summary>Details</summary>
Motivation: 将形式概念分析与粗糙集理论中的约简概念扩展到模糊上下文，探索两者之间的内在联系和等价条件。

Method: 基于完全剩余格L，建立L-上下文的约简理论，通过数学证明分析形式概念分析和粗糙集理论中约简概念的等价关系。

Result: 证明了形式概念分析和粗糙集理论的L-上下文约简通过否定可相互定义当且仅当L满足双重否定律。

Conclusion: 双重否定律是连接形式概念分析和粗糙集理论中模糊上下文约简概念的关键条件，为模糊信息处理提供了理论基础。

Abstract: We postulate the intuitive idea of reducts of fuzzy contexts based on formal
concept analysis and rough set theory. For a complete residuated lattice $L$,
it is shown that reducts of $L$-contexts in formal concept analysis are
interdefinable with reducts of $L$-contexts in rough set theory via negation
if, and only if, $L$ satisfies the law of double negation.

</details>


### [15] [Proceedings of the Sixteenth International Symposium on Games, Automata, Logics, and Formal Verification](https://arxiv.org/abs/2509.13258)
*Giorgio Bacci,Adrian Francalanza*

Main category: cs.LO

TL;DR: GandALF 2025会议论文集，聚焦游戏、自动机、逻辑和形式验证领域的研究


<details>
  <summary>Details</summary>
Motivation: 汇集学术界和工业界在游戏、自动机、逻辑和形式验证领域的研究人员，促进理论到应用的广泛主题交流与跨领域融合

Method: 通过国际研讨会形式，组织学术交流会议和论文集出版

Result: 成功举办了第十六届国际研讨会，并出版了包含相关领域最新研究成果的会议论文集

Conclusion: GandALF会议为相关领域研究者提供了重要的交流平台，促进了理论研究和实际应用的交叉融合

Abstract: This volume contains the proceedings of GandALF 2025, the Sixteenth
International Symposium on Games, Automata, Logics, and Formal Verification.
GandALF 2025 took place on 16-17th September 2025, in Valletta, Malta. The aim
of GandALF 2025 is to bring together researchers from academia and industry who
are actively working in the fields of Games, Automata, Logics, and Formal
Verification. The idea is to cover an ample spectrum of themes, ranging from
theory to applications, and stimulate cross-fertilisation.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [16] [Evaluating Large Language Models for Functional and Maintainable Code in Industrial Settings: A Case Study at ASML](https://arxiv.org/abs/2509.12395)
*Yash Mundhra,Max Valk,Maliheh Izadi*

Main category: cs.SE

TL;DR: 本研究探讨了大型语言模型在工业专有代码环境中的代码生成性能，开发了针对ASML专有代码库的评估框架和新基准，提出了build@k评估指标，发现提示技术和模型大小对输出质量有显著影响。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在开源领域的代码生成表现出色，但在工业专有环境中，由于领域特定约束和代码相互依赖性的存在，其适用性尚未得到充分探索。

Method: 与ASML合作进行案例研究，开发定制评估框架和新基准，提出build@k编译集成评估指标，比较不同提示技术、通用与代码专用LLMs的性能，分析模型大小的影响。

Result: 提示技术和模型大小对输出质量有显著影响，few-shot和chain-of-thought提示技术获得最高构建成功率，代码专用LLMs与通用LLMs的性能差异较小且因模型家族而异。

Conclusion: LLMs在工业专有代码环境中具有应用潜力，但需要针对性的评估框架和优化技术来确保生成的代码能够成功编译和集成。

Abstract: Large language models have shown impressive performance in various domains,
including code generation across diverse open-source domains. However, their
applicability in proprietary industrial settings, where domain-specific
constraints and code interdependencies are prevalent, remains largely
unexplored. We present a case study conducted in collaboration with the
leveling department at ASML to investigate the performance of LLMs in
generating functional, maintainable code within a closed, highly specialized
software environment.
  We developed an evaluation framework tailored to ASML's proprietary codebase
and introduced a new benchmark. Additionally, we proposed a new evaluation
metric, build@k, to assess whether LLM-generated code successfully compiles and
integrates within real industrial repositories. We investigate various
prompting techniques, compare the performance of generic and code-specific
LLMs, and examine the impact of model size on code generation capabilities,
using both match-based and execution-based metrics. The findings reveal that
prompting techniques and model size have a significant impact on output
quality, with few-shot and chain-of-thought prompting yielding the highest
build success rates. The difference in performance between the code-specific
LLMs and generic LLMs was less pronounced and varied substantially across
different model families.

</details>


### [17] [Understanding Prompt Management in GitHub Repositories: A Call for Best Practices](https://arxiv.org/abs/2509.12421)
*Hao Li,Hicham Masri,Filipe R. Cogo,Abdul Ali Bangash,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 对24,800个开源提示的实证分析显示，提示管理存在格式不一致、重复问题以及可读性和拼写问题，提出了改进建议


<details>
  <summary>Details</summary>
Motivation: 随着基础模型（如大语言模型）的快速采用，基于自然语言提示构建的软件（promptware）日益普及，但有效的提示管理（如组织和质量保证）至关重要且具有挑战性

Method: 对来自92个GitHub仓库的24,800个开源提示进行实证分析，研究提示管理实践和质量属性

Result: 发现关键挑战：提示格式存在显著不一致、大量内部和外部提示重复、频繁的可读性和拼写问题

Conclusion: 基于研究发现，为开发者在快速发展的promptware生态系统中提高开源提示的可用性和可维护性提供了可行的建议

Abstract: The rapid adoption of foundation models (e.g., large language models) has
given rise to promptware, i.e., software built using natural language prompts.
Effective management of prompts, such as organization and quality assurance, is
essential yet challenging. In this study, we perform an empirical analysis of
24,800 open-source prompts from 92 GitHub repositories to investigate prompt
management practices and quality attributes. Our findings reveal critical
challenges such as considerable inconsistencies in prompt formatting,
substantial internal and external prompt duplication, and frequent readability
and spelling issues. Based on these findings, we provide actionable
recommendations for developers to enhance the usability and maintainability of
open-source prompts within the rapidly evolving promptware ecosystem.

</details>


### [18] [From Legacy Fortran to Portable Kokkos:An Autonomous Agentic AI Workflow](https://arxiv.org/abs/2509.12443)
*Sparsh Gupta,Kamalavasan Kamalakkannan,Maxim Moraru,Galen Shipman,Patrick Diehl*

Main category: cs.SE

TL;DR: 本文提出了一种基于AI代理的工作流，使用专门的大语言模型代理协作将Fortran内核转换为可移植的Kokkos C++程序，实现了科学计算代码的自动化现代化改造。


<details>
  <summary>Details</summary>
Motivation: 随着高性能计算向GPU加速架构转变，许多加速器缺乏原生Fortran绑定，需要将传统Fortran代码现代化以实现跨硬件平台的性能可移植性。手动移植工作量大且需要专业知识。

Method: 采用AI代理工作流，多个专门的LLM代理协作完成翻译、验证、编译、运行、测试、调试和优化等步骤，将Fortran内核转换为可移植的Kokkos C++程序。

Result: 工作流成功现代化了多个基准内核，生成的Kokkos代码在不同硬件分区上实现了性能可移植性。付费OpenAI模型仅需几美元即可完成工作流，生成优化代码性能超过Fortran基线，而开源模型往往无法生成功能代码。

Conclusion: 这项工作证明了AI代理在Fortran到Kokkos转换中的可行性，为自主现代化传统科学应用提供了途径，使其能够在多样化超级计算机上可移植且高效运行，展示了LLM驱动代理系统在科学和系统应用中进行结构化、领域特定推理任务的潜力。

Abstract: Scientific applications continue to rely on legacy Fortran codebases
originally developed for homogeneous, CPU-based systems. As High-Performance
Computing (HPC) shifts toward heterogeneous GPU-accelerated architectures, many
accelerators lack native Fortran bindings, creating an urgent need to modernize
legacy codes for portability. Frameworks like Kokkos provide performance
portability and a single-source C++ abstraction, but manual Fortran-to-Kokkos
porting demands significant expertise and time. Large language models (LLMs)
have shown promise in source-to-source code generation, yet their use in fully
autonomous workflows for translating and optimizing parallel code remains
largely unexplored, especially for performance portability across diverse
hardware.
  This paper presents an agentic AI workflow where specialized LLM "agents"
collaborate to translate, validate, compile, run, test, debug, and optimize
Fortran kernels into portable Kokkos C++ programs. Results show the pipeline
modernizes a range of benchmark kernels, producing performance-portable Kokkos
codes across hardware partitions. Paid OpenAI models such as GPT-5 and
o4-mini-high executed the workflow for only a few U.S. dollars, generating
optimized codes that surpassed Fortran baselines, whereas open-source models
like Llama4-Maverick often failed to yield functional codes.
  This work demonstrates the feasibility of agentic AI for Fortran-to-Kokkos
transformation and offers a pathway for autonomously modernizing legacy
scientific applications to run portably and efficiently on diverse
supercomputers. It further highlights the potential of LLM-driven agentic
systems to perform structured, domain-specific reasoning tasks in scientific
and systems-oriented applications.

</details>


### [19] [Perspectives, Needs and Challenges for Sustainable Software Engineering Teams: A FinServ Case Study](https://arxiv.org/abs/2509.12466)
*Satwik Ghanta,Peggy Gregory,Gul Calikli*

Main category: cs.SE

TL;DR: 该研究通过案例研究发现金融服务公司中高层管理和开发人员对可持续软件工程的理解存在显著差异，管理层关注技术和经济可持续性，而开发者更关注工作负荷和压力等人本因素。


<details>
  <summary>Details</summary>
Motivation: 可持续软件工程(SSE)缺乏针对特定组织背景(如金融服务行业)的研究，这些行业具有数据驱动、严格监管和高交易量的特点，需要理解其独特的可持续性需求。

Method: 采用探索性定性案例研究，通过对金融服务公司6名高管和16名软件工程师进行访谈和焦点小组讨论，涵盖从初级开发人员到团队负责人的不同经验层级。

Result: 研究发现组织层级间对可持续性的认知存在明显分歧：管理层强调技术经济可持续性(云迁移、数据可用性)，开发者关注人本因素(工作负荷管理、压力减少)，且开发者对组织倡议持怀疑态度，认为可能是公关策略。

Conclusion: 组织目标与开发者需求之间的脱节凸显了需要针对具体情境、共同设计的干预措施的重要性，许多参与者倾向于建立专门的可持续性团队，类似于安全治理的内部结构。

Abstract: Sustainable Software Engineering (SSE) is slowly becoming an industry need
for reasons including reputation enhancement, improved profits and more
efficient practices. However, SSE has many definitions, and this is a challenge
for organisations trying to build a common and broadly agreed understanding of
the term. Although much research effort has gone into identifying general SSE
practices, there is a gap in understanding the sustainability needs of specific
organisational contexts, such as financial services, which are highly
data-driven, operate under strict regulatory requirements, and handle millions
of transactions day to day. To address this gap, our research focuses on a
financial services company (FinServCo) that invited us to investigate
perceptions of sustainability in their IT function: how it could be put into
practice, who is responsible for it, and what the challenges are. We conducted
an exploratory qualitative case study using interviews and a focus group with
six higher management employees and 16 software engineers comprising various
experience levels from junior developers to team leaders. Our study found a
clear divergence in how sustainability is perceived between organisational
levels. Higher management emphasised technical and economic sustainability,
focusing on cloud migration and business continuity through data availability.
In contrast, developers highlighted human-centric concerns such as workload
management and stress reduction. Scepticism toward organisational initiatives
was also evident, with some developers viewing them as a PR strategy. Many
participants expressed a preference for a dedicated sustainability team,
drawing analogies to internal structures for security governance. The
disconnect between organisational goals and individual developer needs
highlights the importance of context-sensitive, co-designed interventions.

</details>


### [20] [Good Vibrations? A Qualitative Study of Co-Creation, Communication, Flow, and Trust in Vibe Coding](https://arxiv.org/abs/2509.12491)
*Veronica Pimenova,Sarah Fakhoury,Christian Bird,Margaret-Anne Storey,Madeline Endres*

Main category: cs.SE

TL;DR: 本文首次系统性地定性研究了vibe coding这一新兴AI辅助编程范式，通过分析访谈和社交媒体数据，揭示了开发者如何通过对话式AI交互实现协同创造和心流体验。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注代码产物或理论探讨，缺乏对开发者实际体验vibe coding的实证理解，需要系统研究这一编程范式的实践感知和应用情况。

Method: 采用定性研究方法，分析超过19万字的半结构化访谈、Reddit讨论和LinkedIn帖子，系统调查vibe coding的认知和实践。

Result: 提出了以对话式AI交互、协同创造和开发者心流为核心的vibe coding理论框架，发现AI信任在委托到协同创造的连续体中起调节作用，并识别了规范、可靠性、调试等痛点和最佳实践。

Conclusion: 研究为AI开发工具的未来发展提供了重要启示，并为vibe coding的进一步研究指明了方向，强调了持续心流和开发者体验的重要性。

Abstract: Vibe coding, a term coined by Andrej Karpathy in February 2025, has quickly
become a compelling and controversial natural language programming paradigm in
AI-assisted software development. Centered on iterative co-design with an AI
assistant, vibe coding emphasizes flow and experimentation over strict upfront
specification. While initial studies have begun to explore this paradigm, most
focus on analyzing code artifacts or proposing theories with limited empirical
backing. There remains a need for a grounded understanding of vibe coding as it
is perceived and experienced by developers. We present the first systematic
qualitative investigation of vibe coding perceptions and practice. Drawing on
over 190,000 words from semi-structured interviews, Reddit threads, and
LinkedIn posts, we characterize what vibe coding is, why and how developers use
it, where it breaks down, and which emerging practices aim to support it. We
propose a qualitatively grounded theory of vibe coding centered on
conversational interaction with AI, co-creation, and developer flow and joy. We
find that AI trust regulates movement along a continuum from delegation to
co-creation and supports the developer experience by sustaining flow. We
surface recurring pain points and risks in areas including specification,
reliability, debugging, latency, code review burden, and collaboration. We also
present best practices that have been discovered and shared to mitigate these
challenges. We conclude with implications for the future of AI dev tools and
directions for researchers investigating vibe coding.

</details>


### [21] [Ensembling Large Language Models for Code Vulnerability Detection: An Empirical Evaluation](https://arxiv.org/abs/2509.12629)
*Zhihong Sun,Jia Li,Yao Wan,Chuanyi Li,Hongyu Zhang,Zhi jin,Ge Li,Hong Liu,Chen Lyu,Songlin Hu*

Main category: cs.SE

TL;DR: 本研究探索集成学习在提升大语言模型代码漏洞检测性能方面的潜力，通过多种集成策略和提出的动态门控堆叠方法，显著提高了检测效果，特别是在处理不平衡数据集和多分类任务时表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在代码漏洞检测中存在结果不一致性问题，这种不一致性虽然影响检测稳定性，但也揭示了模型间的潜在互补性，可以通过集成学习来构建更鲁棒的漏洞检测系统。

Method: 使用5个大语言模型（DeepSeek-Coder-6.7B、CodeLlama-7B、CodeLlama-13B、CodeQwen1.5-7B、StarCoder2-15B）和3种集成策略（Bagging、Boosting、Stacking），在3个数据集（Devign、ReVeal、BigVul）上进行实验，并提出了基于混合专家技术的动态门控堆叠（DGS）方法。

Result: 集成方法显著提高了检测性能，Boosting在不平衡数据集场景中表现优异，DGS在传统Stacking基础上表现更好，特别是在处理类别不平衡和多分类任务时。

Conclusion: 集成学习能够有效提升基于大语言模型的漏洞检测系统的可靠性和有效性，为构建更稳健的检测系统提供了有价值的见解。

Abstract: Code vulnerability detection is crucial for ensuring the security and
reliability of modern software systems. Recently, Large Language Models (LLMs)
have shown promising capabilities in this domain. However, notable
discrepancies in detection results often arise when analyzing identical code
segments across different training stages of the same model or among
architecturally distinct LLMs. While such inconsistencies may compromise
detection stability, they also highlight a key opportunity: the latent
complementarity among models can be harnessed through ensemble learning to
create more robust vulnerability detection systems. In this study, we explore
the potential of ensemble learning to enhance the performance of LLMs in source
code vulnerability detection. We conduct comprehensive experiments involving
five LLMs (i.e., DeepSeek-Coder-6.7B, CodeLlama-7B, CodeLlama-13B,
CodeQwen1.5-7B, and StarCoder2-15B), using three ensemble strategies (i.e.,
Bagging, Boosting, and Stacking). These experiments are carried out across
three widely adopted datasets (i.e., Devign, ReVeal, and BigVul). Inspired by
Mixture of Experts (MoE) techniques, we further propose Dynamic Gated Stacking
(DGS), a Stacking variant tailored for vulnerability detection. Our results
demonstrate that ensemble approaches can significantly improve detection
performance, with Boosting excelling in scenarios involving imbalanced
datasets. Moreover, DGS consistently outperforms traditional Stacking,
particularly in handling class imbalance and multi-class classification tasks.
These findings offer valuable insights into building more reliable and
effective LLM-based vulnerability detection systems through ensemble learning.

</details>


### [22] [When Large Language Models Meet UAVs: How Far Are We?](https://arxiv.org/abs/2509.12795)
*Yihua Chen,Xingle Que,Jiashuo Zhang,Ting Chen,Guangshun Li,Jiachi Chen*

Main category: cs.SE

TL;DR: 本研究通过分析74篇论文和56个GitHub项目，发现学术界与工业界在LLM-UAV融合应用上存在差异，识别了9种任务类型和阻碍实际应用的关键因素。


<details>
  <summary>Details</summary>
Motivation: UAV与LLM的融合具有巨大潜力，但现有研究多为初步探索，缺乏对实际应用的深入理解，存在学术研究与实际需求脱节的风险。

Method: 采用实证研究方法，分析74篇学术论文和56个公开GitHub项目，识别LLM在UAV系统中的任务类型分布，并通过在线问卷获取52份有效反馈了解行业视角。

Result: 发现学术界侧重理论建模和任务优化，而工业界更关注飞行控制、任务规划和人机交互；40.4%的从业者尝试过将LLM应用于UAV任务，识别出技术成熟度、性能、安全、成本等阻碍因素。

Conclusion: 研究揭示了学术与工业实践的差距，提出了未来发展的挑战和建议，为LLM-UAV系统的实际应用提供了重要指导。

Abstract: The integration of unmanned aerial vehicles (UAVs) and large language models
(LLMs) has emerged as a research direction of growing interest, with the
potential to address challenges in autonomous decision-making, human-UAV
interaction, and real-time adaptability. However, existing studies have
remained largely in preliminary exploration with a limited understanding of
real-world practice, risking a misalignment between academic research and
practical needs and hindering the translation of results. To examine and
address these potential challenges, we conducted an empirical study of 74
selected papers and 56 public GitHub projects, identified nine task types for
LLMs in UAV systems, and quantified their distribution. Our findings show that
academic research emphasizes theoretical modeling and task optimization with
dispersed attention across tasks. In contrast, industrial projects focus on
flight control, task planning, and human-machine interaction, prioritizing
operability and efficiency. To further capture industry perspectives, we
distributed an online questionnaire. We obtained 52 valid responses: 40.4% of
practitioners have attempted to apply LLMs to UAV tasks. We further identify
factors that impede real-world integration, including technological maturity,
performance, safety, cost, and other considerations. Finally, we highlight
challenges for future development and provide recommendations.

</details>


### [23] [LLM-Based Approach for Enhancing Maintainability of Automotive Architectures](https://arxiv.org/abs/2509.12798)
*Nenad Petrovic,Lukasz Mazur,Alois Knoll*

Main category: cs.SE

TL;DR: 本文探讨了大型语言模型在提高汽车系统灵活性方面的潜力，通过三个案例研究展示了LLMs在自动化更新、硬件抽象、合规性检查、接口兼容性验证和架构修改建议方面的应用。


<details>
  <summary>Details</summary>
Motivation: 汽车系统存在许多降低灵活性的瓶颈，包括长期维护困难、后期更新扩展复杂、重工程序冗长、标准化合规流程繁琐，以及设备和软件组件的异构性和数量庞大。

Method: 采用OpenAI的GPT-4o模型进行概念验证实现，通过三个案例研究：1)更新、硬件抽象和合规性检查；2)接口兼容性验证；3)架构修改建议。

Result: 早期研究表明LLMs在自动化汽车系统相关任务方面具有潜力，能够提高系统灵活性并简化维护和更新流程。

Conclusion: 大型语言模型在汽车系统自动化任务中展现出应用前景，有望解决传统汽车系统面临的灵活性和维护挑战，但仍需进一步研究验证。

Abstract: There are many bottlenecks that decrease the flexibility of automotive
systems, making their long-term maintenance, as well as updates and extensions
in later lifecycle phases increasingly difficult, mainly due to long
re-engineering, standardization, and compliance procedures, as well as
heterogeneity and numerosity of devices and underlying software components
involved. In this paper, we explore the potential of Large Language Models
(LLMs) when it comes to the automation of tasks and processes that aim to
increase the flexibility of automotive systems. Three case studies towards
achieving this goal are considered as outcomes of early-stage research: 1)
updates, hardware abstraction, and compliance, 2) interface compatibility
checking, and 3) architecture modification suggestions. For proof-of-concept
implementation, we rely on OpenAI's GPT-4o model.

</details>


### [24] [SateLight: A Satellite Application Update Framework for Satellite Computing](https://arxiv.org/abs/2509.12809)
*Jinfeng Wen,Jianshu Zhao,Zixi Zhu,Xiaomin Zhang,Qi Liang,Ao Zhou,Shangguang Wang*

Main category: cs.SE

TL;DR: SateLight是一个针对卫星计算的应用更新框架，通过容器化、差分更新和容错机制，显著减少传输延迟并确保更新可靠性


<details>
  <summary>Details</summary>
Motivation: 卫星计算需要应用软件更新，但现有地面系统更新方法无法应对卫星环境的异构性、带宽限制和恶劣空间条件

Method: 采用容器化封装异构应用，集成内容感知差分策略减少通信量，细粒度星上更新设计重构目标应用，以及基于层的容错恢复机制

Result: 在卫星模拟环境中，传输延迟降低最高91.18%（平均56.54%），所有评估应用实现100%更新正确性，真实在轨卫星案例验证了实用性

Conclusion: SateLight为卫星计算提供了一种实用有效的应用更新解决方案，解决了卫星环境下的特殊约束问题

Abstract: Satellite computing is an emerging paradigm that empowers satellites to
perform onboard processing tasks (i.e., \textit{satellite applications}),
thereby reducing reliance on ground-based systems and improving responsiveness.
However, enabling application software updates in this context remains a
fundamental challenge due to application heterogeneity, limited
ground-to-satellite bandwidth, and harsh space conditions. Existing software
update approaches, designed primarily for terrestrial systems, fail to address
these constraints, as they assume abundant computational capacity and stable
connectivity.
  To address this gap, we propose SateLight, a practical and effective
satellite application update framework tailored for satellite computing.
SateLight leverages containerization to encapsulate heterogeneous applications,
enabling efficient deployment and maintenance. SateLight further integrates
three capabilities: (1) a content-aware differential strategy that minimizes
communication data volume, (2) a fine-grained onboard update design that
reconstructs target applications, and (3) a layer-based fault-tolerant recovery
mechanism to ensure reliability under failure-prone space conditions.
Experimental results on a satellite simulation environment with 10
representative satellite applications demonstrate that SateLight reduces
transmission latency by up to 91.18% (average 56.54%) compared to the best
currently available baseline. It also consistently ensures 100% update
correctness across all evaluated applications. Furthermore, a case study on a
real-world in-orbit satellite demonstrates the practicality of our approach.

</details>


### [25] [Evaluating Large Language Models for Code Translation: Effects of Prompt Language and Prompt Design](https://arxiv.org/abs/2509.12973)
*Aamer Aljagthami,Mohammed Banabila,Musab Alshehri,Mohammed Kabini,Mohammad D. Alahmadi*

Main category: cs.SE

TL;DR: 本研究系统评估了大型语言模型在C++、Java、Python和C#之间的代码翻译性能，发现详细提示和英语提示能显著提升翻译质量，所有LLM均优于传统TransCoder基准。


<details>
  <summary>Details</summary>
Motivation: 当前关于模型选择、提示设计和提示语言如何影响多编程语言代码翻译质量的比较证据有限，需要系统评估LLM在代码翻译中的表现。

Method: 使用BLEU和CodeBLEU指标，在两种提示风格（简洁指令和详细规范）和两种提示语言（英语和阿拉伯语）下，对最先进的LLM进行跨语言对的代码翻译评估。

Result: 详细提示在所有模型和翻译方向上均带来一致提升，英语提示比阿拉伯语提示性能高13-15%。最佳模型在Java到C#和Python到C++等挑战性语言对上获得最高CodeBLEU分数，所有LLM均超越传统TransCoder基准。

Conclusion: 研究证明了精心设计的提示工程和提示语言选择的重要性，为软件现代化和跨语言互操作性提供了实用指导。

Abstract: Large language models (LLMs) have shown promise for automated source-code
translation, a capability critical to software migration, maintenance, and
interoperability. Yet comparative evidence on how model choice, prompt design,
and prompt language shape translation quality across multiple programming
languages remains limited. This study conducts a systematic empirical
assessment of state-of-the-art LLMs for code translation among C++, Java,
Python, and C#, alongside a traditional baseline (TransCoder). Using BLEU and
CodeBLEU, we quantify syntactic fidelity and structural correctness under two
prompt styles (concise instruction and detailed specification) and two prompt
languages (English and Arabic), with direction-aware evaluation across language
pairs. Experiments show that detailed prompts deliver consistent gains across
models and translation directions, and English prompts outperform Arabic by
13-15%. The top-performing model attains the highest CodeBLEU on challenging
pairs such as Java to C# and Python to C++. Our evaluation shows that each LLM
outperforms TransCoder across the benchmark. These results demonstrate the
value of careful prompt engineering and prompt language choice, and provide
practical guidance for software modernization and cross-language
interoperability.

</details>


### [26] [Validating Solidity Code Defects using Symbolic and Concrete Execution powered by Large Language Models](https://arxiv.org/abs/2509.13023)
*Ştefan-Claudiu Susan,Andrei Arusoaie,Dorel Lucanu*

Main category: cs.SE

TL;DR: 提出了一种结合Slither检测器、LLMs、Kontrol和Forge的新型智能合约漏洞检测管道，能够可靠检测缺陷并生成证明，显著减少误报和人工验证负担。


<details>
  <summary>Details</summary>
Motivation: 静态分析工具和大型语言模型在Solidity智能合约漏洞检测中存在高误报率，需要能够正式或经验证明缺陷存在的方法。

Method: 集成自定义Slither检测器、LLMs、Kontrol和Forge的检测管道，通过符号执行或具体执行来正确分类代码故障。

Result: 对七种关键缺陷类型进行了实验，展示了在重入攻击、复杂回退函数和错误访问控制策略等三种漏洞上的有效性。

Conclusion: 虽然存在LLMs不一致性和成本等潜在限制，但该方法为结合启发式分析和形式验证提供了稳健框架，可实现更可靠的智能合约自动化审计。

Abstract: The high rate of false alarms from static analysis tools and Large Language
Models (LLMs) complicates vulnerability detection in Solidity Smart Contracts,
demanding methods that can formally or empirically prove the presence of
defects. This paper introduces a novel detection pipeline that integrates
custom Slither-based detectors, LLMs, Kontrol, and Forge. Our approach is
designed to reliably detect defects and generate proofs. We currently perform
experiments with promising results for seven types of critical defects. We
demonstrate the pipeline's efficacy by presenting our findings for three
vulnerabilities -- Reentrancy, Complex Fallback, and Faulty Access Control
Policies -- that are challenging for current verification solutions, which
often generate false alarms or fail to detect them entirely. We highlight the
potential of either symbolic or concrete execution in correctly classifying
such code faults. By chaining these instruments, our method effectively
validates true positives, significantly reducing the manual verification
burden. Although we identify potential limitations, such as the inconsistency
and the cost of LLMs, our findings establish a robust framework for combining
heuristic analysis with formal verification to achieve more reliable and
automated smart contract auditing.

</details>


### [27] [GView: A Survey of Binary Forensics via Visual, Semantic, and AI-Enhanced Analysis](https://arxiv.org/abs/2509.13025)
*Raul Zaharia,Dragoş Gavriluţ,Gheorghiţă Mutu*

Main category: cs.SE

TL;DR: GView是一个开源取证分析框架，结合可视化与AI增强推理，利用大语言模型动态改进取证工作流程，通过逻辑推理和可扩展架构连接实践与学术研究。


<details>
  <summary>Details</summary>
Motivation: 网络安全威胁日益复杂多样，需要更先进的取证分析工具来应对不断增长的威胁数量和复杂性。

Method: 开发开源取证分析框架GView，集成大语言模型进行动态推理增强，采用谓词和推理规则进行逻辑推理，支持对分析文档和用户行为的智能建议。

Result: GView已发展成为连接实践取证与学术研究的桥梁工具，具有可扩展架构，能够显著简化取证工作流程。

Conclusion: GView框架通过AI增强和可视化推理，有效应对现代网络安全威胁的复杂性，为取证分析提供了创新的解决方案。

Abstract: Cybersecurity threats continue to become more sophisticated and diverse in
their artifacts, boosting both their volume and complexity. To overcome those
challenges, we present GView, an open-source forensic analysis framework with
visual and AI-enhanced reasoning. It started with focus on the practical
cybersecurity industry. It has evolved significantly, incorporating large
language models (LLMs) to dynamically enhance reasoning and ease the forensic
workflows. This paper surveys both the current state of GView with its
published papers alongside those that are in the publishing process. It also
includes its innovative use of logical inference through predicates and
inference rules for both the analyzed documents and the user's actions for
better suggestions. We highlight the extensible architecture, showcasing its
potential as a bridge between the practical forensics worlds with the academic
research.

</details>


### [28] [Automating Code Generation for Semiconductor Equipment Control from Developer Utterances with LLMs](https://arxiv.org/abs/2509.13055)
*Youngkyoung Kim,Sanghyeok Park,Misoo Kim,Gangho Yoon,Eunseok Lee,Simon S. Woo*

Main category: cs.SE

TL;DR: 提出渐进式知识增强(PKE)框架，通过多阶段提示逐步提取LLM潜在知识，显著提升半导体设备语言ALPG代码生成准确率


<details>
  <summary>Details</summary>
Motivation: 半导体设备编程语言(如ALPG)语法底层、学习曲线陡峭，现有LLM在生成这类低级设备语言代码方面效果有限

Method: 渐进式知识增强(PKE)框架，通过从简单到复杂的多阶段提示逐步激活LLM的潜在知识，无需大量微调

Result: 在工业ALPG数据集上，PKE显著优于标准提示方法，比次优技术高出11.1%和15.2%的精确匹配分数

Conclusion: PKE为提升LLM在专业低级编程方面的能力提供了实用方法，支持半导体软件开发生产力的提升

Abstract: Semiconductors form the backbone of modern electronics, with their
manufacturing and testing relying on highly specialized equipment and
domain-specific programming languages. Equipment languages such as the
Algorithmic Pattern Generator (ALPG) are critical for precise hardware control
but are challenging to program due to their low-level syntax and steep learning
curve. While large language models (LLMs) have shown promise in generating
high-level code from natural language, their effectiveness on low-level
equipment languages remains limited. To address this, we propose Progressive
Knowledge Enhancement (PKE), a novel multi-stage prompting framework that
progressively extracts and activates the latent knowledge within LLMs, guiding
them from simple to complex examples without extensive fine-tuning. Empirical
evaluation on an industrial ALPG dataset shows that PKE significantly
outperforms standard prompting and surpasses state-of-the-art methods in
generating correct ALPG code, achieving 11.1\% and 15.2\% higher exact match
scores compared to the second-best technique. Further analysis of individual
components confirms that progressive knowledge extraction based on difficulty
enhances accuracy. Our study offer a practical approach to boosting LLM
capabilities for specialized low-level programming, supporting greater
productivity in semiconductor software development.

</details>


### [29] [Accelerating Discovery: Rapid Literature Screening with LLMs](https://arxiv.org/abs/2509.13103)
*Santiago Matalonga,Domenico Amalfitano,Jean Carlo Rossa Hauck,Martín Solari,Guilherme H. Travassos*

Main category: cs.SE

TL;DR: 开发了一个基于LLM的工具来支持多声部文献综述中的文档搜索和筛选，该工具在识别不相关文献方面与人工研究者有90%的一致性，能够节省研究人员时间用于更高级别的抽象任务。


<details>
  <summary>Details</summary>
Motivation: 多声部文献综述(MVLR)过程耗时耗力，研究人员需要审查和筛选大量非结构化文档，其中很多信息稀疏且不太可能被纳入最终研究。在航空电子领域的上下文感知软件系统测试MVLR中，需要审查超过8000份高度异构的文档，因此需要开发LLM助手来支持文档搜索和筛选。

Method: 应用健全的工程实践开发基于本地部署的LLM工具，整合检索增强生成(RAG)技术处理候选文献源。使用阳性一致性百分比(PPA)作为主要指标量化工具性能，通过便利抽样、人工判断和统计抽样来验证工具的使用质量。

Result: 当前工具在识别与研究不相关的文献源方面，与人工研究者的一致性达到90%。分享了开发细节以支持工具的领域特定适配。

Conclusion: 使用基于LLM的工具支持学术研究人员进行严谨的MVLR是可行的，这些工具可以释放宝贵时间用于更高级别的抽象任务，但研究人员的参与仍然是确保工具支持彻底研究的关键。

Abstract: Background: Conducting Multi Vocal Literature Reviews (MVLRs) is often time
and effort-intensive. Researchers must review and filter a large number of
unstructured sources, which frequently contain sparse information and are
unlikely to be included in the final study. Our experience conducting an MVLR
on Context-Aware Software Systems (CASS) Testing in the avionics domain
exemplified this challenge, with over 8,000 highly heterogeneous documents
requiring review. Therefore, we developed a Large Language Model (LLM)
assistant to support the search and filtering of documents. Aims: To develop
and validate an LLM based tool that can support researchers in performing the
search and filtering of documents for an MVLR without compromising the rigor of
the research protocol. Method: We applied sound engineering practices to
develop an on-premises LLM-based tool incorporating Retrieval Augmented
Generation (RAG) to process candidate sources. Progress towards the aim was
quantified using the Positive Percent Agreement (PPA) as the primary metric to
ensure the performance of the LLM based tool. Convenience sampling, supported
by human judgment and statistical sampling, were used to verify and validate
the tool's quality-in-use. Results: The tool currently demonstrates a PPA
agreement with human researchers of 90% for sources that are not relevant to
the study. Development details are shared to support domain-specific adaptation
of the tool. Conclusions: Using LLM-based tools to support academic researchers
in rigorous MVLR is feasible. These tools can free valuable time for
higher-level, abstract tasks. However, researcher participation remains
essential to ensure that the tool supports thorough research.

</details>


### [30] [Vulnerability Patching Across Software Products and Software Components: A Case Study of Red Hat's Product Portfolio](https://arxiv.org/abs/2509.13117)
*Jukka Ruohonen,Sani Abdullahi,Abhishek Tiwari*

Main category: cs.SE

TL;DR: 对Red Hat产品1999-2024年漏洞修复时间序列分析显示，易受攻击产品和组件数量不稳定，呈线性增长趋势，但与整体漏洞趋势不一致，且存在转折点表明安全债务增长可能趋于稳定。


<details>
  <summary>Details</summary>
Motivation: 基于软件维护和安全债务概念，研究Red Hat产品漏洞修复的时间序列特征，了解安全债务的演变趋势。

Method: 采用分段回归分析方法对Red Hat产品1999-2024年间的漏洞修复数据进行时间序列分析。

Result: 易受攻击产品和组件数量不稳定，多数序列呈现线性增长趋势，但与整体漏洞趋势不一致，存在明显转折点。

Conclusion: 线性趋势并非普遍适用，安全债务的增长可能正在趋于稳定，需要持续监控和管理安全债务。

Abstract: Motivated by software maintenance and the more recent concept of security
debt, the paper presents a time series analysis of vulnerability patching of
Red Hat's products and components between 1999 and 2024. According to the
results based on segmented regression analysis, the amounts of vulnerable
products and components have not been stable; a linear trend describes many of
the series well. Nor do the amounts align well with trends characterizing
vulnerabilities in general. There are also visible breakpoints indicating that
the linear trend is not universally applicable and that the growing security
debt may be stabilizing.

</details>


### [31] [Optimizing Code Embeddings and ML Classifiers for Python Source Code Vulnerability Detection](https://arxiv.org/abs/2509.13134)
*Talaya Farasat,Joachim Posegga*

Main category: cs.SE

TL;DR: 本研究评估了Word2Vec、CodeBERT和GraphCodeBERT三种代码嵌入技术与BiLSTM和CNN两种深度学习分类器在Python源代码漏洞检测中的组合效果。研究发现，尽管GraphCodeBERT+CNN表现良好，但Word2Vec+BiLSTM组合始终获得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 随着源代码复杂性和规模的增加，手动漏洞检测变得不切实际，需要研究自动化的机器学习方法。本研究旨在寻找代码嵌入技术和机器学习分类器的最佳组合，以提高Python源代码漏洞检测的效果。

Method: 评估了三种代码嵌入技术（Word2Vec、CodeBERT、GraphCodeBERT）和两种深度学习分类器（双向长短期记忆网络BiLSTM、卷积神经网络CNN）的不同组合。

Result: CNN与GraphCodeBERT组合表现良好，但BiLSTM与Word2Vec组合始终获得最优的整体性能，表明经典嵌入技术与序列模型的组合仍具有优势。

Conclusion: 选择合适的嵌入技术和分类器组合对于提高自动化漏洞检测系统的有效性至关重要，特别是在Python源代码检测中，经典方法如Word2Vec与BiLSTM的组合仍能提供稳定优势。

Abstract: In recent years, the growing complexity and scale of source code have
rendered manual software vulnerability detection increasingly impractical. To
address this challenge, automated approaches leveraging machine learning and
code embeddings have gained substantial attention. This study investigates the
optimal combination of code embedding techniques and machine learning
classifiers for vulnerability detection in Python source code. We evaluate
three embedding techniques, i.e., Word2Vec, CodeBERT, and GraphCodeBERT
alongside two deep learning classifiers, i.e., Bidirectional Long Short-Term
Memory (BiLSTM) networks and Convolutional Neural Networks (CNN). While CNN
paired with GraphCodeBERT exhibits strong performance, the BiLSTM model using
Word2Vec consistently achieves superior overall results. These findings suggest
that, despite the advanced architectures of recent models like CodeBERT and
GraphCodeBERT, classical embeddings such as Word2Vec, when used with
sequence-based models like BiLSTM, can offer a slight yet consistent
performance advantage. The study underscores the critical importance of
selecting appropriate combinations of embeddings and classifiers to enhance the
effectiveness of automated vulnerability detection systems, particularly for
Python source code.

</details>


### [32] [Towards the Next Generation of Software: Insights from Grey Literature on AI-Native Applications](https://arxiv.org/abs/2509.13144)
*Lingli Cao,Shanshan Li,Ying Fan,Danyang Li,Chenxing Zhong*

Main category: cs.SE

TL;DR: 本研究通过灰色文献综述首次提出了AI原生应用的双层工程蓝图，明确了其以AI为核心智能范式和概率性本质的两大支柱特征，识别了关键质量属性和典型技术栈。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，AI原生应用作为一种新的软件工程范式正在兴起，但目前缺乏统一的工程定义和架构蓝图，从业者缺乏系统性的设计、质量保证和技术选择指导。

Method: 采用灰色文献综述方法，结合Google和Bing搜索获取的概念视角与GitHub上领先开源项目的实践洞察，通过包含来源选择、质量评估和主题分析的结构化协议对异构来源进行综合分析。

Result: 基于选择标准最终确定了106项研究，分析发现AI原生应用具有两大核心支柱：AI作为系统智能范式的核心作用及其固有的概率性、非确定性本质。关键质量属性包括可靠性、可用性、性能效率和AI特定的可观测性。典型技术栈包括LLM编排框架、向量数据库和AI原生可观测性平台。

Conclusion: 本研究首次提出了AI原生应用的双层工程蓝图，为从业者提供了系统性的设计指导，强调了响应质量、成本效益和结果可预测性等区别于传统软件系统的特点。

Abstract: Background: The rapid advancement of large language models (LLMs) has given
rise to AI-native applications, a new paradigm in software engineering that
fundamentally redefines how software is designed, developed, and evolved.
Despite their growing prominence, AI-native applications still lack a unified
engineering definition and architectural blueprint, leaving practitioners
without systematic guidance for system design, quality assurance, and
technology selection.
  Objective: This study seeks to establish a comprehensive understanding of
AI-native applications by identifying their defining characteristics, key
quality attributes, and typical technology stacks, as well as by clarifying the
opportunities and challenges they present.
  Method: We conducted a grey literature review, integrating conceptual
perspectives retrieved from targeted Google and Bing searches with practical
insights derived from leading open-source projects on GitHub. A structured
protocol encompassing source selection, quality assessment, and thematic
analysis was applied to synthesize findings across heterogeneous sources.
  Results: We finally identified 106 studies based on the selection criteria.
The analysis reveals that AI-native applications are distinguished by two core
pillars: the central role of AI as the system's intelligence paradigm and their
inherently probabilistic, non-deterministic nature. Critical quality attributes
include reliability, usability, performance efficiency, and AI-specific
observability. In addition, a typical technology stack has begun to emerge,
comprising LLM orchestration frameworks, vector databases, and AI-native
observability platforms. These systems emphasize response quality,
cost-effectiveness, and outcome predictability, setting them apart from
conventional software systems.
  Conclusion: This study is the first to propose a dual-layered engineering
blueprint...

</details>
