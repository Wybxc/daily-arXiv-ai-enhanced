<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 22]
- [cs.LO](#cs.LO) [Total: 4]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Autonomous QA Agent: A Retrieval-Augmented Framework for Reliable Selenium Script Generation](https://arxiv.org/abs/2601.06034)
*Dudekula Kasim Vali*

Main category: cs.SE

TL;DR: 基于RAG的自主QA代理，通过项目文档和HTML结构来生成Selenium测试脚本，显著减少LLM幻觉，在电商测试场景中达到100%语法有效性和90%执行成功率。


<details>
  <summary>Details</summary>
Motivation: 软件测试中，将需求转化为可执行测试脚本的过程通常是手动且容易出错的。虽然大语言模型可以生成代码，但经常会产生不存在的UI元素幻觉。

Method: 提出了自主QA代理，这是一个基于检索增强生成(RAG)的系统，通过将多种格式文档（Markdown、PDF、HTML）存入向量数据库，在生成Selenium脚本前检索相关上下文，确保生成基于实际DOM结构。

Result: 在20个电商测试场景中，RAG方法实现了100%（20/20）的语法有效性和90%（18/20，95% CI：[85%，95%]，p < 0.001）的执行成功率，而标准LLM生成只有30%成功率。

Conclusion: 虽然评估仅限于单一领域，但该方法通过将生成过程基于实际DOM结构，显著减少了幻觉，展示了RAG在自动化UI测试中的潜力。

Abstract: Software testing is critical in the software development lifecycle, yet translating requirements into executable test scripts remains manual and error-prone. While Large Language Models (LLMs) can generate code, they often hallucinate non-existent UI elements. We present the Autonomous QA Agent, a Retrieval-Augmented Generation (RAG) system that grounds Selenium script generation in project-specific documentation and HTML structure. By ingesting diverse formats (Markdown, PDF, HTML) into a vector database, our system retrieves relevant context before generation. Evaluation on 20 e-commerce test scenarios shows our RAG approach achieves 100% (20/20) syntax validity and 90% (18/20, 95% CI: [85%, 95%], p < 0.001) execution success, compared to 30% for standard LLM generation. While our evaluation is limited to a single domain, our method significantly reduces hallucinations by grounding generation in actual DOM structure, demonstrating RAG's potential for automated UI testing.

</details>


### [2] [Contract2Plan: Verified Contract-Grounded Retrieval-Augmented Optimization for BOM-Aware Procurement and Multi-Echelon Inventory Planning](https://arxiv.org/abs/2601.06164)
*Sahil Agarwal*

Main category: cs.SE

TL;DR: Contract2Plan：一个经过验证的GenAI到优化器管道，在合同条款提取后加入求解器验证门，确保采购和库存计划的可行性和合规性


<details>
  <summary>Details</summary>
Motivation: 传统基于LLM的合同条款提取方法存在脆弱性，容易产生遗漏条款、单位错误和未解决冲突等问题，导致不可行计划或合同违规，特别是在BOM耦合的情况下

Method: 系统通过检索带溯源的条款证据，提取类型化约束模式，将约束编译为BOM感知的MILP模型，并使用求解器诊断验证基础、资格、一致性和可行性，触发针对性修复或弃权

Result: 在包含500个实例的合成微基准测试中，仅提取方法显示出重尾后悔分布和非平凡的MOQ违规发生率，验证了验证机制的必要性

Conclusion: 验证应成为合同基础规划系统的首要组成部分，系统形式化了哪些条款类别允许保守修复并保证合同安全可行性，哪些需要人工确认

Abstract: Procurement and inventory planning is governed not only by demand forecasts and bills of materials (BOMs), but also by operational terms in contracts and supplier documents (e.g., MOQs, lead times, price tiers, allocation caps, substitution approvals). LLM-based extraction can speed up structuring these terms, but extraction-only or LLM-only decision pipelines are brittle: missed clauses, unit errors, and unresolved conflicts can yield infeasible plans or silent contract violations, amplified by BOM coupling. We introduce Contract2Plan, a verified GenAI-to-optimizer pipeline that inserts a solver-based compliance gate before plans are emitted. The system retrieves clause evidence with provenance, extracts a typed constraint schema with evidence spans, compiles constraints into a BOM-aware MILP, and verifies grounding, eligibility, consistency, and feasibility using solver diagnostics, triggering targeted repair or abstention when automation is unsafe. We formalize which clause classes admit conservative repair with contract-safe feasibility guarantees and which require human confirmation. A self-contained synthetic micro-benchmark (500 instances; T=5) computed by exact enumeration under an execution model with MOQ uplift and emergency purchases shows heavy-tailed regret and nontrivial MOQ-violation incidence for extraction-only planning, motivating verification as a first-class component of contract-grounded planning systems.

</details>


### [3] [Attention Mechanism and Heuristic Approach: Context-Aware File Ranking Using Multi-Head Self-Attention](https://arxiv.org/abs/2601.06185)
*Pradeep Kumar Sharma,Shantanu Godbole,Sarada Prasad Jena,Hritvik Shrivastava*

Main category: cs.SE

TL;DR: 使用多头自注意力机制改进软件变更影响分析中的文件排名，将Top-50召回率从62-65%提升至78-82%


<details>
  <summary>Details</summary>
Motivation: 现有确定性方法在变更影响分析中虽然有效，但召回率存在瓶颈，因为它们将特征视为线性独立，忽略了专家推理模式中的上下文依赖关系和度量间关系

Method: 提出将多头自注意力作为后确定性评分细化机制，学习特征间的上下文权重，根据候选文件集的关系行为动态调整每个文件的重要性，注意力机制产生上下文感知调整并与确定性分数相加

Result: 在200个测试案例上，自注意力机制将Top-50召回率从约62-65%提升至78-82%（取决于仓库复杂性），在Top-50文件中达到80%召回率，专家验证显示主观准确性对齐从6.5/10提升至8.6/10

Conclusion: 该方法通过上下文感知的注意力机制弥合了确定性自动化与专家判断之间的推理能力差距，提高了仓库感知工作量估计中的召回率，同时保持了可解释性

Abstract: The identification and ranking of impacted files within software reposi-tories is a key challenge in change impact analysis. Existing deterministic approaches that combine heuristic signals, semantic similarity measures, and graph-based centrality metrics have demonstrated effectiveness in nar-rowing candidate search spaces, yet their recall plateaus. This limitation stems from the treatment of features as linearly independent contributors, ignoring contextual dependencies and relationships between metrics that characterize expert reasoning patterns. To address this limitation, we propose the application of Multi-Head Self-Attention as a post-deterministic scoring refinement mechanism. Our approach learns contextual weighting between features, dynamically adjust-ing importance levels per file based on relational behavior exhibited across candidate file sets. The attention mechanism produces context-aware adjustments that are additively combined with deterministic scores, pre-serving interpretability while enabling reasoning similar to that performed by experts when reviewing change surfaces. We focus on recall rather than precision, as false negatives (missing impacted files) are far more costly than false positives (irrelevant files that can be quickly dismissed during review). Empirical evaluation on 200 test cases demonstrates that the introduc-tion of self-attention improves Top-50 recall from approximately 62-65% to between 78-82% depending on repository complexity and structure, achiev-ing 80% recall at Top-50 files. Expert validation yields improvement from 6.5/10 to 8.6/10 in subjective accuracy alignment. This transformation bridges the reasoning capability gap between deterministic automation and expert judgment, improving recall in repository-aware effort estimation.

</details>


### [4] [RiskBridge: Turning CVEs into Business-Aligned Patch Priorities](https://arxiv.org/abs/2601.06201)
*Yelena Mujibur Sheikh,Awez Akhtar Khatik,Luoxi Tang,Yuqiao Meng,Zhaohan Xi*

Main category: cs.SE

TL;DR: RiskBridge：一个可解释、合规感知的漏洞管理框架，整合CVSS、EPSS和CISA KEV等多源情报，通过概率零日暴露模拟和政策即代码引擎，实现动态、业务对齐的补丁优先级排序，相比现有商业基线显著降低风险并提升修复效率。


<details>
  <summary>Details</summary>
Motivation: 企业面临前所未有的网络安全漏洞激增，每月有数千个新CVE披露。传统CVSS等优先级框架提供静态严重性指标，无法考虑利用概率、合规紧迫性和运营影响，导致修复效率低下和延迟。

Method: RiskBridge整合CVSS v4、EPSS和CISA KEV多源情报，采用概率零日暴露模拟(ZDES)模型预测近期利用可能性，政策即代码引擎将PCI DSS、NIST SP 800-53等监管要求转换为自动化SLA逻辑，以及ROI驱动的优化器最大化每个修复工作的累积风险降低。

Result: 使用实时CVE数据集的实验评估显示：相比最先进的商业基线，RiskBridge实现88%的残余风险降低、18天的SLA合规改进和35%的修复效率提升。

Conclusion: RiskBridge是一个实用且可审计的决策智能系统，统一了概率建模、合规推理和优化分析，代表了现代企业环境中自动化、可解释和业务中心化漏洞管理的重要进展。

Abstract: Enterprises are confronted with an unprece- dented escalation in cybersecurity vulnerabil- ities, with thousands of new CVEs disclosed each month. Conventional prioritization frame- works such as CVSS offer static severity met- rics that fail to account for exploit probabil- ity, compliance urgency, and operational im- pact, resulting in inefficient and delayed re- mediation. This paper introduces RiskBridge, an explainable and compliance-aware vulner- ability management framework that integrates multi-source intelligence from CVSS v4, EPSS, and CISA KEV to produce dynamic, business- aligned patch priorities. RiskBridge employs a probabilistic Zero-Day Exposure Simulation (ZDES) model to fore- cast near-term exploit likelihood, a Policy-as- Code Engine to translate regulatory mandates (e.g., PCI DSS, NIST SP 800-53) into auto- mated SLA logic, and an ROI-driven Opti- mizer to maximize cumulative risk reduction per remediation effort. Experimental evalua- tions using live CVE datasets demonstrate an 88% reduction in residual risk, an 18-day improvement in SLA compliance, and a 35% increase in remediation efficiency compared to state-of-the-art commercial baselines. These findings validate RiskBridge as a prac- tical and auditable decision-intelligence sys- tem that unifies probabilistic modeling, com- pliance reasoning, and optimization analytics. The framework represents a step toward auto- mated, explainable, and business-centric vul- nerability management in modern enterprise environments

</details>


### [5] [Self-Admitted Technical Debt in LLM Software: An Empirical Comparison with ML and Non-ML Software](https://arxiv.org/abs/2601.06266)
*Niruthiha Selvanayagam,Manel Abdellatif,Taher A. Ghaleb*

Main category: cs.SE

TL;DR: 首次对LLM时代自承认技术债务(SATD)的实证研究，发现LLM系统技术债务积累率与ML系统相似，但保持无债务状态时间更长，并识别出三种LLM特有的新债务类型。


<details>
  <summary>Details</summary>
Motivation: 虽然SATD在ML和非ML软件中已有广泛研究，但LLM系统在架构、工作流程和依赖关系上与传统软件及前LLM时代的ML软件存在根本差异，目前对LLM系统中SATD的表现和演化知之甚少。

Method: 对477个仓库（LLM、ML、非ML各159个）进行SATD流行度比较，进行SATD引入和移除的生存分析，并对377个SATD实例进行定性分析。

Result: LLM仓库SATD积累率与ML系统相似（3.95% vs 4.10%），但保持无债务状态时间比ML仓库长2.4倍（中位数492天 vs 204天）。识别出三种LLM特有的新债务类型：模型堆栈绕行债务、模型依赖债务和性能优化债务。

Conclusion: LLM系统技术债务动态与传统软件不同，虽然积累率相似但保持无债务状态时间更长，随后快速积累。研究发现了LLM特有的新债务类型，为LLM开发中的技术债务管理提供了新见解。

Abstract: Self-admitted technical debt (SATD), referring to comments flagged by developers that explicitly acknowledge suboptimal code or incomplete functionality, has received extensive attention in machine learning (ML) and traditional (Non-ML) software. However, little is known about how SATD manifests and evolves in contemporary Large Language Model (LLM)-based systems, whose architectures, workflows, and dependencies differ fundamentally from both traditional and pre-LLM ML software. In this paper, we conduct the first empirical study of SATD in the LLM era, replicating and extending prior work on ML technical debt to modern LLM-based systems. We compare SATD prevalence across LLM, ML, and non-ML repositories across a total of 477 repositories (159 per category). We perform survival analysis of SATD introduction and removal to understand the dynamics of technical debt across different development paradigms. Surprisingly, despite their architectural complexity, our results reveal that LLM repositories accumulate SATD at similar rates to ML systems (3.95% vs. 4.10%). However, we observe that LLM repositories remain debt-free 2.4x longer than ML repositories (a median of 492 days vs. 204 days), and then start to accumulate technical debt rapidly. Moreover, our qualitative analysis of 377 SATD instances reveals three new forms of technical debt unique to LLM-based development that have not been reported in prior research: Model-Stack Workaround Debt, Model Dependency Debt, and Performance Optimization Debt. Finally, by mapping SATD to stages of the LLM development pipeline, we observe that debt concentrates

</details>


### [6] [Automated QoR improvement in OpenROAD with coding agents](https://arxiv.org/abs/2601.06268)
*Amur Ghose,Junyeong Jang,Andrew B. Kahng,Jakang Lee*

Main category: cs.SE

TL;DR: AuDoPEDA是一个基于LLM的自主EDA代码开发系统，能够在OpenROAD中自动提出研究方向、实现改进并提交代码，显著减少布线长度和时钟周期。


<details>
  <summary>Details</summary>
Motivation: EDA开发受限于专家资源稀缺，虽然大语言模型在编码和科学推理任务中表现出色，但其在推动EDA技术发展方面的潜力尚未得到充分测试。

Method: 基于OpenAI模型和Codex类代理构建的自主、基于代码库的编码系统，能够读取OpenROAD、提出研究方向、扩展为实施步骤并提交可执行的代码差异。

Result: 在OpenROAD实验中，实现了布线长度减少高达5.9%，有效时钟周期减少高达10.0%。

Conclusion: AuDoPEDA展示了LLM在EDA代码改进中的潜力，为自动化EDA开发提供了端到端的解决方案，减少了对人工监督的需求。

Abstract: EDA development and innovation has been constrained by scarcity of expert engineering resources. While leading LLMs have demonstrated excellent performance in coding and scientific reasoning tasks, their capacity to advance EDA technology itself has been largely untested. We present AuDoPEDA, an autonomous, repository-grounded coding system built atop OpenAI models and a Codex-class agent that reads OpenROAD, proposes research directions, expands them into implementation steps, and submits executable diffs. Our contributions include (i) a closed-loop LLM framework for EDA code changes; (ii) a task suite and evaluation protocol on OpenROAD for PPA-oriented improvements; and (iii) end-to-end demonstrations with minimal human oversight. Experiments in OpenROAD achieve routed wirelength reductions of up to 5.9%, and effective clock period reductions of up to 10.0%.

</details>


### [7] [Mining Quantum Software Patterns in Open-Source Projects](https://arxiv.org/abs/2601.06281)
*Neilson Carlos Leite Ramalho,Erico A. da Silva,Higor Amario de Souza,Marcos Lordello Chaim*

Main category: cs.SE

TL;DR: 对985个量子计算Jupyter Notebook的实证研究发现，开发者使用量子模式分为三个层次：基础电路工具、算法原语和领域应用，表明量子软件工程正在成熟化。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算在密码学、优化和材料科学等领域的应用前景，量子软件工程面临挑战和机遇。现有量子框架从基于电路模型转向更高层次的抽象，但缺乏对实践中量子模式应用的实证研究。

Method: 1. 从三个量子计算框架（Qiskit、PennyLane、Classiq）构建知识库，识别并记录了9个新模式；2. 开发可复用的语义搜索工具，在985个Jupyter Notebook的大规模数据集中自动检测这些模式。

Result: 研究发现开发者使用量子模式分为三个层次：基础电路工具、常见算法原语（如振幅放大）、以及金融和优化等领域的特定应用。这表明开发者越来越多地使用高级构建块解决实际问题。

Conclusion: 量子软件工程领域正在成熟，开发者通过不同抽象层次的模式构建量子程序，从底层电路到高级应用，反映了该领域向实用化发展的趋势。

Abstract: Quantum computing has become an active research field in recent years, as its applications in fields such as cryptography, optimization, and materials science are promising. Along with these developments, challenges and opportunities exist in the field of Quantum Software Engineering, as the development of frameworks and higher-level abstractions has attracted practitioners from diverse backgrounds. Unlike initial quantum frameworks based on the circuit model, recent frameworks and libraries leverage higher-level abstractions for creating quantum programs. This paper presents an empirical study of 985 Jupyter Notebooks from 80 open-source projects to investigate how quantum patterns are applied in practice. Our work involved two main stages. First, we built a knowledge base from three quantum computing frameworks (Qiskit, PennyLane, and Classiq). This process led us to identify and document 9 new patterns that refine and extend the existing quantum computing pattern catalog. Second, we developed a reusable semantic search tool to automatically detect these patterns across our large-scale dataset, providing a practitioner-focused analysis. Our results show that developers use patterns in three levels: from foundational circuit utilities, to common algorithmic primitives (e.g., Amplitude Amplification), up to domain-specific applications for finance and optimization. This indicates a maturing field where developers are increasingly using high-level building blocks to solve real-world problems.

</details>


### [8] [Foundational Analysis of Safety Engineering Requirements (SAFER)](https://arxiv.org/abs/2601.06335)
*Noga Chemo,Yaniv Mordecai,Yoram Reich*

Main category: cs.SE

TL;DR: SAFER是一个基于生成式AI的模型驱动框架，用于改进复杂安全关键系统的安全需求生成和分析，通过映射需求到系统功能、识别不足需求、检测重复和矛盾，提升安全工程效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 安全需求通常由多个目标不协调的利益相关者指定，导致需求存在缺口、重复和矛盾，威胁系统安全和合规性。现有方法大多是非正式的，无法有效解决这些问题。

Method: SAFER增强基于模型的系统工程(MBSE)，通过消费需求规范模型，生成以下结果：1) 将需求映射到系统功能；2) 识别需求规范不足的功能；3) 检测重复需求；4) 识别需求集中的矛盾。提供结构化分析、报告和决策支持。

Result: 在自主无人机系统上演示SAFER，显著提高了需求不一致性的检测能力，增强了安全工程过程的效率和可靠性。证明生成式AI需要形式化模型和系统化查询的增强。

Conclusion: 生成式AI必须通过形式化模型进行增强并系统化查询，才能提供有意义的安全需求规范和稳健的安全架构。SAFER框架为复杂安全关键系统提供了有效的安全需求分析方法。

Abstract: We introduce a framework for Foundational Analysis of Safety Engineering Requirements (SAFER), a model-driven methodology supported by Generative AI to improve the generation and analysis of safety requirements for complex safety-critical systems. Safety requirements are often specified by multiple stakeholders with uncoordinated objectives, leading to gaps, duplications, and contradictions that jeopardize system safety and compliance. Existing approaches are largely informal and insufficient for addressing these challenges. SAFER enhances Model-Based Systems Engineering (MBSE) by consuming requirement specification models and generating the following results: (1) mapping requirements to system functions, (2) identifying functions with insufficient requirement specifications, (3) detecting duplicate requirements, and (4) identifying contradictions within requirement sets. SAFER provides structured analysis, reporting, and decision support for safety engineers. We demonstrate SAFER on an autonomous drone system, significantly improving the detection of requirement inconsistencies, enhancing both efficiency and reliability of the safety engineering process. We show that Generative AI must be augmented by formal models and queried systematically, to provide meaningful early-stage safety requirement specifications and robust safety architectures.

</details>


### [9] [Architecting AgentOps Needs CHANGE](https://arxiv.org/abs/2601.06456)
*Shaunak Biswas,Hiya Bhatt,Karthik Vaidhyanathan*

Main category: cs.SE

TL;DR: 提出了CHANGE框架，用于应对Agentic AI系统的非确定性特性，通过六大能力来构建适应持续演化的AgentOps平台


<details>
  <summary>Details</summary>
Motivation: Agentic AI系统与传统软件和传统ML系统有本质区别，其行为在部署后仍会持续演化，而现有的DevOps/MLOps原则基于确定性假设，无法有效管理这类系统的非确定性和学习轨迹发散问题

Method: 提出CHANGE概念框架，包含六大能力：Contextualize（情境化）、Harmonize（协调）、Anticipate（预测）、Negotiate（协商）、Generate（生成）、Evolve（演化），为构建AgentOps平台提供基础

Result: 通过客户支持系统场景展示了CHANGE框架的应用，重新定义了软件架构，使其能够适应不确定性和持续演化成为系统的固有属性

Conclusion: 需要从管理控制循环转向支持智能体、基础设施和人类监督之间的动态协同演化，CHANGE框架为实现这一转变提供了指导，为Agentic AI系统的架构设计奠定了基础

Abstract: The emergence of Agentic AI systems has outpaced the architectural thinking required to operate them effectively. These agents differ fundamentally from traditional software: their behavior is not fixed at deployment but continuously shaped by experience, feedback, and context. Applying operational principles inherited from DevOps or MLOps, built for deterministic software and traditional ML systems, assumes that system behavior can be managed through versioning, monitoring, and rollback. This assumption breaks down for Agentic AI systems whose learning trajectories diverge over time. This introduces non-determinism making system reliability a challenge at runtime. We argue that architecting such systems requires a shift from managing control loops to enabling dynamic co-evolution among agents, infrastructure, and human oversight. To guide this shift, we introduce CHANGE, a conceptual framework comprising six capabilities for operationalizing Agentic AI systems: Contextualize, Harmonize, Anticipate, Negotiate, Generate, and Evolve. CHANGE provides a foundation for architecting an AgentOps platform to manage the lifecycle of evolving Agentic AI systems, illustrated through a customer-support system scenario. In doing so, CHANGE redefines software architecture for an era where adaptation to uncertainty and continuous evolution are inherent properties of the system.

</details>


### [10] [Coding in a Bubble? Evaluating LLMs in Resolving Context Adaptation Bugs During Code Adaptation](https://arxiv.org/abs/2601.06497)
*Tanghaoran Zhang,Xinjun Mao,Shangwen Wang,Yuxin Zhao,Yao Lu,Zezhou Tang,Wenyu Xu,Longfei Sun,Changrong Xie,Kang Yang,Yue Yu*

Main category: cs.SE

TL;DR: CtxBugGen框架生成上下文适应错误来评估LLMs，发现LLMs在解决跨上下文代码适应错误方面表现不佳，揭示其在跨上下文推理方面的关键弱点。


<details>
  <summary>Details</summary>
Motivation: 代码适应是软件开发中的基本但具有挑战性的任务，需要解决上下文适应错误。虽然LLMs在自动化代码相关任务方面显示出巨大潜力，但它们在解决CtxBugs方面的能力仍然是一个重要且未探索的障碍，阻碍了其在代码适应中的实际应用。

Method: 提出CtxBugGen框架，通过四步过程生成CtxBugs：(1)适应任务选择，(2)任务特定扰动，(3)基于LLM的变体生成，(4)CtxBugs识别。使用该框架构建基准，对四个最先进的LLMs进行实证研究。

Result: LLMs在CtxBug解决方面表现不佳，最佳模型Kimi-K2在Pass@1上仅达到55.93%，仅解决52.47%的CtxBugs。CtxBugs的存在使LLMs的适应性能下降高达30%。失败分析表明LLMs经常忽略CtxBugs并在输出中复制它们。

Conclusion: 该研究揭示了LLMs在跨上下文推理方面的关键弱点，强调需要新方法来增强其上下文意识，以实现可靠的代码适应。

Abstract: Code adaptation is a fundamental but challenging task in software development, requiring developers to modify existing code for new contexts. A key challenge is to resolve Context Adaptation Bugs (CtxBugs), which occurs when code correct in its original context violates constraints in the target environment. Unlike isolated bugs, CtxBugs cannot be resolved through local fixes and require cross-context reasoning to identify semantic mismatches. Overlooking them may lead to critical failures in adaptation. Although Large Language Models (LLMs) show great potential in automating code-related tasks, their ability to resolve CtxBugs remains a significant and unexplored obstacle to their practical use in code adaptation. To bridge this gap, we propose CtxBugGen, a novel framework for generating CtxBugs to evaluate LLMs. Its core idea is to leverage LLMs' tendency to generate plausible but context-free code when contextual constraints are absent. The framework generates CtxBugs through a four-step process to ensure their relevance and validity: (1) Adaptation Task Selection, (2) Task-specific Perturbation,(3) LLM-based Variant Generation and (4) CtxBugs Identification. Based on the benchmark constructed by CtxBugGen, we conduct an empirical study with four state-of-the-art LLMs. Our results reveal their unsatisfactory performance in CtxBug resolution. The best performing LLM, Kimi-K2, achieves 55.93% on Pass@1 and resolves just 52.47% of CtxBugs. The presence of CtxBugs degrades LLMs' adaptation performance by up to 30%. Failure analysis indicates that LLMs often overlook CtxBugs and replicate them in their outputs. Our study highlights a critical weakness in LLMs' cross-context reasoning and emphasize the need for new methods to enhance their context awareness for reliable code adaptation.

</details>


### [11] [Fixturize: Bridging the Fixture Gap in Test Generation](https://arxiv.org/abs/2601.06615)
*Pengyu Xue,Chengyi Wang,Zhen Yang,Xiapu Luo,Yuxuan Zhang,Xiran Lyu,Yifei Pei,Zonghan Jia,Yichen Sun,Linhao Wu,Kunwu Zheng*

Main category: cs.SE

TL;DR: Fixturize框架解决了LLM生成单元测试时忽略测试夹具的问题，通过诊断框架识别夹具依赖函数并合成测试夹具，显著提升测试套件质量。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在自动生成单元测试时存在关键局限：经常忽略构建必要的测试夹具（测试运行所需的环境设置），导致生成的测试套件质量不足。

Method: 提出Fixturize诊断框架，主动识别夹具依赖函数，通过迭代反馈驱动过程合成测试夹具。同时创建FixtureEval基准，包含600个Python和Java函数，带有明确的夹具依赖标签。

Result: Fixturize在识别测试夹具依赖方面达到88.38%-97.00%准确率，将测试套件通过率提升18.03%-42.86%。集成现有测试工具后，行覆盖率/分支覆盖率分别提升16.85%/24.08%（LLM基）和31.54%/119.66%（搜索基）。

Conclusion: 测试夹具意识是现代自动测试流程中缺失的关键组件，Fixturize通过解决这一问题显著提升了自动生成测试的质量和覆盖率。

Abstract: Current Large Language Models (LLMs) have advanced automated unit test generation but face a critical limitation: they often neglect to construct the necessary test fixtures, which are the environmental setups required for a test to run. To bridge this gap, this paper proposes Fixturize, a diagnostic framework that proactively identifies fixture-dependent functions and synthesizes test fixtures accordingly through an iterative, feedback-driven process, thereby improving the quality of auto-generated test suites of existing approaches. For rigorous evaluation, the authors introduce FixtureEval, a dedicated benchmark comprising 600 curated functions across two Programming Languages (PLs), i.e., Python and Java, with explicit fixture dependency labels, enabling both the corresponding classification and generation tasks. Empirical results demonstrate that Fixturize is highly effective, achieving 88.38%-97.00% accuracy across benchmarks in identifying the dependence of test fixtures and significantly enhancing the Suite Pass rate (SuitePS) by 18.03%-42.86% on average across both PLs with the auto-generated fixtures. Owing to the maintenance of test fixtures, Fixturize further improves line/branch coverage when integrated with existing testing tools of both LLM-based and Search-based by 16.85%/24.08% and 31.54%/119.66% on average, respectively. The findings establish fixture awareness as an essential, missing component in modern auto-testing pipelines.

</details>


### [12] [An Exploratory Pilot Survey on Technical Quality Control Practices in Agile R&D Projects](https://arxiv.org/abs/2601.06689)
*Mateus Costa Lucena*

Main category: cs.SE

TL;DR: 敏捷研发软件项目中技术质量管理面临挑战，特别是在技术不确定性和实验压力高的环境中。本研究调查了巴西玛瑙斯科技机构中敏捷团队在Scrum环境下技术质量控制实践和指标的使用情况。


<details>
  <summary>Details</summary>
Motivation: 在敏捷研发软件项目中，特别是在技术不确定性和实验压力高的环境中，技术质量管理是一个持续存在的挑战。需要了解敏捷研发团队如何在Scrum环境中报告技术质量控制实践和指标的使用情况。

Method: 采用结构化问卷调查方法，对巴西玛瑙斯科技机构的专业人士进行调查，收集报告实践、质量感知和常见挑战。定量数据辅以定性回答以支持上下文解释。

Result: 结果显示，虽然自动化测试、代码审查和持续集成等实践被广泛认可，但在迭代中的实际应用往往不一致。在技术质量指标监控和从业务角度评估技术债务的机制报告方面也存在差距。

Conclusion: 本研究提供了探索性基线，描述了在区域创新生态系统中敏捷研发项目如何管理技术质量，而非追求普遍性结论。揭示了实践认可与实际应用之间的差距，以及技术质量监控和债务评估方面的不足。

Abstract: Managing technical quality in agile Research and Development (R&D) software projects represents a persistent challenge, particularly in contexts characterized by high technical uncertainty and experimental pressure. This exploratory pilot survey explores how agile R&D software teams report the use of practices and metrics related to technical quality control within Scrum-based environments. The study employed a structured questionnaire administered to professionals from Science and Technology Institutions (STIs) located in Manaus, Brazil, aiming to capture reported practices, perceptions of quality, and recurrent challenges. Quantitative data were complemented by qualitative responses to support contextual interpretation. The results indicate that although practices such as automated testing, code review, and continuous integration are widely acknowledged, their reported application is often inconsistent across iterations. Gaps were also observed in the monitoring of technical quality metrics and in the reporting of mechanisms for assessing technical debt from a business perspective. Rather than aiming for generalization, this study offers an exploratory baseline that describes how technical quality is managed in agile R&D projects within a regional innovation ecosystem.

</details>


### [13] [Comparative Separation: Evaluating Separation on Comparative Judgment Test Data](https://arxiv.org/abs/2601.06761)
*Xiaoyin Xi,Neeku Capak,Kate Stockwell,Zhe Yu*

Main category: cs.SE

TL;DR: 提出了一种新的群体公平性概念"比较分离"，用于在比较判断测试数据上评估机器学习软件的公平性，避免了传统分离准则需要真实标签的限制。


<details>
  <summary>Details</summary>
Motivation: 机器学习软件越来越多地用于高风险决策，需要确保软件在不同敏感群体间表现一致（满足分离准则）。然而，评估分离需要每个测试数据点的真实标签，这在实际应用中难以获取。比较判断（如"A比B好"）比提供评分或分类标签对人类来说认知负担更低，因此研究能否在比较判断测试数据上评估分离准则。

Method: 首先定义了在比较判断测试数据上的新公平性概念"比较分离"及其评估指标。然后，在理论上和经验上证明了在二分类问题中，比较分离等价于传统分离。最后分析了评估分离和比较分离分别需要的测试数据点和测试数据对数量，以达到相同的统计功效水平。

Result: 证明了在二分类问题中，比较分离与传统分离是等价的。这是首次探索在比较判断测试数据上进行公平性评估的研究，展示了使用比较判断测试数据进行模型评估的可行性和实际优势。

Conclusion: 该研究提出了比较分离这一新公平性概念，为在缺乏真实标签的情况下评估机器学习公平性提供了可行方案。通过比较判断数据，可以降低人类标注的认知负担，同时保持与传统分离准则的等价性，为实际应用中的公平性评估提供了实用工具。

Abstract: This research seeks to benefit the software engineering society by proposing comparative separation, a novel group fairness notion to evaluate the fairness of machine learning software on comparative judgment test data. Fairness issues have attracted increasing attention since machine learning software is increasingly used for high-stakes and high-risk decisions. It is the responsibility of all software developers to make their software accountable by ensuring that the machine learning software do not perform differently on different sensitive groups -- satisfying the separation criterion. However, evaluation of separation requires ground truth labels for each test data point. This motivates our work on analyzing whether separation can be evaluated on comparative judgment test data. Instead of asking humans to provide the ratings or categorical labels on each test data point, comparative judgments are made between pairs of data points such as A is better than B. According to the law of comparative judgment, providing such comparative judgments yields a lower cognitive burden for humans than providing ratings or categorical labels. This work first defines the novel fairness notion comparative separation on comparative judgment test data, and the metrics to evaluate comparative separation. Then, both theoretically and empirically, we show that in binary classification problems, comparative separation is equivalent to separation. Lastly, we analyze the number of test data points and test data pairs required to achieve the same level of statistical power in the evaluation of separation and comparative separation, respectively. This work is the first to explore fairness evaluation on comparative judgment test data. It shows the feasibility and the practical benefits of using comparative judgment test data for model evaluations.

</details>


### [14] [MemGovern: Enhancing Code Agents through Learning from Governed Human Experiences](https://arxiv.org/abs/2601.06789)
*Qihao Wang,Ziming Cheng,Shuo Zhang,Fan Liu,Rui Xu,Heng Lian,Kunyi Wang,Xiaoming Yu,Jianghao Yin,Sen Hu,Yue Hu,Shaolei Zhang,Yanbing Liu,Ronghao Chen,Huacan Wang*

Main category: cs.SE

TL;DR: MemGovern框架将GitHub历史数据转化为可操作的体验记忆卡，帮助自主软件工程代理提升bug修复能力


<details>
  <summary>Details</summary>
Motivation: 当前自主软件工程代理存在"封闭世界"限制，仅从零开始或依赖本地上下文修复bug，忽略了GitHub等平台上丰富的历史人类经验。真实世界的问题跟踪数据非结构化且碎片化，阻碍了访问这些开放世界经验。

Method: MemGovern框架通过经验治理将原始GitHub数据转化为代理友好的体验卡，并引入代理化经验搜索策略，实现逻辑驱动的人类专业知识检索。

Result: MemGovern生成了135K个治理后的体验卡，在SWE-bench Verified上将解决率提升了4.65%。

Conclusion: MemGovern作为一种插件方法，为代理友好的记忆基础设施提供了解决方案，能够显著提升自主软件工程代理的性能。

Abstract: While autonomous software engineering (SWE) agents are reshaping programming paradigms, they currently suffer from a "closed-world" limitation: they attempt to fix bugs from scratch or solely using local context, ignoring the immense historical human experience available on platforms like GitHub. Accessing this open-world experience is hindered by the unstructured and fragmented nature of real-world issue-tracking data. In this paper, we introduce MemGovern, a framework designed to govern and transform raw GitHub data into actionable experiential memory for agents. MemGovern employs experience governance to convert human experience into agent-friendly experience cards and introduces an agentic experience search strategy that enables logic-driven retrieval of human expertise. By producing 135K governed experience cards, MemGovern achieves a significant performance boost, improving resolution rates on the SWE-bench Verified by 4.65%. As a plug-in approach, MemGovern provides a solution for agent-friendly memory infrastructure.

</details>


### [15] [PenForge: On-the-Fly Expert Agent Construction for Automated Penetration Testing](https://arxiv.org/abs/2601.06910)
*Huihui Huang,Jieke Shi,Junkai Chen,Ting Zhang,Yikun Li,Chengran Yang,Eng Lieh Ouh,Lwin Khin Shar,David Lo*

Main category: cs.SE

TL;DR: PenForge是一个动态构建专家代理的渗透测试框架，在零日漏洞场景下比现有方法提升3倍成功率


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的渗透测试方法存在局限性：单一通用代理在复杂场景中表现不佳，而专门化代理无法适应多样化的漏洞类型。需要一种能够动态适应不同测试场景的解决方案。

Method: PenForge框架通过自动侦察潜在攻击面，并在测试过程中动态实例化上下文感知的专家代理，而不是依赖预先准备的代理。实现了按需构建代理的能力。

Result: 在CVE-Bench的零日漏洞设置中，PenForge实现了30.0%的利用成功率（12/40），比现有最先进方法提升了3倍。

Conclusion: PenForge代表了动态代理构建的范式转变，具有显著潜力。未来工作需要：提供更丰富的工具使用知识、扩展基准测试范围、通过可解释机制和人工审查建立开发者信任。

Abstract: Penetration testing is essential for identifying vulnerabilities in web applications before real adversaries can exploit them. Recent work has explored automating this process with Large Language Model (LLM)-powered agents, but existing approaches either rely on a single generic agent that struggles in complex scenarios or narrowly specialized agents that cannot adapt to diverse vulnerability types. We therefore introduce PenForge, a framework that dynamically constructs expert agents during testing rather than relying on those prepared beforehand. By integrating automated reconnaissance of potential attack surfaces with agents instantiated on the fly for context-aware exploitation, PenForge achieves a 30.0% exploit success rate (12/40) on CVE-Bench in the particularly challenging zero-day setting, which is a 3 times improvement over the state-of-the-art. Our analysis also identifies three opportunities for future work: (1) supplying richer tool-usage knowledge to improve exploitation effectiveness; (2) extending benchmarks to include more vulnerabilities and attack types; and (3) fostering developer trust by incorporating explainable mechanisms and human review. As an emerging result with substantial potential impact, PenForge embodies the early-stage yet paradigm-shifting idea of on-the-fly agent construction, marking its promise as a step toward scalable and effective LLM-driven penetration testing.

</details>


### [16] [MicLog: Towards Accurate and Efficient LLM-based Log Parsing via Progressive Meta In-Context Learning](https://arxiv.org/abs/2601.07005)
*Jianbo Yu,Yixuan Li,Hai Xu,Kang Xu,Junjielong Xu,Zhijing Li,Pinjia He,Wanyuan Wang*

Main category: cs.SE

TL;DR: MicLog提出首个渐进式元上下文学习日志解析框架，结合元学习与小规模开源LLM，显著提升解析精度并大幅降低时间成本


<details>
  <summary>Details</summary>
Motivation: 传统基于语法和语义的日志解析器难以处理日志语义变化和数据稀缺问题，而现有LLM解析器存在上下文学习能力利用不足、跨域泛化能力弱、LLM查询耗时昂贵等挑战

Method: 提出ProgMeta-ICL框架：1) 采用零样本到k样本渐进式元上下文学习范式，使用加权DBSCAN候选采样和增强BM25演示选择；2) 构建多级预查询缓存，动态匹配和优化最近解析的模板

Result: 在Loghub-2.0数据集上，MicLog比最先进解析器提升10.3%的解析精度，同时减少42.4%的解析时间

Conclusion: MicLog通过结合元学习与上下文学习，有效解决了LLM日志解析中的性能不一致和高成本问题，为日志分析提供了高效可靠的解决方案

Abstract: Log parsing converts semi-structured logs into structured templates, forming a critical foundation for downstream analysis. Traditional syntax and semantic-based parsers often struggle with semantic variations in evolving logs and data scarcity stemming from their limited domain coverage. Recent large language model (LLM)-based parsers leverage in-context learning (ICL) to extract semantics from examples, demonstrating superior accuracy. However, LLM-based parsers face two main challenges: 1) underutilization of ICL capabilities, particularly in dynamic example selection and cross-domain generalization, leading to inconsistent performance; 2) time-consuming and costly LLM querying. To address these challenges, we present MicLog, the first progressive meta in-context learning (ProgMeta-ICL) log parsing framework that combines meta-learning with ICL on small open-source LLMs (i.e., Qwen-2.5-3B). Specifically, MicLog: i) enhances LLMs' ICL capability through a zero-shot to k-shot ProgMeta-ICL paradigm, employing weighted DBSCAN candidate sampling and enhanced BM25 demonstration selection; ii) accelerates parsing via a multi-level pre-query cache that dynamically matches and refines recently parsed templates. Evaluated on Loghub-2.0, MicLog achieves 10.3% higher parsing accuracy than the state-of-the-art parser while reducing parsing time by 42.4%.

</details>


### [17] [Between Policy and Practice: GenAI Adoption in Agile Software Development Teams](https://arxiv.org/abs/2601.07051)
*Michael Neumann,Lasse Bischof,Nic Elias Hinz,Luca Stockmann,Dennis Schrader,Ana Carolina Ahaus,Erim Can Demirci,Benjamin Gabel,Maria Rauschenberger,Philipp Diebold,Henning Fritzemeier,Adam Przybylek*

Main category: cs.SE

TL;DR: 敏捷环境中生成式AI工具的采用研究：通过多案例研究分析实际使用情况、收益与障碍，发现技术-组织-环境框架下的错配问题


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具正在重塑软件工程活动，但在敏捷环境中的采用情况尚未充分探索。本研究旨在调查敏捷从业者在真实组织环境中如何采用GenAI工具，关注监管条件、使用案例、收益和障碍。

Method: 采用探索性多案例研究方法，在三个德国组织中进行，包括17个半结构化访谈和文档分析。应用跨案例主题分析来识别GenAI采用模式。

Result: 研究发现GenAI主要用于创造性任务、文档编写和代码辅助。收益包括效率提升和创造力增强，而障碍涉及数据隐私、验证工作量和缺乏治理。使用技术-组织-环境框架分析发现，这些障碍源于三个维度之间的错配。监管压力往往转化为不考虑实际技术使用模式或组织约束的政策，导致政策与实践之间存在系统性差距。

Conclusion: GenAI在增强敏捷角色方面具有显著潜力，但需要在技术-组织-环境维度上实现对齐，包括明确的政策、数据保护措施和用户培训，以确保负责任和有效的集成。

Abstract: Context: The rapid emergence of generative AI (GenAI) tools has begun to reshape various software engineering activities. Yet, their adoption within agile environments remains underexplored. Objective: This study investigates how agile practitioners adopt GenAI tools in real-world organizational contexts, focusing on regulatory conditions, use cases, benefits, and barriers. Method: An exploratory multiple case study was conducted in three German organizations, involving 17 semi-structured interviews and document analysis. A cross-case thematic analysis was applied to identify GenAI adoption patterns. Results: Findings reveal that GenAI is primarily used for creative tasks, documentation, and code assistance. Benefits include efficiency gains and enhanced creativity, while barriers relate to data privacy, validation effort, and lack of governance. Using the Technology-Organization-Environment (TOE) framework, we find that these barriers stem from misalignments across the three dimensions. Regulatory pressures are often translated into policies without accounting for actual technological usage patterns or organizational constraints. This leads to systematic gaps between policy and practice. Conclusion: GenAI offers significant potential to augment agile roles but requires alignment across TOE dimensions, including clear policies, data protection measures, and user training to ensure responsible and effective integration.

</details>


### [18] [A Large-Scale Study on the Development and Issues of Multi-Agent AI Systems](https://arxiv.org/abs/2601.07136)
*Daniel Liu,Krishna Upadhyay,Vinaik Chhetri,A. B. Siddique,Umar Farooq*

Main category: cs.SE

TL;DR: 对8个主流多智能体AI系统的首次大规模实证研究，分析了4.2万次提交和4.7千个已解决问题，揭示了三种开发模式、维护优先级分布和问题解决时间特征。


<details>
  <summary>Details</summary>
Motivation: 多智能体AI系统（如LangChain、CrewAI、AutoGen）快速发展，但缺乏对其实际演进和维护实践的了解。本研究旨在填补这一空白，通过实证分析揭示这些系统的开发模式和维护现状。

Method: 对8个领先的开源多智能体AI系统进行大规模实证研究，分析超过42,000次唯一提交和4,700多个已解决问题，识别开发模式、维护类型分布和问题解决时间特征。

Result: 识别出三种开发模式：持续型、稳定型和爆发驱动型；完美性维护占40.8%，纠正性维护占27.4%，适应性维护占24.3%；最常见问题包括bug（22%）、基础设施（14%）和智能体协调（10%）；中位解决时间从不到1天到约2周不等。

Conclusion: 当前生态系统既有发展势头又显脆弱，需要改进测试基础设施、文档质量和维护实践，以确保长期可靠性和可持续性。问题报告自2023年起急剧增加，反映了系统复杂性和使用量的增长。

Abstract: The rapid emergence of multi-agent AI systems (MAS), including LangChain, CrewAI, and AutoGen, has shaped how large language model (LLM) applications are developed and orchestrated. However, little is known about how these systems evolve and are maintained in practice. This paper presents the first large-scale empirical study of open-source MAS, analyzing over 42K unique commits and over 4.7K resolved issues across eight leading systems. Our analysis identifies three distinct development profiles: sustained, steady, and burst-driven. These profiles reflect substantial variation in ecosystem maturity. Perfective commits constitute 40.8% of all changes, suggesting that feature enhancement is prioritized over corrective maintenance (27.4%) and adaptive updates (24.3%). Data about issues shows that the most frequent concerns involve bugs (22%), infrastructure (14%), and agent coordination challenges (10%). Issue reporting also increased sharply across all frameworks starting in 2023. Median resolution times range from under one day to about two weeks, with distributions skewed toward fast responses but a minority of issues requiring extended attention. These results highlight both the momentum and the fragility of the current ecosystem, emphasizing the need for improved testing infrastructure, documentation quality, and maintenance practices to ensure long-term reliability and sustainability.

</details>


### [19] [Engineering Decisions in MBSE: Insights for a Decision Capture Framework Development](https://arxiv.org/abs/2601.07301)
*Nidhal Selmi,Jean-michel Bruel,Sébastien Mosser,Matthieu Crespo,Alain Kerbrat*

Main category: cs.SE

TL;DR: 提出一个轻量级框架，将决策捕获集成到基于模型的系统工程中，通过将决策备选方案表示为系统模型切片来减少捕获工作量并保持与系统元素的明确链接。


<details>
  <summary>Details</summary>
Motivation: 决策是工程设计中的核心活动，捕获决策知识对工程团队具有重要价值，但传统决策捕获方法需要大量努力且难以捕获足够的上下文信息以供重用。

Method: 提出一个轻量级框架，将决策捕获集成到MBSE工作流中，通过将决策备选方案表示为系统模型切片，减少捕获工作量并保持与需求、行为和架构元素的明确链接。

Result: 使用简化的飞机架构行业示例讨论了决策捕获的主要挑战，并提出了初步解决方案来应对这些挑战。

Conclusion: 基于模型的系统工程（MBSE）通过将决策直接嵌入系统模型中，可以成为解决决策捕获挑战的有前景的解决方案，能够减少捕获工作量同时保持与系统元素的明确链接。

Abstract: Decision-making is a core engineering design activity that conveys the engineer's knowledge and translates it into courses of action. Capturing this form of knowledge can reap potential benefits for the engineering teams and enhance development efficiency. Despite its clear value, traditional decision capture often requires a significant amount of effort and still falls short of capturing the necessary context for reuse. Model-based systems engineering (MBSE) can be a promising solution to address these challenges by embedding decisions directly within system models, which can reduce the capture workload while maintaining explicit links to requirements, behaviors, and architectural elements. This article discusses a lightweight framework for integrating decision capture into MBSE workflows by representing decision alternatives as system model slices. Using a simplified industry example from aircraft architecture, we discuss the main challenges associated with decision capture and propose preliminary solutions to address these challenges.

</details>


### [20] [FairRF: Multi-Objective Search for Single and Intersectional Software Fairness](https://arxiv.org/abs/2601.07537)
*Giordano d'Alosio,Max Hort,Rebecca Moussa,Federica Sarro*

Main category: cs.SE

TL;DR: FairRF是一种基于多目标进化搜索的新方法，使用随机森林作为基础分类器，通过优化超参数配置和数据变异来同时最大化公平性和有效性，最终提供一组帕累托最优解供利益相关者选择。


<details>
  <summary>Details</summary>
Motivation: AI/ML系统在敏感领域的广泛应用引发了严重的公平性担忧。现有的大多数公平性增强方法都是黑盒式的，不允许利益相关者根据自身需求在公平性和有效性之间进行权衡。

Method: 基于多目标进化搜索，使用随机森林作为基础分类器，搜索最佳超参数配置和数据变异以最大化公平性和有效性。最终返回一组帕累托最优解供选择。

Result: 在11个不同场景下与26个基线方法进行比较，使用5个有效性指标和3个公平性指标（包含交叉偏见的两个变体）。结果显示FairRF能显著提高基础分类器的公平性，同时保持一致的预测有效性，在所有公平性定义下提供更一致的优化，并在交叉偏见缓解方面超越了现有最先进方法。

Conclusion: FairRF是一种有效的偏见缓解方法，允许利益相关者根据特定需求调整公平软件系统的开发。

Abstract: Background: The wide adoption of AI- and ML-based systems in sensitive domains raises severe concerns about their fairness. Many methods have been proposed in the literature to enhance software fairness. However, the majority behave as a black-box, not allowing stakeholders to prioritise fairness or effectiveness (i.e., prediction correctness) based on their needs. Aims: In this paper, we introduce FairRF, a novel approach based on multi-objective evolutionary search to optimise fairness and effectiveness in classification tasks. FairRF uses a Random Forest (RF) model as a base classifier and searches for the best hyperparameter configurations and data mutation to maximise fairness and effectiveness. Eventually, it returns a set of Pareto optimal solutions, allowing the final stakeholders to choose the best one based on their needs. Method: We conduct an extensive empirical evaluation of FairRF against 26 different baselines in 11 different scenarios using five effectiveness and three fairness metrics. Additionally, we also include two variations of the fairness metrics for intersectional bias for a total of six definitions analysed. Result: Our results show that FairRF can significantly improve the fairness of base classifiers, while maintaining consistent prediction effectiveness. Additionally, FairRF provides a more consistent optimisation under all fairness definitions compared to state-of-the-art bias mitigation methods and overcomes the existing state-of-the-art approach for intersectional bias mitigation. Conclusions: FairRF is an effective approach for bias mitigation also allowing stakeholders to adapt the development of fair software systems based on their specific needs.

</details>


### [21] [OODEval: Evaluating Large Language Models on Object-Oriented Design](https://arxiv.org/abs/2601.07602)
*Bingxu Xiao,Yunwei Dong,Yiqi Tang,Manqing Zhang,Yifan Zhou,Chunyan Ma,Yepang Liu*

Main category: cs.SE

TL;DR: 论文提出了OODEval和OODEval-Human两个面向对象设计评估基准，以及CLUE评估指标，对29个LLM进行系统评估，发现LLM在语法准确性高但语义设计能力不足，最佳模型接近本科生平均水平但仍远低于优秀人类设计师。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估主要集中在代码层面，缺乏对软件设计能力的系统研究。面向对象设计是软件工程的核心能力，需要专门的基准和指标来评估LLM在这方面的表现。

Method: 1) 构建OODEval基准：包含50个不同难度的OOD任务；2) 创建OODEval-Human基准：包含940个本科生提交的类图并由教师评分；3) 提出CLUE评估指标：评估全局正确性和细粒度设计质量；4) 对29个LLM进行实证研究，分析五个研究问题。

Result: LLM在语法准确性方面表现良好，但在语义设计（特别是方法和关系生成）方面存在显著缺陷。Qwen3-Coder-30B整体表现最佳，与DeepSeek-R1和GPT-4o相当；Gemma3-4B-IT虽然参数规模小但优于GPT-4o-Mini。最佳LLM接近本科生平均水平，但远低于优秀人类设计师。参数规模、代码专业化、指令调优对性能有积极影响，而设计复杂度和需求可读性降低性能。

Conclusion: LLM在面向对象设计任务中表现出潜力但仍有局限，特别是在语义理解和设计质量方面。需要更专业的评估基准和指标来推动LLM在软件设计领域的发展。模型规模、专业化和调优策略是关键影响因素。

Abstract: Recent advances in large language models (LLMs) have driven extensive evaluations in software engineering. however, most prior work concentrates on code-level tasks, leaving software design capabilities underexplored. To fill this gap, we conduct a comprehensive empirical study evaluating 29 LLMs on object-oriented design (OOD) tasks. Owing to the lack of standardized benchmarks and metrics, we introduce OODEval, a manually constructed benchmark comprising 50 OOD tasks of varying difficulty, and OODEval-Human, the first human-rated OOD benchmark, which includes 940 undergraduate-submitted class diagrams evaluated by instructors. We further propose CLUE (Class Likeness Unified Evaluation), a unified metric set that assesses both global correctness and fine-grained design quality in class diagram generation. Using these benchmarks and metrics, we investigate five research questions: overall correctness, comparison with humans, model dimension analysis, task feature analysis, and bad case analysis. The results indicate that while LLMs achieve high syntactic accuracy, they exhibit substantial semantic deficiencies, particularly in method and relationship generation. Among the evaluated models, Qwen3-Coder-30B achieves the best overall performance, rivaling DeepSeek-R1 and GPT-4o, while Gemma3-4B-IT outperforms GPT-4o-Mini despite its smaller parameter scale. Although top-performing LLMs nearly match the average performance of undergraduates, they remain significantly below the level of the best human designers. Further analysis shows that parameter scale, code specialization, and instruction tuning strongly influence performance, whereas increased design complexity and lower requirement readability degrade it. Bad case analysis reveals common failure modes, including keyword misuse, missing classes or relationships, and omitted methods.

</details>


### [22] ["TODO: Fix the Mess Gemini Created": Towards Understanding GenAI-Induced Self-Admitted Technical Debt](https://arxiv.org/abs/2601.07786)
*Abdullah Al Mujahid,Mia Mohammad Imran*

Main category: cs.SE

TL;DR: 研究分析AI辅助编程中开发者承认的技术债务，提出GIST概念来描述AI生成代码导致的技术债务


<details>
  <summary>Details</summary>
Motivation: 随着ChatGPT、Copilot等大语言模型融入软件开发流程，开发者在代码注释中留下AI参与痕迹，其中一些注释既承认使用了生成式AI，又承认存在技术缺陷。研究者希望了解AI辅助如何影响技术债务的出现时机和原因。

Method: 分析2022年11月至2025年7月期间公共Python和JavaScript GitHub仓库中的6,540条引用LLM的代码注释，识别出81条同时自我承认技术债务(SATD)的注释。

Result: 开发者最常描述推迟测试、不完全适配、对AI生成代码理解有限等情况。这表明AI辅助既影响技术债务出现的时间，也影响其产生原因。

Conclusion: 提出GenAI诱导的自我承认技术债务(GIST)概念框架，用于描述开发者在使用AI生成代码时明确表达对其行为或正确性不确定性的重复性案例。

Abstract: As large language models (LLMs) such as ChatGPT, Copilot, Claude, and Gemini become integrated into software development workflows, developers increasingly leave traces of AI involvement in their code comments. Among these, some comments explicitly acknowledge both the use of generative AI and the presence of technical shortcomings. Analyzing 6,540 LLM-referencing code comments from public Python and JavaScript-based GitHub repositories (November 2022-July 2025), we identified 81 that also self-admit technical debt(SATD). Developers most often describe postponed testing, incomplete adaptation, and limited understanding of AI-generated code, suggesting that AI assistance affects both when and why technical debt emerges. We term GenAI-Induced Self-admitted Technical debt (GIST) as a proposed conceptual lens to describe recurring cases where developers incorporate AI-generated code while explicitly expressing uncertainty about its behavior or correctness.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [23] [FO-Complete Program Verification for Heap Logics](https://arxiv.org/abs/2601.06719)
*Adithya Murali,Hrishikesh Balakrishnan,Aaron Councilman,P. Madhusudan*

Main category: cs.LO

TL;DR: 提出了两种具有隐式堆片段且支持FO完备程序验证的堆逻辑：帧逻辑(FL)和分离逻辑(SL-FL)，实现了基于量化实例化和SMT求解器的FO完备推理。


<details>
  <summary>Details</summary>
Motivation: 现有的堆逻辑验证系统缺乏理论保证，无法确保所有有效定理最终都能被证明。本文旨在开发具有隐式堆片段且提供FO完备性理论保证的堆逻辑，确保递归定义解释为不动点定义时所有有效定理最终都能被证明。

Method: 开发了两种新的堆逻辑：1) 帧逻辑(FL)，具有隐式堆片段；2) 分离逻辑(SL-FL)，采用受帧逻辑启发的替代语义。为FL设计了适用于FO完备推理的验证条件生成方法，使用量化实例化和SMT求解器。展示了SL-FL可以转换为FL以获得FO完备推理能力。

Result: 实现了基于该技术的工具，并在操作数据结构的基准测试套件上展示了逻辑的表达能力和验证技术的有效性。验证条件生成方法适用于FO完备推理，SL-FL到FL的转换成功实现了FO完备推理。

Conclusion: 本文首次提出了具有隐式堆片段且支持FO完备程序验证的堆逻辑，为程序验证提供了理论保证，并通过实现工具和基准测试验证了方法的有效性和表达能力。

Abstract: We develop the first two heap logics that have implicit heaplets and that admit FO-complete program verification. The notion of FO-completeness is a theoretical guarantee that all theorems that are valid when recursive definitions are interpreted as fixpoint definitions (instead of least fixpoint) are guaranteed to be eventually proven by the system. The logics we develop are a frame logic ($\textit{FL}$) and a separation logic ($\textit{SL-FL}$) that has an alternate semantics inspired by frame logic. We show verification condition generation for FL that is amenable to FO-complete reasoning using quantifier instantiation and SMT solvers. We show $\textit{SL-FL}$ can be translated to FL in order to obtain FO-complete reasoning. We implement tools that realize our technique and show the expressiveness of our logics and the efficacy of the verification technique on a suite of benchmarks that manipulate data structures.

</details>


### [24] [Formalization of Amicable Numbers Theory](https://arxiv.org/abs/2601.07444)
*Zhipeng Chen,Haolun Tang,Jingyi Zhan*

Main category: cs.LO

TL;DR: 在Lean 4中形式化亲和数理论，包括定义、经典公式证明、生成规则、Borho-Hoffmann繁殖方法，以及扩展到社交数、订婚数等概念。


<details>
  <summary>Details</summary>
Motivation: 亲和数是数论中一个经典概念，但缺乏在证明助手中的完整形式化。本文旨在填补这一空白，为亲和数理论提供严格的机械化验证，包括历史著名结果和现代生成方法。

Method: 使用Lean 4证明助手，定义亲和数相关概念（亲和对、亲和数、社交数等），形式化基本结构定理，证明经典公式（Thābit公式、Thābit规则、欧拉广义规则），并首次完整形式化Borho-Hoffmann繁殖方法。

Result: 成功形式化2076行Lean代码，包含139个定理，验证了历史著名亲和对，证明了经典公式，首次完整实现了Borho-Hoffmann繁殖方法（540行，33个定理），并扩展到社交数、订婚数等概念。

Conclusion: 本文在Lean 4中建立了亲和数理论的全面形式化框架，不仅验证了经典结果，还首次机械化实现了现代生成方法，为数论的形式化验证提供了重要基础。

Abstract: This paper presents a formalization of the theory of amicable numbers in the Lean~4 proof assistant. Two positive integers $m$ and $n$ are called an amicable pair if the sum of proper divisors of $m$ equals $n$ and the sum of proper divisors of $n$ equals $m$. Our formalization introduces the proper divisor sum function $\propersum(n) = σ(n) - n$, defines the concepts of amicable pairs and amicable numbers, and computationally verifies historically famous amicable pairs. Furthermore, we formalize basic structural theorems, including symmetry, non-triviality, and connections to abundant/deficient numbers. A key contribution is the complete formal proof of the classical Thābit formula (9th century), using index-shifting and the \texttt{zify} tactic. Additionally, we provide complete formal proofs of both Thābit's rule and Euler's generalized rule (1747), two fundamental theorems for generating amicable pairs. A major achievement is the first complete formalization of the Borho-Hoffmann breeding method (1986), comprising 540 lines with 33 theorems and leveraging automated algebra tactics (\texttt{zify} and \texttt{ring}) to verify complex polynomial identities. We also formalize extensions including sociable numbers (aliquot cycles), betrothed numbers (quasi-amicable pairs), parity constraint theorems, and computational search bounds for coprime pairs ($>10^{65}$). We verify the smallest sociable cycle of length 5 (Poulet's cycle) and computationally verify specific instances. The formalization comprises 2076 lines of Lean code organized into Mathlib-candidate and paper-specific modules, with 139 theorems and all necessary infrastructure for divisor sum multiplicativity and coprimality reasoning.

</details>


### [25] [Simplicial Belief](https://arxiv.org/abs/2601.07669)
*Christian Cachin,David Lehnherr,Thomas Studer*

Main category: cs.LO

TL;DR: 该论文提出了一种在单纯复形中建模信念的新方法，通过引入多色单纯复形来定义各种信念概念。


<details>
  <summary>Details</summary>
Motivation: 虽然模态逻辑的单纯解释已有大量研究，分布式知识概念也得到了充分探索，但在单纯模型中如何建模信念一直是一个开放问题。现有研究缺乏在单纯复形框架中表达信念的合适方法。

Method: 引入多色单纯复形，这种结构自然地给状态施加了一个似然关系。基于这种似然关系，可以定义各种信念概念。

Result: 成功开发了在单纯模型中建模信念的框架，填补了该领域的研究空白，为在单纯复形中表达信念概念提供了理论基础。

Conclusion: 多色单纯复形为解决单纯模型中信念建模的开放问题提供了有效方案，扩展了模态逻辑单纯解释的应用范围。

Abstract: Recently, much work has been carried out to study simplicial interpretations of modal logic. While notions of (distributed) knowledge have been well investigated in this context, it has been open how to model belief in simplicial models. We introduce polychromatic simplicial complexes, which naturally impose a plausibility relation on states. From this, we can define various notions of belief.

</details>


### [26] [On Angels and Demons: Strategic (De)Construction of Dynamic Models](https://arxiv.org/abs/2601.07690)
*Davide Catta,Rustam Galimullin,Munyque Mittelmann*

Main category: cs.LO

TL;DR: 本文提出了三种用于推理加权图拓扑修改策略的逻辑：Strategic Deconstruction Logic（破坏性代理移除边）、Strategic Construction Logic（建设性代理添加边）和Strategic Update Logic（两者结合），并研究了它们的表达能力和模型检查复杂度。


<details>
  <summary>Details</summary>
Motivation: 近年来，对能够形式化代理修改模型结构策略推理的逻辑越来越感兴趣。这一研究方向受到通信网络、安全协议和多智能体规划等建模系统随时间演化的应用驱动。

Method: 提出了三种逻辑：1) Strategic Deconstruction Logic - 破坏性代理（demon）在成本限制内移除边；2) Strategic Construction Logic - 建设性代理（angel）在成本限制内添加边；3) Strategic Update Logic - 结合两种代理，它们可以合作或竞争。

Result: 研究了这些逻辑的表达能力（expressive power）和模型检查问题（model checking problems）的复杂度。

Conclusion: 本文为加权图拓扑修改的策略推理提供了三种新的逻辑框架，并对其理论性质进行了初步分析，为动态系统建模中的策略推理提供了形式化工具。

Abstract: In recent years, there has been growing interest in logics that formalise strategic reasoning about agents capable of modifying the structure of a given model. This line of research has been motivated by applications where a modelled system evolves over time, such as communication networks, security protocols, and multi-agent planning. In this paper, we introduce three logics for reasoning about strategies that modify the topology of weighted graphs. In Strategic Deconstruction Logic, a destructive agent (the demon) removes edges up to a certain cost. In Strategic Construction Logic, a constructive agent (the angel) adds edges within a cost bound. Finally, Strategic Update Logic combines both agents, who may cooperate or compete. We study the expressive power of these logics and the complexity of their model checking problems.

</details>
