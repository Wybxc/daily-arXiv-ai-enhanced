<div id=toc></div>

# Table of Contents

- [cs.LO](#cs.LO) [Total: 9]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.SE](#cs.SE) [Total: 30]
- [cs.PL](#cs.PL) [Total: 12]


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [1] [Modular Counting over 3-Element and Conservative Domains](https://arxiv.org/abs/2510.09950)
*Andrei A. Bulatov,Amirhossein Kazeminia*

Main category: cs.LO

TL;DR: 本文研究了约束满足问题(CSP)的模计数版本，即计算从给定关系结构到固定关系结构的同态数量模素数p的问题。


<details>
  <summary>Details</summary>
Motivation: 模计数在过去十年中得到了深入研究，但主要集中在图同态领域。本文旨在系统研究一般关系结构的模同态计数问题。

Method: 提出了一种将模计数问题约简到更小域的新方法，并研究了3元素域和保守域上此类问题的复杂性。

Result: 主要成果包括开发了约简模计数问题到更小域的技术，并分析了3元素域和保守域上模计数问题的复杂性特征。

Conclusion: 本文为关系结构模同态计数的系统研究提供了新的约简方法和复杂性分析结果，扩展了模计数理论的研究范围。

Abstract: In the Constraint Satisfaction Problem (CSP for short) the goal is to decide
the existence of a homomorphism from a given relational structure $G$ to a
given relational structure $H$. If the structure $H$ is fixed and $G$ is the
only input, the problem is denoted $CSP(H)$. In its counting version,
$\#CSP(H)$, the task is to find the number of such homomorphisms. The CSP and
#CSP have been used to model a wide variety of combinatorial problems and have
received a tremendous amount of attention from researchers from multiple
disciplines.
  In this paper we consider the modular version of the counting CSPs, that is,
problems of the form $\#_pCSP(H)$ of counting the number of homomorphisms to
$H$ modulo a fixed prime number $p$. Modular counting has been intensively
studied during the last decade, although mainly in the case of graph
homomorphisms. Here we continue the program of systematic research of modular
counting of homomorphisms to general relational structures. The main results of
the paper include a new way of reducing modular counting problems to smaller
domains and a study of the complexity of such problems over 3-element domains
and over conservative domains, that is, relational structures that allow to
express (in a certain exact way) every possible unary predicate.

</details>


### [2] [Proof Strategy Extraction from LLMs for Enhancing Symbolic Provers](https://arxiv.org/abs/2510.10131)
*Jian Fang,Yican Sun,Yingfei Xiong*

Main category: cs.LO

TL;DR: Strat2Rocq从LLMs中提取证明策略并形式化为Rocq中的引理，增强符号证明器CoqHammer的能力，使其在软件验证中的成功率提升13.41%。


<details>
  <summary>Details</summary>
Motivation: 交互式定理证明需要大量人工努力，传统符号证明器与LLMs互补。但LLMs提示成本高且有安全风险，因此需要利用LLMs的策略来增强符号证明器。

Method: 分析LLMs在训练集上的证明轨迹，让LLM生成自然语言证明并总结为形式化引理，采用标准代理方法减少形式化过程中的错误。

Result: 在开源Rocq项目的软件验证中，Strat2Rocq将CoqHammer的成功率提高了13.41%。

Conclusion: 从LLMs中提取策略可以有效地增强符号证明器的能力，为软件验证提供了一种成本效益高且安全的方法。

Abstract: One important approach to software verification is interactive theorem
proving. However, writing formal proofs often requires substantial human
effort, making proof automation highly important. Traditionally, proof
automation has relied on symbolic provers. Recently, large language models
(LLMs) have demonstrated strong capabilities in theorem proving, complementing
symbolic provers. Nonetheless, prompting LLMs can be expensive and may pose
security risks for confidential codebases. As a result, purely symbolic
approaches remain important even in the LLM era, as they are cost-effective,
secure, and complement the strengths of LLMs.
  Motivated by these considerations, we ask a new research question: can we
extract the internal strategies of LLMs to enhance the capabilities of symbolic
provers? As an initial attempt to answer this question, we propose Strat2Rocq,
which extracts proof strategies from LLMs and formalizes them as lemmas in
Rocq. These lemmas are accessible to symbolic provers such as CoqHammer. With
the addition of these LLM-extracted lemmas, CoqHammer is able to prove more
theorems. The knowledge extraction process involves analyzing the proof
trajectories of LLMs on a training set of proved theorems. For each theorem, we
prompt the LLM to generate a natural language proof, then ask it to summarize
this proof into formalized lemmas with proofs. We also employ a standard
agentic approach to mitigate errors during formalization. Our evaluation
demonstrates that, on open-source Rocq projects for software verification,
Strat2Rocq enhances the success rate of CoqHammer by 13.41%.

</details>


### [3] [Formally Verified Certification of Unsolvability of Temporal Planning Problems](https://arxiv.org/abs/2510.10189)
*David Wang,Mohammad Abdulaziz*

Main category: cs.LO

TL;DR: 提出一种基于时间自动机网络编码和模型检查的时间规划不可解性认证方法，强调通过Isabelle/HOL形式化验证确保认证可信度


<details>
  <summary>Details</summary>
Motivation: 需要确保时间规划不可解性认证的可信度，传统方法缺乏形式化验证保证

Method: 将规划问题编码为时间自动机网络，使用高效模型检查器验证，再用形式化验证的证书检查器认证结果

Result: 开发了经过Isabelle/HOL形式化验证的时间自动机编码实现和证书检查流程

Conclusion: 该方法通过形式化验证确保了时间规划不可解性认证的可信度

Abstract: We present an approach to unsolvability certification of temporal planning.
Our approach is based on encoding the planning problem into a network of timed
automata, and then using an efficient model checker on the network followed by
a certificate checker to certify the output of the model checker. Our approach
prioritises trustworthiness of the certification: we formally verify our
implementation of the encoding to timed automata using the theorem prover
Isabelle/HOL and we use an existing certificate checker (also formally verified
in Isabelle/HOL) to certify the model checking result.

</details>


### [4] [Non-Expansive Fuzzy Coalgebraic Logic](https://arxiv.org/abs/2510.11080)
*Stefan Gebhart,Lutz Schröder,Paul Wild*

Main category: cs.LO

TL;DR: 本文研究了非扩张性coalgebraic模糊模态逻辑，提出了一个PSpace可判定性标准，并应用于概率和度量转换系统的定量模态逻辑。


<details>
  <summary>Details</summary>
Motivation: 模糊模态逻辑在Zadeh基下计算易处理但表达能力有限，而在Lukasiewicz基下表达能力强但计算复杂。本文旨在寻找在表达能力和计算复杂性之间取得平衡的中间方案。

Method: 使用非扩张性命题基的coalgebraic模糊模态逻辑框架，提供了一个PSpace可判定性标准。

Result: 恢复了非扩张性模糊ALC的PSpace复杂度结果，并为概率和度量转换系统的各种定量模态逻辑获得了新的PSpace上界。

Conclusion: 非扩张性coalgebraic模糊模态逻辑在表达能力和计算复杂性之间提供了良好的平衡，为多种定量模态逻辑提供了PSpace可判定性保证。

Abstract: Fuzzy logic extends the classical truth values "true" and "false" with
additional truth degrees in between, typically real numbers in the unit
interval. More specifically, fuzzy modal logics in this sense are given by a
choice of fuzzy modalities and a fuzzy propositional base. It has been noted
that fuzzy modal logics over the Zadeh base, which interprets disjunction as
maximum, are often computationally tractable but on the other hand add little
in the way of expressivity to their classical counterparts. Contrastingly,
fuzzy modal logics over the more expressive Lukasiewicz base have attractive
logical properties but are often computationally less tractable or even
undecidable. In the basic case of the modal logic of fuzzy relations, sometimes
termed fuzzy ALC, it has recently been shown that an intermediate non-expansive
propositional base, known from characteristic logics for behavioural distances
of quantitative systems, strikes a balance between these poles: It provides
increased expressiveness over the Zadeh base while avoiding the computational
problems of the Lukasiewicz base, in fact allowing for reasoning in PSpace.
Modal logics, in particular fuzzy modal logics, generally vary widely in terms
of syntax and semantics, involving, for instance, probabilistic, preferential,
or weighted structures. Coalgebraic modal logic provides a unifying framework
for wide ranges of semantically different modal logics, both two-valued and
fuzzy. In the present work, we focus on non-expansive coalgebraic fuzzy modal
logics, providing a criterion for decidability in PSpace. Using this criterion,
we recover the mentioned complexity result for non-expansive fuzzy ALC and
moreover obtain new PSpace upper bounds for various quantitative modal logics
for probabilistic and metric transition systems.

</details>


### [5] [Proceedings Twentieth International Workshop on Logical Frameworks and Meta-Languages: Theory and Practice](https://arxiv.org/abs/2510.11199)
*Kaustuv Chaudhuri,Daniele Nantes-Sobrinho*

Main category: cs.LO

TL;DR: 这是LFMTP 2025研讨会的论文集，包含在FSCD会议卫星活动中展示的论文


<details>
  <summary>Details</summary>
Motivation: 收集和展示逻辑框架与元语言领域的最新研究成果，促进该领域的学术交流

Method: 通过国际研讨会形式，邀请研究人员提交论文，经过同行评审后收录

Result: 成功举办了第20届LFMTP研讨会，汇集了该领域的最新研究进展

Conclusion: LFMTP继续作为逻辑框架和元语言领域的重要学术交流平台

Abstract: These are the contributed papers presented at the 20th International Workshop
on Logical Frameworks and Meta-Languages: Theory and Practice (LFMTP 2025), at
Birmingham, UK on 19 July as a satellite event of the FSCD conference. The
program committee for this edition of LFMTP was chaired by Kaustuv Chaudhuri
and Daniele Nantes-Sobrinho. More information about LFMTP can be found on
https://lfmtp.org.

</details>


### [6] [Cut-elimination for the alternation-free modal mu-calculus](https://arxiv.org/abs/2510.11293)
*Bahareh Afshari,Johannes Kloibhofer*

Main category: cs.LO

TL;DR: 提出了模态μ演算无交替片段的语法切割消除程序，在循环证明系统中进行切割约简，利用非良基证明结构直接将有切割的循环证明转换为无切割证明


<details>
  <summary>Details</summary>
Motivation: 为模态μ演算的无交替片段开发直接的切割消除方法，避免通过其他逻辑或依赖正则化中间机制

Method: 在循环证明系统中使用多切割和良拟序理论，利用非良基但有限分支的证明结构进行直接转换

Result: 成功实现了从有切割循环证明到无切割证明的直接转换

Conclusion: 该方法为模态μ演算无交替片段提供了新颖的语法切割消除程序

Abstract: We present a syntactic cut-elimination procedure for the alternation-free
fragment of the modal mu-calculus. Cut reduction is carried out within a cyclic
proof system, where proofs are finitely branching but may be non-wellfounded.
The structure of such proofs is exploited to directly transform a cyclic proof
with cuts into a cut-free one, without detouring through other logics or
relying on intermediate machinery for regularisation. Novel ingredients include
the use of multicuts and results from the theory of well-quasi-orders, the
later used in the termination argument.

</details>


### [7] [A Denotational Product Construction for Temporal Verification of Effectful Higher-Order Programs](https://arxiv.org/abs/2510.11320)
*Kazuki Watanabe,Mayuko Kori,Taro Sekiyama,Satoshi Kura,Hiroshi Unno*

Main category: cs.LO

TL;DR: 提出了一个范畴框架，用于对带有副效应的高阶程序进行线性时序验证，包括概率性高阶程序。该框架通过范畴化乘积构造，将线性时序安全验证转化为乘积程序的最弱前置条件计算。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以自动化验证带有副效应（如概率性）的高阶程序的线性时序性质，需要开发通用框架来统一处理这类问题。

Method: 使用范畴化乘积构造（denotational product construction），将效应高阶程序的线性时序验证转化为乘积程序的最弱前置条件计算，并利用现有求解器。

Result: 实现了概率性和天使非确定性高阶程序的实例化，并基于Kura和Unno的求解器开发了概率情况的自动化验证器。

Conclusion: 这是首个支持递归概率高阶程序线性时序验证的自动化验证器，通过范畴框架实现了通用且可扩展的验证方法。

Abstract: We propose a categorical framework for linear-time temporal verification of
effectful higher-order programs, including probabilistic higher-order programs.
Our framework provides a generic denotational reduction -- namely, a
denotational product construction -- from linear-time safety verification of
effectful higher-order programs to computation of weakest pre-conditions of
product programs. This reduction enables us to apply existing algorithms for
such well-studied computations of weakest pre-conditions, some of which are
available as off-the-shelf solvers. We show the correctness of our denotational
product construction by proving a preservation theorem under strong monad
morphisms and an existence of suitable liftings along a fibration. We
instantiate our framework with both probabilistic and angelic nondeterministic
higher-order programs, and implement an automated solver for the probabilistic
case based on the existing solver developed by Kura and Unno. To the best of
our knowledge, this is the first automated verifier for linear-time temporal
verification of probabilistic higher-order programs with recursion.

</details>


### [8] [Representations](https://arxiv.org/abs/2510.11419)
*Paul Brunet*

Main category: cs.LO

TL;DR: 提出一个名为Representations的软件分析系统元模型，旨在简化验证框架的开发，特别关注逻辑完备性证明。


<details>
  <summary>Details</summary>
Motivation: 自动化系统的形式分析需要不断开发新的验证框架来处理新编程特性或错误类型，其中逻辑完备性证明往往特别困难。

Method: 构建一个通用的软件分析系统（SAS）元模型，该模型对建模的SAS要求较少假设，能够捕获广泛的系统类别。

Result: 该模型有助于理解现有完备性证明的结构，并能利用这种结构构建新系统并证明其完备性。

Conclusion: 提出的Representations模型为软件分析系统的开发和完备性证明提供了实用的理论基础和方法指导。

Abstract: The formal analysis of automated systems is an important and growing
industry. This activity routinely requires new verification frameworks to be
developed to tackle new programming features, or new considerations (bugs of
interest). Often, one particular property can prove frustrating to establish:
completeness of the logic with respect to the semantics. In this paper, we try
and make such developments easier, with a particular attention on completeness.
Towards that aim, we propose a formal (meta-)model of software analysis systems
(SAS), the eponymous Representations. This model requires few assumptions on
the SAS being modeled, and as such is able to capture a large class of such
systems. We then show how our approach can be fruitful, both to understand how
existing completeness proofs can be structured, and to leverage this structure
to build new systems and prove their completeness.

</details>


### [9] [Lecture Notes on Verifying Graph Neural Networks](https://arxiv.org/abs/2510.11617)
*François Schwarzentruber*

Main category: cs.LO

TL;DR: 本文探讨了图神经网络与Weisfeiler-Lehman测试、一阶逻辑和分级模态逻辑之间的联系，并提出了一种包含线性不等式中计数模态的模态逻辑来解决图神经网络的验证任务。


<details>
  <summary>Details</summary>
Motivation: 建立图神经网络与形式逻辑之间的理论联系，并开发能够验证图神经网络属性的逻辑框架。

Method: 提出了一种包含计数模态的模态逻辑，其计数模态出现在线性不等式中。设计了一种基于经典模态逻辑表方法并扩展了带Presburger算术的布尔代数无量化片段推理的满足性算法。

Result: 开发了一种新的模态逻辑形式化方法，能够表达图神经网络的验证属性，并提供了相应的满足性判定算法。

Conclusion: 通过将图神经网络与形式逻辑连接，提出了一种有效的验证框架，为图神经网络的形式化验证提供了理论基础和实用工具。

Abstract: In these lecture notes, we first recall the connection between graph neural
networks, Weisfeiler-Lehman tests and logics such as first-order logic and
graded modal logic. We then present a modal logic in which counting modalities
appear in linear inequalities in order to solve verification tasks on graph
neural networks. We describe an algorithm for the satisfiability problem of
that logic. It is inspired from the tableau method of vanilla modal logic,
extended with reasoning in quantifier-free fragment Boolean algebra with
Presburger arithmetic.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [10] [The Tribonacci constant and finite automata](https://arxiv.org/abs/2510.10834)
*Jeffrey Shallit*

Main category: cs.FL

TL;DR: 该论文证明了不存在接受Tribonacci表示和Sturmian特征词的自动机


<details>
  <summary>Details</summary>
Motivation: 研究Tribonacci常数相关的自动机表示问题，探索Tribonacci表示与Sturmian词之间的关系

Method: 通过数学证明方法，分析Tribonacci表示和Sturmian特征词的自动机可接受性

Result: 证明了不存在接受n和⌊nψ⌋的Tribonacci表示的并行自动机，也不存在生成斜率为ψ-1的Sturmian特征词的Tribonacci自动机

Conclusion: Tribonacci常数相关的某些表示问题无法用自动机解决，这揭示了Tribonacci系统与Sturmian系统之间的根本差异

Abstract: We show that there is no automaton accepting the Tribonacci representations
of $n$ and $x$ in parallel, where $\psi = 1.839\cdots$ is the Tribonacci
constant, and $x= \lfloor n \psi \rfloor$. Similarly, there is no Tribonacci
automaton generating the Sturmian characteristic word with slope $\psi-1$.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [11] [A Comprehensive Survey on Benchmarks and Solutions in Software Engineering of LLM-Empowered Agentic System](https://arxiv.org/abs/2510.09721)
*Jiale Guo,Suizhi Huang,Mei Li,Dong Huang,Xingsheng Chen,Regina Zhang,Zhijiang Guo,Han Yu,Siu-Ming Yiu,Christian Jensen,Pietro Lio,Kwok-Yan Lam*

Main category: cs.SE

TL;DR: 该论文首次对LLM赋能的软件工程进行全面分析，提出了一个包含解决方案和基准测试的双维度分类法，分析了150多篇论文，揭示了从简单提示工程到复杂智能体系统的演进过程。


<details>
  <summary>Details</summary>
Motivation: LLM在软件工程中的应用引发了从传统规则系统到智能自主系统的范式转变，但领域缺乏对基准测试与解决方案之间相互关联的全面理解，阻碍了系统性进展和评估。

Method: 分析了150多篇最新论文，构建了一个全面的分类法，涵盖两个主要维度：解决方案（基于提示、微调和智能体的范式）和基准测试（代码生成、翻译、修复等任务）。

Result: 揭示了该领域从简单提示工程演进到包含规划分解、推理自优化、记忆机制和工具增强的复杂智能体系统，提出了连接50多个基准测试与对应解决方案策略的统一框架。

Conclusion: 该调查为研究人员和实践者提供了理解、评估和推进LLM赋能软件工程系统的基础资源，识别了关键研究空白并提出了可行的未来方向，包括多智能体协作框架和自进化代码生成系统。

Abstract: The integration of LLMs into software engineering has catalyzed a paradigm
shift from traditional rule-based systems to sophisticated agentic systems
capable of autonomous problem-solving. Despite this transformation, the field
lacks a comprehensive understanding of how benchmarks and solutions
interconnect, hindering systematic progress and evaluation. This survey
presents the first holistic analysis of LLM-empowered software engineering,
bridging the critical gap between evaluation and solution approaches. We
analyze 150+ recent papers and organize them into a comprehensive taxonomy
spanning two major dimensions: (1) Solutions, categorized into prompt-based,
fine-tuning-based, and agent-based paradigms, and (2) Benchmarks, covering code
generation, translation, repair, and other tasks. Our analysis reveals how the
field has evolved from simple prompt engineering to complex agentic systems
incorporating planning and decomposition, reasoning and self-refinement, memory
mechanisms, and tool augmentation. We present a unified pipeline that
illustrates the complete workflow from task specification to final
deliverables, demonstrating how different solution paradigms address varying
complexity levels across software engineering tasks. Unlike existing surveys
that focus on isolated aspects, we provide full-spectrum coverage connecting
50+ benchmarks with their corresponding solution strategies, enabling
researchers to identify optimal approaches for specific evaluation criteria.
Furthermore, we identify critical research gaps and propose actionable future
directions, including multi-agent collaboration frameworks, self-evolving code
generation systems, and integration of formal verification with LLM-based
methods. This survey serves as a foundational resource for researchers and
practitioners seeking to understand, evaluate, and advance LLM-empowered
software engineering systems.

</details>


### [12] [InteractScience: Programmatic and Visually-Grounded Evaluation of Interactive Scientific Demonstration Code Generation](https://arxiv.org/abs/2510.09724)
*Qiaosheng Chen,Yang Liu,Lei Li,Kai Chen,Qipeng Guo,Gong Cheng,Fei Yuan*

Main category: cs.SE

TL;DR: InteractScience是一个评估大语言模型在科学领域生成交互式演示代码能力的基准测试，结合了程序化功能测试和视觉定性测试来验证交互逻辑和渲染输出。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试要么评估知识问答而不涉及代码，要么评估静态网页代码生成而不涉及科学交互性。需要评估模型将准确科学知识与交互式前端代码实现相结合的能力。

Method: 设计了一个混合框架，结合程序化功能测试来严格验证交互逻辑，以及基于视觉的定性测试来评估渲染输出与参考快照的对比。构建了InteractScience基准，包含五个科学领域的精心设计问题，每个问题都配有单元测试、参考快照和检查清单。

Result: 评估了30个领先的开源和闭源LLM，结果显示在将领域知识与交互式前端编码整合方面仍存在持续弱点。

Conclusion: InteractScience是首个能够自动测量这种结合能力并具有现实交互操作的基准测试，为推进可靠且教育有用的科学演示代码生成奠定了基础。

Abstract: Large Language Models (LLMs) are increasingly capable of generating complete
applications from natural language instructions, creating new opportunities in
science and education. In these domains, interactive scientific demonstrations
are particularly valuable for explaining concepts, supporting new teaching
methods, and presenting research findings. Generating such demonstrations
requires models to combine accurate scientific knowledge with the ability to
implement interactive front-end code that behaves correctly and responds to
user actions. This capability goes beyond the scope of existing benchmarks,
which typically evaluate either knowledge question answering without grounding
in code or static web code generation without scientific interactivity. To
evaluate this integrated ability, we design a hybrid framework that combines
programmatic functional testing to rigorously verify interaction logic with
visually-grounded qualitative testing to assess rendered outputs against
reference snapshots. Building on this framework, we present InteractScience, a
benchmark consisting of a substantial set of carefully designed questions
across five scientific domains, each paired with unit tests, reference
snapshots, and checklists. We evaluate 30 leading open- and closed-source LLMs
and report results that highlight ongoing weaknesses in integrating domain
knowledge with interactive front-end coding. Our work positions InteractScience
as the first benchmark to automatically measure this combined capability with
realistic interactive operations, providing a foundation for advancing reliable
and educationally useful scientific demonstration code generation. All code and
data are publicly available at https://github.com/open-compass/InteractScience.

</details>


### [13] [Agentic Property-Based Testing: Finding Bugs Across the Python Ecosystem](https://arxiv.org/abs/2510.09907)
*Muhammad Maaz,Liam DeVoe,Zac Hatfield-Dodds,Nicholas Carlini*

Main category: cs.SE

TL;DR: 开发了一个基于LLM的代理，能够自动分析Python代码、推断属性、生成和执行基于属性的测试，并输出可操作的bug报告。在100个流行Python包上的评估显示，56%的bug报告是有效的，32%值得向维护者报告。


<details>
  <summary>Details</summary>
Motivation: 基于属性的测试(PBT)是一种轻量级形式化方法，但需要用户手动指定输入域和属性。本研究旨在利用LLM自动分析代码、推断属性并执行PBT，实现自动化软件测试。

Method: 开发LLM代理分析Python模块，从代码和文档中推断函数特定和跨函数属性，合成并执行基于属性的测试，通过测试输出确认真实bug，最终输出可操作的bug报告。

Result: 在100个流行Python包上评估，56%的bug报告有效，32%值得报告给维护者。使用排名标准后，21个高分bug中86%有效，81%值得报告。报告了5个bug（其中4个带补丁），3个补丁成功合并。

Conclusion: LLM与PBT结合提供了一种严谨且可扩展的自主软件测试方法，能够发现从序列化失败到数值精度错误等多种类型的bug。

Abstract: Property-based testing (PBT) is a lightweight formal method, typically
implemented as a randomized testing framework. Users specify the input domain
for their test using combinators supplied by the PBT framework, and the
expected properties or invariants as a unit-test function. The framework then
searches for a counterexample, e.g. by generating inputs and calling the test
function. In this work, we demonstrate an LLM-based agent which analyzes Python
modules, infers function-specific and cross-function properties from code and
documentation, synthesizes and executes PBTs, reflects on outputs of these
tests to confirm true bugs, and finally outputs actionable bug reports for the
developer. We perform an extensive evaluation of our agent across 100 popular
Python packages. Of the bug reports generated by the agent, we found after
manual review that 56\% were valid bugs and 32\% were valid bugs that we would
report to maintainers. We then developed a ranking rubric to surface
high-priority valid bugs to developers, and found that of the 21 top-scoring
bugs, 86\% were valid and 81\% we would report. The bugs span diverse failure
modes from serialization failures to numerical precision errors to flawed cache
implementations. We reported 5 bugs, 4 with patches, including to NumPy and
cloud computing SDKs, with 3 patches merged successfully. Our results suggest
that LLMs with PBT provides a rigorous and scalable method for autonomously
testing software. Our code and artifacts are available at:
https://github.com/mmaaz-git/agentic-pbt.

</details>


### [14] [OFP-Repair: Repairing Floating-point Errors via Original-Precision Arithmetic](https://arxiv.org/abs/2510.09938)
*Youshuai Tan,Zishuo Ding,Jinfu Chen,Weiyi Shang*

Main category: cs.SE

TL;DR: OFP-Repair是一种新的浮点程序错误修复方法，能够区分并修复原始精度和高精度计算错误，在多个数据集上表现优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 浮点程序错误在关键领域可能导致严重后果，现有修复工具要么需要高精度实现（开发耗时且需要专业知识），要么只能修复有限错误或产生次优结果。

Method: 提出OFP-Repair方法，能够区分原始精度可修复和高精度计算需求的错误，无需依赖高精度实现。

Result: 在ACESO数据集上，修复补丁在四个精度指标上分别提升了3、7、3和8个数量级；在真实案例中成功检测并修复了更多错误；在GSL库中修复了15个bug中的5个。

Conclusion: OFP-Repair方法具有实际应用价值，开发人员已表示兴趣并考虑将其集成到开发工作流中。

Abstract: Errors in floating-point programs can lead to severe consequences,
particularly in critical domains such as military, aerospace, and financial
systems, making their repair a crucial research problem. In practice, some
errors can be fixed using original-precision arithmetic, while others require
high-precision computation. Developers often avoid addressing the latter due to
excessive computational resources required. However, they sometimes struggle to
distinguish between these two types of errors, and existing repair tools fail
to assist in this differentiation. Most current repair tools rely on
high-precision implementations, which are time-consuming to develop and demand
specialized expertise. Although a few tools do not require high-precision
programs, they can only fix a limited subset of errors or produce suboptimal
results.
  To address these challenges, we propose a novel method, named OFP-Repair.On
ACESO's dataset, our patches achieve improvements of three, seven, three, and
eight orders of magnitude across four accuracy metrics. In real-world cases,
our method successfully detects all five original-precision-repairable errors
and fixes three, whereas ACESO only repairs one. Notably, these results are
based on verified data and do not fully capture the potential of OFP-Repair. To
further validate our method, we deploy it on a decade-old open bug report from
GNU Scientific Library (GSL), successfully repairing five out of 15 bugs. The
developers have expressed interest in our method and are considering
integrating our tool into their development workflow. We are currently working
on applying our patches to GSL. The results are highly encouraging,
demonstrating the practical applicability of our technique.

</details>


### [15] [Operationalizing AI: Empirical Evidence on MLOps Practices, User Satisfaction, and Organizational Context](https://arxiv.org/abs/2510.09968)
*Stefan Pasch*

Main category: cs.SE

TL;DR: 该研究分析了8000多条AI开发平台的用户评论，发现7种MLOps实践与用户满意度显著正相关，表明有效的MLOps实施为AI开发带来实际价值。组织背景影响MLOps实践的讨论频率，但不影响MLOps与满意度的关系。


<details>
  <summary>Details</summary>
Motivation: 尽管MLOps作为整合软件工程原则与ML生命周期管理的最佳实践已经出现，但关于这些实践是否以及如何支持用户开发和运营AI应用的实证证据仍然有限。

Method: 研究分析了G2.com上8000多条AI开发平台的用户评论，使用零样本分类方法测量评论对9种已确立的MLOps实践的情感倾向。

Result: 9种MLOps实践中的7种与用户满意度呈现显著正相关关系，表明有效的MLOps实施为AI开发带来实际价值。小型企业的评论者较少讨论某些MLOps实践，但企业规模不调节MLOps与满意度之间的联系。

Conclusion: 一旦应用，MLOps实践在不同组织环境中都被认为是普遍有益的。组织背景影响MLOps实践的讨论频率，但不影响MLOps与满意度的关系。

Abstract: Organizational efforts to utilize and operationalize artificial intelligence
(AI) are often accompanied by substantial challenges, including scalability,
maintenance, and coordination across teams. In response, the concept of Machine
Learning Operations (MLOps) has emerged as a set of best practices that
integrate software engineering principles with the unique demands of managing
the ML lifecycle. Yet, empirical evidence on whether and how these practices
support users in developing and operationalizing AI applications remains
limited. To address this gap, this study analyzes over 8,000 user reviews of AI
development platforms from G2.com. Using zero-shot classification, we measure
review sentiment toward nine established MLOps practices, including continuous
integration and delivery (CI/CD), workflow orchestration, reproducibility,
versioning, collaboration, and monitoring. Seven of the nine practices show a
significant positive relationship with user satisfaction, suggesting that
effective MLOps implementation contributes tangible value to AI development.
However, organizational context also matters: reviewers from small firms
discuss certain MLOps practices less frequently, suggesting that organizational
context influences the prevalence and salience of MLOps, though firm size does
not moderate the MLOps-satisfaction link. This indicates that once applied,
MLOps practices are perceived as universally beneficial across organizational
settings.

</details>


### [16] [SLEAN: Simple Lightweight Ensemble Analysis Network for Multi-Provider LLM Coordination: Design, Implementation, and Vibe Coding Bug Investigation Case Study](https://arxiv.org/abs/2510.10010)
*Matheus J. T. Vargas*

Main category: cs.SE

TL;DR: SLEAN是一个简单的轻量级集成分析框架，通过文本提示编排协调多个LLM提供商，采用三阶段协议过滤有害的AI生成代码建议，减少83-90%的代码变更表面，实现最小因果编辑。


<details>
  <summary>Details</summary>
Motivation: 解决AI辅助调试中产生的不必要复杂性、破坏现有功能或错误解决问题的修改，提供可靠的多提供商合成和端到端可审计性。

Method: 使用基于.txt模板的简单提示桥接LLM，采用独立分析、交叉批评和仲裁的三阶段协议，无需专业技术知识即可部署。

Result: 在15个软件bug评估中，从69个AI生成的修复建议中接受了22个修复（31.9%），拒绝了47个有害建议，仲裁过程将代码变更表面减少了83-90%。

Conclusion: SLEAN的文件驱动、提供商无关架构使其适用于安全审计、代码审查、文档验证等领域，无需专门编码专业知识即可部署。

Abstract: We present SLEAN (Simple Lightweight Ensemble Analysis Network), a
deterministic framework for coordinating multiple LLM providers through
text-based prompt orchestration. Unlike complex multi-agent systems requiring
specialized infrastructure, SLEAN operates as a simple prompt bridge between
LLMs using .txt templates, requiring no deep technical knowledge for
deployment. The three-phase protocol formed by independent analysis,
cross-critique, and arbitration, filters harmful AI-generated code suggestions
before production deployment, addressing how AI-assisted debugging increasingly
produces modifications that introduce unnecessary complexity, break existing
functionality, or address problems. Evaluating 15 software bugs, we analyzed 69
AI-generated fix propositions. SLEAN's filtering accepted 22 fixes (31.9%, 95%
CI 20.9-42.9%) while rejecting 47 that would have been harmful if applied
verbatim. The arbitration process reduced code change surface by 83-90%
relative to raw AI outputs, enforcing minimal causal edits over scope-expanding
modifications. Minimal Type 2 inputs proved more efficient than detailed Type 1
inputs, requiring 2.85 versus 3.56 propositions per accepted fix (35.1% versus
28.1% acceptance, about a 20% efficiency gain). Agreement between AI systems
showed weak correlation with fix quality: high convergence (at least 80%)
occurred in 4 of 15 cases and improved acceptance by only 2.4% points;
arbitration appeared only at exactly 10% convergence in 2 of 15 cases, although
low convergence alone did not necessitate arbitration. The file-driven,
provider-agnostic architecture enables deployment without specialized coding
expertise, making it applicable to security auditing, code review, document
verification, and other domains requiring reliable multi-provider synthesis
with end-to-end auditability.

</details>


### [17] [OBsmith: Testing JavaScript Obfuscator using LLM-powered sketching](https://arxiv.org/abs/2510.10066)
*Shan Jiang,Chenguang Zhu,Sarfraz Khurshid*

Main category: cs.SE

TL;DR: OBsmith是一个基于大语言模型的JavaScript混淆器测试框架，通过生成程序模板和从真实程序中提取模板来系统性地测试混淆器的语义保持能力，发现了11个未知的正确性bug。


<details>
  <summary>Details</summary>
Motivation: 现有的混淆器评估主要关注抗反混淆能力，而忽略了语义保持这一关键问题。不正确的转换可能无声地改变功能、损害可靠性和安全性，违背了混淆的根本目的。

Method: OBsmith利用LLM生成程序模板（捕捉不同语言结构、习语和边界情况），并将这些模板实例化为可执行程序，在不同配置下进行混淆测试。同时从真实程序中自动提取模板，实现更有针对性的测试。

Result: 发现了11个之前未知的正确性bug。在相同程序预算下，五个最先进的JavaScript模糊测试工具都无法检测到这些问题。消融实验表明除通用MR外的所有组件都至少贡献了一个bug类。

Conclusion: OBsmith是朝着自动化测试和混淆器等语义保持工具链质量保证的重要一步，结果还引发了关于如何平衡混淆预设和性能成本的讨论。

Abstract: JavaScript obfuscators are widely deployed to protect intellectual property
and resist reverse engineering, yet their correctness has been largely
overlooked compared to performance and resilience. Existing evaluations
typically measure resistance to deobfuscation, leaving the critical question of
whether obfuscators preserve program semantics unanswered. Incorrect
transformations can silently alter functionality, compromise reliability, and
erode security-undermining the very purpose of obfuscation. To address this
gap, we present OBsmith, a novel framework to systematically test JavaScript
obfuscators using large language models (LLMs). OBsmith leverages LLMs to
generate program sketches abstract templates capturing diverse language
constructs, idioms, and corner cases-which are instantiated into executable
programs and subjected to obfuscation under different configurations. Besides
LLM-powered sketching, OBsmith also employs a second source: automatic
extraction of sketches from real programs. This extraction path enables more
focused testing of project specific features and lets developers inject domain
knowledge into the resulting test cases. OBsmith uncovers 11 previously unknown
correctness bugs. Under an equal program budget, five general purpose
state-of-the-art JavaScript fuzzers (FuzzJIT, Jsfunfuzz, Superion, DIE,
Fuzzilli) failed to detect these issues, highlighting OBsmith's complementary
focus on obfuscation induced misbehavior. An ablation shows that all components
except our generic MRs contribute to at least one bug class; the negative MR
result suggests the need for obfuscator-specific metamorphic relations. Our
results also seed discussion on how to balance obfuscation presets and
performance cost. We envision OBsmith as an important step towards automated
testing and quality assurance of obfuscators and other semantic-preserving
toolchains.

</details>


### [18] [A Mathematics-Guided Approach to Floating-Point Error Detection](https://arxiv.org/abs/2510.10081)
*Youshuai Tan,Zhanwei Zhang,Zishuo Ding,Lianyu Zheng,Jinfu Chen,Weiyi Shang*

Main category: cs.SE

TL;DR: 提出了一种基于数学指导的MGDE方法，用于高效检测浮点程序中的错误诱导输入，相比现有FPCC方法在检测效果和效率上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 浮点程序错误在关键领域可能造成严重后果，现有方法存在计算成本高和长距离收敛能力有限的问题。

Method: 采用具有二次收敛特性的牛顿-拉弗森方法，通过数学指导来识别错误诱导输入。

Result: 在88个单输入浮点程序中，MGDE检测到44个程序中的89个错误，而FPCC仅检测到29个程序中的48个错误；MGDE速度比FPCC快6.4倍。在多输入程序中，MGDE检测到9个错误，平均耗时0.6443秒，而FPCC未能检测到任何错误且耗时更长。

Conclusion: MGDE方法在浮点程序错误检测方面比现有最先进方法更有效和高效，特别是在多输入程序中表现突出。

Abstract: Floating-point program errors can lead to severe consequences, particularly
in critical domains such as military applications. Only a small subset of
inputs may induce substantial floating-point errors, prompting researchers to
develop methods for identifying these error-inducing inputs. Although existing
approaches have achieved some success, they still suffer from two major
limitations: (1) High computational cost: The evaluation of error magnitude for
candidate inputs relies on high-precision programs, which are prohibitively
time-consuming. (2) Limited long-range convergence capability: Current methods
exhibit inefficiency in search, making the process akin to finding a needle in
a haystack.
  To address these two limitations, we propose a novel method, named MGDE, to
detect error-inducing inputs based on mathematical guidance. By employing the
Newton-Raphson method, which exhibits quadratic convergence properties, we
achieve highly effective and efficient results. Since the goal of identifying
error-inducing inputs is to uncover the underlying bugs, we use the number of
bugs detected in floating-point programs as the primary evaluation metric in
our experiments. As FPCC represents the most effective state-of-the-art
approach to date, we use it as the baseline for comparison. The dataset of FPCC
consists of 88 single-input floating-point programs. FPCC is able to detect 48
bugs across 29 programs, whereas our method successfully identifies 89 bugs
across 44 programs. Moreover, FPCC takes 6.4096 times as long as our proposed
method. We also deploy our method to multi-input programs, identifying a total
of nine bugs with an average detection time of 0.6443 seconds per program. In
contrast, FPCC fails to detect any bugs while requiring an average computation
time of 100 seconds per program.

</details>


### [19] [IntrinTrans: LLM-based Intrinsic Code Translator for RISC-V Vector](https://arxiv.org/abs/2510.10119)
*Liutong Han,Zhiyuan Tan,Hongbin Zhang,Pengcheng Wang,Chu Kang,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: IntrinTrans是一个基于LLM的多智能体方法，利用编译测试反馈自动跨架构翻译SIMD内联函数代码，并通过活跃度分析优化RISC-V Vector内联函数性能。


<details>
  <summary>Details</summary>
Motivation: 随着RISC-V生态系统的快速发展，对RISC-V Vector扩展支持的需求日益增长。现有跨架构翻译主要依赖手动重写，耗时且易错，而基于规则的方法翻译成功率和性能有限。

Method: 使用基于LLM的多智能体方法，结合编译测试反馈进行自动翻译，并通过活跃度分析获取寄存器使用信息来优化生成的RVV内联函数。

Result: 实验表明，高级LLM在有限迭代次数内能在大多数情况下生成语义正确的RISC-V Vector内联函数，在某些情况下性能达到开源社区原生实现的5.93倍。

Conclusion: IntrinTrans提供了一种自动化的跨架构内联函数翻译方法，能够生成高性能的RISC-V Vector代码，显著优于手动翻译和基于规则的方法。

Abstract: The use of intrinsic functions to exploit hardware-specific capabilities is
an important approach for optimizing library performance. Many mainstream
libraries implement a large number of vectorized algorithms on Arm or x86 SIMD
intrinsic functions. With the rapid expansion of the RISC-V hardware-software
ecosystem, there is a growing demand for support of the RISC-V Vector (RVV)
extension. Translating existing vectorized intrinsic code onto RVV intrinsics
is a practical and effective approach. However, current cross-architecture
translation largely relies on manual rewriting, which is time-consuming and
error-prone. Furthermore, while some rule-based methods can reduce the need for
manual intervention, their translation success rate is limited by incomplete
rule coverage and syntactic constraints, and the performance suffers from
inadequate utilization of RVV-specific features. We present IntrinTrans, a
LLM-based multi-agent approach that utilizes compile-and-test feedback to
translate intrinsic code across architectures automatically, and further
optimizes the generated RVV intrinsics using register-usage information derived
from liveness analysis. To evaluate the effectiveness of our approach, we
collected 34 vectorized algorithm cases from open-source libraries. Each case
includes an Arm Neon intrinsics implementation and a RVV intrinsics
implementation contributed by the open-source community, together with
correctness and performance tests. Our experiments show that advanced LLMs
produce semantically correct RISC-V Vector intrinsics in most cases within a
limited number of iterations, and in some cases achieve up to 5.93x the
performance of the native implementation from the open-source community.

</details>


### [20] [A Systematic Study on Generating Web Vulnerability Proof-of-Concepts Using Large Language Models](https://arxiv.org/abs/2510.10148)
*Mengyao Zhao,Kaixuan Li,Lyuye Zhang,Wenjing Dang,Chenggong Ding,Sen Chen,Zheli Liu*

Main category: cs.SE

TL;DR: LLMs能够利用公开CVE信息自动生成有效的PoC漏洞利用代码，在8%-34%的情况下成功，通过代码上下文和自适应推理策略可将成功率提升至68%-72%。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在代码理解方面的进步，研究其是否能够利用公开的CVE信息自动生成有效的PoC漏洞利用代码，这对漏洞复现、理解和缓解具有重要意义。

Method: 评估GPT-4o和DeepSeek-R1在100个真实可复现CVE上的表现，涵盖三个漏洞披露阶段：仅有描述的新披露漏洞、有补丁的1-day漏洞、有完整代码上下文的N-day漏洞。

Result: LLMs仅使用公开数据就能在8%-34%的情况下生成有效的PoC，DeepSeek-R1表现优于GPT-4o。添加代码上下文可将成功率提升17%-20%，函数级上下文比文件级提升9%-13%。结合自适应推理策略后成功率可达68%-72%。

Conclusion: LLMs能够显著改变漏洞利用的动态，已有23个新生成的PoC被NVD和Exploit DB接受，表明LLM生成的PoC具有实际价值。

Abstract: Recent advances in Large Language Models (LLMs) have brought remarkable
progress in code understanding and reasoning, creating new opportunities and
raising new concerns for software security. Among many downstream tasks,
generating Proof-of-Concept (PoC) exploits plays a central role in
vulnerability reproduction, comprehension, and mitigation. While previous
research has focused primarily on zero-day exploitation, the growing
availability of rich public information accompanying disclosed CVEs leads to a
natural question: can LLMs effectively use this information to automatically
generate valid PoCs? In this paper, we present the first empirical study of
LLM-based PoC generation for web application vulnerabilities, focusing on the
practical feasibility of leveraging publicly disclosed information. We evaluate
GPT-4o and DeepSeek-R1 on 100 real-world and reproducible CVEs across three
stages of vulnerability disclosure: (1) newly disclosed vulnerabilities with
only descriptions, (2) 1-day vulnerabilities with patches, and (3) N-day
vulnerabilities with full contextual code. Our results show that LLMs can
automatically generate working PoCs in 8%-34% of cases using only public data,
with DeepSeek-R1 consistently outperforming GPT-4o. Further analysis shows that
supplementing code context improves success rates by 17%-20%, with
function-level providing 9%-13% improvement than file-level ones. Further
integrating adaptive reasoning strategies to prompt refinement significantly
improves success rates to 68%-72%. Our findings suggest that LLMs could reshape
vulnerability exploitation dynamics. To date, 23 newly generated PoCs have been
accepted by NVD and Exploit DB.

</details>


### [21] [LLMs are All You Need? Improving Fuzz Testing for MOJO with Large Language Models](https://arxiv.org/abs/2510.10179)
*Linghan Huang,Peizhou Zhao,Huaming Chen*

Main category: cs.SE

TL;DR: MOJOFuzzer是首个针对新兴编程语言的零样本学习环境下自适应LLM模糊测试框架，通过多阶段过滤和动态提示调整，显著提高了测试用例的有效性和错误检测性能。


<details>
  <summary>Details</summary>
Motivation: MOJO作为新兴高性能AI编程语言缺乏完善的测试框架和语料库，导致LLM在模糊测试中产生语法正确但语义错误的代码，降低了测试效果。

Method: 提出多阶段框架系统消除低质量生成输入，并基于运行时反馈动态调整LLM提示进行测试用例变异，实现迭代学习过程。

Result: MOJOFuzzer显著提升了测试有效性、API覆盖率和错误检测性能，在MOJO语言中发现了13个先前未知的错误。

Conclusion: 该研究不仅推进了LLM驱动的软件测试领域，还为利用LLM测试新兴编程语言建立了基础方法论。

Abstract: The rapid development of large language models (LLMs) has revolutionized
software testing, particularly fuzz testing, by automating the generation of
diverse and effective test inputs. This advancement holds great promise for
improving software reliability. Meanwhile, the introduction of MOJO, a
high-performance AI programming language blending Python's usability with the
efficiency of C and C++, presents new opportunities to enhance AI model
scalability and programmability. However, as a new language, MOJO lacks
comprehensive testing frameworks and a sufficient corpus for LLM-based testing,
which exacerbates model hallucination. In this case, LLMs will generate
syntactically valid but semantically incorrect code, significantly reducing the
effectiveness of fuzz testing. To address this challenge, we propose
MOJOFuzzer, the first adaptive LLM-based fuzzing framework designed for
zero-shot learning environments of emerging programming languages. MOJOFuzzer
integrates a mutil-phase framework that systematically eliminates low-quality
generated inputs before execution, significantly improving test case validity.
Furthermore, MOJOFuzzer dynamically adapts LLM prompts based on runtime
feedback for test case mutation, enabling an iterative learning process that
continuously enhances fuzzing efficiency and bug detection performance. Our
experimental results demonstrate that MOJOFuzzer significantly enhances test
validity, API coverage, and bug detection performance, outperforming
traditional fuzz testing and state-of-the-art LLM-based fuzzing approaches.
Using MOJOFuzzer, we have conducted a first large-scale fuzz testing evaluation
of MOJO, uncorvering 13 previous unknown bugs. This study not only advances the
field of LLM-driven software testing but also establishes a foundational
methodology for leveraging LLMs in the testing of emerging programming
languages.

</details>


### [22] [Grounded AI for Code Review: Resource-Efficient Large-Model Serving in Enterprise Pipelines](https://arxiv.org/abs/2510.10290)
*Sayan Mandal,Hua Jiang*

Main category: cs.SE

TL;DR: 提出了一个基于静态分析和AST指导的自动化代码审查系统，使用量化开源模型在单GPU上运行，为合规性强的C/C++代码提供快速、准确的审查反馈。


<details>
  <summary>Details</summary>
Motivation: 在合规性要求高的环境中，传统静态分析工具产生大量低解释性输出，而直接使用LLM存在幻觉风险和成本问题，需要更可靠的自动化代码审查方案。

Method: 结合静态分析结果与AST指导的上下文提取，采用量化开源模型和分层缓存技术，构建解耦的系统架构，支持独立采用不同组件。

Result: 在安全导向的C/C++标准评估中，实现中位数59.8秒的首次反馈时间，违规减少效果与大型专有模型相当但违规率更低。内部调查显示减少了人工审查迭代次数。

Conclusion: 该系统在保持竞争力的同时降低了成本，强调可重现性、可审计性和向更广泛标准及辅助修补的扩展路径。

Abstract: Automated code review adoption lags in compliance-heavy settings, where
static analyzers produce high-volume, low-rationale outputs, and naive LLM use
risks hallucination and incurring cost overhead. We present a production system
for grounded, PR-native review that pairs static-analysis findings with
AST-guided context extraction and a single-GPU, on-demand serving stack
(quantized open-weight model, multi-tier caching) to deliver concise
explanations and remediation guidance. Evaluated on safety-oriented C/C++
standards, the approach achieves sub-minute median first-feedback (offline p50
build+LLM 59.8s) while maintaining competitive violation reduction and lower
violation rates versus larger proprietary models. The architecture is
decoupled: teams can adopt the grounding/prompting layer or the serving layer
independently. A small internal survey (n=8) provides directional signals of
reduced triage effort and moderate perceived grounding, with participants
reporting fewer human review iterations. We outline operational lessons and
limitations, emphasizing reproducibility, auditability, and pathways to broader
standards and assisted patching.

</details>


### [23] [Prepared for the Unknown: Adapting AIOps Capacity Forecasting Models to Data Changes](https://arxiv.org/abs/2510.10320)
*Lorena Poenaru-Olaru,Wouter van 't Hof,Adrian Stando,Arkadiusz P. Trawinski,Eileen Kapel,Jan S. Rellermeyer,Luis Cruz,Arie van Deursen*

Main category: cs.SE

TL;DR: 比较基于数据漂移检测的重新训练与定期重新训练在容量预测模型中的效果，发现漂移检测方法在大多数情况下能达到相近的准确性且更经济高效。


<details>
  <summary>Details</summary>
Motivation: 容量管理中频繁重新训练预测模型成本高昂且难以扩展，需要找到平衡准确性和效率的解决方案。

Method: 研究基于时间序列数据变化检测的重新训练策略，与定期重新训练进行对比分析。

Result: 漂移检测重新训练在大多数情况下能达到与定期重新训练相当的预测准确性，但在数据快速变化时定期重新训练仍更优。

Conclusion: 基于数据漂移的重新训练是成本效益更高的策略，能够减少重新训练开销同时保持稳健性能。

Abstract: Capacity management is critical for software organizations to allocate
resources effectively and meet operational demands. An important step in
capacity management is predicting future resource needs often relies on
data-driven analytics and machine learning (ML) forecasting models, which
require frequent retraining to stay relevant as data evolves. Continuously
retraining the forecasting models can be expensive and difficult to scale,
posing a challenge for engineering teams tasked with balancing accuracy and
efficiency. Retraining only when the data changes appears to be a more
computationally efficient alternative, but its impact on accuracy requires
further investigation. In this work, we investigate the effects of retraining
capacity forecasting models for time series based on detected changes in the
data compared to periodic retraining. Our results show that drift-based
retraining achieves comparable forecasting accuracy to periodic retraining in
most cases, making it a cost-effective strategy. However, in cases where data
is changing rapidly, periodic retraining is still preferred to maximize the
forecasting accuracy. These findings offer actionable insights for software
teams to enhance forecasting systems, reducing retraining overhead while
maintaining robust performance.

</details>


### [24] [Bridging Semantics & Structure for Software Vulnerability Detection using Hybrid Network Models](https://arxiv.org/abs/2510.10321)
*Jugal Gajjar,Kaustik Ranaware,Kamalasankari Subramaniakuppusamy*

Main category: cs.SE

TL;DR: 提出了一种结合异构图表示和轻量级本地LLM的混合框架，用于软件漏洞检测，在Java漏洞检测中达到93.57%的准确率，比现有方法提升8.36-17.81%。


<details>
  <summary>Details</summary>
Motivation: 现有静态和动态分析方法往往忽略影响不安全行为的结构依赖关系，且大型云模型存在成本和隐私问题。

Method: 将程序建模为异构图，捕获控制和数据流关系，结合轻量级(<4B)本地LLM，统一拓扑特征与语义推理。

Result: 在Java漏洞检测(二元分类)中达到93.57%准确率，比基于图注意力网络的嵌入方法提升8.36%，比预训练LLM基线(Qwen2.5 Coder 3B)提升17.81%。

Conclusion: 该方法为可扩展、可解释且本地可部署的漏洞分析工具铺平了道路，使分析从纯语法检查转向更深层次的结构和语义洞察。

Abstract: Software vulnerabilities remain a persistent risk, yet static and dynamic
analyses often overlook structural dependencies that shape insecure behaviors.
Viewing programs as heterogeneous graphs, we capture control- and data-flow
relations as complex interaction networks. Our hybrid framework combines these
graph representations with light-weight (<4B) local LLMs, uniting topological
features with semantic reasoning while avoiding the cost and privacy concerns
of large cloud models. Evaluated on Java vulnerability detection (binary
classification), our method achieves 93.57% accuracy-an 8.36% gain over Graph
Attention Network-based embeddings and 17.81% over pretrained LLM baselines
such as Qwen2.5 Coder 3B. Beyond accuracy, the approach extracts salient
subgraphs and generates natural language explanations, improving
interpretability for developers. These results pave the way for scalable,
explainable, and locally deployable tools that can shift vulnerability analysis
from purely syntactic checks to deeper structural and semantic insights,
facilitating broader adoption in real-world secure software development.

</details>


### [25] [Testing and Enhancing Multi-Agent Systems for Robust Code Generation](https://arxiv.org/abs/2510.10460)
*Zongyi Lyu,Songqiang Chen,Zhenlan Ji,Liwen Wang,Shuai Wang,Daoyuan Wu,Wenxuan Wang,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: 本文首次对多智能体系统在代码生成中的鲁棒性进行了全面研究，通过基于模糊测试的方法揭示了主流MAS存在的严重鲁棒性缺陷，并提出了有效的修复方法。


<details>
  <summary>Details</summary>
Motivation: 尽管多智能体系统在代码生成方面表现出色，但其鲁棒性尚未得到充分探索，这对实际部署构成了关键挑战。

Method: 设计了包含语义保留变异算子和新型适应度函数的模糊测试管道，评估主流MAS在不同数据集和LLM上的表现，并提出了包含多提示生成和监控智能体的修复方法。

Result: 研究发现各种流行MAS在应用语义保留变异后，无法解决7.9%-83.3%原本成功解决的问题。修复方法能有效解决40.0%-88.9%的已识别故障。

Conclusion: 该工作揭示了MAS在代码生成中的关键鲁棒性缺陷，提供了有效的缓解策略，为开发更可靠的代码生成MAS提供了重要见解。

Abstract: Multi-agent systems (MASs) have emerged as a promising paradigm for automated
code generation, demonstrating impressive performance on established benchmarks
by decomposing complex coding tasks across specialized agents with different
roles. Despite their prosperous development and adoption, their robustness
remains pressingly under-explored, raising critical concerns for real-world
deployment. This paper presents the first comprehensive study examining the
robustness of MASs for code generation through a fuzzing-based testing
approach. By designing a fuzzing pipeline incorporating semantic-preserving
mutation operators and a novel fitness function, we assess mainstream MASs
across multiple datasets and LLMs. Our findings reveal substantial robustness
flaws of various popular MASs: they fail to solve 7.9%-83.3% of problems they
initially resolved successfully after applying the semantic-preserving
mutations. Through comprehensive failure analysis, we identify a common yet
largely overlooked cause of the robustness issue: miscommunications between
planning and coding agents, where plans lack sufficient detail and coding
agents misinterpret intricate logic, aligning with the challenges inherent in a
multi-stage information transformation process. Accordingly, we also propose a
repairing method that encompasses multi-prompt generation and introduces a new
monitor agent to address this issue. Evaluation shows that our repairing method
effectively enhances the robustness of MASs by solving 40.0%-88.9% of
identified failures. Our work uncovers critical robustness flaws in MASs and
provides effective mitigation strategies, contributing essential insights for
developing more reliable MASs for code generation.

</details>


### [26] [How Students Use Generative AI for Software Testing: An Observational Study](https://arxiv.org/abs/2510.10551)
*Baris Ardic,Quentin Le Dilavrec,Andy Zaidman*

Main category: cs.SE

TL;DR: 研究新手开发者使用生成式AI进行单元测试的交互策略、依赖程度和感知收益挑战，发现四种交互策略和两种提示风格，AI辅助能节省时间但存在信任和质量问题。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具在软件工程中的集成带来了生产力提升机会，但也改变了开发者角色，特别是对新手开发者可能带来控制、输出质量和学习方面的担忧。

Method: 对12名具有软件测试基础知识的本科生进行观察研究，分析他们使用生成式AI进行单元测试任务的交互策略和提示风格。

Result: 识别出四种交互策略（测试想法和实施来源不同组合）和两种提示风格（一次性或迭代测试生成）。学生报告了节省时间、减轻认知负担等好处，但也存在信任度降低、测试质量问题等缺点。策略和提示风格影响工作流程，但不显著影响测试有效性或代码质量。

Conclusion: 生成式AI辅助单元测试对新手开发者既有益处也有挑战，需要平衡自动化辅助与学习机会，策略选择影响工作流程但不决定最终测试质量。

Abstract: The integration of generative AI tools like ChatGPT into software engineering
workflows opens up new opportunities to boost productivity in tasks such as
unit test engineering. However, these AI-assisted workflows can also
significantly alter the developer's role, raising concerns about control,
output quality, and learning, particularly for novice developers. This study
investigates how novice software developers with foundational knowledge in
software testing interact with generative AI for engineering unit tests. Our
goal is to examine the strategies they use, how heavily they rely on generative
AI, and the benefits and challenges they perceive when using generative
AI-assisted approaches for test engineering. We conducted an observational
study involving 12 undergraduate students who worked with generative AI for
unit testing tasks. We identified four interaction strategies, defined by
whether the test idea or the test implementation originated from generative AI
or the participant. Additionally, we singled out prompting styles that focused
on one-shot or iterative test generation, which often aligned with the broader
interaction strategy. Students reported benefits including time-saving, reduced
cognitive load, and support for test ideation, but also noted drawbacks such as
diminished trust, test quality concerns, and lack of ownership. While strategy
and prompting styles influenced workflow dynamics, they did not significantly
affect test effectiveness or test code quality as measured by mutation score or
test smells.

</details>


### [27] [Generative AI and the Transformation of Software Development Practices](https://arxiv.org/abs/2510.10819)
*Vivek Acharya*

Main category: cs.SE

TL;DR: 本文探讨生成式AI如何改变软件工程实践，包括聊天编程、多智能体系统等新技术，分析其带来的机遇与挑战。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI和大型语言模型如何重塑软件开发方式，探索AI辅助技术对软件工程实践的影响及相关信任、责任和技能转变问题。

Method: 通过案例研究和行业数据，调查迭代式聊天开发、多智能体系统、动态提示编排和模型上下文协议集成等技术。

Result: 生成式AI在编码中带来了更快的开发周期和编程民主化等机会，但也面临模型可靠性和成本等挑战。

Conclusion: 需要为AI在软件开发中的负责任和有效使用定义新的角色、技能和最佳实践。

Abstract: Generative AI is reshaping how software is designed, written, and maintained.
Advances in large language models (LLMs) are enabling new development styles -
from chat-oriented programming and 'vibe coding' to agentic programming - that
can accelerate productivity and broaden access. This paper examines how
AI-assisted techniques are changing software engineering practice, and the
related issues of trust, accountability, and shifting skills. We survey
iterative chat-based development, multi-agent systems, dynamic prompt
orchestration, and integration via the Model Context Protocol (MCP). Using case
studies and industry data, we outline both the opportunities (faster cycles,
democratized coding) and the challenges (model reliability and cost) of
applying generative AI to coding. We describe new roles, skills, and best
practices for using AI in a responsible and effective way.

</details>


### [28] [Agentic RAG for Software Testing with Hybrid Vector-Graph and Multi-Agent Orchestration](https://arxiv.org/abs/2510.10824)
*Mohanakrishnan Hariharan,Satish Arvapalli,Seshu Barma,Evangeline Sheela*

Main category: cs.SE

TL;DR: 使用基于Agentic RAG系统的软件测试自动化方法，通过自主AI代理和混合向量-图知识系统来自动生成测试计划、测试用例和质量工程指标，显著提升测试准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决传统软件测试的局限性，通过结合大型语言模型和多代理协调来改进质量工程工件的创建过程。

Method: 结合自主AI代理与混合向量-图知识系统，利用Gemini和Mistral等LLM、多代理编排和增强的情境化技术来自动化测试计划、用例和QE指标生成。

Result: 准确率从65%提升至94.8%，企业级验证显示测试时间线减少85%，测试套件效率提升85%，预计成本节省35%，上线时间提前2个月。

Conclusion: 该方法在软件测试自动化方面取得了显著成效，通过AI驱动的质量工程实现了高效、准确且具有全面文档可追溯性的测试流程。

Abstract: We present an approach to software testing automation using Agentic
Retrieval-Augmented Generation (RAG) systems for Quality Engineering (QE)
artifact creation. We combine autonomous AI agents with hybrid vector-graph
knowledge systems to automate test plan, case, and QE metric generation. Our
approach addresses traditional software testing limitations by leveraging LLMs
such as Gemini and Mistral, multi-agent orchestration, and enhanced
contextualization. The system achieves remarkable accuracy improvements from
65% to 94.8% while ensuring comprehensive document traceability throughout the
quality engineering lifecycle. Experimental validation of enterprise Corporate
Systems Engineering and SAP migration projects demonstrates an 85% reduction in
testing timeline, an 85% improvement in test suite efficiency, and projected
35% cost savings, resulting in a 2-month acceleration of go-live.

</details>


### [29] [Software Defect Prediction using Autoencoder Transformer Model](https://arxiv.org/abs/2510.10840)
*Seshu Barma,Mohanakrishnan Hariharan,Satish Arvapalli*

Main category: cs.SE

TL;DR: 提出ADE-QVAET模型，结合自适应差分进化和量子变分自编码器-Transformer，用于软件缺陷预测，在90%训练数据下达到98.08%准确率。


<details>
  <summary>Details</summary>
Motivation: 现有ML模型在处理噪声数据、类别不平衡、模式识别、特征提取和泛化方面存在困难，需要开发更有效的软件质量评估方法。

Method: 开发ADE-QVAET模型，结合自适应差分进化优化和量子变分自编码器-Transformer，获取高维潜在特征并保持序列依赖关系，通过超参数调优实现可扩展的缺陷预测。

Result: 在90%训练比例下，ADE-QVAET达到98.08%准确率、92.45%精确率、94.67%召回率和98.12% F1分数，优于差分进化ML模型。

Conclusion: ADE-QVAET通过AI-ML技术集成，为质量工程提供了有效的软件缺陷预测解决方案，显著提升了预测性能。

Abstract: An AI-ML-powered quality engineering approach uses AI-ML to enhance software
quality assessments by predicting defects. Existing ML models struggle with
noisy data types, imbalances, pattern recognition, feature extraction, and
generalization. To address these challenges, we develop a new model, Adaptive
Differential Evolution (ADE) based Quantum Variational Autoencoder-Transformer
(QVAET) Model (ADE-QVAET). ADE combines with QVAET to obtain high-dimensional
latent features and maintain sequential dependencies, resulting in enhanced
defect prediction accuracy. ADE optimization enhances model convergence and
predictive performance. ADE-QVAET integrates AI-ML techniques such as tuning
hyperparameters for scalable and accurate software defect prediction,
representing an AI-ML-driven technology for quality engineering. During
training with a 90% training percentage, ADE-QVAET achieves high accuracy,
precision, recall, and F1-score of 98.08%, 92.45%, 94.67%, and 98.12%,
respectively, when compared to the Differential Evolution (DE) ML model.

</details>


### [30] [Generative AI for Software Project Management: Insights from a Review of Software Practitioner Literature](https://arxiv.org/abs/2510.10887)
*Lakshana Iruni Assalaarachchi,Zainab Masood,Rashina Hoda,John Grundy*

Main category: cs.SE

TL;DR: 软件项目经理将GenAI视为助手而非替代品，支持自动化常规任务、预测分析、沟通协作和敏捷实践，但需负责任使用以应对幻觉、伦理隐私等问题。


<details>
  <summary>Details</summary>
Motivation: 了解软件项目管理中GenAI转型的现状，通过从业者公开讨论来掌握实际应用情况。

Method: 对47个公开从业者资源（博客、文章、行业报告）进行灰色文献综述。

Result: 软件项目经理主要将GenAI视为"助手"、"副驾驶"或"朋友"而非"PM替代品"，支持自动化常规任务、预测分析、沟通协作和敏捷实践。从业者强调负责任使用GenAI，关注幻觉、伦理隐私、缺乏情感智能和人类判断等问题。

Conclusion: 提出了GenAI时代软件项目经理的技能提升要求，映射到项目管理协会人才三角模型，并为从业者和研究人员提供了关键建议。

Abstract: Software practitioners are discussing GenAI transformations in software
project management openly and widely. To understand the state of affairs, we
performed a grey literature review using 47 publicly available practitioner
sources including blogs, articles, and industry reports. We found that software
project managers primarily perceive GenAI as an "assistant", "copilot", or
"friend" rather than as a "PM replacement", with support of GenAI in automating
routine tasks, predictive analytics, communication and collaboration, and in
agile practices leading to project success. Practitioners emphasize responsible
GenAI usage given concerns such as hallucinations, ethics and privacy, and lack
of emotional intelligence and human judgment. We present upskilling
requirements for software project managers in the GenAI era mapped to the
Project Management Institute's talent triangle. We share key recommendations
for both practitioners and researchers.

</details>


### [31] [Project-Level C-to-Rust Translation via Synergistic Integration of Knowledge Graphs and Large Language Models](https://arxiv.org/abs/2510.10956)
*Zhiqiang Yuan,Wenjun Mao,Zhuo Chen,Xiyue Shang,Chong Wang,Yiling Lou,Xin Peng*

Main category: cs.SE

TL;DR: 提出了一种基于指针知识图谱的C到Rust项目级翻译方法，通过全局指针语义信息显著提升翻译代码的安全性和正确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的C到Rust翻译方法在项目级别存在困难，特别是对指针的处理缺乏全局视角，导致生成的Rust代码仍然不够安全。

Method: 构建C-Rust指针知识图谱，包含指针使用信息和Rust导向的注解，结合LLM进行项目级翻译。

Result: 相比基于规则的翻译和传统LLM重写，减少了99.9%的不安全使用，功能正确性比模糊测试增强的LLM方法平均提高29.3%。

Conclusion: 指针知识图谱能够有效提升LLM在C到Rust翻译中的表现，生成更安全、更符合Rust习惯的代码。

Abstract: Translating C code into safe Rust is an effective way to ensure its memory
safety. Compared to rule-based translation which produces Rust code that
remains largely unsafe, LLM-based methods can generate more idiomatic and safer
Rust code because LLMs have been trained on vast amount of human-written
idiomatic code. Although promising, existing LLM-based methods still struggle
with project-level C-to-Rust translation. They typically partition a C project
into smaller units (\eg{} functions) based on call graphs and translate them
bottom-up to resolve program dependencies. However, this bottom-up,
unit-by-unit paradigm often fails to translate pointers due to the lack of a
global perspective on their usage. To address this problem, we propose a novel
C-Rust Pointer Knowledge Graph (KG) that enriches a code-dependency graph with
two types of pointer semantics: (i) pointer-usage information which record
global behaviors such as points-to flows and map lower-level struct usage to
higher-level units; and (ii) Rust-oriented annotations which encode ownership,
mutability, nullability, and lifetime. Synthesizing the \kg{} with LLMs, we
further propose \ourtool{}, which implements a project-level C-to-Rust
translation technique. In \ourtool{}, the \kg{} provides LLMs with
comprehensive pointer semantics from a global perspective, thus guiding LLMs
towards generating safe and idiomatic Rust code from a given C project. Our
experiments show that \ourtool{} reduces unsafe usages in translated Rust by
99.9\% compared to both rule-based translation and traditional LLM-based
rewriting, while achieving an average 29.3\% higher functional correctness than
those fuzzing-enhanced LLM methods.

</details>


### [32] [RepoSummary: Feature-Oriented Summarization and Documentation Generation for Code Repositories](https://arxiv.org/abs/2510.11039)
*Yifeng Zhu,Xianlin Zhao,Xutian Li,Yanzhen Zou,Haizhuo Yuan,Yue Wang,Bing Xie*

Main category: cs.SE

TL;DR: RepoSummary是一种面向特征的代码仓库摘要方法，能自动生成仓库文档并建立从功能特征到代码元素的准确可追溯链接，相比现有方法在特征覆盖率和可追溯性方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有仓库摘要技术主要基于目录树结构进行代码摘要，无法有效追踪高层次特征到协作实现它们的对应方法，这限制了代码理解和维护的效率。

Method: 提出RepoSummary方法，采用面向特征的方法进行代码仓库摘要，同时自动生成仓库文档，并建立从功能特征到相应代码元素的准确可追溯链接。

Result: 相比最先进的基线方法HGEN，RepoSummary在特征覆盖率和可追溯性方面显著提升：完全覆盖特征的比例从61.2%提高到71.1%，文件级可追溯召回率从29.9%提升到53.0%，生成的文档在概念一致性、可理解性和格式方面也更优。

Conclusion: RepoSummary通过特征导向的方法有效解决了现有仓库摘要技术的局限性，为开发者在代码理解和维护过程中快速定位相关方法和文件提供了有力支持。

Abstract: Repository summarization is a crucial research question in development and
maintenance for software engineering. Existing repository summarization
techniques primarily focus on summarizing code according to the directory tree,
which is insufficient for tracing high-level features to the methods that
collaboratively implement them. To address these limitations, we propose
RepoSummary, a feature-oriented code repository summarization approach that
simultaneously generates repository documentation automatically. Furthermore,
it establishes more accurate traceability links from functional features to the
corresponding code elements, enabling developers to rapidly locate relevant
methods and files during code comprehension and maintenance. Comprehensive
experiments against the state-of-the-art baseline (HGEN) demonstrate that
RepoSummary achieves higher feature coverage and more accurate traceability. On
average, it increases the rate of completely covered features in manual
documentation from 61.2% to 71.1%, improves file-level traceability recall from
29.9% to 53.0%, and generates documentation that is more conceptually
consistent, easier to understand, and better formatted than that produced by
existing approaches.

</details>


### [33] [Defects4C: Benchmarking Large Language Model Repair Capability with C/C++ Bugs](https://arxiv.org/abs/2510.11059)
*Jian Wang,Xiaofei Xie,Qiang Hu,Shangqing Liu,Jiongchi Yu,Jiaolong Klong,Yi Li*

Main category: cs.SE

TL;DR: 提出了Defects4C基准数据集，用于C/C++程序修复研究，并评估了24个大型语言模型在修复C/C++错误方面的效果。


<details>
  <summary>Details</summary>
Motivation: 当前Java程序修复研究已有Defects4J等基准，但C/C++程序修复研究存在明显差距，缺乏高质量的开源基准数据集。

Method: 构建了Defects4C基准数据集，包含真实C/C++仓库中的900万个bug相关提交、248个高质量bug函数和102个漏洞函数，并配有测试用例。使用该数据集评估了24个最先进的大型语言模型。

Result: 研究揭示了当前基于LLM的C/C++程序修复技术的优势和局限性，强调了需要更鲁棒的方法。

Conclusion: Defects4C在推动未来C/C++程序修复研究中发挥着关键作用，填补了该领域基准数据集的空白。

Abstract: Automated Program Repair (APR) plays a critical role in enhancing the quality
and reliability of software systems. While substantial progress has been made
in Java-based APR, largely facilitated by benchmarks like Defects4J, there
remains a significant gap in research on C/C++ program repair, despite the
widespread use of C/C++ and the prevalence of associated vulnerabilities. This
gap is primarily due to the lack of high-quality, open-source benchmarks
tailored for C/C++.
  To address this issue, we introduce Defects4C, a comprehensive and executable
benchmark specifically designed for C/C++ program repair. Our dataset is
constructed from real-world C/C++ repositories and includes a large collection
of bug-relevant commits (9M in total), 248 high-quality buggy functions, and
102 vulnerable functions, all paired with test cases for reproduction. These
resources enable rigorous evaluation of repair techniques and support the
retraining of learning-based approaches for enhanced performance.
  Using Defects4C, we conduct a comprehensive empirical study evaluating the
effectiveness of 24 state-of-the-art large language models (LLMs) in repairing
C/C++ faults. Our findings offer valuable insights into the strengths and
limitations of current LLM-based APR techniques in this domain, highlighting
both the need for more robust methods and the critical role of Defects4C in
advancing future research

</details>


### [34] [DebugTA: An LLM-Based Agent for Simplifying Debugging and Teaching in Programming Education](https://arxiv.org/abs/2510.11076)
*Lingyue Fu,Haowei Yuan,Datong Chen,Xinyi Dai,Qingyao Li,Weinan Zhang,Weiwen Liu,Yong Yu*

Main category: cs.SE

TL;DR: 提出DebugTA，一种基于LLM的调试和教学代理，通过专用工具和显式教学原则，将复杂调试任务分解为顺序LLM交互，提高教学效果并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 编程教育中的调试与教学任务面临输入复杂性和异质性挑战，现有方法未能充分利用标准代码，限制了LLM的潜力。

Method: 开发DebugTA代理，包含标准代码检索、变量替换对齐参考代码、外部编译器实时分析等专用工具，通过分解任务为顺序LLM交互简化推理。

Result: 在三个真实世界代码数据集上的实验表明，DebugTA持续提高教学效果，同时显著降低计算成本。

Conclusion: DebugTA通过工具调用和任务分解有效解决了DT任务中的推理挑战，为编程教育中的调试教学提供了有效解决方案。

Abstract: In programming education, Debugging and Teaching (DT) task is a common
scenario where students receive assistance in correcting their erroneous code.
The task involves multiple inputs, including erroneous code, error messages,
reference solutions, and the question description, with the goal of generating
modification suggestions to the erroneous code. However, two key challenges
hinder the effectiveness of existing approaches. Firstly, the complexity and
heterogeneity of inputs inherent in DT tasks significantly elevate the
reasoning challenges faced by LLMs. Second, existing approaches often fail to
fully leverage the availability of standard code in DT tasks, forcing models to
rely solely on complex multi-step reasoning, which limits the potential of LLMs
in addressing DT tasks effectively. To address these challenges, we propose
DebugTA, a novel LLM-based debugging and teaching agent with specialized tools
for standard code retrieval, variable substitution to align reference code, and
an external compiler for real-time code analysis. Guided by explicit
pedagogical and debugging principles, DebugTA acts as an agent that decomposes
a complex task into sequential LLM interactions, each utilizing distinct tools
for specific subtasks, thereby simplifying the logical reasoning at each step
and reducing overall reasoning complexity. Furthermore, DebugTA utilizes tool
calls to align the standard code with the erroneous code as much as possible,
allowing the LLM to focus on logic errors within the erroneous code and
improving the accuracy of the generated suggestions. To rigorously assess the
quality of modification suggestions, we introduce a student simulator-teacher
interaction paradigm. Experimental results on three real-world code datasets
demonstrate that DebugTA consistently improves teaching effectiveness while
significantly reducing computational costs.

</details>


### [35] [What Slows Down FMware Development? An Empirical Study of Developer Challenges and Resolution Times](https://arxiv.org/abs/2510.11138)
*Zitao Wang,Zhimin Zhao,Michael W. Godfrey*

Main category: cs.SE

TL;DR: 本文首次对FMware（基于基础模型的应用和基础设施）在云端平台和开源仓库中的开发进行了大规模分析，揭示了主要应用领域、开发挑战和耗时问题。


<details>
  <summary>Details</summary>
Motivation: 基础模型（如GPT）正在彻底改变软件工程实践，但FMware的设计、实现和演进带来了新的挑战，特别是在云端和本地平台中，其目标、流程和工具与传统软件开发存在差异。

Method: 通过分析GitHub仓库和主要FMware平台（HuggingFace、GPTStore、Ora、Poe）的数据，从三个维度实证研究FMware生态系统：常见应用领域、开发者遇到的关键挑战、需要最大努力解决的问题类型。

Result: 研究发现FMware主要聚焦教育、内容创作和商业战略领域；存在内存管理、依赖处理和分词器配置等技术挑战；在GitHub上，错误报告和核心功能问题最常被报告，而代码审查、相似性搜索和提示模板设计最耗时。

Conclusion: 通过揭示开发实践和痛点，本研究为改进FMware工具、工作流程和社区支持提供了机会，并为FMware开发的未来提供了可操作的见解。

Abstract: Foundation Models (FMs), such as OpenAI's GPT, are fundamentally transforming
the practice of software engineering by enabling the development of
\emph{FMware} -- applications and infrastructures built around these models.
FMware systems now support tasks such as code generation, natural-language
interaction, knowledge integration, and multi-modal content creation,
underscoring their disruptive impact on current software engineering workflows.
However, the design, implementation, and evolution of FMware present
significant new challenges, particularly across cloud-based and on-premise
platforms where goals, processes, and tools often diverge from those of
traditional software development.
  To our knowledge, this is the first large-scale analysis of FMware
development across both cloud-based platforms and open-source repositories. We
empirically investigate the FMware ecosystem through three focus areas: (1) the
most common application domains of FMware, (2) the key challenges developers
encounter, and (3) the types of issues that demand the greatest effort to
resolve. Our analysis draws on data from GitHub repositories and from leading
FMware platforms, including HuggingFace, GPTStore, Ora, and Poe. Our findings
reveal a strong focus on education, content creation, and business strategy,
alongside persistent technical challenges in memory management, dependency
handling, and tokenizer configuration. On GitHub, bug reports and core
functionality issues are the most frequently reported problems, while code
review, similarity search, and prompt template design are the most
time-consuming to resolve.
  By uncovering developer practices and pain points, this study points to
opportunities to improve FMware tools, workflows, and community support, and
provides actionable insights to help guide the future of FMware development.

</details>


### [36] [Interoperability From OpenTelemetry to Kieker: Demonstrated as Export from the Astronomy Shop](https://arxiv.org/abs/2510.11179)
*David Georg Reichelt,Shinhyung Yang,Wilhelm Hasselbring*

Main category: cs.SE

TL;DR: 将OpenTelemetry追踪数据转换为Kieker框架格式，使Kieker能够分析更多编程语言的数据


<details>
  <summary>Details</summary>
Motivation: Kieker框架目前只支持有限的语言（Java、C、Fortran、Python），而OpenTelemetry标准支持更多语言（如C#、JavaScript），需要将两者集成以扩展分析能力

Method: 开发数据转换方法，将OpenTelemetry的追踪数据格式转换为Kieker框架可处理的格式

Result: 成功实现了数据转换，能够从OpenTelemetry检测的应用（如Astronomy Shop演示应用）生成调用树等分析结果

Conclusion: 通过集成OpenTelemetry和Kieker，显著扩展了Kieker框架的语言支持范围，增强了其可观测性分析能力

Abstract: The observability framework Kieker provides a range of analysis capabilities,
but it is currently only able to instrument a smaller selection of languages
and technologies, including Java, C, Fortran, and Python. The OpenTelemetry
standard aims for providing reference implementations for most programming
languages, including C# and JavaScript, that are currently not supported by
Kieker. In this work, we describe how to transform OpenTelemetry tracing data
into the Kieker framework. Thereby, it becomes possible to create for example
call trees from OpenTelemetry instrumentations. We demonstrate the usability of
our approach by visualizing trace data of the Astronomy Shop, which is an
OpenTelemetry demo application.

</details>


### [37] [Detection of Performance Changes in MooBench Results Using Nyrkiö on GitHub Actions](https://arxiv.org/abs/2510.11310)
*Shinhyung Yang,David Georg Reichelt,Henrik Ingo,Wilhelm Hasselbring*

Main category: cs.SE

TL;DR: 将Nyrkiö变更检测服务集成到MooBench中，实现了GitHub项目性能变化的自动检测，发现了一个由Linux内核版本变更导致的性能回归问题。


<details>
  <summary>Details</summary>
Motivation: GitHub上有5.18亿个项目，性能变化对项目用户至关重要，但性能变更检测是一个具有挑战性的问题。

Method: 在MooBench中集成Nyrkiö变更检测服务，将测量数据上传到该服务以检测性能变化。

Result: 识别出一个主要的性能回归问题，该问题可通过GitHub Actions重现，且由Linux内核版本变更引起。

Conclusion: 通过集成变更检测服务，成功实现了GitHub项目性能变化的自动检测，并发现了具体的性能回归原因。

Abstract: In GitHub with its 518 million hosted projects, performance changes within
these projects are highly relevant to the project's users. Although performance
measurement is supported by GitHub CI/CD, performance change detection is a
challenging topic.
  In this paper, we demonstrate how we incorporated Nyrki\"o to MooBench. Prior
to this work, Moobench continuously ran on GitHub virtual machines, measuring
overhead of tracing agents, but without change detection. By adding the upload
of the measurements to the Nyrki\"o change detection service, we made it
possible to detect performance changes. We identified one major performance
regression and examined the performance change in depth. We report that (1) it
is reproducible with GitHub actions, and (2) the performance regression is
caused by a Linux Kernel version change.

</details>


### [38] [Cracking CodeWhisperer: Analyzing Developers' Interactions and Patterns During Programming Tasks](https://arxiv.org/abs/2510.11516)
*Jeena Javahar,Tanya Budhrani,Manaal Basha,Cleidson R. B. de Souza,Ivan Beschastnikh,Gema Rodriguez-Perez*

Main category: cs.SE

TL;DR: 研究分析了开发者使用Amazon CodeWhisperer（基于LLM的代码生成工具）的行为模式，通过用户研究识别出四种关键交互模式。


<details>
  <summary>Details</summary>
Motivation: 随着AI代码生成工具的普及，理解开发者如何采用这些工具变得重要，特别是针对Amazon CodeWhisperer的使用行为。

Method: 进行了两个用户研究，每组10名参与者，第一个研究确定关键交互，第二个研究使用自定义遥测插件收集低级别交互数据，采用混合方法分析。

Result: 识别出四种行为模式：1）增量代码精炼 2）使用自然语言注释的显式指令 3）基于模型建议的基线结构化 4）与外部资源的整合使用。

Conclusion: 提供了对这些行为模式的全面分析，有助于理解开发者如何与AI代码生成工具互动。

Abstract: The use of AI code-generation tools is becoming increasingly common, making
it important to understand how software developers are adopting these tools. In
this study, we investigate how developers engage with Amazon's CodeWhisperer,
an LLM-based code-generation tool. We conducted two user studies with two
groups of 10 participants each, interacting with CodeWhisperer - the first to
understand which interactions were critical to capture and the second to
collect low-level interaction data using a custom telemetry plugin. Our
mixed-methods analysis identified four behavioral patterns: 1) incremental code
refinement, 2) explicit instruction using natural language comments, 3)
baseline structuring with model suggestions, and 4) integrative use with
external sources. We provide a comprehensive analysis of these patterns .

</details>


### [39] [CodeWatcher: IDE Telemetry Data Extraction Tool for Understanding Coding Interactions with LLMs](https://arxiv.org/abs/2510.11536)
*Manaal Basha,Aimeê M. Ribeiro,Jeena Javahar,Cleidson R. B. de Souza,Gema Rodríguez-Pérez*

Main category: cs.SE

TL;DR: CodeWatcher是一个轻量级、无干扰的客户端-服务器系统，用于在VS Code编辑器中捕获开发者与代码生成工具的细粒度交互事件，支持编程行为研究。


<details>
  <summary>Details</summary>
Motivation: 理解开发者如何与代码生成工具交互需要详细的实时编程行为数据，但收集这些数据往往会影响工作流程。

Method: 开发了包含VS Code插件、Python RESTful API和MongoDB后端的系统，记录插入、删除、复制粘贴和焦点切换等语义化事件。

Result: 系统能够连续监控开发者活动而不修改用户工作流程，支持编程会话的重构和行为分析。

Conclusion: 该基础设施对于负责任AI、开发者生产力以及代码生成工具的人本评估研究至关重要。

Abstract: Understanding how developers interact with code generation tools (CGTs)
requires detailed, real-time data on programming behavior which is often
difficult to collect without disrupting workflow. We present
\textit{CodeWatcher}, a lightweight, unobtrusive client-server system designed
to capture fine-grained interaction events from within the Visual Studio Code
(VS Code) editor. \textit{CodeWatcher} logs semantically meaningful events such
as insertions made by CGTs, deletions, copy-paste actions, and focus shifts,
enabling continuous monitoring of developer activity without modifying user
workflows. The system comprises a VS Code plugin, a Python-based RESTful API,
and a MongoDB backend, all containerized for scalability and ease of
deployment. By structuring and timestamping each event, \textit{CodeWatcher}
enables post-hoc reconstruction of coding sessions and facilitates rich
behavioral analyses, including how and when CGTs are used during development.
This infrastructure is crucial for supporting research on responsible AI,
developer productivity, and the human-centered evaluation of CGTs. Please find
the demo, diagrams, and tool here: https://osf.io/j2kru/overview.

</details>


### [40] [Automatically Generating Questions About Scratch Programs](https://arxiv.org/abs/2510.11658)
*Florian Obermüller,Gordon Fraser*

Main category: cs.SE

TL;DR: 该论文提出了一个自动为Scratch程序生成理解性问题的方法，基于LitterBox静态分析工具，在60多万个项目中生成了5400多万个问题，并通过实验验证了这些问题的有效性。


<details>
  <summary>Details</summary>
Motivation: 学生在编程学习中，仅仅完成编程任务并不能保证真正理解了编程概念。需要通过对代码提问来评估程序理解能力，但为每个学生程序手动创建针对性问题很繁琐。

Method: 扩展LitterBox静态分析工具，为Scratch程序自动生成30种不同类型的问题，这些问题基于已建立的程序理解模型。

Result: 在600,913个Scratch项目中自动生成了54,118,694个问题。对34名九年级学生的初步实验表明，该方法能生成有意义的问题，且学生回答问题的能力与其整体表现相关。

Conclusion: 自动生成程序理解问题的方法在Scratch编程环境中是可行的，能够有效评估学生的程序理解能力，并与学生的整体学习表现相关联。

Abstract: When learning to program, students are usually assessed based on the code
they wrote. However, the mere completion of a programming task does not
guarantee actual comprehension of the underlying concepts. Asking learners
questions about the code they wrote has therefore been proposed as a means to
assess program comprehension. As creating targeted questions for individual
student programs can be tedious and challenging, prior work has proposed to
generate such questions automatically. In this paper we generalize this idea to
the block-based programming language Scratch. We propose a set of 30 different
questions for Scratch code covering an established program comprehension model,
and extend the LitterBox static analysis tool to automatically generate
corresponding questions for a given Scratch program. On a dataset of 600,913
projects we generated 54,118,694 questions automatically. Our initial
experiments with 34 ninth graders demonstrate that this approach can indeed
generate meaningful questions for Scratch programs, and we find that the
ability of students to answer these questions on their programs relates to
their overall performance.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [41] [Abstract String Domain Defined with Word Equations as a Reduced Product (Extended Version)](https://arxiv.org/abs/2510.11007)
*Antonina Nepeivoda,Ilya Afanasyev*

Main category: cs.PL

TL;DR: 提出了一个字符串区间抽象域，通过字方程和字不等式来表征字符串值，构建了基于长度非增态射的抽象字符串对象格结构，并定义了高效的抽象字符串操作来支持JavaScript字符串程序分析。


<details>
  <summary>Details</summary>
Motivation: 为了更精确地分析JavaScript等语言中的字符串操作程序，需要开发能够处理字符串约束的抽象域，以捕获字符串值的上下界信息。

Method: 定义字符串区间抽象域，使用字方程表示下界、字不等式表示上界；构建基于长度非增态射的字符串属性半格上的约化积；设计多种约化策略使字符串对象域形成格结构。

Result: 建立了字符串区间抽象域的格结构，定义了高效的抽象字符串操作，最小化了约化计算开销，能够有效分析JavaScript字符串操作程序的性质。

Conclusion: 该字符串抽象域为程序分析提供了强大的理论基础，能够精确处理字符串约束，特别适用于JavaScript等动态语言中字符串操作的静态分析。

Abstract: We introduce a string-interval abstract domain, where string intervals are
characterized by systems of word equations (encoding lower bounds on string
values) and word disequalities (encoding upper bounds). Building upon the
lattice structure of string intervals, we define an abstract string object as a
reduced product on a string property semilattice, determined by
length-non-increasing morphisms. We consider several reduction strategies for
abstract string objects and show that upon these strategies the string object
domain forms a lattice. We define basic abstract string operations on the
domain, aiming to minimize computational overheads on the reduction, and show
how the domain can be used to analyse properties of JavaScript string
manipulating programs.

</details>


### [42] [Herb.jl: A Unifying Program Synthesis Library](https://arxiv.org/abs/2510.09726)
*Tilman Hinnerichs,Reuben Gardos Reid,Jaap de Jong,Bart Swinkels,Pamela Wochner,Nicolae Filat,Tudor Magurescu,Issa Hanou,Sebastijan Dumancic*

Main category: cs.PL

TL;DR: Herb.jl是一个用Julia编程语言编写的统一程序合成库，旨在模块化合成算法，便于重用和扩展现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的程序合成工具虽然众多，但重用和混合先前开发的方法既繁琐又耗时。

Method: 将底层合成算法模块化为可通信和完全可扩展的子组件，允许直接重用这些模块。

Result: 展示了三个常见用例：实现简单问题和语法并解决、用少量代码实现先前开发的合成器、在基准测试上运行合成器。

Conclusion: Herb.jl提供了一个统一的程序合成框架，简化了合成方法的重用和扩展。

Abstract: Program synthesis -- the automatic generation of code given a specification
-- is one of the most fundamental tasks in artificial intelligence (AI) and
many programmers' dream. Numerous synthesizers have been developed to tackle
program synthesis, manifesting different ideas to approach the exponentially
growing program space. While numerous smart program synthesis tools exist,
reusing and remixing previously developed methods is tedious and
time-consuming. We propose Herb.jl, a unifying program synthesis library
written in the Julia programming language, to address these issues. Since
current methods rely on similar building blocks, we aim to modularize the
underlying synthesis algorithm into communicating and fully extendable
sub-compartments, allowing for straightforward reapplication of these modules.
To demonstrate the benefits of using Herb.jl, we show three common use cases:
1. how to implement a simple problem and grammar, and how to solve it, 2. how
to implement a previously developed synthesizer with just a few lines of code,
and 3. how to run a synthesizer against a benchmark.

</details>


### [43] [ACT: Automatically Generating Compiler Backends from Tensor Accelerator ISA Descriptions](https://arxiv.org/abs/2510.09932)
*Devansh Jain,Akash Pardeshi,Marco Frigo,Krut Patel,Kaustubh Khulbe,Jai Arora,Charith Mendis*

Main category: cs.PL

TL;DR: ACT是一个编译器后端生成器，能够根据张量加速器的指令集架构描述自动生成编译器后端，解决了新硬件缺乏编译器支持的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的张量加速器缺乏编译器后端支持，且硬件设计迭代快速，手动开发编译器后端成本高、周期长，阻碍了新硬件的采用和软件开发。

Method: ACT通过形式化规范描述张量加速器ISA，采用参数化等式饱和的指令选择方法和基于约束规划的内存分配策略，自动生成编译器后端。

Result: 为三个工业和学术界的加速器平台生成了编译器后端，性能达到或超过手动优化的内核库，同时保持低编译开销。

Conclusion: ACT能够自动生成正确且完整的编译器后端，显著降低了新张量加速器的软件支持成本，加速了硬件创新和采用。

Abstract: Tensor compilers play a key role in enabling high-performance implementations
of deep learning workloads. These compilers rely on existing CPU and GPU code
generation backends to generate device-specific code. Recently, many tensor
accelerators (neural processing units) have been proposed to further accelerate
these workloads. Compared to commodity hardware, however, most of the proposed
tensor accelerators do not have compiler backends with code generation support.
Moreover, the accelerator designs are subject to fast iteration cycles, making
it difficult to manually develop compiler backends similar to commodity
hardware platforms. Therefore, to increase adoption and enable faster software
development cycles for novel tensor accelerator designs, we need to make the
compiler backend construction process more agile.
  To address this gap, we introduce ACT, a compiler backend generator that
automatically generates compiler backends for tensor accelerators, given just
the instruction set architecture (ISA) descriptions. We first formally specify
the compiler backend generation problem that introduces a novel specification
for describing tensor accelerator ISAs. Next, we design ACT such that it
supports user-programmable memories and complex parameterized instructions that
are prevalent in tensor accelerators. ACT uses a novel parameterized equality
saturation-based instruction selection phase and a constraint programming-based
memory allocation phase. We prove that compiler backends generated by ACT are
sound and complete. Finally, we generate compiler backends for three
accelerator platforms from industry and academia, and show that they match or
outperform code written using hand-optimized kernel libraries while maintaining
low compilation overheads.

</details>


### [44] [End-to-end Compositional Verification of Program Safety through Verified and Verifying Compilation](https://arxiv.org/abs/2510.10015)
*Jinhua Wu,Yuting Wang,Liukun Yu,Linglong Meng*

Main category: cs.PL

TL;DR: 提出基于开放标记转移系统的开放安全概念，支持模块化安全验证和组合编译，解决Rust等安全语言中混合安全/不安全模块的端到端安全验证问题。


<details>
  <summary>Details</summary>
Motivation: 现代安全编程语言如Rust需要混合安全和不安全模块，传统端到端安全验证方法无法处理模块化场景，需要新的模块化安全定义和验证框架。

Method: 基于开放标记转移系统定义开放安全概念，支持模块边界组合和模块化保持，通过验证组合编译实现异构模块的分离验证和目标级安全结果组合。

Result: 开发了所有权语言Oolang的验证编译器，通过哈希表案例展示了组合安全验证的有效性，支持验证编译和验证编译的结合。

Conclusion: 开放安全框架为混合安全/不安全模块的系统提供了模块化端到端安全验证的理论基础和实践方法。

Abstract: Program safety (i.e., absence of undefined behaviors) is critical for correct
operation of computer systems. It is usually verified at the source level
(e.g., by separation logics) and preserved to the target by verified compilers
(e.g., CompCert), thereby achieving end-to-end verification of safety. However,
modern safe programming languages like Rust pose new problems in achieving
end-to-end safety. Because not all functionalities can be implemented in the
safe language, mixing safe and unsafe modules is needed. Therefore, verified
compilation must preserve a modular notion of safety which can be composed at
the target level. Furthermore, certain classes of errors (e.g., memory errors)
are automatically excluded by verifying compilation (e.g., borrow checking) for
modules written in safe languages. As a result, verified compilation needs to
cooperate with verifying compilation to ensure end-to-end safety.
  To address the above problems, we propose a modular and generic definition of
safety called open safety based on program semantics described as open labeled
transition systems (LTS). Open safety is composable at the boundary of modules
and can be modularly preserved by verified compositional compilation. Those
properties enable separate verification of safety for heterogeneous modules and
composition of the safety results at the target level. Open safety can be
generalized to partial safety (i.e., only a certain class of errors can occur).
By this we formalized the correctness of verifying compilation as derivation of
total safety from partial safety. We demonstrate how our framework can combine
verified and verifying compilation by developing a verified compiler for an
ownership language (called Owlang) inspired by Rust. We evaluate our approach
on the compositional safety verification using a hash map implemented by Owlang
and C.

</details>


### [45] [LOOPerSet: A Large-Scale Dataset for Data-Driven Polyhedral Compiler Optimization](https://arxiv.org/abs/2510.10209)
*Massinissa Merouani,Afif Boudaoud,Riyadh Baghdadi*

Main category: cs.PL

TL;DR: LOOPerSet是一个包含2800万个标记数据点的大规模公共数据集，用于解决机器学习在编译器优化中数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习在编译器优化（特别是多面体模型）中的发展受到大规模公共性能数据集稀缺的限制，这导致研究人员需要进行昂贵的数据生成活动，减缓了创新速度并阻碍了可重现的研究。

Method: 通过220,000个独特的合成生成的多面体程序，创建了包含2800万个标记数据点的LOOPerSet数据集。每个数据点将程序和复杂的语义保持转换序列映射到真实的性能测量值。

Result: LOOPerSet数据集具有规模大和多样性高的特点，为训练和评估学习成本模型、基准测试新模型架构以及探索自动化多面体调度前沿提供了宝贵资源。

Conclusion: 该数据集在宽松许可下发布，旨在促进可重现研究并降低数据驱动编译器优化的入门门槛。

Abstract: The advancement of machine learning for compiler optimization, particularly
within the polyhedral model, is constrained by the scarcity of large-scale,
public performance datasets. This data bottleneck forces researchers to
undertake costly data generation campaigns, slowing down innovation and
hindering reproducible research learned code optimization. To address this gap,
we introduce LOOPerSet, a new public dataset containing 28 million labeled data
points derived from 220,000 unique, synthetically generated polyhedral
programs. Each data point maps a program and a complex sequence of
semantics-preserving transformations (such as fusion, skewing, tiling, and
parallelism)to a ground truth performance measurement (execution time). The
scale and diversity of LOOPerSet make it a valuable resource for training and
evaluating learned cost models, benchmarking new model architectures, and
exploring the frontiers of automated polyhedral scheduling. The dataset is
released under a permissive license to foster reproducible research and lower
the barrier to entry for data-driven compiler optimization.

</details>


### [46] [Learning to Guarantee Type Correctness in Code Generation through Type-Guided Program Synthesis](https://arxiv.org/abs/2510.10216)
*Zhechong Huang,Zhao Zhang,Ruyi Ji,Tingxuan Xia,Qihao Zhu,Qinxiang Cao,Zeyu Sun,Yingfei Xiong*

Main category: cs.PL

TL;DR: TyFlow是一个将类型推理内部化到代码生成中的新系统，通过类型引导的程序合成来指导模型学习类型系统，显著提高了代码的功能正确性。


<details>
  <summary>Details</summary>
Motivation: 语言模型在代码生成方面表现出色，但确保类型正确性仍然是一个挑战。传统方法如约束解码只能外部拒绝不可类型化的代码，但模型本身没有有效学习类型推理，这限制了整体性能。

Method: TyFlow采用类型引导的程序合成系统，在类型推导树和合成推导树之间保持同构关系，使用基于合成决策序列的新代码表示而非传统的基于文本的标记序列。

Result: 评估显示TyFlow不仅消除了类型错误，还显著提高了功能正确性，表明将语言模型与类型系统内部对齐的重要性。

Conclusion: 通过将类型系统学习的复杂性卸载到表示本身，模型可以将计算资源重新导向更高级的程序语义，从而在代码生成中实现更好的类型正确性和功能正确性。

Abstract: Language models have shown remarkable proficiency in code generation;
nevertheless, ensuring type correctness remains a challenge. Although
traditional methods, such as constrained decoding, alleviate this problem by
externally rejecting untypable code, the model itself does not effectively
learn type reasoning internally, which ultimately limits its overall
performance. This paper introduces TyFlow, a novel system that internalizes
type reasoning within code generation to guide the model to learn the type
system. The core of our approach is a novel type-guided program synthesis
system that maintains an isomorphism between type derivation trees and
synthesis derivation trees, enabling a new code representation based on
synthesis decision sequences rather than traditional text-based token
sequences. By offloading the complexity of type system learning to the
representation itself, models can redirect their computational resources toward
higher-level program semantics. Our evaluation shows that TyFlow not only
eliminates type errors but also significantly improves functional correctness,
highlighting the importance of aligning LMs with type systems internally.

</details>


### [47] [Old is Gold: Optimizing Single-threaded Applications with Exgen-Malloc](https://arxiv.org/abs/2510.10219)
*Ruihao Li,Lizy K. John,Neeraja J. Yadwadkar*

Main category: cs.PL

TL;DR: Exgen-Malloc是一个专为单线程应用设计的内存分配器，通过消除多线程环境下的复杂元数据和简化控制流，显著提升了分配效率和内存利用率。


<details>
  <summary>Details</summary>
Motivation: 现代内存分配器为多线程环境优化，但在单线程场景下会引入不必要的开销。在超大规模数据中心，即使1%的效率提升也能带来数百万美元的节省和能耗降低。

Method: 采用集中式堆、单一空闲块列表、平衡的内存提交和重定位策略，并借鉴现代多线程分配器的设计原则，同时消除不必要的元数据和简化控制流。

Result: 在两个Intel Xeon平台上测试，相比dlmalloc在SPEC CPU2017、redis-benchmark和mimalloc-bench上分别获得1.17x、1.10x和1.93x的速度提升；相比mimalloc分别节省6.2%、0.1%和25.2%的内存。

Conclusion: 专为单线程应用定制的内存分配器能够显著提升性能和内存效率，证明了在特定场景下简化设计的重要性。

Abstract: Memory allocators hide beneath nearly every application stack, yet their
performance footprint extends far beyond their code size. Even small
inefficiencies in the allocators ripple through caches and the rest of the
memory hierarchy, collectively imposing what operators often call a "datacenter
tax". At hyperscale, even a 1% improvement in allocator efficiency can unlock
millions of dollars in savings and measurable reductions in datacenter energy
consumption. Modern memory allocators are designed to optimize allocation speed
and memory fragmentation in multi-threaded environments, relying on complex
metadata and control logic to achieve high performance. However, the overhead
introduced by this complexity prompts a reevaluation of allocator design.
Notably, such overhead can be avoided in single-threaded scenarios, which
continue to be widely used across diverse application domains.
  In this paper, we introduce Exgen-Malloc, a memory allocator purpose-built
for single-threaded applications. By specializing for single-threaded
execution, Exgen-Malloc eliminates unnecessary metadata, simplifies the control
flow, thereby reducing overhead and improving allocation efficiency. Its core
design features include a centralized heap, a single free-block list, and a
balanced strategy for memory commitment and relocation. Additionally,
Exgen-Malloc incorporates design principles in modern multi-threaded
allocators, which do not exist in legacy single-threaded allocators such as
dlmalloc. We evaluate Exgen-Malloc on two Intel Xeon platforms. Across both
systems, Exgen-Malloc achieves a speedup of 1.17x, 1.10x, and 1.93x over
dlmalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench, respectively. In
addition to performance, Exgen-Malloc achieves 6.2%, 0.1%, and 25.2% memory
savings over mimalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench,
respectively.

</details>


### [48] [A Trace-based Approach for Code Safety Analysis](https://arxiv.org/abs/2510.10410)
*Hui Xu*

Main category: cs.PL

TL;DR: 本文系统分析了Rust语言中的不安全代码和未定义行为问题，建立了理解框架并总结了Rust代码的正确性标准，为安全封装提供了实践指导。


<details>
  <summary>Details</summary>
Motivation: Rust作为内存安全编程语言虽然禁止未定义行为，但不安全代码仍然是关键问题。通过审查Rust的安全设计和分析实际项目，需要建立系统框架来理解不安全代码和未定义行为。

Method: 回顾Rust的安全设计，分析真实世界的Rust项目，建立系统化的框架来理解不安全代码和未定义行为，总结Rust代码的正确性标准。

Result: 建立了理解不安全代码和未定义行为的系统框架，总结了Rust代码的正确性标准，并推导出实现安全封装的可操作指导。

Conclusion: 该研究为Rust语言中的不安全代码问题提供了系统性分析框架和实践指导，有助于提高Rust代码的安全性和可靠性。

Abstract: Rust is a memory-safe programming language that disallows undefined behavior.
Its safety guarantees have been extensively examined by the community through
empirical studies, which has led to its remarkable success. However, unsafe
code remains a critical concern in Rust. By reviewing the safety design of Rust
and analyzing real-world Rust projects, this paper establishes a systematic
framework for understanding unsafe code and undefined behavior, and summarizes
the soundness criteria for Rust code. It further derives actionable guidance
for achieving sound encapsulation.

</details>


### [49] [ECO: Enhanced Code Optimization via Performance-Aware Prompting for Code-LLMs](https://arxiv.org/abs/2510.10517)
*Su-Hyeon Kim,Joonghyuk Hahn,Sooyoung Cha,Yo-Sub Han*

Main category: cs.PL

TL;DR: ECO是一个性能感知的代码优化提示框架，通过分析性能瓶颈和优化原理来指导代码LLM生成更高效的代码，无需微调即可实现高达7.81倍的加速。


<details>
  <summary>Details</summary>
Motivation: 现有基于慢-快代码对的优化方法往往只是表面模式模仿，无法揭示性能提升的根本原因，需要更深入的性能推理方法。

Method: ECO框架从参考代码对中提取运行时优化指令(ROI)，同时使用符号顾问进行瓶颈诊断和ROI检索器获取相关优化原理，将这些信息组合成性能感知提示来指导代码LLM。

Result: ECO显著提升了代码LLM生成高效代码的能力，实现了高达7.81倍的加速，同时最小化了正确性损失。

Conclusion: ECO提供了一个模型无关、无需微调的代码优化提示框架，能够有效指导代码LLM进行性能推理和优化。

Abstract: Code runtime optimization-the task of rewriting a given code to a faster
one-remains challenging, as it requires reasoning about performance trade-offs
involving algorithmic and structural choices. Recent approaches employ
code-LLMs with slow-fast code pairs provided as optimization guidance, but such
pair-based methods obscure the causal factors of performance gains and often
lead to superficial pattern imitation rather than genuine performance
reasoning. We introduce ECO, a performance-aware prompting framework for code
optimization. ECO first distills runtime optimization instructions (ROIs) from
reference slow-fast code pairs; Each ROI describes root causes of inefficiency
and the rationales that drive performance improvements. For a given input code,
ECO in parallel employs (i) a symbolic advisor to produce a bottleneck
diagnosis tailored to the code, and (ii) an ROI retriever to return related
ROIs. These two outputs are then composed into a performance-aware prompt,
providing actionable guidance for code-LLMs. ECO's prompts are model-agnostic,
require no fine-tuning, and can be easily prepended to any code-LLM prompt. Our
empirical studies highlight that ECO prompting significantly improves
code-LLMs' ability to generate efficient code, achieving speedups of up to
7.81x while minimizing correctness loss.

</details>


### [50] [A Verified High-Performance Composable Object Library for Remote Direct Memory Access (Extended Version)](https://arxiv.org/abs/2510.10531)
*Guillaume Ambal,George Hodgkins,Mark Madler,Gregory Chockler,Brijesh Dongol,Joseph Izraelevitz,Azalea Raad,Viktor Vafeiadis*

Main category: cs.PL

TL;DR: LOCO是一个经过形式化验证的RDMA多节点对象库，填补了共享内存和分布式系统编程之间的空白。它提供了性能接近定制RDMA系统但编程模型更简单的对象，并开发了Mowgli验证框架来确保正确性。


<details>
  <summary>Details</summary>
Motivation: RDMA虽然能实现低延迟高吞吐量的网络通信，但其弱内存模型难以在实际中使用，且最近才被形式化。需要一种既能利用RDMA性能优势又易于编程和验证的解决方案。

Method: 开发了LOCO库，构建封装良好的多节点对象，利用RDMA的强局部性和弱一致性特性。同时创建了Mowgli模块化声明式验证框架，该框架灵活且独立于内存一致性模型。

Result: LOCO对象性能与定制RDMA系统相当（如分布式映射），但编程模型更简单，适合形式化正确性证明。Mowgli框架成功实例化RDMA内存模型并验证了LOCO库的正确性。

Conclusion: LOCO提供了一个实用的RDMA编程抽象，通过形式化验证确保了系统正确性，填补了共享内存和分布式系统编程之间的重要空白。

Abstract: Remote Direct Memory Access (RDMA) is a memory technology that allows remote
devices to directly write to and read from each other's memory, bypassing
components such as the CPU and operating system. This enables low-latency
high-throughput networking, as required for many modern data centres, HPC
applications and AI/ML workloads. However, baseline RDMA comprises a highly
permissive weak memory model that is difficult to use in practice and has only
recently been formalised. In this paper, we introduce the Library of Composable
Objects (LOCO), a formally verified library for building multi-node objects on
RDMA, filling the gap between shared memory and distributed system programming.
LOCO objects are well-encapsulated and take advantage of the strong locality
and the weak consistency characteristics of RDMA. They have performance
comparable to custom RDMA systems (e.g. distributed maps), but with a far
simpler programming model amenable to formal proofs of correctness. To support
verification, we develop a novel modular declarative verification framework,
called Mowgli, that is flexible enough to model multinode objects and is
independent of a memory consistency model. We instantiate Mowgli with the RDMA
memory model, and use it to verify correctness of LOCO libraries.

</details>


### [51] [HUGR: A Quantum-Classical Intermediate Representation](https://arxiv.org/abs/2510.11420)
*Mark Koch,Agustín Borgna,Seyon Sivarajah,Alan Lawrence,Alec Edgington,Douglas Wilson,Craig Roy,Luca Mondada,Lukas Heidemann,Ross Duncan*

Main category: cs.PL

TL;DR: HUGR是一种新颖的基于图的中间表示，用于混合量子-经典程序，具有高表达性和可扩展性，支持模式匹配编译技术，并提供安全保证。


<details>
  <summary>Details</summary>
Motivation: 为了捕捉近期和未来量子计算设备的能力，以及新兴量子编程范式中的抽象概念，需要一个表达力强且可扩展的中间表示。

Method: 设计基于图的层次化统一图表示(HUGR)，受MLIR启发，支持多抽象层次推理和平滑降级，包含严格静态类型和线性量子类型等安全保证。

Result: 开发了完整的HUGR规范和开源参考实现，支持机器友好的图结构和强大的编译技术。

Conclusion: HUGR为量子程序编译提供了安全、可扩展的中间表示框架，有助于快速开发编译工具链。

Abstract: We introduce the Hierarchical Unified Graph Representation (HUGR): a novel
graph based intermediate representation for mixed quantum-classical programs.
HUGR's design features high expressivity and extensibility to capture the
capabilities of near-term and forthcoming quantum computing devices, as well as
new and evolving abstractions from novel quantum programming paradigms. The
graph based structure is machine-friendly and supports powerful pattern
matching based compilation techniques. Inspired by MLIR, HUGR's extensibility
further allows compilation tooling to reason about programs at multiple levels
of abstraction, lowering smoothly between them. Safety guarantees in the
structure including strict, static typing and linear quantum types allow rapid
development of compilation tooling without fear of program invalidation. A full
specification of HUGR and reference implementation are open-source and
available online.

</details>


### [52] [(Dis)Proving Spectre Security with Speculation-Passing Style](https://arxiv.org/abs/2510.11573)
*Santiago Arranz-Olmos,Gilles Barthe,Lionel Blatter,Xingyu Xie,Zhiyuan Zhang*

Main category: cs.PL

TL;DR: 本文提出了一种称为推测传递风格(SPS)的程序转换方法，将推测常数时间(SCT)验证简化为常数时间(CT)验证，为SCT工具提供了形式化基础。


<details>
  <summary>Details</summary>
Motivation: 现有的推测常数时间(SCT)验证工具通常是常数时间(CT)验证工具的简单扩展，但这些扩展缺乏精确的定义和形式化分析。本文旨在填补这一空白，为这些扩展建立形式化基础。

Method: 引入推测传递风格(SPS)程序转换，通过添加攻击者控制的预测输入来修改程序，使其遵循这些预测。该方法将SCT验证问题转化为CT验证问题。

Result: 证明了程序是SCT当且仅当其SPS转换是CT，从而可以利用现有CT验证工具证明SCT。在Spectre-v1基准测试中与EasyCrypt、BINSEC和ctgrind等工具结合验证了有效性。

Conclusion: SPS转换提供了一种将SCT验证简化为CT验证的通用方法，为检测Spectre漏洞提供了形式化基础和实用工具。

Abstract: Constant-time (CT) verification tools are commonly used for detecting
potential side-channel vulnerabilities in cryptographic libraries. Recently, a
new class of tools, called speculative constant-time (SCT) tools, has also been
used for detecting potential Spectre vulnerabilities. In many cases, these SCT
tools have emerged as liftings of CT tools. However, these liftings are seldom
defined precisely and are almost never analyzed formally. The goal of this
paper is to address this gap, by developing formal foundations for these
liftings, and to demonstrate that these foundations can yield practical
benefits.
  Concretely, we introduce a program transformation, coined Speculation-Passing
Style (SPS), for reducing SCT verification to CT verification. Essentially, the
transformation instruments the program with a new input that corresponds to
attacker-controlled predictions and modifies the program to follow them. This
approach is sound and complete, in the sense that a program is SCT if and only
if its SPS transform is CT. Thus, we can leverage existing CT verification
tools to prove SCT; we illustrate this by combining SPS with three standard
methodologies for CT verification, namely reducing it to non-interference,
assertion safety and dynamic taint analysis. We realize these combinations with
three existing tools, EasyCrypt, BINSEC, and ctgrind, and we evaluate them on
Kocher's benchmarks for Spectre-v1. Our results focus on Spectre-v1 in the
standard CT leakage model; however, we also discuss applications of our method
to other variants of Spectre and other leakage models.

</details>
