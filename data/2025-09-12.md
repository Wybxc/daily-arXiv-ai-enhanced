<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.SE](#cs.SE) [Total: 13]
- [cs.LO](#cs.LO) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Towards Verified Compilation of Floating-point Optimization in Scientific Computing Programs](https://arxiv.org/abs/2509.09019)
*Mohit Tekriwal,John Sarracino*

Main category: cs.PL

TL;DR: 利用Rocq定理证明器中的Verified LLVM框架，证明LLVM IR层次浮点数优化（特别是FMA优化）的正确性，并提出扩展方案


<details>
  <summary>Details</summary>
Motivation: 科学计算程序需要精强的编译器优化来获得高性能，但同时必须确保这些优化的正确性，特别是浮点数优化

Method: 基于Rocq定理证明器中的Verified LLVM框架，对基本块实现算术表达式$a * b + c$进行FMA融合乘加优化的正确性证明

Result: 完成了FMA优化在基本块上的正确性证明，为更复杂的浮点数优化验证奠定基础

Conclusion: 这项预研工作为验证LLVM浮点数优化正确性提供了基础，并提出了扩展更多程序特征和fast math优化的方向

Abstract: Scientific computing programs often undergo aggressive compiler optimization
to achieve high performance and efficient resource utilization. While
performance is critical, we also need to ensure that these optimizations are
correct. In this paper, we focus on a specific class of optimizations,
floating-point optimizations, notably due to fast math, at the LLVM IR level.
We present a preliminary work, which leverages the Verified LLVM framework in
the Rocq theorem prover, to prove the correctness of Fused-Multiply-Add (FMA)
optimization for a basic block implementing the arithmetic expression $a * b +
c$ . We then propose ways to extend this preliminary results by adding more
program features and fast math floating-point optimizations.

</details>


### [2] [Dependent-Type-Preserving Memory Allocation](https://arxiv.org/abs/2509.09059)
*Paulette Koronkevich,William J. Bowman*

Main category: cs.PL

TL;DR: 本文探讨依赖类型语言在编译后类型信息被擦除导致的外部程序链接安全问题，提出通过类型保持编译来确保链接时的类型检查。


<details>
  <summary>Details</summary>
Motivation: 依赖类型语言如Coq、Agda等允许编写详细程序规范，但这些规范在编译后被擦除，外部链接程序可能违反原始程序规范，即使使用验证编译器也无法避免。

Method: 开发支持依赖内存分配的中间语言，以及保持依赖类型的内存分配编译器传递，实现类型保持编译以便在链接时进行类型检查。

Result: 正在进行的工作，提出了解决依赖类型语言编译后类型安全问题的技术路线。

Conclusion: 通过类型保持编译可以防止与类型不正确的程序链接，从而确保依赖类型程序在编译后的规范完整性。

Abstract: Dependently typed programming languages such as Coq, Agda, Idris, and F*,
allow programmers to write detailed specifications of their programs and prove
their programs meet these specifications. However, these specifications can be
violated during compilation since they are erased after type checking. External
programs linked with the compiled program can violate the specifications of the
original program and change the behavior of the compiled program -- even when
compiled with a verified compiler. For example, since Coq does not allow
explicitly allocating memory, a programmer might link their Coq program with a
C program that can allocate memory. Even if the Coq program is compiled with a
verified compiler, the external C program can still violate the memory-safe
specification of the Coq program by providing an uninitialized pointer to
memory. This error could be ruled out by type checking in a language expressive
enough to indicate whether memory is initialized versus uninitialized. Linking
with a program with an uninitialized pointer could be considered ill-typed, and
our linking process could prevent linking with ill-typed programs. To
facilitate type checking during linking, we can use type-preserving
compilation, which preserves the types through the compilation process. In this
ongoing work, we develop a typed intermediate language that supports dependent
memory allocation, as well as a dependent-type-preserving compiler pass for
memory allocation.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [Pattern-Based File and Data Access with Python Glob: A Comprehensive Guide for Computational Research](https://arxiv.org/abs/2509.08843)
*Sidney Shapiro*

Main category: cs.SE

TL;DR: 这篇论文介绍了Python glob模块在科研工作流中的重要作用，通过文件模式匹配实现大规模数据摘取、数据分析和AI数据集构建，提高可复现研究效率。


<details>
  <summary>Details</summary>
Motivation: 文件模式访问是计算科学研究的基础但缺乏文档记载，需要提供一个简洁而强大的工具来支持可扩展的工作流。

Method: 通过具体的Python示例，结合pandas、scikit-learn、matplotlib等库，展示glob在数据科学、商业分析和人工智能应用中的实践方法。

Result: 证明了glob能够有效地进行文件遍历和与分析管道集成，成为可复现研究和数据工程的方法论基础。

Conclusion: 该论文为研究人员和实践者提供了一个简洁的参考指南，使glob成为Python基础研究工作流中文件模式匹配的标准引用。

Abstract: Pattern-based file access is a fundamental but often under-documented aspect
of computational research. The Python glob module provides a simple yet
powerful way to search, filter, and ingest files using wildcard patterns,
enabling scalable workflows across disciplines. This paper introduces glob as a
versatile tool for data science, business analytics, and artificial
intelligence applications. We demonstrate use cases including large-scale data
ingestion, organizational data analysis, AI dataset construction, and
reproducible research practices. Through concrete Python examples with widely
used libraries such as pandas,scikit-learn, and matplotlib, we show how glob
facilitates efficient file traversal and integration with analytical pipelines.
By situating glob within the broader context of reproducible research and data
engineering, we highlight its role as a methodological building block. Our goal
is to provide researchers and practitioners with a concise reference that
bridges foundational concepts and applied practice, making glob a default
citation for file pattern matching in Python-based research workflows.

</details>


### [4] [A Systematic Mapping Study on Chatbots in Programming Education](https://arxiv.org/abs/2509.08857)
*Marcelino Garcia,Renato Garcia,Arthur Parizotto,Andre Mendes,Pedro Valle,Ricardo Vilela,Renato Balancieri,Williamson Silva*

Main category: cs.SE

TL;DR: 这是一份关于教育聊天机器在编程教育中应用的系统浏览研究，分析了54项研究的发展趋势、使用的编程语言、教育内容和交互模型等。


<details>
  <summary>Details</summary>
Motivation: 教育聊天机器在编程教育中越来越重要，需要系统性地研究它们的发展状况和应用方式，以指导未来教育工具的开发。

Method: 采用系统浏览研究方法，从3,216篇公开出版物中筛选54项研究进行分析，重点关注聊天机器类型、编程语言、教育内容、交互模型和应用场景五个方面。

Result: 研究发现主要趋势是使用Python语言的聊天机器，重点教授基础编程概念，采用多种教学方法和技术架构。识别了领域内的研究趋势和空白。

Conclusion: 该研究为编程教育工具的开发提供了有价值的见解，帮助指导未来教育聊天机器的设计和应用。

Abstract: Educational chatbots have gained prominence as support tools for teaching
programming, particularly in introductory learning contexts. This paper
presents a Systematic Mapping Study (SMS) that investigated how such agents
have been developed and applied in programming education. From an initial set
of 3,216 publications, 54 studies were selected and analyzed based on five
research subquestions, addressing chatbot types, programming languages used,
educational content covered, interaction models, and application contexts. The
results reveal a predominance of chatbots designed for Python instruction,
focusing on fundamental programming concepts, and employing a wide variety of
pedagogical approaches and technological architectures. In addition to
identifying trends and gaps in the literature, this study provides insights to
inform the development of new educational tools for programming instruction.

</details>


### [5] [GeoJSON Agents:A Multi-Agent LLM Architecture for Geospatial Analysis-Function Calling vs Code Generation](https://arxiv.org/abs/2509.08863)
*Qianqian Luo,Liuchang Xu,Qingming Lin,Sensen Wu,Ruichen Mao,Chao Wang,Hailin Feng,Bo Huang,Zhenhong Du*

Main category: cs.SE

TL;DR: 这篇论文提出了GeoJSON Agents多段式LLM架构，通过函数调用和代码生成两种方法来处理空间数据，显著提升了GIS自动化的性能和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs在任务自动化和自然语言理解方面取得了显著进步，但缺乏GIS专业知识仍然限制了其能力，需要解决LLMs在空间数据处理方面的限制。

Method: 设计了一种多段式LLM架构，包含任务解析、段式协作和结果集成三个组件。Planner段式将自然语言任务转换为结构化的GeoJSON命令，Worker段式通过函数调用或代码生成来处理空间数据。

Result: 在70个任务的测试中，基于函数调用的方法达到85.71%的准确率，基于代码生成的方法达到97.14%的准确率，都显著超过了通用模型的最佳表现（48.57%）。

Conclusion: 这是首次提出用于GeoJSON数据的LLM多段式架构，并对两种主流的LLM增强方法进行了比较。代码生成方法更灵活，函数调用方法执行更稳定，为提升GeoAI系统性能提供了新的视角。

Abstract: LLMs have made substantial progress in task automation and natural language
understanding.However,without expertise in GIS,they continue to encounter
limitations.To address these issues, we propose GeoJSON Agents-a multi-agent
LLM architecture.This framework transforms natural language tasks into
structured GeoJSON operation commands and processes spatial data using two
widely adopted LLM enhancement techniques:Function Calling and Code
Generation.The architecture consists of three components-task parsing,agent
collaboration,and result integration-aimed at enhancing both the performance
and scalability of GIS automation.The Planner agent interprets natural language
tasks into structured GeoJSON commands.Then,specialized Worker agents
collaborate according to assigned roles to perform spatial data processing and
analysis,either by invoking predefined function APIs or by dynamically
generating and executing Python-based spatial analysis code.Finally,the system
integrates the outputs from multiple execution rounds into
reusable,standards-compliant GeoJSON files.To systematically evaluate the
performance of the two approaches,we constructed a benchmark dataset of 70
tasks with varying complexity and conducted experiments using OpenAI's GPT-4o
as the core model.Results indicate that the Function Calling-based GeoJSON
Agent achieved an accuracy of 85.71%,while the Code Generation-based agent
reached 97.14%,both significantly outperforming the best-performing
general-purpose model (48.57%).Further analysis reveals that the Code
Generation provides greater flexibility,whereas the Function Calling approach
offers more stable execution.This study is the first to introduce an LLM
multi-agent framework for GeoJSON data and to compare the strengths and
limitations of two mainstream LLM enhancement methods,offering new perspectives
for improving GeoAI system performance.

</details>


### [6] [TraceRAG: A LLM-Based Framework for Explainable Android Malware Detection and Behavior Analysis](https://arxiv.org/abs/2509.08865)
*Guangyu Zhang,Xixuan Wang,Shiyu Sun,Peiyan Xiao,Kun Sun,Yanhai Xiong*

Main category: cs.SE

TL;DR: TraceRAG是一个基于检索增强生成(RAG)的框架，通过自然语言查询和Java代码分析来提供可解释的Android恶意软件检测和分析，实现了96%的恶意软件检测准确率和83.81%的行为识别准确率。


<details>
  <summary>Details</summary>
Motivation: Android恶意应用采用复杂的规避策略，将恶意逻辑隐藏在合法功能中，传统分析方法难以恢复深度隐藏行为或提供人类可读的决策解释。

Method: TraceRAG首先生成方法级代码片段的摘要并索引到向量数据库中；查询时通过行为聚焦问题检索语义最相关的代码片段进行深度检查；基于多轮分析结果生成包含恶意行为及其对应代码实现的人类可读报告。

Result: 实验结果显示，基于更新的VirusTotal扫描和人工验证，该方法达到96%的恶意软件检测准确率和83.81%的行为识别准确率。专家评估确认了TraceRAG生成报告的实际效用。

Conclusion: TraceRAG框架成功地将自然语言查询与代码分析相结合，提供了可解释的恶意软件检测和分析解决方案，在准确性和实用性方面都表现出色。

Abstract: Sophisticated evasion tactics in malicious Android applications, combined
with their intricate behavioral semantics, enable attackers to conceal
malicious logic within legitimate functions, underscoring the critical need for
robust and in-depth analysis frameworks. However, traditional analysis
techniques often fail to recover deeply hidden behaviors or provide
human-readable justifications for their decisions. Inspired by advances in
large language models (LLMs), we introduce TraceRAG, a retrieval-augmented
generation (RAG) framework that bridges natural language queries and Java code
to deliver explainable malware detection and analysis. First, TraceRAG
generates summaries of method-level code snippets, which are indexed in a
vector database. At query time, behavior-focused questions retrieve the most
semantically relevant snippets for deeper inspection. Finally, based on the
multi-turn analysis results, TraceRAG produces human-readable reports that
present the identified malicious behaviors and their corresponding code
implementations. Experimental results demonstrate that our method achieves 96\%
malware detection accuracy and 83.81\% behavior identification accuracy based
on updated VirusTotal (VT) scans and manual verification. Furthermore, expert
evaluation confirms the practical utility of the reports generated by TraceRAG.

</details>


### [7] [Benchmarking Energy Efficiency of Large Language Models Using vLLM](https://arxiv.org/abs/2509.08867)
*K. Pronk,Q. Zhao*

Main category: cs.SE

TL;DR: 这篇论文提出了LLM效能基准测试，使用vLLM模拟真实使用场景，分析模型大小、架构和并发请求对能消的影响，为开发者提供可持续AI系统的见解。


<details>
  <summary>Details</summary>
Motivation: 大语言模型(LLMs)的普及对气候产生了越来越大的影响，需要收集更多关于LLM能消的信息。现有的基准测试往往无法反映真实生产环境。

Method: 使用vLLM（一个高吞吐量、生产准备就绪的LLM服务后端）来模拟真实使用条件，分析模型大小、架构和并发请求量对推理能效的影响。

Result: 证明了可以创建更好反映实际部署条件的能效基准测试。

Conclusion: 该基准测试为开发者提供了有价值的见解，帮助建设更可持续的AI系统。

Abstract: The prevalence of Large Language Models (LLMs) is having an growing impact on
the climate due to the substantial energy required for their deployment and
use. To create awareness for developers who are implementing LLMs in their
products, there is a strong need to collect more information about the energy
efficiency of LLMs. While existing research has evaluated the energy efficiency
of various models, these benchmarks often fall short of representing realistic
production scenarios. In this paper, we introduce the LLM Efficiency Benchmark,
designed to simulate real-world usage conditions. Our benchmark utilizes vLLM,
a high-throughput, production-ready LLM serving backend that optimizes model
performance and efficiency. We examine how factors such as model size,
architecture, and concurrent request volume affect inference energy efficiency.
Our findings demonstrate that it is possible to create energy efficiency
benchmarks that better reflect practical deployment conditions, providing
valuable insights for developers aiming to build more sustainable AI systems.

</details>


### [8] [CLARA: A Developer's Companion for Code Comprehension and Analysis](https://arxiv.org/abs/2509.09072)
*Ahmed Adnan,Mushfiqur Rahman,Saad Sakib Noor,Kazi Sakib*

Main category: cs.SE

TL;DR: CLARA是一个基于先进推理模型的浏览器扩展工具，帮助开发者和研究人员进行代码理解、重构和质量检测，通过用户研究证明其实用性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有代码分析工具需要项目设置、缺乏上下文感知且需要大量手动操作，CLARA旨在解决这些问题。

Method: 开发浏览器扩展工具，使用最先进的推理模型，通过定性评估和10名开发者的用户研究来验证工具效果。

Result: 研究结果表明CLARA在代码理解和分析任务中实用、准确且实用。

Conclusion: CLARA是一个开源工具，为代码理解和分析提供了有效的解决方案。

Abstract: Code comprehension and analysis of open-source project codebases is a task
frequently performed by developers and researchers. However, existing tools
that practitioners use for assistance with such tasks often require prior
project setup, lack context-awareness, and involve significant manual effort.
To address this, we present CLARA, a browser extension that utilizes a
state-of-the-art inference model to assist developers and researchers in: (i)
comprehending code files and code fragments, (ii) code refactoring, and (iii)
code quality attribute detection. We qualitatively evaluated CLARA's inference
model using existing datasets and methodology, and performed a comprehensive
user study with 10 developers and academic researchers to assess its usability
and usefulness. The results show that CLARA is useful, accurate, and practical
in code comprehension and analysis tasks. CLARA is an open-source tool
available at https://github.com/SaadNoor555/CLARA_tool_demo. A video showing
the full capabilities of CLARA can be found at
https://youtu.be/VDKVXvIH41Q?si=qBFsmS_Y4m_9x3YH.

</details>


### [9] [Probing Pre-trained Language Models on Code Changes: Insights from ReDef, a High-Confidence Just-in-Time Defect Prediction Dataset](https://arxiv.org/abs/2509.09192)
*Doha Nam,Taehyoun Kim,Duksan Ryu,Jongmoon Baik*

Main category: cs.SE

TL;DR: 这篇论文提出了ReDef数据集，通过revert commit和GPT辅助过滤构建了高信度的软件缺陷预测数据集，并系统评估了预训练语言模型在代码修改理解方面的能力和局限性。


<details>
  <summary>Details</summary>
Motivation: 解决现有JIT-SDP数据集标签噪声大、精确度低的问题，以及评估预训练语言模型是否真正理解代码修改语义。

Method: 通过revert commit标记缺陷修改、历史检查验证清洁修改、GPT辅助多次投票审核过滤模糊实例，构建高信度ReDef数据集。使用5种编码策略微调CodeBERT、CodeT5+、UniXcoder模型，并通过反事实扰动探针模型敏感性。

Result: 构建了3,164个缺陷修改和10,268个清洁修改的高信度数据集。diff格式编码在所有PLM中都明显优于整体函数格式。反事实测试显示模型性能减退很少，说明模型依赖表面线索而非真正语义理解。

Conclusion: 不同于短距零任务，当前的预训练语言模型在理解代码修改方面仍有限，依赖表面特征而非深层语义理解。

Abstract: Just-in-Time software defect prediction (JIT-SDP) plays a critical role in
prioritizing risky code changes during code review and continuous integration.
However, existing datasets often suffer from noisy labels and low precision in
identifying bug-inducing commits. To address this, we present ReDef
(Revert-based Defect dataset), a high-confidence benchmark of function-level
modifications curated from 22 large-scale C/C++ projects. Defective cases are
anchored by revert commits, while clean cases are validated through post-hoc
history checks. Ambiguous instances are conservatively filtered out via a
GPT-assisted triage process involving multiple votes and audits. This pipeline
yields 3,164 defective and 10,268 clean modifications, offering substantially
more reliable labels than prior existing resources. Beyond dataset
construction, we provide the first systematic evaluation of how pre-trained
language models (PLMs) reason about code modifications -- specifically, which
input encodings most effectively expose change information, and whether models
genuinely capture edit semantics. We fine-tune CodeBERT, CodeT5+, and UniXcoder
under five encoding strategies, and further probe their sensitivity through
counterfactual perturbations that swap added/deleted blocks, invert diff
polarity, or inject spurious markers. Our results show that compact diff-style
encodings consistently outperform whole-function formats across all PLMs, with
statistical tests confirming large, model-independent effects. However, under
counterfactual tests, performance degrades little or not at all -- revealing
that what appears to be robustness in fact reflects reliance on superficial
cues rather than true semantic understanding. These findings indicate that,
unlike in snapshot-based tasks, current PLMs remain limited in their ability to
genuinely comprehend code modifications.

</details>


### [10] [On Integrating Large Language Models and Scenario-Based Programming for Improving Software Reliability](https://arxiv.org/abs/2509.09194)
*Ayelet Berzack,Guy Katz*

Main category: cs.SE

TL;DR: 本文提出了一种将大语言模型与传统软件工程技术结合的方法，通过基于场景的编程范式来提高LLM在软件开发中的可靠性和可验证性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM能显著减少开发时间并生成组织良好的代码，但它们经常引入严重错误并以说服性自信呈现错误代码，需要更可靠的方法将LLM整合到软件开发周期中。

Method: 采用基于场景编程（SBP）范式，将人类专家的知识注入LLM，并检查和验证其输出，结合传统软件工程技术进行结构化整合。

Result: 通过Connect4游戏的案例研究，结合LLM和SBP创建了高性能的智能体，能够击败现有强智能体，并在某些情况下能够形式化验证智能体的正确性。

Conclusion: 该方法展示了将LLM与传统软件工程结合的可行性，提高了开发过程的可靠性和可验证性，同时揭示了该方法在易用性方面的有趣见解。

Abstract: Large Language Models (LLMs) are fast becoming indispensable tools for
software developers, assisting or even partnering with them in crafting complex
programs. The advantages are evident -- LLMs can significantly reduce
development time, generate well-organized and comprehensible code, and
occasionally suggest innovative ideas that developers might not conceive on
their own. However, despite their strengths, LLMs will often introduce
significant errors and present incorrect code with persuasive confidence,
potentially misleading developers into accepting flawed solutions.
  In order to bring LLMs into the software development cycle in a more reliable
manner, we propose a methodology for combining them with ``traditional''
software engineering techniques in a structured way, with the goal of
streamlining the development process, reducing errors, and enabling users to
verify crucial program properties with increased confidence. Specifically, we
focus on the Scenario-Based Programming (SBP) paradigm -- an event-driven,
scenario-based approach for software engineering -- to allow human developers
to pour their expert knowledge into the LLM, as well as to inspect and verify
its outputs.
  To evaluate our methodology, we conducted a significant case study, and used
it to design and implement the Connect4 game. By combining LLMs and SBP we were
able to create a highly-capable agent, which could defeat various strong
existing agents. Further, in some cases, we were able to formally verify the
correctness of our agent. Finally, our experience reveals interesting insights
regarding the ease-of-use of our proposed approach. The full code of our
case-study will be made publicly available with the final version of this
paper.

</details>


### [11] [Altered Histories in Version Control System Repositories: Evidence from the Trenches](https://arxiv.org/abs/2509.09294)
*Solal Rapaport,Laurent Pautet,Samuel Tardieu,Stefano Zacchiroli*

Main category: cs.SE

TL;DR: 这篇论文对Git公共代码仓库中的历史修改进行了首次大规模调查，发现了122万个仓库存在8.7万次历史重写，并开发了自动化工具GitHistorian来检测这些修改。


<details>
  <summary>Details</summary>
Motivation: 调查Git公共分支中的历史修改行为，这些修改对下游用户造成推送/拉取流程失效、娱子库完整性和可复现性问题，为供应链攻击提供可之机。

Method: 分析了软件遗产存档案案的111万个仓库，对历史修改进行分类和统计，包括修改发生的仓库、分支以及修改的内容（文件或提交元数据）。

Result: 在1.22万个仓库中发现8.7万次历史重写。通过两个目标案例研究发现，历史修改常用于追溯性更改许可证或移除漏浃的秘密信息，这些都是不良的项目管理或安全实践。

Conclusion: 研究强调了公共Git仓库中历史修改的常见性和潜在风险，并提供了GitHistorian工具来帮助开发者识别和描述这些修改，以避免不良实践对软件接收者造成的风险。

Abstract: Version Control Systems (VCS) like Git allow developers to locally rewrite
recorded history, e.g., to reorder and suppress commits or specific data in
them. These alterations have legitimate use cases, but become problematic when
performed on public branches that have downstream users: they break push/pull
workflows, challenge the integrity and reproducibility of repositories, and
create opportunities for supply chain attackers to sneak into them nefarious
changes. We conduct the first large-scale investigation of Git history
alterations in public code repositories. We analyze 111 M (millions)
repositories archived by Software Heritage, which preserves VCS histories even
across alterations. We find history alterations in 1.22 M repositories, for a
total of 8.7 M rewritten histories. We categorize changes by where they happen
(which repositories, which branches) and what is changed in them (files or
commit metadata). Conducting two targeted case studies we show that altered
histories recurrently change licenses retroactively, or are used to remove
''secrets'' (e.g., private keys) committed by mistake. As these behaviors
correspond to bad practices-in terms of project governance or security
management, respectively-that software recipients might want to avoid, we
introduce GitHistorian, an automated tool, that developers can use to spot and
describe history alterations in public Git repositories.

</details>


### [12] [Cross-Domain Evaluation of Transformer-Based Vulnerability Detection on Open & Industry Data](https://arxiv.org/abs/2509.09313)
*Moritz Mock,Thomas Forrer,Barbara Russo*

Main category: cs.SE

TL;DR: 研究评估了CodeBERT在产业和开源软件中检测漏洞的性能，开发了集成到CI/CD流程的AI-DO工具，并通过调查验证其实用性。


<details>
  <summary>Details</summary>
Motivation: 深度学习漏洞检测技术在产业环境中遇到信任性、继承系统、技能差距和工作流集成等挑战，需要开发更实用的解决方案。

Method: 使用CodeBERT模型，在开源和产业数据上进行交叉域测试，应用类不平衡处理技术，开发CI/CD集成的推荐系统AI-DO。

Result: 培训于产业数据的模型在同域检测准确，但在开源代码上性能下降；使用适当的下采样技术在开源数据上微调的深度学习模型能提高漏洞检测能力。

Conclusion: AI-DO系统能够在不打断工作流的情况下检测和定位漏洞，通过IT专业人员调查验证了其实用性。

Abstract: Deep learning solutions for vulnerability detection proposed in academic
research are not always accessible to developers, and their applicability in
industrial settings is rarely addressed. Transferring such technologies from
academia to industry presents challenges related to trustworthiness, legacy
systems, limited digital literacy, and the gap between academic and industrial
expertise. For deep learning in particular, performance and integration into
existing workflows are additional concerns. In this work, we first evaluate the
performance of CodeBERT for detecting vulnerable functions in industrial and
open-source software. We analyse its cross-domain generalisation when
fine-tuned on open-source data and tested on industrial data, and vice versa,
also exploring strategies for handling class imbalance. Based on these results,
we develop AI-DO(Automating vulnerability detection Integration for Developers'
Operations), a Continuous Integration-Continuous Deployment (CI/CD)-integrated
recommender system that uses fine-tuned CodeBERT to detect and localise
vulnerabilities during code review without disrupting workflows. Finally, we
assess the tool's perceived usefulness through a survey with the company's IT
professionals. Our results show that models trained on industrial data detect
vulnerabilities accurately within the same domain but lose performance on
open-source code, while a deep learner fine-tuned on open data, with
appropriate undersampling techniques, improves the detection of
vulnerabilities.

</details>


### [13] [ORCA: Unveiling Obscure Containers In The Wild](https://arxiv.org/abs/2509.09322)
*Jacopo Bufalino,Agathe Blaise,Stefano Secci*

Main category: cs.SE

TL;DR: 这篇论文研究了容器图像中的模糊现象对软件组成分析(SCA)工具的影响，并提出了一种可以抵御模糊效果的分析方法ORCA，在文件覆盖率上比现有工具提高40%。


<details>
  <summary>Details</summary>
Motivation: 容器化环境中使用过时或存在漏洞的第三方组件带来安全风险，SCA工具能够识别这些风险，但容器文件系统的无意修改会导致图像不完整，影响SCA工具的可靠性。

Method: 分析了600个流行容器的模糊现象，并提出了一种可以抵御模糊效果的容器分析方法，实现为开源工具ORCA。

Result: 发现流行注册表和可信图像中存在模糊容器，许多SCA工具无法有效分析这类容器。ORCA能够有效检测模糊容器内容，文件覆盖率比Docker Scout和Syft提高了40%。

Conclusion: 当前的SCA工具在处理模糊容器时存在显著局限性，ORCA提供了一种更加可靠的解决方案，能够在容器环境中更有效地识别和管理安全风险。

Abstract: Modern software development increasingly depends on open-source libraries and
third-party components, which are often encapsulated into containerized
environments. While improving the development and deployment of applications,
this approach introduces security risks, particularly when outdated or
vulnerable components are inadvertently included in production environments.
Software Composition Analysis (SCA) is a critical process that helps identify
and manage packages and dependencies inside a container. However, unintentional
modifications to the container filesystem can lead to incomplete container
images, which compromise the reliability of SCA tools. In this paper, we
examine the limitations of both cloud-based and open-source SCA tools when
faced with such obscure images. An analysis of 600 popular containers revealed
that obscure containers exist in well-known registries and trusted images and
that many tools fail to analyze such containers. To mitigate these issues, we
propose an obscuration-resilient methodology for container analysis and
introduce ORCA (Obscuration-Resilient Container Analyzer), its open-source
implementation. We reported our findings to all vendors using their appropriate
channels. Our results demonstrate that ORCA effectively detects the content of
obscure containers and achieves a median 40% improvement in file coverage
compared to Docker Scout and Syft.

</details>


### [14] [LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering](https://arxiv.org/abs/2509.09614)
*Jielin Qiu,Zuxin Liu,Zhiwei Liu,Rithesh Murthy,Jianguo Zhang,Haolin Chen,Shiyu Wang,Ming Zhu,Liangwei Yang,Juntao Tan,Zhepeng Cen,Cheng Qian,Shelby Heinecke,Weiran Yao,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.SE

TL;DR: LoCoBench是一个专门评估长上下文LLM在复杂软件开发场景中表现的基准测试，包含8000个评估场景，覆盖10种编程语言，上下文长度从10K到1M tokens，包含8个任务类别和17个评估指标。


<details>
  <summary>Details</summary>
Motivation: 随着支持百万token上下文窗口的语言模型出现，需要专门的基准测试来评估这些模型在真实复杂软件开发场景中的长上下文理解能力，填补现有代码评估基准在长上下文能力评估方面的空白。

Method: 通过5阶段流水线系统生成多样化的高质量评估场景，包含8个任务类别：架构理解、跨文件重构、多会话开发、bug调查、功能实现、代码理解、集成测试和安全分析。使用17个指标（包括8个新指标）在4个维度上进行综合评估，形成LoCoBench Score (LCBS)。

Result: 对最先进的长上下文模型评估显示存在显著的性能差距，表明复杂软件开发中的长上下文理解仍然是一个重要的未解决挑战。

Conclusion: 长上下文理解在复杂软件开发中是一个需要更多关注的重要挑战，LoCoBench为评估和推动这一领域的发展提供了全面的基准测试框架。

Abstract: The emergence of long-context language models with context windows extending
to millions of tokens has created new opportunities for sophisticated code
understanding and software development evaluation. We propose LoCoBench, a
comprehensive benchmark specifically designed to evaluate long-context LLMs in
realistic, complex software development scenarios. Unlike existing code
evaluation benchmarks that focus on single-function completion or short-context
tasks, LoCoBench addresses the critical evaluation gap for long-context
capabilities that require understanding entire codebases, reasoning across
multiple files, and maintaining architectural consistency across large-scale
software systems. Our benchmark provides 8,000 evaluation scenarios
systematically generated across 10 programming languages, with context lengths
spanning 10K to 1M tokens, a 100x variation that enables precise assessment of
long-context performance degradation in realistic software development
settings. LoCoBench introduces 8 task categories that capture essential
long-context capabilities: architectural understanding, cross-file refactoring,
multi-session development, bug investigation, feature implementation, code
comprehension, integration testing, and security analysis. Through a 5-phase
pipeline, we create diverse, high-quality scenarios that challenge LLMs to
reason about complex codebases at unprecedented scale. We introduce a
comprehensive evaluation framework with 17 metrics across 4 dimensions,
including 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our
evaluation of state-of-the-art long-context models reveals substantial
performance gaps, demonstrating that long-context understanding in complex
software development represents a significant unsolved challenge that demands
more attention. LoCoBench is released at:
https://github.com/SalesforceAIResearch/LoCoBench.

</details>


### [15] [I Know Who Clones Your Code: Interpretable Smart Contract Similarity Detection](https://arxiv.org/abs/2509.09630)
*Zhenguang Liu,Lixun Ma,Zhongzheng Mu,Chengkun Wei,Xiaojun Xu,Yingying Jiao,Kui Ren*

Main category: cs.SE

TL;DR: SmartDetector是一种新颖的智能合约函数相似性检测方法，通过将AST分解为语句树并进行细粒度比较，在三个真实数据集上平均F1分数达到95.88%，比现有方法提升14.01%。


<details>
  <summary>Details</summary>
Motivation: 智能合约开发中广泛重用开源代码显著提高了编程效率，但也放大了漏洞传播风险。现有的基于AST的方法难以处理复杂树结构，而深度学习方法往往忽略代码语法和可解释性，导致性能不佳。

Method: SmartDetector将智能合约函数的AST分解为一系列较小的语句树，每个语句树反映源代码的结构元素。然后使用分类器通过比较每对语句树来计算两个函数的相似性得分。采用余弦扩散过程高效搜索最优超参数。

Result: 在三个大型真实数据集上的广泛实验表明，SmartDetector在F1分数上平均比当前最先进方法提升14.01%，总体平均F1分数达到95.88%。

Conclusion: SmartDetector提供了一种在细粒度语句级别可解释的智能合约函数相似性计算方法，有效解决了现有方法在处理复杂树结构和保持可解释性方面的局限性，显著提升了检测性能。

Abstract: Widespread reuse of open-source code in smart contract development boosts
programming efficiency but significantly amplifies bug propagation across
contracts, while dedicated methods for detecting similar smart contract
functions remain very limited. Conventional abstract-syntax-tree (AST) based
methods for smart contract similarity detection face challenges in handling
intricate tree structures, which impedes detailed semantic comparison of code.
Recent deep-learning based approaches tend to overlook code syntax and
detection interpretability, resulting in suboptimal performance.
  To fill this research gap, we introduce SmartDetector, a novel approach for
computing similarity between smart contract functions, explainable at the
fine-grained statement level. Technically, SmartDetector decomposes the AST of
a smart contract function into a series of smaller statement trees, each
reflecting a structural element of the source code. Then, SmartDetector uses a
classifier to compute the similarity score of two functions by comparing each
pair of their statement trees. To address the infinite hyperparameter space of
the classifier, we mathematically derive a cosine-wise diffusion process to
efficiently search optimal hyperparameters. Extensive experiments conducted on
three large real-world datasets demonstrate that SmartDetector outperforms
current state-of-the-art methods by an average improvement of 14.01% in
F1-score, achieving an overall average F1-score of 95.88%.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [16] [Guarded Fragments Meet Dynamic Logic: The Story of Regular Guards (Extended Version)](https://arxiv.org/abs/2509.09218)
*Bartosz Bednarczyk,Emanuel Kieroński*

Main category: cs.LO

TL;DR: 研究了带正则守卫的守卫片段(RGF)，结合了守卫片段(GF)和带交集与逆的命题动态逻辑(ICPDL)的表达能力，证明了RGF的可满足性问题是2EXPTIME完全的，查询蕴含问题是不可判定的，并识别了最大的EXPSPACE完全片段


<details>
  <summary>Details</summary>
Motivation: 统一地推广之前研究的GF扩展，包括传递或等价守卫、传递或等价闭包等，探索RGF的表达能力和计算复杂性

Method: 结合GF和ICPDL的形式化方法，通过逻辑推理和复杂性分析技术，证明可满足性问题的2EXPTIME完全性和查询蕴含问题的不可判定性

Result: RGF的可满足性问题是2EXPTIME完全的，与ICPDL或GF的难度相同；查询蕴含问题是不可判定的；识别出了RGF中最大的EXPSPACE完全片段

Conclusion: RGF在保持与GF相同计算复杂性的同时显著增强了表达能力，但查询蕴含问题的不可判定性限制了其实际应用，最大EXPSPACE完全片段的识别为实际应用提供了可行方案

Abstract: We study the Guarded Fragment with Regular Guards (RGF), which combines the
expressive power of the Guarded Fragment (GF) with Propositional Dynamic Logic
with Intersection and Converse (ICPDL). Our logic generalizes, in a uniform
way, many previously-studied extensions of GF, including (conjunctions of)
transitive or equivalence guards, transitive or equivalence closure and more.
We prove 2EXPTIME-completeness of the satisfiability problem for RGF, showing
that RGF is not harder than ICPDL or GF. Shifting to the query entailment
problem, we provide undecidability results that significantly strengthen and
solidify earlier results along those lines. We conclude by identifying, in a
natural sense, the maximal EXPSPACE-complete fragment of RGF.

</details>
