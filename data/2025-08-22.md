<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 8]
- [cs.SE](#cs.SE) [Total: 10]
- [cs.FL](#cs.FL) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Homomorphism Calculus for User-Defined Aggregations](https://arxiv.org/abs/2508.15109)
*Ziteng Wang,Ruijie Fang,Linus Zheng,Dixin Tang,Isil Dillig*

Main category: cs.PL

TL;DR: 这篇论文提出了一种新的同态形式微积分方法，用于验证和拒绝用户定义聚合函数是否满足同态性质，以支持高效的并行执行和增量计算。


<details>
  <summary>Details</summary>
Motivation: 虽然Spark和Flink等框架支持用户定义聚合函数，但要实现高效执行，函数必须满足同态性质，这是当前的技术挑战。

Method: 设计了一种同态形式微积分方法，能够自动验证UDAF是否满足同态性质，并为满足条件的函数构造合并运算符。

Result: 在真实世界UDAF上评估实现算法，表现显著超过了两个领先的综合器。

Conclusion: 该方法有效解决了数据处理框架中UDAF高效执行的同态性验证问题，为并行化和增量计算提供了可靠支持。

Abstract: Data processing frameworks like Apache Spark and Flink provide built-in
support for user-defined aggregation functions (UDAFs), enabling the
integration of domain-specific logic. However, for these frameworks to support
\emph{efficient} UDAF execution, the function needs to satisfy a
\emph{homomorphism property}, which ensures that partial results from
independent computations can be merged correctly. Motivated by this problem,
this paper introduces a novel \emph{homomorphism calculus} that can both verify
and refute whether a UDAF is a dataframe homomorphism. If so, our calculus also
enables the construction of a corresponding merge operator which can be used
for incremental computation and parallel execution. We have implemented an
algorithm based on our proposed calculus and evaluate it on real-world UDAFs,
demonstrating that our approach significantly outperforms two leading
synthesizers.

</details>


### [2] [Software Model Checking via Summary-Guided Search (Extended Version)](https://arxiv.org/abs/2508.15137)
*Ruijie Fang,Zachary Kincaid,Thomas Reps*

Main category: cs.PL

TL;DR: GPS是一种新的软件模型检测算法，结合了静态分析和定向搜索，能够高效发现程序安全证明和反例，在性能测试中优于现有最先进工具。


<details>
  <summary>Details</summary>
Motivation: 现有的软件模型检测工具在处理包含长输入依赖错误路径的程序时效率不高，需要一种能够同时保证完备性和高效性的新方法。

Method: GPS采用组合式基于摘要的静态分析来指导程序状态的定向搜索，使用静态分析摘要来剪枝不可行路径并驱动测试生成，采用双层搜索策略和插桩技术实现反例完备性。

Result: 在包含SV-COMP和先前文献的基准测试中，GPS在解决的问题数量和运行时间方面都优于最先进的软件模型检测器（包括SV-COMP ReachSafety-Loops类别的顶级工具）。

Conclusion: GPS通过结合静态分析和定向搜索的新颖方法，在保持反例完备性的同时显著提高了软件模型检测的效率，特别是在处理复杂错误路径方面表现出色。

Abstract: In this work, we describe a new software model-checking algorithm called GPS.
GPS treats the task of model checking a program as a directed search of the
program states, guided by a compositional, summary-based static analysis. The
summaries produced by static analysis are used both to prune away infeasible
paths and to drive test generation to reach new, unexplored program states. GPS
can find both proofs of safety and counter-examples to safety (i.e., inputs
that trigger bugs), and features a novel two-layered search strategy that
renders it particularly efficient at finding bugs in programs featuring long,
input-dependent error paths. To make GPS refutationally complete (in the sense
that it will find an error if one exists, if it is allotted enough time), we
introduce an instrumentation technique and show that it helps GPS achieve
refutation-completeness without sacrificing overall performance. We benchmarked
GPS on a suite of benchmarks including both programs from the Software
Verification Competition (SV-COMP) and from prior literature, and found that
our implementation of GPS outperforms state-of-the-art software model checkers
(including the top performers in SV-COMP ReachSafety-Loops category), both in
terms of the number of benchmarks solved and in terms of running time.

</details>


### [3] [Big-Stop Semantics: A Simple Way to Get the Benefits of Small-Step Semantics in a Big-Step Judgment](https://arxiv.org/abs/2508.15157)
*David M Kahn,Jan Hoffmann,Runming Li*

Main category: cs.PL

TL;DR: 大步步循环语义扩展为大停步语义，通过归纳定义捕获分散计算，保持了大步步语义的便捷性同时提供与小步步语义相等的表达能力。


<details>
  <summary>Details</summary>
Motivation: 大步步语义虽然便捷但无法描述分散计算，小步步语义虽能描述分散但规则复杂。需要找到一种方法结合两者优点。

Method: 通过归纳定义扩展大步步语义为大停步语义，在标准评估规则基础上添加少量规则，定义与小步步过渡关系闭包等价的评估判断。

Result: 大停步语义能够捕获分散计算而不引入错误状态，并在类型化、非类型化和带效果的PCF以及命令式语言中得到验证。

Conclusion: 大停步语义提供了一种简洁而强大的方法，在保持大步步语义便捷性的同时达到了与小步步语义相等的表达能力，避免了其他方案的复杂性。

Abstract: As evident in the programming language literature, many practitioners favor
specifying dynamic program behavior using big-step over small-step semantics.
Unlike small-step semantics, which must dwell on every intermediate program
state, big-step semantics conveniently jump directly to the ever-important
result of the computation. Big-step semantics also typically involve fewer
inference rules than their small-step counterparts. However, in exchange for
ergonomics, big-step semantics give up power: Small-step semantics describes
program behaviors that are outside the grasp of big-step semantics, notably
divergence. This work presents a little-known extension of big-step semantics
with inductive definitions that captures diverging computations without
introducing error states. This big-stop semantics is illustrated for typed,
untyped, and effectful variants of PCF, as well as a while-loop-based
imperative language. Big-stop semantics extends the standard big-step inference
rules with a few additional rules to define an evaluation judgment that is
equivalent to the reflexive-transitive closure of small-step transitions. This
simple extension contrasts with other solutions in the literature which
sacrifice ergonomics by introducing many additional inference rules, global
state, and/or less-commonly-understood reasoning principles like coinduction.

</details>


### [4] [Probabilistic Inference for Datalog with Correlated Inputs](https://arxiv.org/abs/2508.15166)
*Jingbo Wang,Shashin Halalingaiah,Weiyi Chen,Chao Wang,Isil Dillig*

Main category: cs.PL

TL;DR: Praline是一个新的Datalog扩展，用于处理输入事实间存在统计相关性时的精确概率推理，通过约束优化和δ-精确推理算法提供可扩展的紧致概率边界。


<details>
  <summary>Details</summary>
Motivation: 现有的概率逻辑编程语言（如ProbLog）在评估输出关系概率时未考虑输入事实间的统计相关性，这限制了推理的精确性。

Method: 将推理任务建模为约束优化问题，提出δ-精确推理算法，结合约束求解、静态分析和迭代优化来提高可扩展性。

Result: 在包括侧信道分析在内的真实基准测试中，该方法不仅具有良好可扩展性，还能提供紧致的概率边界。

Conclusion: Praline成功解决了概率逻辑编程中输入相关性的处理问题，通过创新的优化方法实现了精确且可扩展的概率推理。

Abstract: Probabilistic extensions of logic programming languages, such as ProbLog,
integrate logical reasoning with probabilistic inference to evaluate
probabilities of output relations; however, prior work does not account for
potential statistical correlations among input facts. This paper introduces
Praline, a new extension to Datalog designed for precise probabilistic
inference in the presence of (partially known) input correlations. We formulate
the inference task as a constrained optimization problem, where the solution
yields sound and precise probability bounds for output facts. However, due to
the complexity of the resulting optimization problem, this approach alone often
does not scale to large programs. To address scalability, we propose a more
efficient $\delta$-exact inference algorithm that leverages constraint solving,
static analysis, and iterative refinement. Our empirical evaluation on
challenging real-world benchmarks, including side-channel analysis,
demonstrates that our method not only scales effectively but also delivers
tight probability bounds.

</details>


### [5] [Exploring the Theory and Practice of Concurrency in the Entity-Component-System Pattern](https://arxiv.org/abs/2508.15264)
*Patrick Redmond,Jonathan Castello,José Manuel Calderón Trilla,Lindsey Kuper*

Main category: cs.PL

TL;DR: 本文提出了Core ECS形式化模型，抽象ECS模式的本质，识别出确定性并发行为类别，并发现现有ECS框架未能充分利用确定性并发机会。


<details>
  <summary>Details</summary>
Motivation: ECS模式在游戏开发中广泛应用但外界了解有限，现有解释过于关注具体框架细节或使用不完美比喻，需要建立严谨的形式化模型来理解其本质。

Method: 设计Core ECS形式化模型，抽象具体实现细节，识别确定性行为的程序类别，并调研多个真实ECS框架进行比较分析。

Result: 发现一类Core ECS程序具有调度无关的确定性行为，现有ECS框架都未能充分利用这种确定性并发机会。

Conclusion: 研究指出了新的ECS实现技术空间，可以更好地利用确定性并发机会，将ECS模式作为确定性构造的并发编程模型。

Abstract: The Entity-Component-System (ECS) software design pattern, long used in game
development, encourages a clean separation of identity (entities), data
properties (components), and computational behaviors (systems). Programs
written using the ECS pattern are naturally concurrent, and the pattern offers
modularity, flexibility, and performance benefits that have led to a
proliferation of ECS frameworks. Nevertheless, the ECS pattern is little-known
and not well understood outside of a few domains. Existing explanations of the
ECS pattern tend to be mired in the concrete details of particular ECS
frameworks, or they explain the pattern in terms of imperfect metaphors or in
terms of what it is not. We seek a rigorous understanding of the ECS pattern
via the design of a formal model, Core ECS, that abstracts away the details of
specific implementations to reveal the essence of software using the ECS
pattern. We identify a class of Core ECS programs that behave deterministically
regardless of scheduling, enabling use of the ECS pattern as a
deterministic-by-construction concurrent programming model. With Core ECS as a
point of comparison, we then survey several real-world ECS frameworks and find
that they all leave opportunities for deterministic concurrency unexploited.
Our findings point out a space for new ECS implementation techniques that
better leverage such opportunities.

</details>


### [6] [Fair Termination for Resource-Aware Active Objects](https://arxiv.org/abs/2508.15333)
*Francesco Dagnino,Paola Giannini,Violet Ka I Pun,Ulises Torrella*

Main category: cs.PL

TL;DR: 开发了一个资源感知的主动对象核心演算和类型系统，确保良类型程序能够公平终止


<details>
  <summary>Details</summary>
Motivation: 主动对象系统是分布式计算模型，用于建模分布式系统和业务流程工作流，需要资源感知的形式化方法

Method: 结合分级语义和类型系统技术（适用于顺序程序）与公平终止技术（适用于同步会话）

Result: 提出了确保程序公平终止的类型系统

Conclusion: 成功将顺序程序的分级类型技术与同步会话的公平终止技术相结合，为资源感知主动对象模型提供了形式化基础

Abstract: Active object systems are a model of distributed computation that has been
adopted for modelling distributed systems and business process workflows. This
field of modelling is, in essence, concurrent and resource-aware, motivating
the development of resource-aware formalisations on the active object model.
The contributions of this work are the development of a core calculus for
resource-aware active objects together with a type system ensuring that
well-typed programs are fairly terminating, i.e., they can always eventually
terminate. To achieve this, we combine techniques from graded semantics and
type systems, which are quite well understood for sequential programs, with
those for fair termination, which have been developed for synchronous~sessions.

</details>


### [7] [Compositional Symbolic Execution for the Next 700 Memory Models (Extended Version)](https://arxiv.org/abs/2508.15576)
*Andreas Lööw,Seung Hoon Park,Daniele Nantes-Sobrinho,Sacha-Élie Ayoun,Opale Sjöstedt,Philippa Gardner*

Main category: cs.PL

TL;DR: 本文为内存模型参数化组合符号执行平台提供了新的形式化基础，支持分离逻辑和错误分离逻辑分析，并在Rocq定理证明器中实现和验证


<details>
  <summary>Details</summary>
Motivation: 现有组合符号执行工具虽然利用分离逻辑进行组合验证，但缺乏对内存模型参数化平台的满意形式化基础，限制了平台的灵活性和适用范围

Method: 基于Gillian平台的启发，在Rocq交互式定理证明器中机械化形式化基础，支持多种内存模型实例化（包括C和CHERI），涵盖SL和ISL分析

Result: 成功建立了内存模型参数化CSE平台的形式化基础，能够实例化到广泛的内存模型，同时支持SL和ISL分析

Conclusion: 该形式化基础为组合符号执行平台提供了更灵活、更标准化的理论基础，支持多种编程语言分析和自定义自动化

Abstract: Multiple successful compositional symbolic execution (CSE) tools and
platforms exploit separation logic (SL) for compositional verification and/or
incorrectness separation logic (ISL) for compositional bug-finding, including
VeriFast, Viper, Gillian, CN, and Infer-Pulse. Previous work on the Gillian
platform, the only CSE platform that is parametric on the memory model, meaning
that it can be instantiated to different memory models, suggests that the
ability to use custom memory models allows for more flexibility in supporting
analysis of a wide range of programming languages, for implementing custom
automation, and for improving performance. However, the literature lacks a
satisfactory formal foundation for memory-model-parametric CSE platforms.
  In this paper, inspired by Gillian, we provide a new formal foundation for
memory-model-parametric CSE platforms. Our foundation advances the state of the
art in four ways. First, we mechanise our foundation (in the interactive
theorem prover Rocq). Second, we validate our foundation by instantiating it to
a broad range of memory models, including models for C and CHERI. Third,
whereas previous memory-model-parametric work has only covered SL analyses, we
cover both SL and ISL analyses. Fourth, our foundation is based on standard
definitions of SL and ISL (including definitions of function specification
validity, to ensure sound interoperation with other tools and platforms also
based on standard definitions).

</details>


### [8] [Active Learning for Neurosymbolic Program Synthesis](https://arxiv.org/abs/2508.15750)
*Celeste Barnaby,Qiaochu Chen,Ramya Ramalingam,Osbert Bastani,Isil Dillig*

Main category: cs.PL

TL;DR: 本文提出了一种新的主动学习方法SmartLabel，专门处理神经符号程序合成中神经网络误预测带来的挑战，通过约束保形评估(CCE)技术显著提高了合成准确率


<details>
  <summary>Details</summary>
Motivation: 传统主动学习方法在纯符号程序合成中有效，但在神经符号程序合成中由于神经网络组件的误预测会导致返回错误程序，需要新的方法来处理这种挑战

Method: 提出约束保形评估(CCE)策略，考虑神经网络误预测并整合用户反馈，通过迭代优化CCE精度直到所有剩余程序保证观测等价

Result: 在三个神经符号领域实验中，SmartLabel在98%的基准测试中识别出真实程序，平均需要不到5轮用户交互，而先前技术最多只能达到65%的准确率

Conclusion: SmartLabel方法有效解决了神经符号程序合成中的主动学习挑战，显著提高了合成准确性和用户交互效率

Abstract: The goal of active learning for program synthesis is to synthesize the
desired program by asking targeted questions that minimize user interaction.
While prior work has explored active learning in the purely symbolic setting,
such techniques are inadequate for the increasingly popular paradigm of
neurosymbolic program synthesis, where the synthesized program incorporates
neural components. When applied to the neurosymbolic setting, such techniques
can -- and, in practice, do -- return an unintended program due to
mispredictions of neural components. This paper proposes a new active learning
technique that can handle the unique challenges posed by neural network
mispredictions. Our approach is based upon a new evaluation strategy called
constrained conformal evaluation (CCE), which accounts for neural
mispredictions while taking into account user-provided feedback. Our proposed
method iteratively makes CCE more precise until all remaining programs are
guaranteed to be observationally equivalent. We have implemented this method in
a tool called SmartLabel and experimentally evaluated it on three neurosymbolic
domains. Our results demonstrate that SmartLabel identifies the ground truth
program for 98% of the benchmarks, requiring under 5 rounds of user interaction
on average. In contrast, prior techniques for active learning are only able to
converge to the ground truth program for at most 65% of the benchmarks.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [9] [On the need to perform comprehensive evaluations of automated program repair benchmarks: Sorald case study](https://arxiv.org/abs/2508.15135)
*Sumudu Liyanage,Sherlock A. Licorish,Markus Wagner,Stephen G. MacDonell*

Main category: cs.SE

TL;DR: 本研究开发了一个全面的自动化程序修复(APR)工具评估框架，并以Sorald工具为例进行评估，发现虽然能修复特定规则违规，但会引入新故障、降低功能正确性和代码结构质量。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅评估APR工具清除违规的能力，忽略了其可能引入新违规、改变代码功能和降低代码结构的潜在问题，需要开发全面的评估框架。

Method: 以Sorald作为概念验证，评估其修复3,529个SonarQube违规的能力，分析2,393个Java代码片段，检查新故障引入、功能正确性和代码结构变化。

Result: Sorald修复了特定规则违规，但引入了2,120个新故障(32个bug，2,088个代码异味)，单元测试失败率达24%，代码结构退化。

Conclusion: 需要能捕捉APR工具全方位影响的评估方法，包括副作用，以确保其安全有效采用。

Abstract: In supporting the development of high-quality software, especially necessary
in the era of LLMs, automated program repair (APR) tools aim to improve code
quality by automatically addressing violations detected by static analysis
profilers. Previous research tends to evaluate APR tools only for their ability
to clear violations, neglecting their potential introduction of new (sometimes
severe) violations, changes to code functionality and degrading of code
structure. There is thus a need for research to develop and assess
comprehensive evaluation frameworks for APR tools. This study addresses this
research gap, and evaluates Sorald (a state-of-the-art APR tool) as a proof of
concept. Sorald's effectiveness was evaluated in repairing 3,529 SonarQube
violations across 30 rules within 2,393 Java code snippets extracted from Stack
Overflow. Outcomes show that while Sorald fixes specific rule violations, it
introduced 2,120 new faults (32 bugs, 2088 code smells), reduced code
functional correctness--as evidenced by a 24% unit test failure rate--and
degraded code structure, demonstrating the utility of our framework. Findings
emphasize the need for evaluation methodologies that capture the full spectrum
of APR tool effects, including side effects, to ensure their safe and effective
adoption.

</details>


### [10] [Foundational Design Principles and Patterns for Building Robust and Adaptive GenAI-Native Systems](https://arxiv.org/abs/2508.15411)
*Frederik Vandeputte*

Main category: cs.SE

TL;DR: 本文提出GenAI原生系统设计原则，将生成式AI与传统软件工程结合，构建可靠、自适应、高效的系统架构


<details>
  <summary>Details</summary>
Motivation: 生成式AI虽然具有变革性能力，但由于不可预测性和低效性，在开发可靠系统时面临重大挑战，需要新的设计范式

Method: 提出基于五大支柱（可靠性、卓越性、可进化性、自依赖性、保障性）的GenAI原生设计原则，以及GenAI原生单元、有机基质、可编程路由器等架构模式

Result: 构建了GenAI原生软件栈的关键要素框架，并从技术、用户采纳、经济、法律等多角度分析了系统影响

Conclusion: 该框架为未来研究提供启发，需要进一步验证和实验，鼓励相关社区实施和完善这一概念框架

Abstract: Generative AI (GenAI) has emerged as a transformative technology,
demonstrating remarkable capabilities across diverse application domains.
However, GenAI faces several major challenges in developing reliable and
efficient GenAI-empowered systems due to its unpredictability and inefficiency.
This paper advocates for a paradigm shift: future GenAI-native systems should
integrate GenAI's cognitive capabilities with traditional software engineering
principles to create robust, adaptive, and efficient systems.
  We introduce foundational GenAI-native design principles centered around five
key pillars -- reliability, excellence, evolvability, self-reliance, and
assurance -- and propose architectural patterns such as GenAI-native cells,
organic substrates, and programmable routers to guide the creation of resilient
and self-evolving systems. Additionally, we outline the key ingredients of a
GenAI-native software stack and discuss the impact of these systems from
technical, user adoption, economic, and legal perspectives, underscoring the
need for further validation and experimentation. Our work aims to inspire
future research and encourage relevant communities to implement and refine this
conceptual framework.

</details>


### [11] [An Empirical Study of Knowledge Distillation for Code Understanding Tasks](https://arxiv.org/abs/2508.15423)
*Ruiqi Wang,Zezhou Yang,Cuiyun Gao,Xin Xia,Qing Liao*

Main category: cs.SE

TL;DR: 本文系统研究了知识蒸馏在代码理解任务中的有效性，发现特征蒸馏方法效果最佳，学生模型仅需5%参数即可达到教师模型98%的性能


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型在代码理解中表现出色但计算密集，知识蒸馏可解决部署时的计算和延迟问题，但在代码理解领域的潜力尚未充分探索

Method: 研究两种主流知识蒸馏方法（基于logit和基于特征），在8个学生模型和2个教师PLM上进行实验，涵盖三个下游任务

Result: 知识蒸馏相比标准微调能显著提升性能，代码专用PLM作为教师效果更好，特征蒸馏方法表现最优，学生模型架构与教师相似性不一定带来更好性能

Conclusion: 知识蒸馏是代码理解任务中有效的模型压缩技术，特征蒸馏方法最具潜力，为高效代码理解模型部署提供了重要指导

Abstract: Pre-trained language models (PLMs) have emerged as powerful tools for code
understanding. However, deploying these PLMs in large-scale applications faces
practical challenges due to their computational intensity and inference
latency. Knowledge distillation (KD), a promising model compression and
acceleration technique, addresses these limitations by transferring knowledge
from large teacher models to compact student models, enabling efficient
inference while preserving most of the teacher models' capabilities. While this
technique has shown remarkable success in natural language processing and
computer vision domains, its potential for code understanding tasks remains
largely underexplored.
  In this paper, we systematically investigate the effectiveness and usage of
KD in code understanding tasks. Our study encompasses two popular types of KD
methods, i.e., logit-based and feature-based KD methods, experimenting across
eight student models and two teacher PLMs from different domains on three
downstream tasks. The experimental results indicate that KD consistently offers
notable performance boosts across student models with different sizes compared
with standard fine-tuning. Notably, code-specific PLM demonstrates better
effectiveness as the teacher model. Among all KD methods, the latest
feature-based KD methods exhibit superior performance, enabling student models
to retain up to 98% teacher performance with merely 5% parameters. Regarding
student architecture, our experiments reveal that similarity with teacher
architecture does not necessarily lead to better performance. We further
discuss the efficiency and behaviors in the KD process and inference, summarize
the implications of findings, and identify promising future directions.

</details>


### [12] [SynthCoder: A Synthetical Strategy to Tune LLMs for Code Completion](https://arxiv.org/abs/2508.15495)
*Dongjun Yu,Xiao Yan,Zhenrui Li,Jipeng Xiao,Haochuan He,Yongda Yu,Hao Zhang,Guoping Rong,Xiaobo Huang*

Main category: cs.SE

TL;DR: SynthCoder是一个集成行业最佳实践的代码补全模型，通过多样化数据集构建、跨文件上下文增强和两阶段训练，在主流代码补全基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有代码补全模型的优化方法往往存在权衡效应，在某些数据集上性能提升的同时在其他数据集上性能下降，甚至低于基线模型。需要一种能够全面优化且避免性能退化的方法。

Method: 1) 结合AST节点提取和启发式方法构建多样化数据集；2) 使用BM25算法和调用图增强跨文件上下文信息；3) 采用两阶段训练：基于Seed-Coder-8B-Base模型，先使用课程学习进行微调，再用DPO进行对齐。

Result: 在aiXcoder、ExecRepoBench、CrossCodeEval和CoLT等主流仓库级代码补全基准测试中表现优异，有效缓解了模型重复现有代码的常见问题。

Conclusion: SynthCoder通过综合运用多种先进技术，成功实现了在代码补全任务上的最优性能，解决了现有方法中的权衡问题，为代码补全领域提供了有效的解决方案。

Abstract: Code completion is a prominent application of Large Language Models (LLMs) in
software engineering. Due to the near real-time response requirements of this
task, base models with small to medium-sized parameters are typically employed,
supplemented by various optimization and post-training techniques. However,
these optimization methods often have trade-offs, leading to a seesaw effect
where performance improvements on certain datasets or metrics are accompanied
by degradations on others -- sometimes even falling below the baseline model's
performance. This paper proposes SynthCoder, a model that integrates leading
industry practices to achieve state-of-the-art performance on the
Fill-in-the-Middle (FIM) code completion task. In specific, we first construct
a diverse dataset by combining Abstract Syntax Tree (AST) node extraction with
heuristics that simulate developer behavior. Then we enrich our training corpus
with cross-file contextual information using the BM25 algorithm and call
graphs, enhancing the model's ability to perform code completion in both
file-level and repository-level scenarios. As the last step, we employ a
two-stage training process using the Seed-Coder-8B-Base as the base model.
First, we fine-tune the model using Curriculum Learning technology. Following
this, we perform alignment using Direct Preference Optimization (DPO) with
preference pairs generated through Rejection Sampling. Experimental results
demonstrate that our final model excels on mainstream repository-level code
completion benchmarks, including aiXcoder, ExecRepoBench, CrossCodeEval, and
CoLT. Furthermore, our carefully curated training set effectively mitigates the
model's tendency to just repeat existing code, a common issue existing in
various code completion models.

</details>


### [13] [Towards the Assessment of Task-based Chatbots: From the TOFU-R Snapshot to the BRASATO Curated Dataset](https://arxiv.org/abs/2508.15496)
*Elena Masserini,Diego Clerissi,Daniela Micucci,João R. Campos,Leonardo Mariani*

Main category: cs.SE

TL;DR: 本文提出了两个数据集TOFU-R和BRASATO，用于支持任务型聊天机器人的可靠性、安全性和鲁棒性评估研究，解决了现有评估技术缺乏大规模高质量数据集的问题。


<details>
  <summary>Details</summary>
Motivation: 任务型聊天机器人应用日益广泛，但对其可靠性、安全性和鲁棒性的评估研究不足，主要原因是缺乏大规模高质量数据集。现有的自动化质量评估技术往往基于有限样本或过时/不可用的代理，难以有效评估这些技术。

Method: 创建了两个数据集：TOFU-R（GitHub上Rasa聊天机器人的快照，代表开源Rasa开发的实践现状）和BRASATO（基于对话复杂性、功能复杂性和实用性的精选聊天机器人集合）。同时提供了创建和维护这些数据集的工具支持。

Result: 提供了两个大规模、高质量的数据集：TOFU-R包含GitHub上可用的Rasa聊天机器人快照；BRASATO是经过精心筛选的最相关聊天机器人集合。这些数据集有助于促进聊天机器人可靠性研究的可重复性。

Conclusion: 提出的TOFU-R和BRASATO数据集及其工具支持，为任务型聊天机器人的可靠性、安全性和鲁棒性评估研究提供了重要资源，解决了该领域缺乏大规模高质量数据集的问题，有助于推动相关研究的发展。

Abstract: Task-based chatbots are increasingly being used to deliver real services, yet
assessing their reliability, security, and robustness remains underexplored,
also due to the lack of large-scale, high-quality datasets. The emerging
automated quality assessment techniques targeting chatbots often rely on
limited pools of subjects, such as custom-made toy examples, or outdated, no
longer available, or scarcely popular agents, complicating the evaluation of
such techniques. In this paper, we present two datasets and the tool support
necessary to create and maintain these datasets. The first dataset is RASA
TASK-BASED CHATBOTS FROM GITHUB (TOFU-R), which is a snapshot of the Rasa
chatbots available on GitHub, representing the state of the practice in
open-source chatbot development with Rasa. The second dataset is BOT RASA
COLLECTION (BRASATO), a curated selection of the most relevant chatbots for
dialogue complexity, functional complexity, and utility, whose goal is to ease
reproducibility and facilitate research on chatbot reliability.

</details>


### [14] [Evaluation Guidelines for Empirical Studies in Software Engineering involving LLMs](https://arxiv.org/abs/2508.15503)
*Sebastian Baltes,Florian Angermeir,Chetan Arora,Marvin Muñoz Barón,Chunyang Chen,Lukas Böhme,Fabio Calefato,Neil Ernst,Davide Falessi,Brian Fitzgerald,Davide Fucci,Marcos Kalinowski,Stefano Lambiase,Daniel Russo,Mircea Lungu,Lutz Prechelt,Paul Ralph,Christoph Treude,Stefan Wagner*

Main category: cs.SE

TL;DR: 该论文提出了一个针对LLM在软件工程研究中使用的分类法和8条指南，旨在解决LLM的非确定性、训练数据不透明等问题，促进研究的可复现性。


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地应用于软件工程研究和实践，但其非确定性、不透明的训练数据和不断演变的架构给实证研究的复现和复制带来了挑战。

Method: 提出了LLM研究的分类法，并制定了8条设计指南，包括声明LLM使用、报告模型配置、记录工具架构、披露提示词和交互日志等。

Result: 建立了一套完整的LLM研究指南体系，包括必需和推荐的标准，为社区提供了可用的在线资源(llm-guidelines.org)。

Conclusion: 这些指南能够帮助克服LLM特有的开放科学障碍，实现研究的可复现性和可复制性，为软件工程领域的LLM研究提供了标准化框架。

Abstract: Large language models (LLMs) are increasingly being integrated into software
engineering (SE) research and practice, yet their non-determinism, opaque
training data, and evolving architectures complicate the reproduction and
replication of empirical studies. We present a community effort to scope this
space, introducing a taxonomy of LLM-based study types together with eight
guidelines for designing and reporting empirical studies involving LLMs. The
guidelines present essential (must) criteria as well as desired (should)
criteria and target transparency throughout the research process. Our
recommendations, contextualized by our study types, are: (1) to declare LLM
usage and role; (2) to report model versions, configurations, and fine-tuning;
(3) to document tool architectures; (4) to disclose prompts and interaction
logs; (5) to use human validation; (6) to employ an open LLM as a baseline; (7)
to report suitable baselines, benchmarks, and metrics; and (8) to openly
articulate limitations and mitigations. Our goal is to enable reproducibility
and replicability despite LLM-specific barriers to open science. We maintain
the study types and guidelines online as a living resource for the community to
use and shape (llm-guidelines.org).

</details>


### [15] [QUPER-MAn: Benchmark-Guided Target Setting for Maintainability Requirements](https://arxiv.org/abs/2508.15512)
*Markus Borg,Martin Larsson,Philip Breid,Nadim Hagatulah*

Main category: cs.SE

TL;DR: QUPER-MAn模型将可维护性从被忽视的开发后果转变为主动管理的目标，通过基准测试和目标设定支持需求工程


<details>
  <summary>Details</summary>
Motivation: 可维护源代码对软件开发至关重要，但现有研究表明可维护性往往得不到足够重视，需求工程可以填补这一空白

Method: 采用设计科学研究方法，开发了QUPER-MAn模型（QUPER模型的可维护性适配版本），集成可维护性基准测试并支持目标设定

Result: 研究发现可维护性仍然是次要质量关注点，明确需求通常只泛泛引用编码规范，工具提供的可维护性代理指标通常只用于隐式需求

Conclusion: QUPER-MAn模型能够通过知情和负责任的工程决策，将可维护性转变为主动管理的目标

Abstract: Maintainable source code is essential for sustainable development in any
software organization. Unfortunately, many studies show that maintainability
often receives less attention than its importance warrants. We argue that
requirements engineering can address this gap the problem by fostering
discussions and setting appropriate targets in a responsible manner. In this
preliminary work, we conducted an exploratory study of industry practices
related to requirements engineering for maintainability. Our findings confirm
previous studies: maintainability remains a second-class quality concern.
Explicit requirements often make sweeping references to coding conventions.
Tools providing maintainability proxies are common but typically only used in
implicit requirements related to engineering practices. To address this, we
propose QUPER-MAn, a maintainability adaption of the QUPER model, which was
originally developed to help organizations set targets for performance
requirements. Developed using a design science approach, QUPER-MAn, integrates
maintainability benchmarks and supports target setting. We posit that it can
shift maintainability from an overlooked development consequence to an actively
managed goal driven by informed and responsible engineering decisions.

</details>


### [16] [A Novel Mutation Based Method for Detecting FPGA Logic Synthesis Tool Bugs](https://arxiv.org/abs/2508.15536)
*Yi Zhang,He Jiang,Xiaochen Li,Shikai Guo,Peiyu Zou,Zun Wang*

Main category: cs.SE

TL;DR: VERMEI是一种测试FPGA逻辑综合工具的新方法，通过僵尸逻辑识别、等效变异和差分测试来发现工具缺陷，在5个月内发现了15个bug


<details>
  <summary>Details</summary>
Motivation: FPGA逻辑综合工具中的缺陷可能导致意外行为和安全隐患，现有测试方法存在测试程序语义和逻辑复杂性不足的问题

Method: 包含三个模块：预处理模块通过仿真和覆盖率分析识别僵尸逻辑；等效变异模块通过贝叶斯采样从历史Verilog设计中提取逻辑片段，在僵尸区域进行修剪或插入生成等效变体；bug识别模块基于差分测试比较种子程序和变体程序的综合输出

Result: 在Yosys、Vivado和Quartus上的实验表明VERMEI优于最先进方法，5个月内向供应商报告了15个bug，其中9个被确认为新bug

Conclusion: VERMEI通过生成具有复杂控制流和结构的测试程序，有效提高了FPGA逻辑综合工具的测试效果

Abstract: FPGA (Field-Programmable Gate Array) logic synthesis tools are key components
in the EDA (Electronic Design Automation) toolchain. They convert hardware
designs written in description languages such as Verilog into gate-level
representations for FPGAs. However, defects in these tools may lead to
unexpected behaviors and pose security risks. Therefore, it is crucial to
harden these tools through testing. Although several methods have been proposed
to automatically test FPGA logic synthesis tools, the challenge remains of
insufficient semantic and logical complexity in test programs. In this paper,
we propose VERMEI, a new method for testing FPGA logic synthesis tools. VERMEI
consists of three modules: preprocessing, equivalent mutation, and bug
identification. The preprocessing module identifies zombie logic (inactive code
with no impact on the circuit output) in seed programs through simulation and
coverage analysis. The equivalent mutation module generates equivalent variants
of seed programs by pruning or inserting logic fragments in zombie areas. It
uses Bayesian sampling to extract logic fragments from historical Verilog
designs, making the generated variants have complex control flows and
structures. The bug identification module, based on differential testing,
compares the synthesized outputs of seed and variant programs to identify bugs.
Experiments on Yosys, Vivado, and Quartus demonstrate that VERMEI outperforms
the state-of-the-art methods. Within five months, VERMEI reported 15 bugs to
vendors, 9 of which were confirmed as new.

</details>


### [17] [Establishing Technical Debt Management -- A Five-Step Workshop Approach and an Action Research Study](https://arxiv.org/abs/2508.15570)
*Marion Wiese,Kamila Serwa,Anastasia Besier,Ariane S. Marion-Jetten,Eva Bittner*

Main category: cs.SE

TL;DR: 通过动作研究在IT公司建立技术债务管理过程，应用工作坊方式推进技术债务评估、优先级排序和归还，并通过多种方法提升团队对技术债务的认知度。


<details>
  <summary>Details</summary>
Motivation: 技术债务管理在研究界广泛关注但实践中采用率低，需要开发可执行的TDM过程并验证其在实际环境中的效果和可持续性。

Method: 采用动作研究方法（16个月内完成5个行动周期），通过工作坊形式建立TDM过程。使用问卷调查、团队会议观察、TD-SAGAT心理测量方法和背调数据分析来评估技术债务认知度。

Result: 实践者偏好基于系统进化和成本计算的技术债务优先级排序和归还，重点处理"低垂果"。在背调项中添加提醒功能（如复选框、文本模板）有效提升了技术债务认知度。

Conclusion: 工作坊基于方法可行且能够实现可持续的过程改变，推出了适用于其他IT团队的新TDM思路，如使用重新提交日期、"讨论过TD"复选框和可视化优先级排序方法。

Abstract: Context. Technical debt (TD) items are constructs in a software system
providing short-term benefits but hindering future changes. TD management (TDM)
is frequently researched but rarely adopted in practice. Goal. This study aimed
to establish a TDM process in an IT company based on a predefined workshop
concept. We analyzed which research approaches practitioners adopted for each
TD activity and the TDM's long-term effect on TD awareness. Method. We used
action research (five action cycles in 16 months) with an IT team that creates
IT solutions for signal processing. To examine TD awareness, we (1) analyzed
questionnaires completed during each workshop, (2) observed team meetings, (3)
adopted a method from psychology for measuring awareness in decision-making
situations called TD-SAGAT, and (4) evaluated the backlog data. Results.
Practitioners preferred TD repayment and prioritization based on the system's
evolution and cost calculations, i.e., repayment of so-called low-hanging
fruits. Reminders in the backlog items, such as checkboxes or text templates,
led to a sustainable rise in TD awareness. Conclusions. We showed that a
workshop-based approach is feasible and leads to sustainable process changes.
New ideas for TDM applicable to other IT teams emerged, e.g., using a
re-submission date, using a Talked about TD checkbox, and using visualizations
for TD prioritization.

</details>


### [18] [From PREVENTion to REACTion: Enhancing Failure Resolution in Naval Systems](https://arxiv.org/abs/2508.15584)
*Maria Teresa Rossi,Leonardo Mariani,Oliviero Riganelli*

Main category: cs.SE

TL;DR: PREVENT和REACT方法在海军系统中的故障预测与诊断应用


<details>
  <summary>Details</summary>
Motivation: 大型工业系统经常因磨损、误用或故障而出现异常行为，需要及时检测、定位问题并实施对策

Method: 采用最先进的故障预测方法PREVENT及其扩展的故障排除模块REACT，应用于Fincantieri开发的海军系统

Result: 展示了如何将异常检测与故障排除程序集成，提供了实际应用结果

Conclusion: 总结了经验教训，有助于将这些分析方法部署和扩展到其他工业产品

Abstract: Complex and large industrial systems often misbehave, for instance, due to
wear, misuse, or faults. To cope with these incidents, it is important to
timely detect their occurrences, localize the sources of the problems, and
implement the appropriate countermeasures. This paper reports our experience
with a state-of-the-art failure prediction method, PREVENT, and its extension
with a troubleshooting module, REACT, applied to naval systems developed by
Fincantieri. Our results show how to integrate anomaly detection with
troubleshooting procedures. We conclude by discussing a lesson learned, which
may help deploy and extend these analyses to other industrial products.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [19] [Transition-based vs stated-based acceptance for automata over infinite words](https://arxiv.org/abs/2508.15402)
*Antonio Casares*

Main category: cs.FL

TL;DR: 这篇论文是一个关于在无限字自动机中从状态接受向过渡接受转变的调查性研究，分析了这种转变的原因和影响。


<details>
  <summary>Details</summary>
Motivation: 传统的无限字自动机使用状态接受条件，但近期趋向于使用过渡接受条件，需要分析这种转变的理由和优势。

Method: 通过调查研究方法，收集并分析了多个问题领域，在这些领域中形式化方法的选择会产生重大影响。

Result: 论文揭示了从状态接受向过渡接变转变的合理性，并讨论了这种变化在不同问题中导致的差异及其根源。

Conclusion: 在无限字自动机中推广使用过渡接受条件是有利的，这种形式方法在许多问题中都显示出优势。

Abstract: Automata over infinite objects are a well-established model with applications
in logic and formal verification. Traditionally, acceptance in such automata is
defined based on the set of states visited infinitely often during a run.
However, there is a growing trend towards defining acceptance based on
transitions rather than states.
  In this survey, we analyse the reasons for this shift and advocate using
transition-based acceptance in the context of automata over infinite words. We
present a collection of problems where the choice of formalism has a major
impact and discuss the causes of these differences.

</details>


### [20] [List of Results on the Černý Conjecture and Reset Thresholds for Synchronizing Automata](https://arxiv.org/abs/2508.15655)
*Mikhail V. Volkov*

Main category: cs.FL

TL;DR: 本文综述了关于Černý猜想在不同有限自动机类别中的研究成果，列出了已被证明成立的类别以及仍保持开放但已知二次上界的类别，反映了截至2025年8月21日的最新研究进展。


<details>
  <summary>Details</summary>
Motivation: Černý猜想是自动机理论中的一个重要未解决问题，该猜想认为n状态同步自动机的最短重置词长度不超过(n-1)^2。本文旨在系统梳理该猜想在不同自动机类别中的研究现状。

Method: 采用文献综述的方法，收集和分析已发表的关于Černý猜想的研究成果，对不同类别的有限自动机进行分类总结。

Result: 整理出了Černý猜想已被证明成立的多个自动机类别，以及那些猜想虽未证明但已知存在二次上界（关于状态数的平方）的类别。

Conclusion: 该综述提供了Černý猜想研究现状的全面概览，指出了已解决和未解决的问题，为后续研究提供了重要参考，反映了该领域截至2025年的最新进展。

Abstract: We survey results in the literature that establish the \v{C}ern\'y conjecture
for various classes of finite automata. We also list classes for which the
conjecture remains open, but a quadratic (in the number of states) upper bound
on the minimum length of reset words is known. The results presented reflect
the state of the art as of August 21, 2025.

</details>
