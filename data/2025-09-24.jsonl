{"id": "2509.18583", "categories": ["cs.PL", "quant-ph"], "pdf": "https://arxiv.org/pdf/2509.18583", "abs": "https://arxiv.org/abs/2509.18583", "authors": ["Liyi Li", "Fenfen An", "Federico Zahariev", "Zhi Xiang Chong", "Amr Sabry", "Mark Gordon"], "title": "A Verified Compiler for Quantum Simulation", "comment": "Paper accepted to the Quantum Programming Languages (QPL) 2025\n  conference; available from: https://qpl2025.github.io/accepted/", "summary": "Hamiltonian simulation is a central application of quantum computing, with\nsignificant potential in modeling physical systems and solving complex\noptimization problems. Existing compilers for such simulations typically focus\non low-level representations based on Pauli operators, limiting programmability\nand offering no formal guarantees of correctness across the compilation\npipeline. We introduce QBlue, a high-level, formally verified framework for\ncompiling Hamiltonian simulations. QBlue is based on the formalism of second\nquantization, which provides a natural and expressive way to describe quantum\nparticle systems using creation and annihilation operators. To ensure safety\nand correctness, QBlue includes a type system that tracks particle types and\nenforces Hermitian structure. The framework supports compilation to both\ndigital and analog quantum circuits and captures multiple layers of semantics,\nfrom static constraints to dynamic evolution. All components of QBlue,\nincluding its language design, type system, and compilation correctness, are\nfully mechanized in the Rocq proof framework, making it the first end-to-end\nverified compiler for second-quantized Hamiltonian simulation."}
{"id": "2509.18337", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.18337", "abs": "https://arxiv.org/abs/2509.18337", "authors": ["Bo Xiong", "Linghao Zhang", "Chong Wang", "Peng Liang"], "title": "CoRaCMG: Contextual Retrieval-Augmented Framework for Commit Message Generation", "comment": "15 pages, 4 images, 6 tables, Manuscript submitted to a Journal\n  (2025)", "summary": "Commit messages play a key role in documenting the intent behind code\nchanges. However, they are often low-quality, vague, or incomplete, limiting\ntheir usefulness. Commit Message Generation (CMG) aims to automatically\ngenerate descriptive commit messages from code diffs to reduce developers'\neffort and improve message quality. Although recent advances in LLMs have shown\npromise in automating CMG, their performance remains limited. This paper aims\nto enhance CMG performance by retrieving similar diff-message pairs to guide\nLLMs to generate commit messages that are more precise and informative. We\nproposed CoRaCMG, a Contextual Retrieval-augmented framework for Commit Message\nGeneration, structured in three phases: (1) Retrieve: retrieving the similar\ndiff-message pairs; (2) Augment: combining them with the query diff into a\nstructured prompt; and (3) Generate: generating commit messages corresponding\nto the query diff via LLMs. CoRaCMG enables LLMs to learn project-specific\nterminologies and writing styles from the retrieved diff-message pairs, thereby\nproducing high-quality commit messages. We evaluated our method on various\nLLMs, including closed-source GPT models and open-source DeepSeek models.\nExperimental results show that CoRaCMG significantly boosts LLM performance\nacross four metrics (BLEU, Rouge-L, METEOR, and CIDEr). Specifically,\nDeepSeek-R1 achieves relative improvements of 76% in BLEU and 71% in CIDEr when\naugmented with a single retrieved example pair. After incorporating the single\nexample pair, GPT-4o achieves the highest improvement rate, with BLEU\nincreasing by 89%. Moreover, performance gains plateau after more than three\nexamples are used, indicating diminishing returns. Further analysis shows that\nthe improvements are attributed to the model's ability to capture the\nterminologies and writing styles of human-written commit messages from the\nretrieved example pairs."}
{"id": "2509.18357", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2509.18357", "abs": "https://arxiv.org/abs/2509.18357", "authors": ["Michael Johnson", "David Jaz Myers"], "title": "Proceedings Seventh International Conference on Applied Category Theory 2024", "comment": null, "summary": "Proceedings of the Seventh International Conference on Applied Category\nTheory, held at the University of Oxford on 17 - 21 June 2024. The\ncontributions to ACT 2024 ranged from pure to applied and included\ncontributions in a wide range of disciplines in science and engineering. ACT\n2024 included talks in classical mechanics, quantum physics, probability\ntheory, linguistics, decision theory, machine learning, epidemiology,\nthermodynamics, engineering, and logic."}
{"id": "2509.18232", "categories": ["cs.FL", "E.1; F.4; I.1"], "pdf": "https://arxiv.org/pdf/2509.18232", "abs": "https://arxiv.org/abs/2509.18232", "authors": ["Baudouin Le Charlier"], "title": "A Layered Implementation Framework for Regular Languages", "comment": null, "summary": "I present the most fundamental features of an implemented system designed to\nmanipulate representations of regular languages. The system is structured into\ntwo layers, allowing regular languages to be represented in an increasingly\ncompact, efficient, and integrated way. Both layers are first presented at a\nhigh level, adequate to design and prove the correctness of abstract\nalgorithms. Then, their low-level implementations are described meticulously.\n  At the high level, the first layer offers a notion of normalized regular\nexpressions ensuring that the set of all syntactic derivatives of an expression\nis finite. At the low level, normalized expressions are uniquely represented by\nidentifiers, i.e. by standard integers.\n  The second layer, called the background, introduces additional notions to\nrecord, integrate, and simplify things computed within the first layer. At the\nhigh level, normalized expressions denoting the same regular language can be\nunified by grouping them into equivalence classes. One shortest expression is\nchosen in each class as its representative, which can be used to form equations\nrelating expressions to their derivatives.\n  This paper also presents extensive experimental results to demonstrate the\nusefulness of the proposed framework and, in particular, the fact that it makes\nit possible to represent large sets of regular languages in a unified way where\ndistinct identifiers designate different languages, represented by both a small\nexpression and a minimal deteministic automaton."}
{"id": "2509.18361", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.18361", "abs": "https://arxiv.org/abs/2509.18361", "authors": ["Daye Nam", "Malgorzata Salawa", "Satish Chandra"], "title": "Reading Between the Lines: Scalable User Feedback via Implicit Sentiment in Developer Prompts", "comment": null, "summary": "Evaluating developer satisfaction with conversational AI assistants at scale\nis critical but challenging. User studies provide rich insights, but are\nunscalable, while large-scale quantitative signals from logs or in-product\nratings are often too shallow or sparse to be reliable. To address this gap, we\npropose and evaluate a new approach: using sentiment analysis of developer\nprompts to identify implicit signals of user satisfaction. With an analysis of\nindustrial usage logs of 372 professional developers, we show that this\napproach can identify a signal in ~8% of all interactions, a rate more than 13\ntimes higher than explicit user feedback, with reasonable accuracy even with an\noff-the-shelf sentiment analysis approach. This new practical approach to\ncomplement existing feedback channels would open up new directions for building\na more comprehensive understanding of the developer experience at scale."}
{"id": "2509.18434", "categories": ["cs.LO", "cs.CC"], "pdf": "https://arxiv.org/pdf/2509.18434", "abs": "https://arxiv.org/abs/2509.18434", "authors": ["Dmitriy Zhuk"], "title": "Singleton algorithms for the Constraint Satisfaction Problem", "comment": null, "summary": "A natural strengthening of an algorithm for the (promise) constraint\nsatisfaction problem is its singleton version: we first fix a constraint to\nsome tuple from the constraint relation, then run the algorithm, and remove the\ntuple from the constraint if the answer is negative. We characterize the power\nof the singleton versions of standard universal algorithms for the (promise)\nCSP over a fixed template in terms of the existence of a minion homomorphism.\nUsing the Hales-Jewett theorem, we show that for finite relational structures\nthis minion condition is equivalent to the existence of polymorphisms with\ncertain symmetries, called palette block symmetric polymorphisms. By proving\nthe existence of such polymorphisms we establish that the singleton version of\nthe BLP+AIP algorithm solves all tractable CSPs over domains of size at most 7.\nFinally, by providing concrete CSP templates, we illustrate the limitations of\nlinear programming, the power of the singleton versions, and the elegance of\nthe palette block symmetric polymorphisms."}
{"id": "2509.18454", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.18454", "abs": "https://arxiv.org/abs/2509.18454", "authors": ["Andrzej Białecki", "Piotr Białecki", "Piotr Sowiński", "Mateusz Budziak", "Jan Gajewski"], "title": "SC2Tools: StarCraft II Toolset and Dataset API", "comment": null, "summary": "Computer games, as fully controlled simulated environments, have been\nutilized in significant scientific studies demonstrating the application of\nReinforcement Learning (RL). Gaming and esports are key areas influenced by the\napplication of Artificial Intelligence (AI) and Machine Learning (ML) solutions\nat scale. Tooling simplifies scientific workloads and is essential for\ndeveloping the gaming and esports research area.\n  In this work, we present ``SC2Tools'', a toolset containing multiple\nsubmodules responsible for working with, and producing larger datasets. We\nprovide a modular structure of the implemented tooling, leaving room for future\nextensions where needed. Additionally, some of the tools are not StarCraft~2\nexclusive and can be used with other types of data for dataset creation.\n  The tools we present were leveraged in creating one of the largest\nStarCraft~2 tournament datasets to date with a separate PyTorch and PyTorch\nLightning application programming interface (API) for easy access to the data.\n  We conclude that alleviating the burden of data collection, preprocessing,\nand custom code development is essential for less technically proficient\nresearchers to engage in the growing gaming and esports research area. Finally,\nour solution provides some foundational work toward normalizing experiment\nworkflow in StarCraft~2"}
{"id": "2509.18548", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.18548", "abs": "https://arxiv.org/abs/2509.18548", "authors": ["Steven R Brandt", "Max Morris", "Patrick Diehl", "Christopher Bowen", "Jacob Tucker", "Lauren Bristol", "Golden G. Richard III"], "title": "Locking Down Science Gateways", "comment": null, "summary": "The most recent Linux kernels have a new feature for securing applications:\nLandlock. Like Seccomp before it, Landlock makes it possible for a running\nprocess to give up access to resources. For applications running as Science\nGateways, network access is required while starting up MPI, but for the sake of\nsecurity, it should be taken away prior to the reading of user-supplied\nparameter files. We explore the usefulness of Landlock by modifying and locking\ndown three mature scientific codes: The Einstein Toolkit (a code that studies\nthe dynamics of relativistic astrophysics, e.g. neutron star collisions),\nOcto-Tiger (a code for studying the dynamics of non-relativistic astrophysics,\ne.g. white dwarfs), and FUKA (an initial data solver for relativistic codes).\nFinally, we implement a fully-functioning FUKA science gateway that relies on\nLandlock (instead of user authentication) for security."}
{"id": "2509.18808", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2509.18808", "abs": "https://arxiv.org/abs/2509.18808", "authors": ["Zexun Zhan", "Shuzheng Gao", "Ruida Hu", "Cuiyun Gao"], "title": "SR-Eval: Evaluating LLMs on Code Generation under Stepwise Requirement Refinement", "comment": null, "summary": "Large language models (LLMs) have achieved remarkable progress in code\ngeneration. However, existing benchmarks mainly formalize the task as a static,\nsingle-turn problem, overlooking the stepwise requirement changes and iterative\nworkflows in real-world software development. This mismatch limits the\nunderstanding of how well LLMs can support real-world development workflows.\nConstructing such iterative benchmarks is challenging due to the lack of public\ninteraction traces and the difficulty of creating discriminative, turn-specific\ntest cases.\n  To bridge this gap, we present SR-Eval, a benchmark specifically designed to\nassess LLMs on iterative code generation under Stepwise requirements\nRefinement. SR-Eval spans both function-level and repository-level tasks in\nPython and Java, enabling fine-grained and progressive evaluation across\nevolving requirements. The construction of SR-Eval follows a carefully designed\npipeline that first leverages a multi-agent-based requirement generation method\nto simulate the development process and recover the multi-round interaction\nprocess from final requirements, then employs a semantic-aware discriminative\ntest case generation component to ensure discriminative and consistent\nevaluation at each turn. SR-Eval comprises 443 multi-turn tasks and 1,857\nquestions at both function and repository levels. Using SR-Eval, we evaluate 11\nrepresentative LLMs with three prompting strategies that simulate different\nusage patterns. Results show that iterative code generation under stepwise\nrequirement refinement remains highly challenging: the best-performing model\nachieves only 22.67% completion rate on function-level tasks and 20.00% on\nrepository-level tasks. We further observe that prompting strategies\nsubstantially influence performance, highlighting the need for the development\nof advanced methods."}
{"id": "2509.19136", "categories": ["cs.SE", "cs.AI", "D.2.4; D.2.5; F.3.1"], "pdf": "https://arxiv.org/pdf/2509.19136", "abs": "https://arxiv.org/abs/2509.19136", "authors": ["Sébastien Salva", "Redha Taguelmimt"], "title": "On the Soundness and Consistency of LLM Agents for Executing Test Cases Written in Natural Language", "comment": null, "summary": "The use of natural language (NL) test cases for validating graphical user\ninterface (GUI) applications is emerging as a promising direction to manually\nwritten executable test scripts, which are costly to develop and difficult to\nmaintain. Recent advances in large language models (LLMs) have opened the\npossibility of the direct execution of NL test cases by LLM agents. This paper\ninvestigates this direction, focusing on the impact on NL test case unsoundness\nand on test case execution consistency. NL test cases are inherently unsound,\nas they may yield false failures due to ambiguous instructions or unpredictable\nagent behaviour. Furthermore, repeated executions of the same NL test case may\nlead to inconsistent outcomes, undermining test reliability. To address these\nchallenges, we propose an algorithm for executing NL test cases with guardrail\nmechanisms and specialised agents that dynamically verify the correct execution\nof each test step. We introduce measures to evaluate the capabilities of LLMs\nin test execution and one measure to quantify execution consistency. We propose\na definition of weak unsoundness to characterise contexts in which NL test case\nexecution remains acceptable, with respect to the industrial quality levels Six\nSigma. Our experimental evaluation with eight publicly available LLMs, ranging\nfrom 3B to 70B parameters, demonstrates both the potential and current\nlimitations of current LLM agents for GUI testing. Our experiments show that\nMeta Llama 3.1 70B demonstrates acceptable capabilities in NL test case\nexecution with high execution consistency (above the level 3-sigma). We provide\nprototype tools, test suites, and results."}
{"id": "2509.19185", "categories": ["cs.SE", "cs.ET"], "pdf": "https://arxiv.org/pdf/2509.19185", "abs": "https://arxiv.org/abs/2509.19185", "authors": ["Mohammed Mehedi Hasan", "Hao Li", "Emad Fallahzadeh", "Gopi Krishnan Rajbahadur", "Bram Adams", "Ahmed E. Hassan"], "title": "An Empirical Study of Testing Practices in Open Source AI Agent Frameworks and Agentic Applications", "comment": null, "summary": "Foundation model (FM)-based AI agents are rapidly gaining adoption across\ndiverse domains, but their inherent non-determinism and non-reproducibility\npose testing and quality assurance challenges. While recent benchmarks provide\ntask-level evaluations, there is limited understanding of how developers verify\nthe internal correctness of these agents during development.\n  To address this gap, we conduct the first large-scale empirical study of\ntesting practices in the AI agent ecosystem, analyzing 39 open-source agent\nframeworks and 439 agentic applications. We identify ten distinct testing\npatterns and find that novel, agent-specific methods like DeepEval are seldom\nused (around 1%), while traditional patterns like negative and membership\ntesting are widely adapted to manage FM uncertainty. By mapping these patterns\nto canonical architectural components of agent frameworks and agentic\napplications, we uncover a fundamental inversion of testing effort:\ndeterministic components like Resource Artifacts (tools) and Coordination\nArtifacts (workflows) consume over 70% of testing effort, while the FM-based\nPlan Body receives less than 5%. Crucially, this reveals a critical blind spot,\nas the Trigger component (prompts) remains neglected, appearing in around 1% of\nall tests.\n  Our findings offer the first empirical testing baseline in FM-based agent\nframeworks and agentic applications, revealing a rational but incomplete\nadaptation to non-determinism. To address it, framework developers should\nimprove support for novel testing methods, application developers must adopt\nprompt regression testing, and researchers should explore barriers to adoption.\nStrengthening these practices is vital for building more robust and dependable\nAI agents."}
