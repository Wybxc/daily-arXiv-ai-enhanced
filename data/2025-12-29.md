<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 17]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [AInsteinBench: Benchmarking Coding Agents on Scientific Repositories](https://arxiv.org/abs/2512.21373)
*Titouan Duston,Shuo Xin,Yang Sun,Daoguang Zan,Aoyan Li,Shulin Xin,Kai Shen,Yixiao Chen,Qiming Sun,Ge Zhang,Jiashuo Liu,Huan Zhou,Jingkai Liu,Zhichen Pu,Yuanheng Wang,Bo-Xuan Ge,Xin Tong,Fei Ye,Zhi-Chao Zhao,Wen-Biao Han,Zhoujian Cao,Yueran Zhao,Weiluo Ren,Qingshen Long,Yuxiao Liu,Anni Huang,Yidi Du,Yuanyuan Rong,Jiahao Peng*

Main category: cs.SE

TL;DR: AInsteinBench是一个评估LLM代理在真实科研软件生态中作为科学计算开发代理能力的大规模基准测试，基于六个领域科学代码库的实际拉取请求构建。


<details>
  <summary>Details</summary>
Motivation: 现有科学推理基准侧重于概念知识，软件工程基准强调通用功能实现和问题解决，缺乏评估LLM在真实科研开发环境中端到端能力的基准。

Method: 基于六个广泛使用的科学代码库（量子化学、量子计算、分子动力学、数值相对论、流体动力学、化学信息学）中维护者编写的拉取请求构建任务，通过多阶段筛选和专家评审确保科学挑战性、充分测试覆盖和适当难度。

Result: 创建了一个包含可执行环境评估、科学意义失败模式和测试驱动验证的基准，能够衡量模型超越表面代码生成、掌握计算科学研究核心能力的情况。

Conclusion: AInsteinBench填补了现有基准的空白，为评估LLM在真实科研软件生态中的科学计算开发能力提供了全面框架，推动模型从代码生成向科研开发核心能力发展。

Abstract: We introduce AInsteinBench, a large-scale benchmark for evaluating whether large language model (LLM) agents can operate as scientific computing development agents within real research software ecosystems. Unlike existing scientific reasoning benchmarks which focus on conceptual knowledge, or software engineering benchmarks that emphasize generic feature implementation and issue resolving, AInsteinBench evaluates models in end-to-end scientific development settings grounded in production-grade scientific repositories. The benchmark consists of tasks derived from maintainer-authored pull requests across six widely used scientific codebases, spanning quantum chemistry, quantum computing, molecular dynamics, numerical relativity, fluid dynamics, and cheminformatics. All benchmark tasks are carefully curated through multi-stage filtering and expert review to ensure scientific challenge, adequate test coverage, and well-calibrated difficulty. By leveraging evaluation in executable environments, scientifically meaningful failure modes, and test-driven verification, AInsteinBench measures a model's ability to move beyond surface-level code generation toward the core competencies required for computational scientific research.

</details>


### [2] [Understanding the Role of Large Language Models in Software Engineering: Evidence from an Industry Survey](https://arxiv.org/abs/2512.21347)
*Vítor Mateus de Brito,Kleinner Farias*

Main category: cs.SE

TL;DR: 该研究通过调查46名行业专业人士，实证分析了LLM在软件工程中的采用情况，发现开发者对LLM持积极态度，但同时也担忧认知依赖、安全风险和技术自主性侵蚀等问题。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在软件工程中的快速发展和深度融入开发者日常工作流程，理解其实际使用情况变得至关重要。本研究旨在填补学术讨论与真实软件开发实践之间的差距，为LLM技术的采用和演进提供实证基础。

Method: 采用实证研究方法，通过对46名具有不同教育背景和经验水平的行业专业人士进行问卷调查，收集和分析LLM在软件工程中的采用情况和感知数据。

Result: 调查结果显示：1）开发者对LLM持积极态度，特别是在快速解决技术问题、改进文档支持和增强源代码标准化方面；2）同时存在对认知依赖、安全风险和技术自主性侵蚀的担忧；3）LLM在开发者的日常工作中已深度嵌入。

Conclusion: 研究强调需要对LLM工具进行批判性和监督性使用，为开发者和研究者提供了采用和演进LLM技术的可操作见解，同时指出了未来在认知、伦理和组织影响方面的研究方向。

Abstract: The rapid advancement of Large Language Models (LLMs) is reshaping software engineering by profoundly influencing coding, documentation, and system maintenance practices. As these tools become deeply embedded in developers' daily workflows, understanding how they are used has become essential. This paper reports an empirical study of LLM adoption in software engineering, based on a survey of 46 industry professionals with diverse educational backgrounds and levels of experience. The results reveal positive perceptions of LLMs, particularly regarding faster resolution of technical questions, improved documentation support, and enhanced source code standardization. However, respondents also expressed concerns about cognitive dependence, security risks, and the potential erosion of technical autonomy. These findings underscore the need for critical and supervised use of LLM-based tools. By grounding the discussion in empirical evidence from industry practice, this study bridges the gap between academic discourse and real-world software development. The results provide actionable insights for developers and researchers seeking to adopt and evolve LLM-based technologies in a more effective, responsible, and secure manner, while also motivating future research on their cognitive, ethical, and organizational implications.

</details>


### [3] [Fairness Is Not Just Ethical: Performance Trade-Off via Data Correlation Tuning to Mitigate Bias in ML Software](https://arxiv.org/abs/2512.21348)
*Ying Xiao,Shangwen Wang,Sicen Liu,Dingyuan Xue,Xian Zhan,Yepang Liu,Jie M. Zhang*

Main category: cs.SE

TL;DR: 提出Correlation Tuning (CoT)预处理方法，通过调整数据相关性来缓解软件公平性偏见，相比现有方法在单属性和多属性场景下分别提升3%和10%的性能。


<details>
  <summary>Details</summary>
Motivation: 传统软件公平性研究多强调伦理和社会必要性，但忽视了公平性本质上是软件质量问题，源于不同敏感用户群体间的性能差异。现有偏见缓解方法面临困境：预处理方法适用性广但效果不如后处理方法。

Method: 提出Correlation Tuning (CoT)预处理方法：1) 引入Phi系数直观量化敏感属性与标签间的相关性；2) 采用多目标优化处理代理偏见；3) 通过调整数据相关性来缓解偏见。

Result: CoT将弱势群体的真阳性率平均提升17.5%，将三个关键偏见指标（SPD、AOD、EOD）平均降低50%以上。在单属性和多属性场景下分别比最先进方法高出3%和10%。

Conclusion: CoT作为预处理方法，在保持广泛适用性的同时，实现了与后处理方法相当甚至更好的偏见缓解效果，将公平性明确作为软件质量维度具有实际效益。

Abstract: Traditional software fairness research typically emphasizes ethical and social imperatives, neglecting that fairness fundamentally represents a core software quality issue arising directly from performance disparities across sensitive user groups. Recognizing fairness explicitly as a software quality dimension yields practical benefits beyond ethical considerations, notably improved predictive performance for unprivileged groups, enhanced out-of-distribution generalization, and increased geographic transferability in real-world deployments. Nevertheless, existing bias mitigation methods face a critical dilemma: while pre-processing methods offer broad applicability across model types, they generally fall short in effectiveness compared to post-processing techniques. To overcome this challenge, we propose Correlation Tuning (CoT), a novel pre-processing approach designed to mitigate bias by adjusting data correlations. Specifically, CoT introduces the Phi-coefficient, an intuitive correlation measure, to systematically quantify correlation between sensitive attributes and labels, and employs multi-objective optimization to address the proxy biases. Extensive evaluations demonstrate that CoT increases the true positive rate of unprivileged groups by an average of 17.5% and reduces three key bias metrics, including statistical parity difference (SPD), average odds difference (AOD), and equal opportunity difference (EOD), by more than 50% on average. CoT outperforms state-of-the-art methods by three and ten percentage points in single attribute and multiple attributes scenarios, respectively. We will publicly release our experimental results and source code to facilitate future research.

</details>


### [4] [CosmoCore-Evo: Evolutionary Dream-Replay Reinforcement Learning for Adaptive Code Generation](https://arxiv.org/abs/2512.21351)
*Santhosh Kumar Ravindran*

Main category: cs.SE

TL;DR: CosmoCore-Evo 在 CosmoCore 基础上引入进化算法，将 RL 轨迹视为"基因组"进行突变和选择，增强代码生成任务的适应性和新颖性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习框架在分布偏移环境（如API变更、新库引入）中适应性有限，难以突破训练模式产生新颖解决方案。受人类进化中自然选择和适应机制的启发，需要一种能促进涌现行为的方法。

Method: 扩展 CosmoCore 的梦境回放框架，将强化学习轨迹视为"基因组"，在夜间回放阶段进行突变和选择。增强梦境队列，包含高适应度轨迹的突变和企业调优的适应度函数（考虑效率、合规性、可扩展性）。

Result: 在 HumanEval 变体、BigCodeBench 和自定义 PySpark 管道模拟等扩展基准测试中，CosmoCore-Evo 相比原始 CosmoCore 和 PPO、REAMER 等基线，解决方案新颖性提高达35%，适应速度加快25%。

Conclusion: 进化算法能有效增强 LLM 代理的适应性和新颖性，在分布偏移环境中表现优异，为缩小 LLM 代理的"感知差距"提供了有效途径。

Abstract: Building on the affective dream-replay reinforcement learning framework of CosmoCore, we introduce CosmoCore-Evo, an extension that incorporates evolutionary algorithms to enhance adaptability and novelty in code generation tasks. Inspired by anthropological aspects of human evolution, such as natural selection and adaptation in early hominids, CosmoCore-Evo treats RL trajectories as ``genomes'' that undergo mutation and selection during the nocturnal replay phase. This mechanism allows agents to break free from trained patterns, fostering emergent behaviors and improved performance in distribution-shifted environments, such as changing APIs or novel libraries. We augment the Dream Queue with evolutionary operations, including mutation of high-fitness trajectories and enterprise-tuned fitness functions that incorporate efficiency, compliance, and scalability metrics. Evaluated on extended benchmarks including HumanEval variants with shifts, BigCodeBench, and a custom PySpark pipeline simulation, CosmoCore-Evo achieves up to 35% higher novelty in solutions and 25% faster adaptation compared to the original CosmoCore and baselines like PPO and REAMER. Ablations confirm the role of evolutionary components in bridging the sentient gap for LLM agents. Code for replication, including a toy simulation, is provided.

</details>


### [5] [Multi-Agent LLM Committees for Autonomous Software Beta Testing](https://arxiv.org/abs/2512.21352)
*Sumanth Bharadwaj Hachalli Karanam,Dhiwahar Adhithya Kennady*

Main category: cs.SE

TL;DR: 提出多智能体委员会框架，通过视觉增强的LLM协作进行软件测试，相比单智能体基线显著提升任务成功率


<details>
  <summary>Details</summary>
Motivation: 传统手动软件Beta测试成本高、耗时长，而单智能体LLM方法存在幻觉和不一致行为问题，需要更可靠的自动化测试方案

Method: 采用多智能体委员会框架，包含多样化视觉增强LLM，通过三轮投票协议达成共识，结合模型多样性、角色驱动行为变化和视觉界面理解来系统探索Web应用

Result: 多智能体委员会在84次实验运行中达到89.5%总体任务成功率，2-4智能体配置达到91.7-100%成功率，相比单智能体基线提升13.7-22.0个百分点；在WebShop基准上达到74.7%成功率（GPT-3基线为50.1%），在OWASP安全测试中覆盖8/10漏洞类别

Conclusion: 多智能体委员会框架通过协作投票机制显著提升软件测试效果，支持实时持续集成测试，开源实现促进可重复研究和实际部署

Abstract: Manual software beta testing is costly and time-consuming, while single-agent large language model (LLM) approaches suffer from hallucinations and inconsistent behavior. We propose a multi-agent committee framework in which diverse vision-enabled LLMs collaborate through a three-round voting protocol to reach consensus on testing actions. The framework combines model diversity, persona-driven behavioral variation, and visual user interface understanding to systematically explore web applications. Across 84 experimental runs with 9 testing personas and 4 scenarios, multi-agent committees achieve an 89.5 percent overall task success rate. Configurations with 2 to 4 agents reach 91.7 to 100 percent success, compared to 78.0 percent for single-agent baselines, yielding improvements of 13.7 to 22.0 percentage points. At the action level, the system attains a 93.1 percent success rate with a median per-action latency of 0.71 seconds, enabling real-time and continuous integration testing. Vision-enabled agents successfully identify user interface elements, with navigation and reporting achieving 100 percent success and form filling achieving 99.2 percent success. We evaluate the framework on WebShop and OWASP benchmarks, achieving 74.7 percent success on WebShop compared to a 50.1 percent published GPT-3 baseline, and 82.0 percent success on OWASP Juice Shop security testing with coverage of 8 of the 10 OWASP Top 10 vulnerability categories. Across 20 injected regressions, the committee achieves an F1 score of 0.91 for bug detection, compared to 0.78 for single-agent baselines. The open-source implementation enables reproducible research and practical deployment of LLM-based software testing in CI/CD pipelines.

</details>


### [6] [What Makes a GitHub Issue Ready for Copilot?](https://arxiv.org/abs/2512.21426)
*Mohammed Sayagh*

Main category: cs.SE

TL;DR: 研究构建了32项详细标准来评估GitHub issue的质量，以提升AI代理（如Copilot）处理issue的成功率，并建立可解释的机器学习模型预测issue能否产生被合并的PR。


<details>
  <summary>Details</summary>
Motivation: AI代理（如Copilot）在编码任务中表现依赖于输入质量，但现有GitHub Copilot的最佳实践建议有限且过于高层。需要更详细的标准来评估issue质量，以提高AI代理实现issue的成功率。

Method: 1. 构建32项详细标准来衡量GitHub issue质量；2. 比较导致合并PR与关闭PR的issue差异；3. 建立可解释的机器学习模型预测issue产生合并PR的可能性。

Result: 成功合并的PR通常来自：更简短、范围明确、有清晰指导和相关工件提示、包含实现指导的issue。包含外部引用（配置、上下文设置、依赖或外部API）的issue合并率较低。模型的中位AUC为72%。

Conclusion: 研究揭示了撰写高质量GitHub issue的关键指标，强调在AI协作时代应将issue撰写视为一等软件工程活动。模型能帮助用户改进issue以提高Copilot处理成功率。

Abstract: AI-agents help developers in different coding tasks, such as developing new features, fixing bugs, and reviewing code. Developers can write a Github issue and assign it to an AI-agent like Copilot for implementation. Based on the issue and its related discussion, the AI-agent performs a plan for the implementation, and executes it. However, the performance of AI-agents and LLMs heavily depends on the input they receive. For instance, a GitHub issue that is unclear or not well scoped might not lead to a successful implementation that will eventually be merged. GitHub Copilot provides a set of best practice recommendations that are limited and high-level. In this paper, we build a set of 32 detailed criteria that we leverage to measure the quality of GitHub issues to make them suitable for AI-agents. We compare the GitHub issues that lead to a merged pull request versus closed pull request. Then, we build an interpretable machine learning model to predict the likelihood of a GitHub issue resulting in a merged pull request. We observe that pull requests that end up being merged are those originating from issues that are shorter, well scoped, with clear guidance and hints about the relevant artifacts for an issue, and with guidance on how to perform the implementation. Issues with external references including configuration, context setup, dependencies or external APIs are associated with lower merge rates. We built an interpretable machine learning model to help users identify how to improve a GitHub issue to increase the chances of the issue resulting in a merged pull request by Copilot. Our model has a median AUC of 72\%. Our results shed light on quality metrics relevant for writing GitHub issues and motivate future studies further investigate the writing of GitHub issues as a first-class software engineering activity in the era of AI-teammates.

</details>


### [7] [Cerberus: Multi-Agent Reasoning and Coverage-Guided Exploration for Static Detection of Runtime Errors](https://arxiv.org/abs/2512.21431)
*Hridya Dhulipala,Xiaokai Rong,Tien N. Nguyen*

Main category: cs.SE

TL;DR: Cerberus是一个基于LLM的预测性、无需执行的覆盖率引导测试框架，用于在不实际执行代码的情况下检测运行时错误。


<details>
  <summary>Details</summary>
Motivation: 在软件开发中，需要在集成代码片段到代码库之前检测运行时错误和异常，但传统方法需要实际执行代码。本文旨在开发一种无需执行的测试框架。

Method: Cerberus使用LLM生成触发运行时错误的输入，并进行代码覆盖率预测和错误检测，无需代码执行。采用两阶段反馈循环：第一阶段同时提高代码覆盖率和检测错误，第二阶段当覆盖率达到100%或最大值时专注于错误检测。

Result: 实证评估表明，Cerberus比传统和基于学习的测试框架表现更好，能更高效地生成高覆盖率测试用例，发现更多运行时错误，适用于完整和不完整的代码片段。

Conclusion: Cerberus是一个有效的无需执行的测试框架，通过LLM和两阶段反馈循环，在提高代码覆盖率和检测运行时错误方面优于现有方法。

Abstract: In several software development scenarios, it is desirable to detect runtime errors and exceptions in code snippets without actual execution. A typical example is to detect runtime exceptions in online code snippets before integrating them into a codebase. In this paper, we propose Cerberus, a novel predictive, execution-free coverage-guided testing framework. Cerberus uses LLMs to generate the inputs that trigger runtime errors and to perform code coverage prediction and error detection without code execution. With a two-phase feedback loop, Cerberus first aims to both increasing code coverage and detecting runtime errors, then shifts to focus only detecting runtime errors when the coverage reaches 100% or its maximum, enabling it to perform better than prompting the LLMs for both purposes. Our empirical evaluation demonstrates that Cerberus performs better than conventional and learning-based testing frameworks for (in)complete code snippets by generating high-coverage test cases more efficiently, leading to the discovery of more runtime errors.

</details>


### [8] [Fuzzwise: Intelligent Initial Corpus Generation for Fuzzing](https://arxiv.org/abs/2512.21440)
*Hridya Dhulipala,Xiaokai Rong,Aashish Yadavally,Tien N. Nguyen*

Main category: cs.SE

TL;DR: FuzzWise：基于LLM的多智能体框架，将初始种子生成与最小化整合为一步，通过预测性代码覆盖评估生成高质量初始语料库


<details>
  <summary>Details</summary>
Motivation: 在基于突变的灰盒模糊测试中，生成高质量的初始种子语料库对有效模糊测试至关重要。传统方法需要先生成大量语料库再最小化，这个过程分离且低效。

Method: FuzzWise采用基于大语言模型的多智能体框架：第一个LLM智能体为目标程序生成测试用例；第二个LLM智能体作为预测性代码覆盖模块，评估每个生成的测试用例是否能提升当前语料库的整体覆盖率。这种流线化过程允许立即评估每个新生成测试种子的贡献。

Result: 实证评估表明，FuzzWise生成的测试用例数量显著少于基线方法。尽管测试用例数量较少，但FuzzWise实现了更高的代码覆盖率，触发了更多的运行时错误。此外，在生成能捕获更多错误的初始语料库方面，FuzzWise更具时间效率和覆盖效率。

Conclusion: FuzzWise通过整合初始种子生成与最小化过程，利用LLM的预测能力，在不实际执行程序的情况下生成高质量的初始语料库，节省计算资源，特别适用于执行不可行或不期望的场景。

Abstract: In mutation-based greybox fuzzing, generating high-quality input seeds for the initial corpus is essential for effective fuzzing. Rather than conducting separate phases for generating a large corpus and subsequently minimizing it, we propose FuzzWise which integrates them into one process to generate the optimal initial corpus of seeds (ICS). FuzzWise leverages a multi-agent framework based on Large Language Models (LLMs). The first LLM agent generates test cases for the target program. The second LLM agent, which functions as a predictive code coverage module, assesses whether each generated test case will enhance the overall coverage of the current corpus. The streamlined process allows each newly generated test seed to be immediately evaluated for its contribution to the overall coverage. FuzzWise employs a predictive approach using an LLM and eliminates the need for actual execution, saving computational resources and time, particularly in scenarios where the execution is not desirable or even impossible. Our empirical evaluation demonstrates that FuzzWise generates significantly fewer test cases than baseline methods. Despite the lower number of test cases, FuzzWise achieves high code coverage and triggers more runtime errors compared to the baselines. Moreover, it is more time-efficient and coverage-efficient in producing an initial corpus catching more errors.

</details>


### [9] [Code Clone Refactoring in C# with Lambda Expressions](https://arxiv.org/abs/2512.21511)
*Takuto Kawamoto,Yoshiki Higo*

Main category: cs.SE

TL;DR: 提出一种针对C#语言的代码克隆重构方法，使用lambda表达式参数化行为差异，评估显示35%的克隆对适合重构，其中28.9%成功重构


<details>
  <summary>Details</summary>
Motivation: 现有的"提取方法"重构技术主要针对Java程序，使用参数化方法处理代码克隆中的差异，但不同编程语言的特性（特别是lambda表达式规范）会影响该技术的适用性，需要针对特定语言（如C#）开发优化方法

Method: 提出C#特定的技术，使用lambda表达式分析和合并代码克隆。首先检测代码克隆（使用NiCad克隆检测器），然后通过lambda表达式参数化行为差异，将多个代码克隆提取为单个方法

Result: 在22个项目中的2,217个克隆对上进行评估：35.0%的克隆对被判定适合重构；在这些适合重构的克隆对中，28.9%成功完成实际重构

Conclusion: 针对特定编程语言（如C#）优化"提取方法"重构技术是必要的，使用lambda表达式可以有效处理代码克隆中的行为差异，但成功率仍有提升空间

Abstract: "Extract Method" refactoring is a technique for consolidating code clones. Parameterization approaches are used to extract a single method from multiple code clones that contain differences. This approach parameterizes expressions and behaviors within a method. In particular, behavior parameterization has been extensively studied in Java programs, but little research has been conducted on other programming languages.
  Lambda expressions can be used to parameterize behaviors, but the specifications of each programming language significantly affect the applicability of this technique. Therefore, the optimal "Extract Method" approach may vary depending on the programming language.
  In this study, we propose a C#-specific technique that uses lambda expressions to analyze and consolidate code clones. We evaluated our proposed method by applying it to code clones detected by the NiCad clone detector and measuring how many of them could be successfully consolidated.
  In total, 2,217 clone pairs from 22 projects were included in our evaluation. For the clone pairs determined to be refactorable, we also attempted refactoring actually. The proposed approach determined that 35.0% of all clone pairs were suitable for refactoring. Among these, 28.9% were successfully refactored.

</details>


### [10] [XTrace: A Non-Invasive Dynamic Tracing Framework for Android Applications in Production](https://arxiv.org/abs/2512.21555)
*Qi Hu,Jiangchao Liu,Xin Yu,Lin Zhang,Edward Jiang*

Main category: cs.SE

TL;DR: XTrace是一个Android动态追踪框架，通过非侵入式代理和优化ART虚拟机的内置插桩机制，实现生产环境中任意方法的高性能拦截，解决了传统方法无法捕获"幽灵bug"的问题。


<details>
  <summary>Details</summary>
Motivation: 随着移动应用复杂度指数级增长和设备环境碎片化加剧，确保在线应用稳定性面临前所未有的挑战。传统静态日志和崩溃后分析方法缺乏实时上下文信息，无法有效应对只在特定场景出现的"幽灵bug"，迫切需要动态运行时可观测性解决方案。

Method: XTrace提出非侵入式代理新范式，避免直接修改虚拟机底层数据结构。通过利用和优化Android ART虚拟机内置的高度稳定插桩机制，实现高性能方法拦截。无需应用发布即可在生产环境中拦截和追踪任意方法。

Result: 在拥有数亿日活用户的字节跳动应用中评估，XTrace展示了生产级稳定性和性能。大规模在线A/B实验确认其稳定性，对崩溃用户率和ANR率无显著影响(p>0.05)，同时保持极低开销(<7ms启动延迟，<0.01ms每次方法调用)和广泛兼容性(Android 5.0-15+)。诊断了11个以上严重在线崩溃和多个性能瓶颈，根本原因定位效率提升超过90%。

Conclusion: XTrace提供了一个生产级解决方案，调和了Android动态追踪中长期存在的稳定性与全面覆盖之间的冲突，为移动应用稳定性保障提供了有效的动态追踪框架。

Abstract: As the complexity of mobile applications grows exponentially and the fragmentation of user device environments intensifies, ensuring online application stability faces unprecedented challenges. Traditional methods, such as static logging and post-crash analysis, lack real-time contextual information, rendering them ineffective against "ghost bugs" that only manifest in specific scenarios. This highlights an urgent need for dynamic runtime observability: intercepting and tracing arbitrary methods in production without requiring an app release. We propose XTrace, a novel dynamic tracing framework. XTrace introduces a new paradigm of non-invasive proxying, which avoids direct modification of the virtual machine's underlying data structures. It achieves high-performance method interception by leveraging and optimizing the highly stable, built-in instrumentation mechanism of the Android ART virtual machine. Evaluated in a ByteDance application with hundreds of millions of daily active users, XTrace demonstrated production-grade stability and performance. Large-scale online A/B experiments confirmed its stability, showing no statistically significant impact (p > 0.05) on Crash User Rate or ANR rate, while maintaining minimal overhead (<7 ms startup latency, <0.01 ms per-method call) and broad compatibility (Android 5.0-15+). Critically, XTrace diagnosed over 11 severe online crashes and multiple performance bottlenecks, improving root-cause localization efficiency by over 90%. This confirms XTrace provides a production-grade solution that reconciles the long-standing conflict between stability and comprehensive coverage in Android dynamic tracing.

</details>


### [11] [Co-Evolution of Types and Dependencies: Towards Repository-Level Type Inference for Python Code](https://arxiv.org/abs/2512.21591)
*Shuo Sun,Shixin Zhang,Jiwei Yan,Jun Yan,Jian Zhang*

Main category: cs.SE

TL;DR: 提出基于LLM的仓库级Python类型推断方法，通过类型与依赖的协同演化实现高精度类型推断


<details>
  <summary>Details</summary>
Motivation: Python的动态类型机制导致运行时类型错误频发，现有工具只能在孤立代码片段中进行类型推断，无法处理仓库级别的复杂跨过程依赖关系

Method: 构建实体依赖图(EDG)建模仓库级类型依赖，采用迭代式类型推断方法让类型和依赖在每轮迭代中协同演化，并引入类型检查器循环策略实时验证和修正推断结果

Result: 在12个复杂Python仓库上评估，TypeSim得分0.89，TypeExact得分0.84，相比最强基线分别提升27%和40%，工具引入的新类型错误减少92.7%

Conclusion: 该方法在仓库级Python类型推断方面取得显著突破，为实现自动化、可靠的Python类型标注迈出重要一步

Abstract: Python's dynamic typing mechanism, while promoting flexibility, is a significant source of runtime type errors that plague large-scale software, which inspires the automatic type inference techniques. Existing type inference tools have achieved advances in type inference within isolated code snippets. However, repository-level type inference remains a significant challenge, primarily due to the complex inter-procedural dependencies that are difficult to model and resolve. To fill this gap, we present \methodName, a novel approach based on LLMs that achieves repository-level type inference through the co-evolution of types and dependencies. \methodName~constructs an Entity Dependency Graph (EDG) to model the objects and type dependencies across the repository. During the inference process, it iteratively refines types and dependencies in EDG for accurate type inference. Our key innovations are: (1) an EDG model designed to capture repository-level type dependencies; (2) an iterative type inference approach where types and dependencies co-evolve in each iteration; and (3) a type-checker-in-the-loop strategy that validates and corrects inferences on-the-fly, thereby reducing error propagation. When evaluated on 12 complex Python repositories, \methodName~significantly outperformed prior works, achieving a \textit{TypeSim} score of 0.89 and a \textit{TypeExact} score of 0.84, representing a 27\% and 40\% relative improvement over the strongest baseline. More importantly, \methodName~removed new type errors introduced by the tool by 92.7\%. This demonstrates a significant leap towards automated, reliable type annotation for real-world Python development.

</details>


### [12] [How Do Agents Perform Code Optimization? An Empirical Study](https://arxiv.org/abs/2512.21757)
*Huiyun Peng,Antonio Zhong,Ricardo Andrés Calvo Méndez,Kelechi G. Kalu,James C. Davis*

Main category: cs.SE

TL;DR: 首个比较AI与人类性能优化提交的实证研究：AI生成的PR在性能验证方面不如人类，但优化模式相似


<details>
  <summary>Details</summary>
Motivation: 虽然AI编码代理在代码生成和bug修复方面取得进展，但它们在真实世界性能优化任务中的表现尚不清楚，需要实证研究来比较AI和人类在性能优化方面的差异

Method: 使用AIDev数据集，分析324个AI生成和83个人类编写的性能优化PR，从采用率、可维护性、优化模式和验证实践四个维度进行比较分析

Result: AI编写的性能PR包含明确性能验证的比例显著低于人类（45.7% vs 63.6%，p=0.007），但AI和人类使用的优化模式基本相同

Conclusion: AI在性能优化方面与人类使用相似的模式，但在性能验证方面存在不足，这为推进智能代码优化代理提供了改进方向

Abstract: Performance optimization is a critical yet challenging aspect of software development, often requiring a deep understanding of system behavior, algorithmic tradeoffs, and careful code modifications. Although recent advances in AI coding agents have accelerated code generation and bug fixing, little is known about how these agents perform on real-world performance optimization tasks. We present the first empirical study comparing agent- and human-authored performance optimization commits, analyzing 324 agent-generated and 83 human-authored PRs from the AIDev dataset across adoption, maintainability, optimization patterns, and validation practices. We find that AI-authored performance PRs are less likely to include explicit performance validation than human-authored PRs (45.7\% vs. 63.6\%, $p=0.007$). In addition, AI-authored PRs largely use the same optimization patterns as humans. We further discuss limitations and opportunities for advancing agentic code optimization.

</details>


### [13] [The State of the SBOM Tool Ecosystems: A Comparative Analysis of SPDX and CycloneDX](https://arxiv.org/abs/2512.21781)
*Abdul Ali Bangash,Tongxu Ge,Zhimin Zhao,Arshdeep Singh,Zitao Wang,Bram Adams*

Main category: cs.SE

TL;DR: 该研究对SBOM（软件物料清单）的两种主流格式SPDX和CycloneDX的工具生态系统进行了定量比较，分析了170个公开工具的用例、生态系统健康度、开源项目采用情况，揭示了两种生态系统的互补优势。


<details>
  <summary>Details</summary>
Motivation: SBOM的采用依赖于工具生态系统，而SPDX和CycloneDX作为两种主流格式，其生态系统在成熟度、工具支持和社区参与方面存在显著差异。需要对这些生态系统进行系统比较，为开发者、贡献者和实践者提供决策依据。

Method: 1. 对170个公开宣传的SBOM工具进行用例定量比较；2. 比较两种生态系统的健康指标（171个CycloneDX工具 vs 470个SPDX工具）；3. 分析36,990个开源工具的问题报告以识别挑战和开发机会；4. 调查使用每种工具生态系统的前250个开源项目并比较其健康指标。

Result: 研究发现两种生态系统具有不同特征：使用CycloneDX工具的项目表现出更高的开发者参与度和某些健康指标，而SPDX工具则受益于更成熟的生态系统、更广泛的工具可用性和已建立的行业采用。两种生态系统各有优势，存在互补性。

Conclusion: 该研究为开发者、贡献者和实践者提供了关于这两种SBOM生态系统互补优势的见解，并识别了相互增强的机会。研究结果表明两种格式都有其价值，生态系统的发展可以通过相互学习和借鉴来共同提升。

Abstract: A Software Bill of Materials (SBOM) provides transparency by documenting software component metadata and dependencies. However, SBOM adoption depends on tool ecosystems. With two dominant formats: SPDX and CycloneDX - the ecosystems vary significantly in maturity, tool support, and community engagement. We conduct a quantitative comparison of use cases for 170 publicly advertised SBOM tools, identifying enhancement areas for each format. We compare health metrics of both ecosystems (171 CycloneDX versus 470 SPDX tools) to evaluate robustness and maturity. We quantitatively compare 36,990 issue reports from open-source tools to identify challenges and development opportunities. Finally, we investigate the top 250 open-source projects using each tool ecosystem and compare their health metrics. Our findings reveal distinct characteristics: projects using CycloneDX tools demonstrate higher developer engagement and certain health indicators, while SPDX tools benefit from a more mature ecosystem with broader tool availability and established industry adoption. This research provides insights for developers, contributors, and practitioners regarding complementary strengths of these ecosystems and identifies opportunities for mutual enhancement.

</details>


### [14] [A Story About Cohesion and Separation: Label-Free Metric for Log Parser Evaluation](https://arxiv.org/abs/2512.21811)
*Qiaolin Qin,Jianchen Zhao,Heng Li,Weiyi Shang,Ettore Merlo*

Main category: cs.SE

TL;DR: 提出PMSS，一种无需标签的日志解析器评估指标，通过聚类质量和模板相似度分析来评估解析器性能，解决了现有指标依赖标注数据的问题。


<details>
  <summary>Details</summary>
Motivation: 现有日志解析器评估指标严重依赖标注数据，限制了在工业场景中的应用；同时不同版本的真实标签会导致不一致的性能结论。

Method: 提出PMSS（parser medoid silhouette score）指标，使用medoid silhouette分析和Levenshtein距离来评估解析器的分组质量和模板质量，具有近似线性的时间复杂度。

Result: PMSS与基于标签的FGA和FTA指标显著正相关（p<1e-8），Spearman相关系数分别为0.648和0.587；PMSS最佳解析器与FGA最佳解析器性能差异仅2.1%。

Conclusion: PMSS为日志解析器评估提供了无需标签的替代方案，特别适用于真实标签不一致或不可用的情况，并提供了使用指南和挑战分析。

Abstract: Log parsing converts log messages into structured event templates, allowing for automated log analysis and reducing manual inspection effort. To select the most compatible parser for a specific system, multiple evaluation metrics are commonly used for performance comparisons. However, existing evaluation metrics heavily rely on labeled log data, which limits prior studies to a fixed set of datasets and hinders parser evaluations and selections in the industry. Further, we discovered that different versions of ground-truth used in existing studies can lead to inconsistent performance conclusions. Motivated by these challenges, we propose a novel label-free template-level metric, PMSS (parser medoid silhouette score), to evaluate log parser performance. PMSS evaluates both parser grouping and template quality with medoid silhouette analysis and Levenshtein distance within a near-linear time complexity in general. To understand its relationship with label-based template-level metrics, FGA and FTA, we compared their evaluation outcomes for six log parsers on the standard corrected Loghub 2.0 dataset. Our results indicate that log parsers achieving the highest PMSS or FGA exhibit comparable performance, differing by only 2.1% on average in terms of the FGA score; the difference is 9.8% for FTA. PMSS is also significantly (p<1e-8) and positively correlated to both FGA and FTA: the Spearman's rho correlation coefficient of PMSS-FGA and PMSS-FTA are respectively 0.648 and 0.587, close to the coefficient between FGA and FTA (0.670). We further extended our discussion on how to interpret the conclusions from different metrics, identifying challenges in using PMSS, and provided guidelines on conducting parser selections with our metric. PMSS provides a valuable evaluation alternative when ground-truths are inconsistent or labels are unavailable.

</details>


### [15] [Analyzing Code Injection Attacks on LLM-based Multi-Agent Systems in Software Development](https://arxiv.org/abs/2512.21818)
*Brian Bowers,Smita Khapre,Jugal Kalita*

Main category: cs.SE

TL;DR: 该论文分析了多智能体系统在软件工程中的漏洞，发现coder-reviewer-tester架构比coder和coder-tester架构更安全但效率较低，通过添加安全分析智能体可提高效率同时增强安全性，但安全分析智能体本身仍易受高级代码注入攻击。


<details>
  <summary>Details</summary>
Motivation: 随着智能体AI和多智能体系统即将主导产业和社会，这些系统具有目标驱动的自主性，代表了生成式AI的强大形式。然而，由于其自主设计和缺乏人工监督，这些系统无法自行识别和应对攻击，存在安全漏洞需要分析。

Method: 提出用于软件工程实现阶段的多智能体系统架构，建立全面的威胁模型，分析不同架构（coder、coder-tester、coder-reviewer-tester）的脆弱性，并研究添加安全分析智能体的效果。

Result: coder-reviewer-tester架构比coder和coder-tester架构更具弹性但编码效率较低；添加安全分析智能体可缓解效率损失同时获得更好的弹性；安全分析智能体本身易受高级代码注入攻击，在注入代码中嵌入有毒few-shot示例可将攻击成功率从0%提升至71.95%。

Conclusion: 多智能体系统虽然能准确生成代码，但存在安全漏洞，特别是代码注入攻击。需要更强大的安全机制来保护这些自主系统，当前的安全分析智能体仍不足以防御高级攻击。

Abstract: Agentic AI and Multi-Agent Systems are poised to dominate industry and society imminently. Powered by goal-driven autonomy, they represent a powerful form of generative AI, marking a transition from reactive content generation into proactive multitasking capabilities. As an exemplar, we propose an architecture of a multi-agent system for the implementation phase of the software engineering process. We also present a comprehensive threat model for the proposed system. We demonstrate that while such systems can generate code quite accurately, they are vulnerable to attacks, including code injection. Due to their autonomous design and lack of humans in the loop, these systems cannot identify and respond to attacks by themselves. This paper analyzes the vulnerability of multi-agent systems and concludes that the coder-reviewer-tester architecture is more resilient than both the coder and coder-tester architectures, but is less efficient at writing code. We find that by adding a security analysis agent, we mitigate the loss in efficiency while achieving even better resiliency. We conclude by demonstrating that the security analysis agent is vulnerable to advanced code injection attacks, showing that embedding poisonous few-shot examples in the injected code can increase the attack success rate from 0% to 71.95%.

</details>


### [16] [HALF: Process Hollowing Analysis Framework for Binary Programs with the Assistance of Kernel Modules](https://arxiv.org/abs/2512.22043)
*Zhangbo Long,Letian Sha,Jiaye Pan,Dongpeng Xu,Yifei Huang,Fu Xiao*

Main category: cs.SE

TL;DR: 提出基于内核模块和进程空洞技术的二进制程序分析框架，提升细粒度分析的可用性和性能


<details>
  <summary>Details</summary>
Motivation: 二进制程序分析在系统安全中很重要，但细粒度分析（如动态污点分析）存在部署性差、内存占用高、性能开销大等问题，需要适应新的分析场景如内存破坏利用和沙箱逃逸恶意软件

Method: 1. 使用内核模块扩展传统动态二进制插桩的分析能力；2. 基于解耦分析思想，通过进程空洞技术在容器进程中构建分析环境；3. 复用现有动态二进制插桩平台功能，减少对目标程序执行的影响

Result: 在Windows平台上实现原型，通过基准测试和实际程序的大量实验验证了框架的有效性和性能，并通过分析实际利用程序和恶意代码验证了实用价值

Conclusion: 该框架提高了细粒度二进制程序分析的可用性和性能，能够有效分析内存破坏利用和沙箱逃逸恶意软件等新场景，具有实际应用价值

Abstract: Binary program analysis is still very important in system security. There are many practical achievements in binary code analysis, but fine-grained analysis such as dynamic taint analysis, is constantly studied due to the problem of deployability, high memory usage, and performance overhead, so as to better adapt to the new analysis scenario, such as memory corruption exploits and sandbox evasion malware. This paper presents a new binary program analysis framework, in order to improve the usability and performance of fine-grained analysis. The framework mainly uses the kernel module to further expand the analysis capability of the traditional dynamic binary instrumentation. Then, based on the idea of decoupling analysis, the analysis environment is constructed in the container process through process hollowing techniques in a new way. It can reuse the functions of the existing dynamic binary instrumentation platforms and also reduce the impact on the execution of the target program. The prototype is implemented on the Windows platform. The validity and performance of the framework are verified by a large number of experiments with benchmark and actual programs. The effectiveness of the framework is also verified by the analysis of actual exploit programs and malicious code, demonstrating the value of the practical application.

</details>


### [17] [Proceedings First Workshop on Adaptable Cloud Architectures](https://arxiv.org/abs/2512.22054)
*Giuseppe De Palma,Saverio Giallorenzo*

Main category: cs.SE

TL;DR: WACA 2025研讨会论文集，聚焦可适应云架构，与DisCoTec 2025联合举办


<details>
  <summary>Details</summary>
Motivation: 随着云计算的快速发展，需要可适应、灵活且高效的云架构来应对动态变化的工作负载和需求

Method: 通过研讨会形式汇集学术界和工业界专家，分享最新研究成果和实践经验，讨论可适应云架构的设计、实现和评估方法

Result: 形成了包含多篇高质量论文的论文集，涵盖了可适应云架构的多个关键方面，为领域发展提供了重要参考

Conclusion: WACA 2025成功促进了可适应云架构领域的知识交流和合作，论文集为该领域的研究和实践提供了宝贵资源

Abstract: This volume contains the post-proceedings of the Workshop on Adaptable Cloud Architectures (WACA 2025), held on June 20, 2025, in Lille, France, co-located with DisCoTec 2025 - 20th International Federated Conference on Distributed Computing Techniques.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [18] [A Note on the NP-Hardness of PARTITION Via First-Order Projections](https://arxiv.org/abs/2512.21448)
*Paúl Risco Iturralde*

Main category: cs.LO

TL;DR: 本文证明PARTITION问题在FO投影归约下是NP难的，填补了文献中关于PARTITION问题归约复杂性的空白。


<details>
  <summary>Details</summary>
Motivation: Murray和Williams的文章暗示PARTITION问题在2^{n^{o(1)}}大小的AC0归约下是否NP难尚不明确。本文旨在填补这一文献空白，证明PARTITION问题在更严格的归约下确实是NP难的。

Method: 通过一阶逻辑公式定义归约，对已知的3SAT到SUBSET-SUM再到PARTITION的归约进行轻微修改，使其成为一阶投影归约。由于一阶归约是多项式大小AC0归约的特例，因此结果成立。

Result: 成功证明PARTITION问题在一阶投影归约下是NP难的，从而也证明了它在多项式大小AC0归约下是NP难的。

Conclusion: 本文填补了文献中关于PARTITION问题归约复杂性的空白，证明了该问题在描述计算复杂性框架下（使用一阶逻辑公式定义归约）确实是NP难的。

Abstract: In the article ''On the (Non) NP-Hardness of Computing Circuit Complexity'', Murray and Williams imply the PARTITION decision problem is not known to be NP-hard via $2^{n^{o(1)}}$-size AC0 reductions. In this note, we show PARTITION is NP-hard via first-order projections. Basically, we slightly modify well-known reductions from 3SAT to SUBSET-SUM and from SUBSET-SUM to PARTITION, but do so in the context of descriptive computational complexity, i.e., we use first-order logical formulas to define them. Hardness under polynomial-size AC0 reductions follows because first-order reductions are a particular type of them. Thus, this note fills a gap in the literature.

</details>


### [19] [The Tensor-Plus Calculus](https://arxiv.org/abs/2512.21965)
*Kostia Chardonnet,Marc de Visme,Benoît Valiron,Renaud Vilmart*

Main category: cs.LO

TL;DR: 提出一种图形语言，包含乘性和加性两种幺半结构，用于建模非确定性、概率性或量子计算，并提供范畴语义和完备的等式理论。


<details>
  <summary>Details</summary>
Motivation: 现有图形语言通常需要显式标注（如胶带、世界注释）来区分并行线是乘性连接还是加性连接，这增加了复杂性。本文旨在开发一种上下文隐式确定的图形语言，更自然地建模不同计算范式。

Method: 设计一种彩色PROP图形语言，包含乘性（配对）和加性（分支）两种幺半结构。将图形作为交换半环的参数元素，为不同计算模型选择相应半环。提供范畴语义，证明语言的通用性，并建立完备的等式理论。

Result: 成功开发了上下文隐式确定连接类型的图形语言，为半加性范畴设计了内部语言，证明了语义的通用性，并建立了完备的等式理论来识别语义等价的图形。

Conclusion: 该图形语言为建模非确定性、概率性和量子计算提供了统一的框架，通过上下文隐式确定连接类型简化了表示，同时保持了语义的严谨性和完备性。

Abstract: We propose a graphical language that accommodates two monoidal structures: a multiplicative one for pairing and an additional one for branching. In this colored PROP, whether wires in parallel are linked through the multiplicative structure or the additive structure is implicit and determined contextually rather than explicitly through tapes, world annotations, or other techniques, as is usually the case in the literature. The diagrams are used as parameter elements of a commutative semiring, whose choice is determined by the kind of computation we want to model, such as non-deterministic, probabilistic, or quantum.
  Given such a semiring, we provide a categorical semantics of diagrams and show the language as universal for it. We also provide an equational theory to identify diagrams that share the same semantics and show that the theory is sound and complete and captures semantical equivalence.
  In categorical terms, we design an internal language for semiadditive categories (C,+,0) with a symmetric monoidal structure (C,x,1) distributive over it, and such that the homset C(1,1) is isomorphic to a given commutative semiring, e.g., the semiring of non-negative real numbers for the probabilistic case.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [20] [Quantitative Verification of Omega-regular Properties in Probabilistic Programming](https://arxiv.org/abs/2512.21596)
*Peixin Wang,Jianhao Bai,Min Zhang,C. -H. Luke Ong*

Main category: cs.PL

TL;DR: 提出TPI框架，将概率编程与时序逻辑结合，计算满足ω-正则规范的执行轨迹的后验分布，并提供严格的概率上下界保证。


<details>
  <summary>Details</summary>
Motivation: 现有概率编程推理技术通常只计算程序终止时的后验分布，无法捕捉概率行为的时序演化。需要一种能够处理时序规范和观测的推理框架。

Method: 提出时序后验推理(TPI)框架，将Rabin接受条件分解为持久性和递归性组件，构建随机屏障证书来严格限定每个组件的概率上下界。

Result: 实现了原型工具TPInfer，在基准测试集上展示了在概率模型中高效推理丰富时序属性的能力。

Conclusion: TPI框架成功统一了概率编程与时序逻辑，为具有时序规范的复杂概率模型提供了严格的定量推理能力。

Abstract: Probabilistic programming provides a high-level framework for specifying statistical models as executable programs with built-in randomness and conditioning. Existing inference techniques, however, typically compute posterior distributions over program states at fixed time points, most often at termination, thereby failing to capture the temporal evolution of probabilistic behaviors. We introduce temporal posterior inference (TPI), a new framework that unifies probabilistic programming with temporal logic by computing posterior distributions over execution traces that satisfy omega-regular specifications, conditioned on possibly temporal observations. To obtain rigorous quantitative guarantees, we develop a new method for computing upper and lower bounds on the satisfaction probabilities of omega-regular properties. Our approach decomposes Rabin acceptance conditions into persistence and recurrence components and constructs stochastic barrier certificates that soundly bound each component. We implement our approach in a prototype tool, TPInfer, and evaluate it on a suite of benchmarks, demonstrating effective and efficient inference over rich temporal properties in probabilistic models.

</details>
