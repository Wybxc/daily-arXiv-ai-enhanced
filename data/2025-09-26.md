<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 18]
- [cs.LO](#cs.LO) [Total: 3]
- [cs.PL](#cs.PL) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [ACCeLLiuM: Supervised Fine-Tuning for Automated OpenACC Pragma Generation](https://arxiv.org/abs/2509.20380)
*Samyak Jhaveri,Vanessa Klotzmann,Crista Lopes*

Main category: cs.SE

TL;DR: ACCeLLiuM是专门针对OpenACC指令生成进行微调的大语言模型，能够自动为数据并行循环生成专家级OpenACC指令，显著提升GPU编程效率。


<details>
  <summary>Details</summary>
Motivation: 随着GPU硬件的普及和并行编程框架的复杂性增加，虽然OpenACC等指令式编程标准简化了GPU编程，但仍需要专业知识才能有效使用指令。

Method: 通过从GitHub公共仓库挖掘4,033个OpenACC指令-循环对构建监督微调数据集，训练专门用于生成OpenACC指令的LLM模型。

Result: 在测试集上，基础LLM无法一致生成有效指令，而ACCeLLiuM模型能生成正确指令类型的指令占87%，完全匹配的指令占50%，即使不完全匹配也常包含有用的附加子句。

Conclusion: ACCeLLiuM显著降低了自动GPU卸载的门槛，为LLM驱动的OpenACC指令生成建立了可复现的基准，并公开了代码、模型和数据集。

Abstract: The increasing ubiquity of GPUs is accompanied by the increasing complexity
of their hardware and parallel programming frameworks. Directive-based parallel
programming standards like OpenACC simplify GPU programming to some extent by
abstracting away low-level complexities, but a fair amount of expertise is
still required in order to use those directives effectively.
  We introduce ACCeLLiuM, two open weights Large Language Models specifically
fine-tuned for generating expert OpenACC directives for data-parallel loops,
along with the supervised fine-tuning dataset that was used to train them. The
ACCeLLiuM SFT dataset contains 4,033 OpenACC pragma-loop pairs mined from
public GitHub C/C++ repositories, with 3,223 pairs for training and 810 for
testing. Experimental evaluations show a pronounced performance gap in
generating correct OpenACC pragmas between base LLMs and our fine-tuned
versions. On the held-out test set, base LLMs fail to consistently generate
valid pragmas, whereas LLMs fine-tuned on the ACCeLLiuM dataset generate valid
pragmas with the correct directive type for $87\%$ of the data-parallel loops,
and exact pragmas--including directives, clauses, clause order, and clause
variables--for $50\%$ of the cases. Even when not exact, generated pragmas
frequently incorporate the correct clauses in a different order than the
ground-truth label, or include additional clauses that enable finer control
over parallel execution, data movement, and concurrency, offering practical
value beyond strict string-matching. By publicly releasing the code, models,
and dataset as ACCeLLiuM we hope to establish a reproducible benchmark for
LLM-powered OpenACC pragma generation, and lower the barrier to automated GPU
offloading of serially written programs.

</details>


### [2] [Enhancing Python Programming Education with an AI-Powered Code Helper: Design, Implementation, and Impact](https://arxiv.org/abs/2509.20518)
*Sayed Mahbub Hasan Amiri,Md Mainul Islam*

Main category: cs.SE

TL;DR: 本文提出了一个基于AI的Python聊天机器人，通过结合静态代码分析、动态执行追踪和大语言模型，帮助学生解决编程学习中的调试错误、语法问题和概念转化等难题。


<details>
  <summary>Details</summary>
Motivation: 传统IDE和静态分析工具缺乏交互式指导，而现有AI代码助手如GitHub Copilot主要关注代码完成而非教育目的。本研究旨在填补这一空白，开发一个注重学习过程的教育型AI助手。

Method: 采用混合架构：使用CodeLlama进行代码嵌入，GPT-4处理自然语言交互，Docker沙箱确保安全执行。结合静态分析和动态追踪技术。

Result: 在1500份学生提交的评估中，系统达到85%的错误解决成功率，优于pylint（62%）和GPT-4（73%）。调试时间减少59.3%，编程能力提升34%。

Conclusion: 该研究展示了AI如何增强编程教育，通过平衡技术创新与教学关怀，优先考虑教育公平和长期技能保持，而非仅仅代码完成。

Abstract: This is the study that presents an AI-Python-based chatbot that helps
students to learn programming by demonstrating solutions to such problems as
debugging errors, solving syntax problems or converting abstract theoretical
concepts to practical implementations. Traditional coding tools like Integrated
Development Environments (IDEs) and static analyzers do not give robotic help
while AI-driven code assistants such as GitHub Copilot focus on getting things
done. To close this gap, our chatbot combines static code analysis, dynamic
execution tracing, and large language models (LLMs) to provide the students
with relevant and practical advice, hence promoting the learning process. The
chatbots hybrid architecture employs CodeLlama for code embedding, GPT-4 for
natural language interactions, and Docker-based sandboxing for secure
execution. Evaluated through a mixed-methods approach involving 1,500 student
submissions, the system demonstrated an 85% error resolution success rate,
outperforming standalone tools like pylint (62%) and GPT-4 (73%). Quantitative
results revealed a 59.3% reduction in debugging time among users, with pre- and
post-test assessments showing a 34% improvement in coding proficiency,
particularly in recursion and exception handling. Qualitative feedback from 120
students highlighted the chatbots clarity, accessibility, and
confidence-building impact, though critiques included occasional latency and
restrictive code sanitization. By balancing technical innovation with
pedagogical empathy, this research provides a blueprint for AI tools that
prioritize educational equity and long-term skill retention over mere code
completion. The chatbot exemplifies how AI can augment human instruction,
fostering deeper conceptual understanding in programming education.

</details>


### [3] [State-of-the-Art in Software Security Visualization: A Systematic Review](https://arxiv.org/abs/2509.20385)
*Ishara Devendra,Chaman Wijesiriwardana,Prasad Wimalaratne*

Main category: cs.SE

TL;DR: 本文对软件安全可视化技术进行了系统综述，建立了包含图基、符号基、矩阵基和隐喻基四种类型的综合分类法，分析了60多篇关键研究论文，突出了该领域的主要问题、最新进展和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统日益复杂和威胁环境不断演变，传统的基于文本和数值的安全分析方法变得越来越低效，需要将复杂的安全数据转化为易于理解的视觉格式。

Method: 通过文献系统综述方法，对60多篇软件安全可视化关键研究论文进行分析，建立四类可视化技术的综合分类法。

Result: 识别出两个主要研究领域：软件开发可视化和操作安全/网络安全可视化，强调了需要适应不断变化安全环境的创新可视化技术。

Conclusion: 研究强调了创新可视化技术对增强威胁检测、改进安全响应策略以及指导未来研究的实际意义。

Abstract: Software security visualization is an interdisciplinary field that combines
the technical complexity of cybersecurity, including threat intelligence and
compliance monitoring, with visual analytics, transforming complex security
data into easily digestible visual formats. As software systems get more
complex and the threat landscape evolves, traditional text-based and numerical
methods for analyzing and interpreting security concerns become increasingly
ineffective. The purpose of this paper is to systematically review existing
research and create a comprehensive taxonomy of software security visualization
techniques through literature, categorizing these techniques into four types:
graph-based, notation-based, matrix-based, and metaphor-based visualization.
This systematic review explores over 60 recent key research papers in software
security visualization, highlighting its key issues, recent advancements, and
prospective future research directions. From the comprehensive analysis, the
two main areas were distinctly highlighted as extensive software development
visualization, focusing on advanced methods for depicting software
architecture: operational security visualization and cybersecurity
visualization. The findings highlight the necessity for innovative
visualization techniques that adapt to the evolving security landscape, with
practical implications for enhancing threat detection, improving security
response strategies, and guiding future research.

</details>


### [4] [Dynamic ReAct: Scalable Tool Selection for Large-Scale MCP Environments](https://arxiv.org/abs/2509.20386)
*Nishant Gaurav,Adit Akarsh,Ankit Ranjan,Manoj Bajaj*

Main category: cs.SE

TL;DR: Dynamic ReAct是一种新颖方法，使ReAct代理能够在超出大型语言模型上下文内存限制的庞大MCP工具集下高效运行，通过搜索加载机制减少50%工具加载量同时保持任务完成准确率


<details>
  <summary>Details</summary>
Motivation: 解决在包含数百或数千个可用工具的环境中，由于无法同时加载所有工具而导致的工具选择基本挑战

Method: 提出并评估了五种逐步优化工具选择过程的架构，最终实现搜索加载机制，以最小计算开销实现智能工具选择

Result: 实验结果表明该方法将工具加载量减少高达50%，同时保持任务完成准确率

Conclusion: 该方法推进了真正通用AI代理的发展，使其能够动态适应多样化任务环境

Abstract: We present Dynamic ReAct, a novel approach for enabling ReAct agents to ef-
ficiently operate with extensive Model Control Protocol (MCP) tool sets that
exceed the contextual memory limitations of large language models. Our approach
addresses the fundamental challenge of tool selection in environments
containing hundreds or thousands of available tools, where loading all tools
simultaneously is computationally infeasible. We propose and evaluate five
distinct architectures that progressively refine the tool selection process,
culminating in a search-and-load mechanism that achieves intelligent tool
selection with minimal computational overhead. Our experimental results
demonstrate that the proposed approach reduces tool loading by up to 50% while
maintaining task completion accuracy, advancing the path towards truly
general-purpose AI agents capable of dynamically adapting to diverse task
environments.

</details>


### [5] [Towards Systematic Specification and Verification of Fairness Requirements: A Position Paper](https://arxiv.org/abs/2509.20387)
*Qusai Ramadan,Jukka Ruohonen,Abhishek Tiwari,Adam Alami,Zeyd Boukhers*

Main category: cs.SE

TL;DR: 本文提出基于知识图谱的框架来解决软件系统中的公平性需求规范和验证问题，以解决因缺乏明确的公平性要求而导致的歧视问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常将软件系统中的歧视问题归因于算法设计缺陷或数据偏见，但忽视了公平性需求规范不明确及其验证缺失这一根本原因。专家关于公平性的知识往往是隐性的，难以转化为精确可验证的需求。

Method: 借鉴安全工程领域的经验，提出开发基于知识图谱的框架来形式化公平性知识，辅助公平性需求的规范和验证。

Result: 本文讨论了相关挑战、研究问题，并提出了解决这些研究问题的路线图。

Conclusion: 基于知识图谱的方法有望为软件系统公平性需求的规范和验证提供有效的形式化机制。

Abstract: Decisions suggested by improperly designed software systems might be prone to
discriminate against people based on protected characteristics, such as gender
and ethnicity. Previous studies attribute such undesired behavior to flaws in
algorithmic design or biased data. However, these studies ignore that
discrimination is often the result of a lack of well-specified fairness
requirements and their verification. The fact that experts' knowledge about
fairness is often implicit makes the task of specifying precise and verifiable
fairness requirements difficult. In related domains, such as security
engineering, knowledge graphs have been proven to be effective in formalizing
knowledge to assist requirements specification and verification. To address the
lack of formal mechanisms for specifying and verifying fairness requirements,
we propose the development of a knowledge graph-based framework for fairness.
In this paper, we discuss the challenges, research questions, and a road map
towards addressing the research questions.

</details>


### [6] [Online-Optimized RAG for Tool Use and Function Calling](https://arxiv.org/abs/2509.20415)
*Yu Pan,Xiaocheng Li,Hanzhao Wang*

Main category: cs.SE

TL;DR: 本文提出Online-Optimized RAG框架，通过在线梯度更新解决检索增强生成中的嵌入对齐问题，提高工具选择的准确性和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 解决实际应用中由于不完美的嵌入模型或噪声描述导致的嵌入对齐问题，这种对齐错误可能导致检索错误和任务失败。

Method: 引入部署时框架，通过最小反馈（如任务成功）从实时交互中持续适应检索嵌入，应用轻量级在线梯度更新，无需改变底层LLM。

Result: 在多样化工具使用和文档检索场景中，该方法持续提高工具选择准确性和最终任务成功率。

Conclusion: Online-Optimized RAG为构建稳健、自我改进的RAG系统提供了简单实用的路径。

Abstract: In many applications, retrieval-augmented generation (RAG) drives tool use
and function calling by embedding the (user) queries and matching them to
pre-specified tool/function descriptions. In this paper, we address an
embedding misalignment issue that often arises in practical applications due to
imperfect embedding models or noisy descriptions; such misalignment may lead to
incorrect retrieval and task failure. We introduce Online-Optimized RAG, a
deployment-time framework that continually adapts retrieval embeddings from
live interactions using minimal feedback (e.g., task success). Online-Optimized
RAG applies lightweight online gradient updates with negligible per-query
latency and requires no changes to the underlying LLM. The method is
plug-and-play: it supports both single- and multi-hop tool use, dynamic tool
inventories, and $K$-retrieval with re-ranking. We provide a problem-dependent
theoretical analysis that quantifies how the method's performance depends on
the initialization quality of the embeddings and other related quantities.
Across diverse tool-use and document-retrieval scenarios, our Online-Optimized
RAG consistently improves tool selection accuracy and end-task success, thus
providing a simple, practical path to robust, self-improving RAG systems.

</details>


### [7] [Formal Verification of Legal Contracts: A Translation-based Approach](https://arxiv.org/abs/2509.20421)
*Reiner Hähnle,Cosimo Laneve,Adele Veschetti*

Main category: cs.SE

TL;DR: Stipula是一种用于建模法律合同的领域特定编程语言，本文提出通过将Stipula合同翻译为带有JML注解的Java代码，并使用KeY工具进行形式化验证的方法。


<details>
  <summary>Details</summary>
Motivation: 旨在为涉及资产转移和义务的法律合同提供可执行属性的形式化验证方法，确保合同的正确性。

Method: 将Stipula合同自动翻译为带有Java Modeling Language注解的Java代码，然后使用KeY演绎验证工具进行部分和完全正确性验证。

Result: 对于具有不相交循环的大型Stipula合同子集，翻译和验证过程完全自动化，验证了通用演绎验证工具在翻译方法中的成功应用。

Conclusion: 研究表明通用演绎验证工具可以成功应用于翻译方法中，为法律合同的形式化验证提供了有效途径。

Abstract: Stipula is a domain-specific programming language designed to model legal
contracts with enforceable properties, especially those involving asset
transfers and obligations. This paper presents a methodology to formally verify
the correctness of Stipula contracts through translation into Java code
annotated with Java Modeling Language specifications. As a verification
backend, the deductive verification tool KeY is used. Both, the translation and
the verification of partial and total correctness for a large subset of Stipula
contracts, those with disjoint cycles, is fully automatic. Our work
demonstrates that a general-purpose deductive verification tool can be used
successfully in a translation approach.

</details>


### [8] [AI-Specific Code Smells: From Specification to Detection](https://arxiv.org/abs/2509.20491)
*Brahim Mahmoudi,Naouel Moha,Quentin Stievenert,Florent Avellaneda*

Main category: cs.SE

TL;DR: SpecDetect4AI是一个基于工具的方法，用于大规模规范和检测AI特定代码异味，结合高级声明式领域特定语言和可扩展静态分析工具，在826个AI系统中达到88.66%精确度和88.89%召回率。


<details>
  <summary>Details</summary>
Motivation: AI系统的兴起带来了新的软件问题，现有检测工具往往无法识别AI特定的代码异味，这些异味可能导致不可重现性、静默故障或模型泛化能力差等深层问题。

Method: 结合高级声明式领域特定语言进行规则规范，配合可扩展的静态分析工具来解析和检测AI系统中的代码异味规则。

Result: 在826个AI系统（2000万行代码）中评估，SpecDetect4AI达到88.66%精确度和88.89%召回率，优于现有检测工具，SUS评分为81.7/100。

Conclusion: SpecDetect4AI通过专用规则支持AI特定代码异味的规范和检测，能够有效分析大型AI系统，展示了效率和可扩展性。

Abstract: The rise of Artificial Intelligence (AI) is reshaping how software systems
are developed and maintained. However, AI-based systems give rise to new
software issues that existing detection tools often miss. Among these, we focus
on AI-specific code smells, recurring patterns in the code that may indicate
deeper problems such as unreproducibility, silent failures, or poor model
generalization. We introduce SpecDetect4AI, a tool-based approach for the
specification and detection of these code smells at scale. This approach
combines a high-level declarative Domain-Specific Language (DSL) for rule
specification with an extensible static analysis tool that interprets and
detects these rules for AI-based systems. We specified 22 AI-specific code
smells and evaluated SpecDetect4AI on 826 AI-based systems (20M lines of code),
achieving a precision of 88.66% and a recall of 88.89%, outperforming other
existing detection tools. Our results show that SpecDetect4AI supports the
specification and detection of AI-specific code smells through dedicated rules
and can effectively analyze large AI-based systems, demonstrating both
efficiency and extensibility (SUS 81.7/100).

</details>


### [9] [PromptDebt: A Comprehensive Study of Technical Debt Across LLM Projects](https://arxiv.org/abs/2509.20497)
*Ahmed Aljohani,Hyunsook Do*

Main category: cs.SE

TL;DR: 本文首次对LLM集成中的技术债务进行了大规模实证研究，发现OpenAI集成是主要债务来源，提示设计是最大的技术债务产生因素。


<details>
  <summary>Details</summary>
Motivation: 随着LLM通过API被广泛集成到软件中，这些集成带来了特有的技术债务问题，需要系统性的研究和理解。

Method: 通过分析93,142个Python文件中主要LLM API的使用情况，研究LLM特定技术债务的来源、普遍性和缓解策略。

Result: 54.49%的技术债务实例来自OpenAI集成，12.35%来自LangChain使用；提示设计是主要债务来源（6.61%），其中基于指令的提示（38.60%）和少样本提示（18.13%）最易产生债务。

Conclusion: 研究揭示了LLM集成中的技术债务模式，发布了全面的技术债务数据集，并为管理LLM系统的技术债务提供了实用指导。

Abstract: Large Language Models (LLMs) are increasingly embedded in software via APIs
like OpenAI, offering powerful AI features without heavy infrastructure. Yet
these integrations bring their own form of self-admitted technical debt (SATD).
In this paper, we present the first large-scale empirical study of LLM-specific
SATD: its origins, prevalence, and mitigation strategies. By analyzing 93,142
Python files across major LLM APIs, we found that 54.49% of SATD instances stem
from OpenAI integrations and 12.35% from LangChain use. Prompt design emerged
as the primary source of LLM-specific SATD, with 6.61% of debt related to
prompt configuration and optimization issues, followed by hyperparameter tuning
and LLM-framework integration. We further explored which prompt techniques
attract the most debt, revealing that instruction-based prompts (38.60%) and
few-shot prompts (18.13%) are particularly vulnerable due to their dependence
on instruction clarity and example quality. Finally, we release a comprehensive
SATD dataset to support reproducibility and offer practical guidance for
managing technical debt in LLM-powered systems.

</details>


### [10] [Enhancing LLM-based Fault Localization with a Functionality-Aware Retrieval-Augmented Generation Framework](https://arxiv.org/abs/2509.20552)
*Xinyu Shi,Zhenhao Li,An Ran Chen*

Main category: cs.SE

TL;DR: FaR-Loc是一个基于大语言模型和检索增强生成的方法级故障定位框架，通过功能提取、语义检索和重排序三个组件，在Defects4J基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于大语言模型的故障定位方法在处理复杂系统时面临项目特定知识缺乏和大型项目导航困难的挑战，需要更有效的解决方案。

Method: FaR-Loc包含三个核心组件：LLM功能提取模块生成失败行为的自然语言描述；语义密集检索组件在共享语义空间中嵌入功能描述和覆盖方法；LLM重排序模块基于上下文相关性重新排序检索到的方法。

Result: 在Defects4J基准测试中，FaR-Loc在Top-1准确率上比SoapFL和AutoFL分别提高14.6%和9.1%，在Top-5准确率上分别提高19.2%和22.1%，且优于所有基于学习和频谱分析的方法。

Conclusion: FaR-Loc通过结合LLM和RAG技术有效提升了故障定位性能，特别是采用包含代码结构的预训练嵌入模型（如UniXcoder）可将Top-1准确率提升高达49.0%。

Abstract: Fault localization (FL) is a critical but time-consuming task in software
debugging, aiming to identify faulty code elements. While recent advances in
large language models (LLMs) have shown promise for FL, they often struggle
with complex systems due to the lack of project-specific knowledge and the
difficulty of navigating large projects. To address these limitations, we
propose FaR-Loc, a novel framework that enhances method-level FL by integrating
LLMs with retrieval-augmented generation (RAG). FaR-Loc consists of three key
components: LLM Functionality Extraction, Semantic Dense Retrieval, and LLM
Re-ranking. First, given a failed test and its associated stack trace, the LLM
Functionality Extraction module generates a concise natural language
description that captures the failing behavior. Next, the Semantic Dense
Retrieval component leverages a pre-trained code-understanding encoder to embed
both the functionality description (natural language) and the covered methods
(code) into a shared semantic space, enabling the retrieval of methods with
similar functional behavior. Finally, the LLM Re-ranking module reorders the
retrieved methods based on their contextual relevance. Our experiments on the
widely used Defects4J benchmark show that FaR-Loc outperforms state-of-the-art
LLM-based baselines SoapFL and AutoFL, by 14.6% and 9.1% in Top-1 accuracy, by
19.2% and 22.1% in Top-5 accuracy, respectively. It also surpasses all
learning-based and spectrum-based baselines across all Top-N metrics without
requiring re-training. Furthermore, we find that pre-trained code embedding
models that incorporate code structure, such as UniXcoder, can significantly
improve fault localization performance by up to 49.0% in Top-1 accuracy.
Finally, we conduct a case study to illustrate the effectiveness of FaR-Loc and
to provide insights for its practical application.

</details>


### [11] [Design, Implementation and Evaluation of a Novel Programming Language Topic Classification Workflow](https://arxiv.org/abs/2509.20631)
*Michael Zhang,Yuan Tian,Mariam Guizani*

Main category: cs.SE

TL;DR: 本文提出了一种新颖的编程语言主题分类工作流，结合多标签支持向量机和滑动窗口投票策略，能够在源代码中精确定位核心语言概念。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统规模和复杂度的增长，理解源代码中编程语言主题的分布对于指导技术决策、改进入职流程以及为工具和教育提供信息变得越来越重要。

Method: 设计了一种结合多标签支持向量机与滑动窗口和投票策略的工作流，在IBM Project CodeNet数据集上进行训练，实现对操作符重载、虚函数、继承和模板等核心语言概念的细粒度定位。

Result: 模型在所有主题上的平均F1得分为0.90，代码-主题高亮得分为0.75。

Conclusion: 研究结果为代码分析和数据驱动软件工程领域的研究人员和实践者提供了经验性见解和可复用的分析管道。

Abstract: As software systems grow in scale and complexity, understanding the
distribution of programming language topics within source code becomes
increasingly important for guiding technical decisions, improving onboarding,
and informing tooling and education. This paper presents the design,
implementation, and evaluation of a novel programming language topic
classification workflow. Our approach combines a multi-label Support Vector
Machine (SVM) with a sliding window and voting strategy to enable fine-grained
localization of core language concepts such as operator overloading, virtual
functions, inheritance, and templates. Trained on the IBM Project CodeNet
dataset, our model achieves an average F1 score of 0.90 across topics and 0.75
in code-topic highlight. Our findings contribute empirical insights and a
reusable pipeline for researchers and practitioners interested in code analysis
and data-driven software engineering.

</details>


### [12] [Exploring Engagement in Hybrid Meetings](https://arxiv.org/abs/2509.20780)
*Daniela Grassi,Fabio Calefato,Darja Smite,Nicole Novielli,Filippo Lanubile*

Main category: cs.SE

TL;DR: 本研究通过多模态方法分析混合会议中的参与度模式，发现现场和远程参与者的参与度相当，但远程参与者在长时间会议中参与度较低，主动角色和会议规模是影响参与度的关键因素。


<details>
  <summary>Details</summary>
Motivation: COVID-19大流行后混合工作的广泛采用改变了软件开发实践，带来了沟通协作的新挑战。远程参与会议可能导致远程团队成员的孤立、疏离和参与度下降，需要客观测量混合会议中的参与度模式。

Method: 研究了三家软件公司的专业人士数周时间，采用多模态方法测量参与度，包括自报告问卷和生物识别设备的生理测量，重点关注现场与远程参与者的差异。

Result: 回归分析显示现场和远程参与者参与度相当，但远程参与者在长时间会议中参与度较低。主动角色与更高参与度正相关，而大型会议和下午会议与较低参与度相关。

Conclusion: 研究结果为混合会议中的参与度和脱离因素提供了见解，提出了会议改进建议，这些见解不仅适用于软件团队，也适用于面临类似混合协作挑战的知识密集型组织。

Abstract: Background. The widespread adoption of hybrid work following the COVID-19
pandemic has fundamentally transformed software development practices,
introducing new challenges in communication and collaboration as organizations
transition from traditional office-based structures to flexible working
arrangements. This shift has established a new organizational norm where even
traditionally office-first companies now embrace hybrid team structures. While
remote participation in meetings has become commonplace in this new
environment, it may lead to isolation, alienation, and decreased engagement
among remote team members. Aims. This study aims to identify and characterize
engagement patterns in hybrid meetings through objective measurements, focusing
on the differences between co-located and remote participants. Method. We
studied professionals from three software companies over several weeks,
employing a multimodal approach to measure engagement. Data were collected
through self-reported questionnaires and physiological measurements using
biometric devices during hybrid meetings to understand engagement dynamics.
Results. The regression analyses revealed comparable engagement levels between
onsite and remote participants, though remote participants show lower
engagement in long meetings regardless of participation mode. Active roles
positively correlate with higher engagement, while larger meetings and
afternoon sessions are associated with lower engagement. Conclusions. Our
results offer insights into factors associated with engagement and
disengagement in hybrid meetings, as well as potential meeting improvement
recommendations. These insights are potentially relevant not only for software
teams but also for knowledge-intensive organizations across various sectors
facing similar hybrid collaboration challenges.

</details>


### [13] [Verification Limits Code LLM Training](https://arxiv.org/abs/2509.20837)
*Srishti Gureja,Elena Tommasone,Jingyi He,Sara Hooker,Matthias Gallé,Marzieh Fadaee*

Main category: cs.SE

TL;DR: 本文研究了代码生成中合成验证的瓶颈问题，提出了验证天花板概念，并通过实验证明当前验证方法过于严格，需要重新校准验证策略来突破这一瓶颈。


<details>
  <summary>Details</summary>
Motivation: 随着代码生成模型越来越依赖合成数据，验证能力成为训练数据质量和多样性的关键瓶颈。本文旨在系统研究验证设计如何影响模型性能，探索如何突破验证天花板。

Method: 通过三个维度研究验证策略：(i)分析测试复杂度和数量的影响；(ii)探索放宽通过阈值和LLM软验证方法；(iii)比较正确与错误解决方案的保留效果。

Result: 研究发现：更丰富的测试套件可提升代码生成能力(+3 pass@1)；放宽验证阈值可恢复有价值训练数据(提升2-4点pass@1)；保留每个问题的多样正确解决方案能带来一致泛化收益。

Conclusion: 当前验证实践过于严格，过滤了有价值的多样性，但不能完全丢弃验证。通过结合校准验证与多样化问题-解决方案对，可以突破验证天花板，开发更强的代码生成模型。

Abstract: Large language models for code generation increasingly rely on synthetic
data, where both problem solutions and verification tests are generated by
models. While this enables scalable data creation, it introduces a previously
unexplored bottleneck: the verification ceiling, in which the quality and
diversity of training data are fundamentally constrained by the capabilities of
synthetic verifiers. In this work, we systematically study how verification
design and strategies influence model performance. We investigate (i) what we
verify by analyzing the impact of test complexity and quantity: richer test
suites improve code generation capabilities (on average +3 pass@1), while
quantity alone yields diminishing returns, (ii) how we verify by exploring
relaxed pass thresholds: rigid 100% pass criteria can be overly restrictive. By
allowing for relaxed thresholds or incorporating LLM-based soft verification,
we can recover valuable training data, leading to a 2-4 point improvement in
pass@1 performance. However, this benefit is contingent upon the strength and
diversity of the test cases used, and (iii) why verification remains necessary
through controlled comparisons of formally correct versus incorrect solutions
and human evaluation: retaining diverse correct solutions per problem yields
consistent generalization gains. Our results show that Verification as
currently practiced is too rigid, filtering out valuable diversity. But it
cannot be discarded, only recalibrated. By combining calibrated verification
with diverse, challenging problem-solution pairs, we outline a path to break
the verification ceiling and unlock stronger code generation models.

</details>


### [14] [PseudoBridge: Pseudo Code as the Bridge for Better Semantic and Logic Alignment in Code Retrieval](https://arxiv.org/abs/2509.20881)
*Yixuan Li,Xinyi Liu,Weidong Yang,Ben Fei,Shuhao Li,Mingjie Zhou,Lipeng Ma*

Main category: cs.SE

TL;DR: PseudoBridge是一个新颖的代码检索框架，通过引入伪代码作为中间模态来更好地对齐自然语言语义和编程语言逻辑，显著提升了代码搜索的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于预训练语言模型的代码搜索方法仍面临两个关键挑战：人类意图与机器执行逻辑之间的语义鸿沟，以及对不同代码风格的鲁棒性有限。

Method: PseudoBridge采用两阶段方法：1）使用大语言模型生成伪代码，实现自然语言查询与伪代码的显式对齐；2）引入逻辑不变代码风格增强策略，生成风格多样但逻辑等价的代码实现，并与伪代码对齐。

Result: 在10个不同预训练语言模型和6种主流编程语言上的实验表明，PseudoBridge持续优于基线方法，在检索准确性和泛化能力方面取得显著提升，特别是在零样本领域迁移场景下。

Conclusion: 研究证明了通过伪代码进行显式逻辑对齐的有效性，PseudoBridge有潜力成为代码检索的鲁棒、可泛化解决方案。

Abstract: Code search aims to precisely find relevant code snippets that match natural
language queries within massive codebases, playing a vital role in software
development. Recent advances leverage pre-trained language models (PLMs) to
bridge the semantic gap between unstructured natural language (NL) and
structured programming languages (PL), yielding significant improvements over
traditional information retrieval and early deep learning approaches. However,
existing PLM-based methods still encounter key challenges, including a
fundamental semantic gap between human intent and machine execution logic, as
well as limited robustness to diverse code styles. To address these issues, we
propose PseudoBridge, a novel code retrieval framework that introduces
pseudo-code as an intermediate, semi-structured modality to better align NL
semantics with PL logic. Specifically, PseudoBridge consists of two stages.
First, we employ an advanced large language model (LLM) to synthesize
pseudo-code, enabling explicit alignment between NL queries and pseudo-code.
Second, we introduce a logic-invariant code style augmentation strategy and
employ the LLM to generate stylistically diverse yet logically equivalent code
implementations with pseudo-code, then align the code snippets of different
styles with pseudo-code, enhancing model robustness to code style variation. We
build PseudoBridge across 10 different PLMs and evaluate it on 6 mainstream
programming languages. Extensive experiments demonstrate that PseudoBridge
consistently outperforms baselines, achieving significant gains in retrieval
accuracy and generalization, particularly under zero-shot domain transfer
scenarios such as Solidity and XLCoST datasets. These results demonstrate the
effectiveness of explicit logical alignment via pseudo-code and highlight
PseudoBridge's potential as a robust, generalizable solution for code
retrieval.

</details>


### [15] [Designing for Novice Debuggers: A Pilot Study on an AI-Assisted Debugging Tool](https://arxiv.org/abs/2509.21067)
*Oka Kurniawan,Erick Chandra,Christopher M. Poskitt,Yannic Noller,Kenny Tsu Wei Choo,Cyrille Jegourel*

Main category: cs.SE

TL;DR: CodeHinter是一个结合传统调试工具和LLM技术的调试助手，旨在帮助编程新手修复语义错误并促进他们在调试过程中的主动参与。


<details>
  <summary>Details</summary>
Motivation: 现有的AI辅助调试工具容易导致学生对AI过度依赖，未能真正促进学生参与调试过程。

Method: 设计CodeHinter调试助手，结合传统调试工具和基于LLM的技术，通过第二版设计迭代并在本科生群体中进行测试。

Result: 学生对工具在解决语义错误方面评价很高，认为比第一版显著更易用，错误定位是最有价值的功能。

Conclusion: AI辅助调试工具应根据用户画像进行个性化定制，以优化与学生的互动效果。

Abstract: Debugging is a fundamental skill that novice programmers must develop.
Numerous tools have been created to assist novice programmers in this process.
Recently, large language models (LLMs) have been integrated with automated
program repair techniques to generate fixes for students' buggy code. However,
many of these tools foster an over-reliance on AI and do not actively engage
students in the debugging process. In this work, we aim to design an intuitive
debugging assistant, CodeHinter, that combines traditional debugging tools with
LLM-based techniques to help novice debuggers fix semantic errors while
promoting active engagement in the debugging process. We present findings from
our second design iteration, which we tested with a group of undergraduate
students. Our results indicate that the students found the tool highly
effective in resolving semantic errors and significantly easier to use than the
first version. Consistent with our previous study, error localization was the
most valuable feature. Finally, we conclude that any AI-assisted debugging tool
should be personalized based on user profiles to optimize their interactions
with students.

</details>


### [16] [An Improved Quantum Software Challenges Classification Approach using Transfer Learning and Explainable AI](https://arxiv.org/abs/2509.21068)
*Nek Dil Khan,Javed Ali Khan,Mobashir Husain,Muhammad Sohail Khan,Arif Ali Khan,Muhammad Azeem Akbar,Shahid Hussain*

Main category: cs.SE

TL;DR: 该研究通过分析Stack Overflow上的量子计算相关帖子，识别量子软件工程中的常见挑战，并使用Transformer模型对帖子进行分类，BERT模型达到95%的准确率。


<details>
  <summary>Details</summary>
Motivation: 量子开发者面临优化量子计算和量子软件工程概念的挑战，需要识别和分类他们在问答平台讨论的常见问题。

Method: 从问答平台提取2829个量子相关帖子，通过内容分析和扎根理论识别挑战类别，使用ChatGPT验证标注，并比较Transformer模型（BERT、DistilBERT、RoBERTa）与深度学习模型（FNN、CNN、LSTM）的分类性能。

Result: Transformer模型平均准确率达到95%，显著优于深度学习模型（89%、86%、84%），使用SHAP增强模型可解释性。

Conclusion: 基于Transformer的方法在量子软件工程挑战分类中表现优异，有助于量子供应商和论坛更好地组织讨论，但需要实际开发者和供应商的实证评估。

Abstract: Quantum Software Engineering (QSE) is a research area practiced by tech
firms. Quantum developers face challenges in optimizing quantum computing and
QSE concepts. They use Stack Overflow (SO) to discuss challenges and label
posts with specialized quantum tags, which often refer to technical aspects
rather than developer posts. Categorizing questions based on quantum concepts
can help identify frequent QSE challenges. We conducted studies to classify
questions into various challenges. We extracted 2829 questions from Q&A
platforms using quantum-related tags. Posts were analyzed to identify frequent
challenges and develop a novel grounded theory. Challenges include Tooling,
Theoretical, Learning, Conceptual, Errors, and API Usage. Through content
analysis and grounded theory, discussions were annotated with common challenges
to develop a ground truth dataset. ChatGPT validated human annotations and
resolved disagreements. Fine-tuned transformer algorithms, including BERT,
DistilBERT, and RoBERTa, classified discussions into common challenges. We
achieved an average accuracy of 95% with BERT DistilBERT, compared to
fine-tuned Deep and Machine Learning (D&ML) classifiers, including Feedforward
Neural Networks (FNN), Convolutional Neural Networks (CNN), and Long Short-Term
Memory networks (LSTM), which achieved accuracies of 89%, 86%, and 84%,
respectively. The Transformer-based approach outperforms the D&ML-based
approach with a 6\% increase in accuracy by processing actual discussions,
i.e., without data augmentation. We applied SHAP (SHapley Additive
exPlanations) for model interpretability, revealing how linguistic features
drive predictions and enhancing transparency in classification. These findings
can help quantum vendors and forums better organize discussions for improved
access and readability. However,empirical evaluation studies with actual
developers and vendors are needed.

</details>


### [17] [Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach](https://arxiv.org/abs/2509.21170)
*Yongda Yu,Guohao Shi,Xianwei Wu,Haochuan He,XueMing Gu,Qianqian Zhao,Kui Liu,Qiushi Wang,Zhao Tian,Haifeng Shen,Guoping Rong*

Main category: cs.SE

TL;DR: MelcotCR是一个基于思维链的微调方法，通过长思维链技术为代码审查提供丰富的结构化信息，使LLM能够同时分析代码审查的多个维度，显著提升代码问题检测和描述的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于微调的代码审查方法受限于训练数据的有限性或模糊性，无法像人类审查者那样同时分析代码审查的多个维度。LLM在代码审查中的潜力尚未充分发挥。

Method: 提出MelcotCR方法，结合最大熵建模原则和预定义推理路径，解决LLM处理长思维链提示时的上下文丢失和推理逻辑丢失问题，增强推理过程的逻辑严密性。

Result: 在MelcotCR数据集和公共CodeReviewer数据集上的实验表明，使用14B参数的Qwen2.5模型经过MelcotCR微调后，在代码问题检测和描述准确性方面超越了现有最优方法，性能与671B的DeepSeek-R1模型相当。

Conclusion: MelcotCR通过思维链微调有效提升了LLM在代码审查任务中的推理能力，证明了即使参数较少的模型通过适当的微调方法也能达到与超大模型相当的性能。

Abstract: Large Language Models (LLMs) have shown great potential in supporting
automated code review due to their impressive capabilities in context
understanding and reasoning. However, these capabilities are still limited
compared to human-level cognition because they are heavily influenced by the
training data. Recent research has demonstrated significantly improved
performance through fine-tuning LLMs with code review data. However, compared
to human reviewers who often simultaneously analyze multiple dimensions of code
review to better identify issues, the full potential of these methods is
hampered by the limited or vague information used to fine-tune the models. This
paper contributes MelcotCR, a chain-of-thought (COT) fine-tuning approach that
trains LLMs with an impressive reasoning ability to analyze multiple dimensions
of code review by harnessing long COT techniques to provide rich structured
information. To address context loss and reasoning logic loss issues that
frequently occur when LLMs process long COT prompts, we propose a solution that
combines the Maximum Entropy (ME) modeling principle with pre-defined reasoning
pathways in MelcotCR to enable more effective utilization of in-context
knowledge within long COT prompts while strengthening the logical tightness of
the reasoning process. Empirical evaluations on our curated MelcotCR dataset
and the public CodeReviewer dataset reveal that a low-parameter base model,
such as 14B Qwen2.5, fine-tuned with MelcotCR can surpass state-of-the-art
methods in terms of the accuracy of detecting and describing code issues, with
its performance remarkably on par with that of the 671B DeepSeek-R1 model.

</details>


### [18] [Semantic Clustering of Civic Proposals: A Case Study on Brazil's National Participation Platform](https://arxiv.org/abs/2509.21292)
*Ronivaldo Ferreira,Guilherme da Silva,Carla Rocha,Gustavo Pinto*

Main category: cs.SE

TL;DR: 提出了一种结合BERTopic、种子词和LLM自动验证的方法，用于自动分类大规模公民参与平台上的贡献，使政府能够将大量公民意见转化为可操作的公共政策数据。


<details>
  <summary>Details</summary>
Motivation: 数字平台上的公民参与日益重要，但由于贡献量巨大，手动分类不可行，需要专家参与并与官方分类法对齐，因此需要自动化解决方案。

Method: 结合BERTopic主题建模、种子词引导和大型语言模型自动验证，实现无需大量人工干预的主题分类。

Result: 初步结果表明，生成的主题具有连贯性且与机构分类对齐，只需最小化的人工努力。

Conclusion: 该方法能够有效帮助政府将大规模公民输入转化为可操作的公共政策数据，提升公民参与平台的实用性。

Abstract: Promoting participation on digital platforms such as Brasil Participativo has
emerged as a top priority for governments worldwide. However, due to the sheer
volume of contributions, much of this engagement goes underutilized, as
organizing it presents significant challenges: (1) manual classification is
unfeasible at scale; (2) expert involvement is required; and (3) alignment with
official taxonomies is necessary. In this paper, we introduce an approach that
combines BERTopic with seed words and automatic validation by large language
models. Initial results indicate that the generated topics are coherent and
institutionally aligned, with minimal human effort. This methodology enables
governments to transform large volumes of citizen input into actionable data
for public policy.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [19] [A Unified Formal Theory on the Logical Limits of Symbol Grounding](https://arxiv.org/abs/2509.20409)
*Zhangchi Liu*

Main category: cs.LO

TL;DR: 本文通过形式化证明构建了符号接地问题的逻辑极限统一理论，证明意义必须来自外部、动态和非算法的过程。


<details>
  <summary>Details</summary>
Motivation: 研究符号接地问题的根本限制，探讨形式系统中意义如何产生以及是否存在算法化的解决方案。

Method: 采用四阶段论证：1) 证明纯符号系统无法内部建立一致的意义基础；2) 扩展至有限静态意义集的固有不完备性；3) 证明符号与外部意义的连接不能是逻辑推理产物；4) 证明任何自动化更新过程都会构建更大的不完备系统。

Result: 形式化证明了意义接地是一个必然开放、非算法的过程，揭示了自包含智能系统的哥德尔式根本限制。

Conclusion: 意义接地过程本质上是非算法化的，任何试图完全算法化符号接地问题的尝试都会面临根本性的逻辑限制。

Abstract: This paper synthesizes a series of formal proofs to construct a unified
theory on the logical limits of the Symbol Grounding Problem. We demonstrate
through a four-stage argument that meaning within a formal system must arise
from a process that is external, dynamic, and non-algorithmic. First, we prove
that any purely symbolic system, devoid of external connections, cannot
internally establish a consistent foundation for meaning due to
self-referential paradoxes. Second, we extend this limitation to systems with
any finite, static set of pre-established meanings, proving they are inherently
incomplete. Third, we demonstrate that the very "act" of connecting an internal
symbol to an external meaning cannot be a product of logical inference within
the system but must be an axiomatic, meta-level update. Finally, we prove that
any attempt to automate this update process using a fixed, external "judgment"
algorithm will inevitably construct a larger, yet equally incomplete, symbolic
system. Together, these conclusions formally establish that the grounding of
meaning is a necessarily open-ended, non-algorithmic process, revealing a
fundamental, G\"odel-style limitation for any self-contained intelligent
system.

</details>


### [20] [Reverse Faà di Bruno's Formula for Cartesian Reverse Differential Categories](https://arxiv.org/abs/2509.20931)
*Aaron Biggin,Jean-Simon Pacaud Lemay*

Main category: cs.LO

TL;DR: 本文提出了笛卡尔反向微分范畴中的高阶反向链式法则，即Faa di Bruno公式的反向微分版本，并定义了部分反向导数和高阶反向导数。


<details>
  <summary>Details</summary>
Motivation: 反向微分是自动微分的基本操作，笛卡尔反向微分范畴在范畴论框架中公理化反向微分，其中主要公理是反向链式法则。本文旨在扩展这一框架，建立高阶反向微分规则。

Method: 在笛卡尔反向微分范畴中定义部分反向导数和高阶反向导数，然后推导出Faa di Bruno公式的反向微分版本作为高阶反向链式法则。

Result: 成功建立了笛卡尔反向微分范畴中的高阶反向微分理论框架，包括部分反向导数和高阶反向导数的定义，以及高阶反向链式法则的数学表达。

Conclusion: 这项工作扩展了反向微分的范畴论框架，为高阶反向微分操作提供了理论基础，对自动微分领域具有重要意义。

Abstract: Reverse differentiation is an essential operation for automatic
differentiation. Cartesian reverse differential categories axiomatize reverse
differentiation in a categorical framework, where one of the primary axioms is
the reverse chain rule, which is the formula that expresses the reverse
derivative of a composition. Here, we present the reverse differential analogue
of Faa di Bruno's Formula, which gives a higher-order reverse chain rule in a
Cartesian reverse differential category. To properly do so, we also define
partial reverse derivatives and higher-order reverse derivatives in a Cartesian
reverse differential category.

</details>


### [21] [A Coalgebraic Model of Quantum Bisimulation](https://arxiv.org/abs/2509.20933)
*Lorenzo Ceragioli,Elena Di Lavore,Giuseppe Lomurno,Gabriele Tedeschi*

Main category: cs.LO

TL;DR: 该论文探索了基于效应代数的分布上的余代数，以统一处理概率和量子效应，并引入了分级单子来遵守量子理论的约束，研究了开放量子系统与其概率对应物之间的关系，并提出了量子效应标记转换系统的操作符。


<details>
  <summary>Details</summary>
Motivation: 定义能够匹配量子能力、并发、非确定性系统的观测特性的行为等价性是一个困难的任务，需要一种能够统一处理概率和量子效应的形式化方法。

Method: 使用基于效应代数的分布上的余代数，引入分级单子来确保量子资源的正确组合（遵循不可克隆定理），比较Aczel-Mendler和核双相似性，并提出了量子效应标记转换系统的操作符。

Result: 核双相似性被证明能够表征在所有输入状态下表现出相同概率行为的量子系统，为参数化量子输入的进程演算语义奠定了基础。

Conclusion: 该工作为量子并发系统的形式化分析提供了理论基础，通过分级单子和效应代数的方法，能够更好地处理量子系统的独特性质，并为未来的量子进程演算研究开辟了道路。

Abstract: Recent works have shown that defining a behavioural equivalence that matches
the observational properties of a quantum-capable, concurrent,
non-deterministic system is a surprisingly difficult task. We explore
coalgebras over distributions taking weights from a generic effect algebra,
which subsumes probabilities and quantum effects, a physical formalism that
represents the probabilistic behaviour of an open quantum system. To abide by
the properties of quantum theory, we introduce monads graded on a partial
commutative monoid, intuitively allowing composition of two processes only if
they use different quantum resources, as prescribed by the no-cloning theorem.
We investigate the relation between an open quantum system and its
probabilistic counterparts obtained when instantiating the input with a
specific quantum state. We consider Aczel-Mendler and kernel bisimilarities,
advocating for the latter as it characterizes quantum systems that exhibit the
same probabilistic behaviour for all input states. Finally, we propose
operators on quantum effect labelled transition systems, paving the way for a
process calculi semantics that is parametric over the quantum input.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [22] [Dual-Language General-Purpose Self-Hosted Visual Language and new Textual Programming Language for Applications](https://arxiv.org/abs/2509.20426)
*Mahmoud Samir Fayed*

Main category: cs.PL

TL;DR: 该论文介绍了PWCT2的开发，这是一个自托管的通用视觉编程语言，使用Ring文本编程语言开发，解决了现有VPLs需要文本编程改进的问题。


<details>
  <summary>Details</summary>
Motivation: 大多数视觉编程语言都是领域特定的，通用VPLs如PWCT需要使用文本编程语言开发和改进，这限制了VPLs的自托管能力。

Method: 首先设计Ring文本编程语言，然后使用PWCT开发Ring编译器，最后用Ring开发PWCT2视觉编程语言，实现自托管。

Result: PWCT2代码生成速度提升36倍，视觉源文件存储减少20倍，支持Ring代码转换为视觉代码，已在Steam平台分发并获得积极反馈。

Conclusion: 成功开发了自托管的通用视觉编程语言PWCT2，证明了VPLs可以自我改进和开发，为未来研究提供了基础。

Abstract: Most visual programming languages (VPLs) are domain-specific, with few
general-purpose VPLs like Programming Without Coding Technology (PWCT). These
general-purpose VPLs are developed using textual programming languages and
improving them requires textual programming. In this thesis, we designed and
developed PWCT2, a dual-language (Arabic/English), general-purpose,
self-hosting visual programming language. Before doing so, we specifically
designed a textual programming language called Ring for its development. Ring
is a dynamically typed language with a lightweight implementation, offering
syntax customization features. It permits the creation of domain-specific
languages through new features that extend object-oriented programming,
allowing for specialized languages resembling Cascading Style Sheets (CSS) or
Supernova language. The Ring Compiler and Virtual Machine are designed using
the PWCT visual programming language where the visual implementation is
composed of 18,945 components that generate 24,743 lines of C code, which
increases the abstraction level and hides unnecessary details. Using PWCT to
develop Ring allowed us to realize several issues in PWCT, which led to the
development of the PWCT2 visual programming language using the Ring textual
programming language. PWCT2 provides approximately 36 times faster code
generation and requires 20 times less storage for visual source files. It also
allows for the conversion of Ring code into visual code, enabling the creation
of a self-hosting VPL that can be developed using itself. PWCT2 consists of
approximately 92,000 lines of Ring code and comes with 394 visual components.
PWCT2 is distributed to many users through the Steam platform and has received
positive feedback, On Steam, 1772 users have launched the software, and the
total recorded usage time exceeds 17,000 hours, encouraging further research
and development.

</details>


### [23] [Efficient Symbolic Computation vis Hash Consing](https://arxiv.org/abs/2509.20534)
*Bowen Zhu,Aayush Sabharwal,Songchen Tan,Yingbo Ma,Alan Edelman,Christopher Rackauckas*

Main category: cs.PL

TL;DR: 本文首次将hash consing技术集成到JuliaSymbolics中，通过全局弱引用哈希表对表达式进行规范化并消除重复存储，显著提升了符号计算的性能和内存效率。


<details>
  <summary>Details</summary>
Motivation: 符号计算系统存在内存效率低下的问题，主要是由于结构相同的子表达式被冗余存储（表达式膨胀），这影响了经典计算机代数和新兴AI驱动数学推理工具的性能。

Method: 在JuliaSymbolics中实现hash consing技术，使用全局弱引用哈希表对表达式进行规范化处理，消除重复存储，同时与Julia的元编程和即时编译基础设施无缝集成。

Result: 基准测试显示显著改进：符号计算加速达3.2倍，内存使用减少达2倍，代码生成快达5倍，函数编译快达10倍，大型模型的数值评估快达100倍。

Conclusion: hash consing对于扩展符号计算规模具有重要意义，为未来将hash consing与e-graphs集成以实现AI驱动管道中增强的等价感知表达式共享铺平了道路。

Abstract: Symbolic computation systems suffer from memory inefficiencies due to
redundant storage of structurally identical subexpressions, commonly known as
expression swell, which degrades performance in both classical computer algebra
and emerging AI-driven mathematical reasoning tools. In this paper, we present
the first integration of hash consing into JuliaSymbolics, a high-performance
symbolic toolkit in Julia, by employing a global weak-reference hash table that
canonicalizes expressions and eliminates duplication. This approach reduces
memory consumption and accelerates key operations such as differentiation,
simplification, and code generation, while seamlessly integrating with Julia's
metaprogramming and just-in-time compilation infrastructure. Benchmark
evaluations across different computational domains reveal substantial
improvements: symbolic computations are accelerated by up to 3.2 times, memory
usage is reduced by up to 2 times, code generation is up to 5 times faster,
function compilation up to 10 times faster, and numerical evaluation up to 100
times faster for larger models. While certain workloads with fewer duplicate
unknown-variable expressions show more modest gains or even slight overhead in
initial computation stages, downstream processing consistently benefits
significantly. These findings underscore the importance of hash consing in
scaling symbolic computation and pave the way for future work integrating hash
consing with e-graphs for enhanced equivalence-aware expression sharing in
AI-driven pipelines.

</details>
