{"id": "2507.22069", "categories": ["cs.PL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22069", "abs": "https://arxiv.org/abs/2507.22069", "authors": ["Tobias Sesterhenn", "Ian Berlot-Attwell", "Janis Zenkner", "Christian Bartelt"], "title": "A Compute-Matched Re-Evaluation of TroVE on MATH", "comment": null, "summary": "Reusing established theorems and formulas is central to mathematical problem\nsolving, serving as essential building blocks for tackling increasingly complex\nchallenges. Recent work, TroVE, argues that code-generating Large Language\nModels (LLMs) can benefit similarly on the MATH benchmark by inducing and\nreusing higher-level toolboxes. By allocating computational budget across an\nensemble of three modes -- directly generating code, creating tools, and\nreusing tools -- TroVE claims to outperform a PRIMITIVE baseline that only\nperforms direct generation. However, recent analysis (Berlot-Attwell et al.,\n2024) casts doubt on these gains, noting that the tools created are often\ntrivial or rarely reused, suggesting that improvements may stem from\nself-consistency or self-correction. In this work, we re-evaluate TroVE on\nMATH, analyze the impact of each of its modes, and show that its benefit does\nnot come from these mechanisms, but simply from a higher computational budget\nspent for TroVE compared to PRIMITIVE. To this end, we also perform a small\ncorrection in the original implementation of TroVE's selection mechanism,\nboosting TroVE's performance on MATH by 3\\% in accuracy. After matching for\ncompute, the benefit of TroVE reduces to a marginal improvement of 1\\%,\nsuggesting that this toolbox approach does not provide a significant benefit on\nMATH.", "AI": {"tldr": "TroVE\u58f0\u79f0\u901a\u8fc7\u5de5\u5177\u7bb1\u65b9\u6cd5\u5728MATH\u57fa\u51c6\u4e0a\u4f18\u4e8e\u76f4\u63a5\u751f\u6210\u4ee3\u7801\u7684PRIMITIVE\uff0c\u4f46\u5206\u6790\u8868\u660e\u5176\u4f18\u52bf\u4e3b\u8981\u6765\u81ea\u66f4\u9ad8\u7684\u8ba1\u7b97\u9884\u7b97\uff0c\u800c\u975e\u5de5\u5177\u7bb1\u673a\u5236\u3002\u4fee\u6b63\u540e\uff0cTroVE\u4ec5\u6bd4PRIMITIVE\u9ad81%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u9a8c\u8bc1TroVE\u5de5\u5177\u7bb1\u65b9\u6cd5\u7684\u5b9e\u9645\u6548\u679c\uff0c\u5206\u6790\u5176\u6027\u80fd\u63d0\u5347\u662f\u5426\u6e90\u4e8e\u5de5\u5177\u7bb1\u673a\u5236\u6216\u8ba1\u7b97\u9884\u7b97\u3002", "method": "\u91cd\u65b0\u8bc4\u4f30TroVE\u5728MATH\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\uff0c\u5206\u6790\u5176\u4e09\u79cd\u6a21\u5f0f\uff08\u76f4\u63a5\u751f\u6210\u4ee3\u7801\u3001\u521b\u5efa\u5de5\u5177\u3001\u91cd\u7528\u5de5\u5177\uff09\u7684\u5f71\u54cd\uff0c\u5e76\u4fee\u6b63\u5176\u9009\u62e9\u673a\u5236\u3002", "result": "TroVE\u7684\u6539\u8fdb\u4e3b\u8981\u6765\u81ea\u66f4\u9ad8\u7684\u8ba1\u7b97\u9884\u7b97\uff0c\u4fee\u6b63\u540e\u4ec5\u6bd4PRIMITIVE\u9ad81%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u5de5\u5177\u7bb1\u65b9\u6cd5\u5728MATH\u57fa\u51c6\u4e0a\u672a\u63d0\u4f9b\u663e\u8457\u4f18\u52bf\uff0c\u6027\u80fd\u63d0\u5347\u4e3b\u8981\u6e90\u4e8e\u8ba1\u7b97\u9884\u7b97\u800c\u975e\u673a\u5236\u672c\u8eab\u3002"}}
{"id": "2507.22065", "categories": ["cs.SE", "cs.AI", "cs.CR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.22065", "abs": "https://arxiv.org/abs/2507.22065", "authors": ["Xiaotao Feng", "Xiaogang Zhu", "Kun Hu", "Jincheng Wang", "Yingjie Cao", "Guang Gong", "Jianfeng Pan"], "title": "Fuzzing: Randomness? Reasoning! Efficient Directed Fuzzing via Large Language Models", "comment": null, "summary": "Fuzzing is highly effective in detecting bugs due to the key contribution of\nrandomness. However, randomness significantly reduces the efficiency of\nfuzzing, causing it to cost days or weeks to expose bugs. Even though directed\nfuzzing reduces randomness by guiding fuzzing towards target buggy locations,\nthe dilemma of randomness still challenges directed fuzzers. Two critical\ncomponents, which are seeds and mutators, contain randomness and are closely\ntied to the conditions required for triggering bugs. Therefore, to address the\nchallenge of randomness, we propose to use large language models (LLMs) to\nremove the randomness in seeds and reduce the randomness in mutators. With\ntheir strong reasoning and code generation capabilities, LLMs can be used to\ngenerate reachable seeds that target pre-determined locations and to construct\nbug-specific mutators tailored for specific bugs. We propose RandLuzz, which\nintegrates LLMs and directed fuzzing, to improve the quality of seeds and\nmutators, resulting in efficient bug exposure. RandLuzz analyzes function call\nchain or functionality to guide LLMs in generating reachable seeds. To\nconstruct bug-specific mutators, RandLuzz uses LLMs to perform bug analysis,\nobtaining information such as bug causes and mutation suggestions, which\nfurther help generate code that performs bug-specific mutations. We evaluate\nRandLuzz by comparing it with four state-of-the-art directed fuzzers, AFLGo,\nBeacon, WindRanger, and SelectFuzz. With RandLuzz-generated seeds, the fuzzers\nachieve an average speedup ranging from 2.1$\\times$ to 4.8$\\times$ compared to\nusing widely-used initial seeds. Additionally, when evaluated on individual\nbugs, RandLuzz achieves up to a 2.7$\\times$ speedup compared to the\nsecond-fastest exposure. On 8 bugs, RandLuzz can even expose them within 60\nseconds.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRandLuzz\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u51cf\u5c11\u6a21\u7cca\u6d4b\u8bd5\u4e2d\u7684\u968f\u673a\u6027\uff0c\u901a\u8fc7\u751f\u6210\u53ef\u8fbe\u79cd\u5b50\u548c\u6784\u5efa\u9488\u5bf9\u7279\u5b9a\u9519\u8bef\u7684\u53d8\u5f02\u5668\uff0c\u663e\u8457\u63d0\u9ad8\u9519\u8bef\u68c0\u6d4b\u6548\u7387\u3002", "motivation": "\u6a21\u7cca\u6d4b\u8bd5\u4e2d\u7684\u968f\u673a\u6027\u867d\u7136\u6709\u52a9\u4e8e\u53d1\u73b0\u9519\u8bef\uff0c\u4f46\u964d\u4f4e\u4e86\u6548\u7387\u3002\u5373\u4f7f\u5b9a\u5411\u6a21\u7cca\u6d4b\u8bd5\u51cf\u5c11\u4e86\u968f\u673a\u6027\uff0c\u79cd\u5b50\u548c\u53d8\u5f02\u5668\u7684\u968f\u673a\u6027\u4ecd\u5f71\u54cd\u6548\u7387\u3002", "method": "\u5229\u7528LLMs\u751f\u6210\u53ef\u8fbe\u79cd\u5b50\u548c\u6784\u5efa\u9488\u5bf9\u7279\u5b9a\u9519\u8bef\u7684\u53d8\u5f02\u5668\uff0c\u7ed3\u5408\u5b9a\u5411\u6a21\u7cca\u6d4b\u8bd5\u5de5\u5177RandLuzz\u3002", "result": "RandLuzz\u5728\u56db\u79cd\u5b9a\u5411\u6a21\u7cca\u6d4b\u8bd5\u5de5\u5177\u4e0a\u5e73\u5747\u63d0\u901f2.1\u00d7\u81f34.8\u00d7\uff0c\u5e76\u572860\u79d2\u5185\u66b4\u97328\u4e2a\u9519\u8bef\u3002", "conclusion": "RandLuzz\u901a\u8fc7LLMs\u51cf\u5c11\u968f\u673a\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u7cca\u6d4b\u8bd5\u7684\u6548\u7387\u548c\u9519\u8bef\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2507.22070", "categories": ["cs.SE", "cs.CE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.22070", "abs": "https://arxiv.org/abs/2507.22070", "authors": ["Y. Du"], "title": "Automated Test Data Generation for Enterprise Protobuf Systems: A Metaclass-Enhanced Statistical Approach", "comment": "7 pages", "summary": "Large-scale enterprise systems utilizing Protocol Buffers (protobuf) present\nsignificant challenges for performance testing, particularly when targeting\nintermediate business interfaces with complex nested data structures.\nTraditional test data generation approaches are inadequate for handling the\nintricate hierarchical and graph-like structures inherent in enterprise\nprotobuf schemas. This paper presents a novel test data generation framework\nthat leverages Python's metaclass system for dynamic type enhancement and\nstatistical analysis of production logs for realistic value domain extraction.\nOur approach combines automatic schema introspection, statistical value\ndistribution analysis, and recursive descent algorithms for handling deeply\nnested structures. Experimental evaluation on three real-world enterprise\nsystems demonstrates up to 95\\% reduction in test data preparation time and\n80\\% improvement in test coverage compared to existing approaches. The\nframework successfully handles protobuf structures with up to 15 levels of\nnesting and generates comprehensive test suites containing over 100,000 test\ncases within seconds.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePython\u5143\u7c7b\u7cfb\u7edf\u548c\u751f\u4ea7\u65e5\u5fd7\u7edf\u8ba1\u5206\u6790\u7684\u6d4b\u8bd5\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f01\u4e1a\u7ea7protobuf\u7cfb\u7edf\u7684\u6d4b\u8bd5\u6548\u7387\u548c\u8986\u76d6\u7387\u3002", "motivation": "\u4f01\u4e1a\u7ea7protobuf\u7cfb\u7edf\u7684\u590d\u6742\u5d4c\u5957\u6570\u636e\u7ed3\u6784\u5bf9\u6027\u80fd\u6d4b\u8bd5\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u4f20\u7edf\u6d4b\u8bd5\u6570\u636e\u751f\u6210\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u3002", "method": "\u7ed3\u5408\u81ea\u52a8\u6a21\u5f0f\u5185\u7701\u3001\u7edf\u8ba1\u503c\u5206\u5e03\u5206\u6790\u548c\u9012\u5f52\u4e0b\u964d\u7b97\u6cd5\uff0c\u52a8\u6001\u589e\u5f3a\u7c7b\u578b\u5e76\u63d0\u53d6\u771f\u5b9e\u503c\u57df\u3002", "result": "\u5728\u4e09\u4e2a\u5b9e\u9645\u4f01\u4e1a\u7cfb\u7edf\u4e2d\uff0c\u6d4b\u8bd5\u6570\u636e\u51c6\u5907\u65f6\u95f4\u51cf\u5c1195%\uff0c\u6d4b\u8bd5\u8986\u76d6\u7387\u63d0\u9ad880%\uff0c\u652f\u630115\u5c42\u5d4c\u5957\u7ed3\u6784\u5e76\u5feb\u901f\u751f\u621010\u4e07+\u6d4b\u8bd5\u7528\u4f8b\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742protobuf\u7ed3\u6784\u7684\u6d4b\u8bd5\u6570\u636e\u751f\u6210\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6d4b\u8bd5\u6548\u7387\u3002"}}
{"id": "2507.22086", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.22086", "abs": "https://arxiv.org/abs/2507.22086", "authors": ["Honghua Dong", "Jiacheng Yang", "Xun Deng", "Yuhe Jiang", "Gennady Pekhimenko", "Fan Long", "Xujie Si"], "title": "TypyBench: Evaluating LLM Type Inference for Untyped Python Repositories", "comment": null, "summary": "Type inference for dynamic languages like Python is a persistent challenge in\nsoftware engineering. While large language models (LLMs) have shown promise in\ncode understanding, their type inference capabilities remain underexplored. We\nintroduce TypyBench, a benchmark designed to evaluate LLMs' type inference\nacross entire Python repositories. TypyBench features two novel metrics:\nTypeSim, which captures nuanced semantic relationships between predicted and\nground truth types, and TypeCheck, which assesses type consistency across\ncodebases. Our evaluation of various LLMs on a curated dataset of 50\nhigh-quality Python repositories reveals that, although LLMs achieve decent\nTypeSim scores, they struggle with complex nested types and exhibit significant\ntype consistency errors. These findings suggest that future research should\nshift focus from improving type similarity to addressing repository-level\nconsistency. TypyBench provides a foundation for this new direction, offering\ninsights into model performance across different type complexities and usage\ncontexts. Our code and data are available at\nhttps://github.com/typybench/typybench.", "AI": {"tldr": "TypyBench\u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728Python\u4ee3\u7801\u5e93\u4e2d\u8fdb\u884c\u7c7b\u578b\u63a8\u65ad\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63d0\u51fa\u4e86TypeSim\u548cTypeCheck\u4e24\u4e2a\u65b0\u6307\u6807\uff0c\u53d1\u73b0LLMs\u5728\u590d\u6742\u5d4c\u5957\u7c7b\u578b\u548c\u7c7b\u578b\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u52a8\u6001\u8bed\u8a00\uff08\u5982Python\uff09\u7684\u7c7b\u578b\u63a8\u65ad\u662f\u4e00\u4e2a\u6301\u7eed\u6311\u6218\uff0cLLMs\u5728\u6b64\u9886\u57df\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5f15\u5165TypyBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542bTypeSim\uff08\u6355\u6349\u9884\u6d4b\u7c7b\u578b\u4e0e\u771f\u5b9e\u7c7b\u578b\u95f4\u7684\u8bed\u4e49\u5173\u7cfb\uff09\u548cTypeCheck\uff08\u8bc4\u4f30\u4ee3\u7801\u5e93\u4e2d\u7684\u7c7b\u578b\u4e00\u81f4\u6027\uff09\u3002", "result": "LLMs\u5728TypeSim\u4e0a\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u5728\u590d\u6742\u5d4c\u5957\u7c7b\u578b\u548c\u7c7b\u578b\u4e00\u81f4\u6027\u4e0a\u5b58\u5728\u663e\u8457\u95ee\u9898\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u4ee3\u7801\u5e93\u7ea7\u522b\u7684\u7c7b\u578b\u4e00\u81f4\u6027\uff0c\u800c\u975e\u4ec5\u6539\u8fdb\u7c7b\u578b\u76f8\u4f3c\u6027\u3002TypyBench\u4e3a\u6b64\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.22063", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22063", "abs": "https://arxiv.org/abs/2507.22063", "authors": ["Wenjie Jacky Mo", "Qin Liu", "Xiaofei Wen", "Dongwon Jung", "Hadi Askari", "Wenxuan Zhou", "Zhe Zhao", "Muhao Chen"], "title": "RedCoder: Automated Multi-Turn Red Teaming for Code LLMs", "comment": null, "summary": "Large Language Models (LLMs) for code generation (i.e., Code LLMs) have\ndemonstrated impressive capabilities in AI-assisted software development and\ntesting. However, recent studies have shown that these models are prone to\ngenerating vulnerable or even malicious code under adversarial settings.\nExisting red-teaming approaches rely on extensive human effort, limiting their\nscalability and practicality, and generally overlook the interactive nature of\nreal-world AI-assisted programming, which often unfolds over multiple turns. To\nbridge these gaps, we present RedCoder, a red-teaming agent that engages victim\nmodels in multi-turn conversation to elicit vulnerable code. The pipeline to\nconstruct RedCoder begins with a multi-agent gaming process that simulates\nadversarial interactions, yielding a set of prototype conversations and an\narsenal of reusable attack strategies. We then fine-tune an LLM on these\nprototype conversations to serve as the backbone of RedCoder. Once deployed,\nRedCoder autonomously engages Code LLMs in multi-turn conversations,\ndynamically retrieving relevant strategies from the arsenal to steer the\ndialogue toward vulnerability-inducing outputs. Experiments across multiple\nCode LLMs show that our approach outperforms prior single-turn and multi-turn\nred-team methods in inducing vulnerabilities in code generation, offering a\nscalable and effective tool for evaluating the security boundaries of modern\ncode-generation systems.", "AI": {"tldr": "RedCoder\u662f\u4e00\u4e2a\u591a\u8f6e\u5bf9\u8bdd\u7ea2\u961f\u4ee3\u7406\uff0c\u901a\u8fc7\u6a21\u62df\u5bf9\u6297\u4ea4\u4e92\u548c\u591a\u8f6e\u5bf9\u8bdd\u8bf1\u5bfc\u4ee3\u7801\u751f\u6210\u6a21\u578b\u751f\u6210\u6f0f\u6d1e\u4ee3\u7801\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7ea2\u961f\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u4e14\u5ffd\u7565\u591a\u8f6e\u4ea4\u4e92\uff0c\u96be\u4ee5\u6269\u5c55\u548c\u5b9e\u7528\u3002", "method": "\u901a\u8fc7\u591a\u4ee3\u7406\u6e38\u620f\u6a21\u62df\u5bf9\u6297\u4ea4\u4e92\u751f\u6210\u539f\u578b\u5bf9\u8bdd\u548c\u653b\u51fb\u7b56\u7565\uff0c\u5fae\u8c03LLM\u4f5c\u4e3aRedCoder\u6838\u5fc3\uff0c\u52a8\u6001\u68c0\u7d22\u7b56\u7565\u8bf1\u5bfc\u6f0f\u6d1e\u3002", "result": "\u5b9e\u9a8c\u8868\u660eRedCoder\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u8bf1\u5bfc\u6f0f\u6d1e\u7684\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RedCoder\u4e3a\u8bc4\u4f30\u4ee3\u7801\u751f\u6210\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2507.22536", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2507.22536", "abs": "https://arxiv.org/abs/2507.22536", "authors": ["Marco Peressotti"], "title": "Infinite Traces by Finality: a Sheaf-Theoretic Approach", "comment": null, "summary": "Kleisli categories have long been recognised as a setting for modelling the\nlinear behaviour of various types of systems. However, the final coalgebra in\nsuch settings does not, in general, correspond to a fixed notion of linear\nsemantics. While there are well-understood conditions under which final\ncoalgebras capture finite trace semantics, a general account of infinite trace\nsemantics via finality has remained elusive. In this work, we present a\nsheaf-theoretic framework for infinite trace semantics in Kleisli categories\nthat systematically constructs final coalgebras capturing infinite traces. Our\napproach combines Kleisli categories, sheaves over ordinals, and guarded\n(co)recursion, enabling infinite behaviours to emerge from coherent families of\nfinite approximations via amalgamation. We introduce the notion of guarded\nbehavioural functor and show that, under mild conditions, their final\ncoalgebras directly characterise infinite traces.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c42\u8bba\u6846\u67b6\u7684\u65b9\u6cd5\uff0c\u5728Kleisli\u8303\u7574\u4e2d\u901a\u8fc7\u6700\u7ec8\u4f59\u4ee3\u6570\u6355\u83b7\u65e0\u9650\u8ff9\u8bed\u4e49\u3002", "motivation": "Kleisli\u8303\u7574\u4e2d\u7f3a\u4e4f\u5bf9\u65e0\u9650\u8ff9\u8bed\u4e49\u7684\u7cfb\u7edf\u6027\u63cf\u8ff0\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u9002\u7528\u4e8e\u6709\u9650\u8ff9\u8bed\u4e49\u3002", "method": "\u7ed3\u5408Kleisli\u8303\u7574\u3001\u5e8f\u6570\u4e0a\u7684\u5c42\u4ee5\u53ca\u5b88\u536b\uff08co\uff09\u9012\u5f52\uff0c\u901a\u8fc7\u878d\u5408\u6709\u9650\u903c\u8fd1\u6784\u9020\u65e0\u9650\u884c\u4e3a\u3002", "result": "\u5f15\u5165\u5b88\u536b\u884c\u4e3a\u51fd\u5b50\u6982\u5ff5\uff0c\u8bc1\u660e\u5728\u6e29\u548c\u6761\u4ef6\u4e0b\u5176\u6700\u7ec8\u4f59\u4ee3\u6570\u53ef\u76f4\u63a5\u8868\u5f81\u65e0\u9650\u8ff9\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aKleisli\u8303\u7574\u4e2d\u7684\u65e0\u9650\u8ff9\u8bed\u4e49\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.22064", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22064", "abs": "https://arxiv.org/abs/2507.22064", "authors": ["Michael Cohoon", "Debbie Furman"], "title": "Machine Learning Experiences: A story of learning AI for use in enterprise software testing that can be used by anyone", "comment": null, "summary": "This paper details the machine learning (ML) journey of a group of people\nfocused on software testing. It tells the story of how this group progressed\nthrough a ML workflow (similar to the CRISP-DM process). This workflow consists\nof the following steps and can be used by anyone applying ML techniques to a\nproject: gather the data; clean the data; perform feature engineering on the\ndata; splitting the data into two sets, one for training and one for testing;\nchoosing a machine learning model; training the model; testing the model and\nevaluating the model performance. By following this workflow, anyone can\neffectively apply ML to any project that they are doing.", "AI": {"tldr": "\u672c\u6587\u63cf\u8ff0\u4e86\u4e00\u7ec4\u4e13\u6ce8\u4e8e\u8f6f\u4ef6\u6d4b\u8bd5\u7684\u56e2\u961f\u5728\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u9879\u76ee\u4e2d\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7c7b\u4f3c\u4e8eCRISP-DM\u8fc7\u7a0b\uff0c\u5305\u62ec\u6570\u636e\u6536\u96c6\u3001\u6e05\u6d17\u3001\u7279\u5f81\u5de5\u7a0b\u3001\u6570\u636e\u5206\u5272\u3001\u6a21\u578b\u9009\u62e9\u3001\u8bad\u7ec3\u3001\u6d4b\u8bd5\u548c\u6027\u80fd\u8bc4\u4f30\u3002", "motivation": "\u5206\u4eab\u56e2\u961f\u5728\u8f6f\u4ef6\u6d4b\u8bd5\u4e2d\u5e94\u7528ML\u7684\u7ecf\u9a8c\uff0c\u4e3a\u5176\u4ed6\u9879\u76ee\u63d0\u4f9b\u53ef\u590d\u7528\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002", "method": "\u91c7\u7528\u7c7b\u4f3cCRISP-DM\u7684ML\u5de5\u4f5c\u6d41\u7a0b\uff0c\u6db5\u76d6\u4ece\u6570\u636e\u51c6\u5907\u5230\u6a21\u578b\u8bc4\u4f30\u7684\u5b8c\u6574\u6b65\u9aa4\u3002", "result": "\u901a\u8fc7\u9075\u5faa\u6b64\u5de5\u4f5c\u6d41\u7a0b\uff0c\u56e2\u961f\u6210\u529f\u5c06ML\u6280\u672f\u5e94\u7528\u4e8e\u8f6f\u4ef6\u6d4b\u8bd5\u9879\u76ee\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u6d41\u7a0b\u5177\u6709\u666e\u9002\u6027\uff0c\u53ef\u5e2e\u52a9\u4efb\u4f55\u4eba\u5728ML\u9879\u76ee\u4e2d\u9ad8\u6548\u5e94\u7528\u76f8\u5173\u6280\u672f\u3002"}}
{"id": "2507.22705", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2507.22705", "abs": "https://arxiv.org/abs/2507.22705", "authors": ["Kristina Sojakova", "Mihai Codescu", "Joshua Gancher"], "title": "Concrete Security Bounds for Simulation-Based Proofs of Multi-Party Computation Protocols", "comment": null, "summary": "The concrete security paradigm aims to give precise bounds on the probability\nthat an adversary can subvert a cryptographic mechanism. This is in contrast to\nasymptotic security, where the probability of subversion may be eventually\nsmall, but large enough in practice to be insecure. Fully satisfactory concrete\nsecurity bounds for Multi-Party Computation (MPC) protocols are difficult to\nattain, as they require reasoning about the running time of cryptographic\nadversaries and reductions. In this paper we close this gap by introducing a\nnew foundational approach that allows us to automatically compute concrete\nsecurity bounds for MPC protocols. We take inspiration from the meta-theory of\nIPDL, a prior approach for formally verified distributed cryptography, to\nsupport reasoning about the runtime of protocols and adversarial advantage. For\npractical proof developments, we implement our approach in Maude, an extensible\nlogic for equational rewriting. We carry out four case studies of concrete\nsecurity for simulation-based proofs. Most notably, we deliver the first formal\nverification of the GMW MPC protocol over N parties. To our knowledge, this is\nthe first time that formally verified concrete security bounds are computed for\na proof of an MPC protocol in the style of Universal Composability. Our tool\nprovides a layer of abstraction that allows the user to write proofs at a high\nlevel, which drastically simplifies the proof size. For comparison, a case\nstudy that in prior works required 2019 LoC only takes 567 LoC, thus reducing\nproof size by 72%", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u8ba1\u7b97\u591a\u65b9\u8ba1\u7b97\uff08MPC\uff09\u534f\u8bae\u7684\u5177\u4f53\u5b89\u5168\u6027\u8fb9\u754c\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "motivation": "\u4f20\u7edf\u6e10\u8fd1\u5b89\u5168\u6027\u65e0\u6cd5\u63d0\u4f9b\u5b9e\u9645\u5b89\u5168\u6027\u7684\u7cbe\u786e\u8fb9\u754c\uff0c\u800c\u5177\u4f53\u5b89\u5168\u6027\u8fb9\u754c\u96be\u4ee5\u624b\u52a8\u63a8\u5bfc\u3002", "method": "\u53d7IPDL\u542f\u53d1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u652f\u6301\u534f\u8bae\u8fd0\u884c\u65f6\u95f4\u548c\u5bf9\u624b\u4f18\u52bf\u7684\u63a8\u7406\uff0c\u5e76\u5728Maude\u4e2d\u5b9e\u73b0\u3002", "result": "\u901a\u8fc7\u56db\u4e2a\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86GMW MPC\u534f\u8bae\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u4e86\u8bc1\u660e\u4ee3\u7801\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aMPC\u534f\u8bae\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\u5de5\u5177\uff0c\u5927\u5e45\u7b80\u5316\u4e86\u8bc1\u660e\u8fc7\u7a0b\u3002"}}
{"id": "2507.22066", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.22066", "abs": "https://arxiv.org/abs/2507.22066", "authors": ["Dylan Manuel", "Paul Rad"], "title": "CodableLLM: Automating Decompiled and Source Code Mapping for LLM Dataset Generation", "comment": null, "summary": "The generation of large, high-quality datasets for code understanding and\ngeneration remains a significant challenge, particularly when aligning\ndecompiled binaries with their original source code. To address this, we\npresent CodableLLM, a Python framework designed to automate the creation and\ncuration of datasets by mapping decompiled functions to their corresponding\nsource functions. This process enhances the alignment between decompiled and\nsource code representations, facilitating the development of large language\nmodels (LLMs) capable of understanding and generating code across multiple\nabstraction levels. CodableLLM supports multiple programming languages and\nintegrates with existing decompilers and parsers to streamline dataset\ngeneration. This paper presents the design and implementation of CodableLLM,\nevaluates its performance in dataset creation, and compares it to existing\ntools in the field. The results demonstrate that CodableLLM offers a robust and\nefficient solution for generating datasets tailored for code-focused LLMS.", "AI": {"tldr": "CodableLLM\u662f\u4e00\u4e2aPython\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u751f\u6210\u548c\u6574\u7406\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5c06\u53cd\u7f16\u8bd1\u7684\u51fd\u6570\u6620\u5c04\u5230\u539f\u59cb\u6e90\u4ee3\u7801\u51fd\u6570\uff0c\u63d0\u5347\u53cd\u7f16\u8bd1\u4ee3\u7801\u4e0e\u6e90\u4ee3\u7801\u7684\u5bf9\u9f50\u8d28\u91cf\uff0c\u652f\u6301\u591a\u8bed\u8a00\u5e76\u96c6\u6210\u73b0\u6709\u5de5\u5177\u3002", "motivation": "\u89e3\u51b3\u4ee3\u7801\u7406\u89e3\u548c\u751f\u6210\u9886\u57df\u4e2d\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u751f\u6210\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u53cd\u7f16\u8bd1\u4e8c\u8fdb\u5236\u6587\u4ef6\u4e0e\u539f\u59cb\u6e90\u4ee3\u7801\u7684\u5bf9\u9f50\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0CodableLLM\u6846\u67b6\uff0c\u652f\u6301\u591a\u8bed\u8a00\uff0c\u96c6\u6210\u73b0\u6709\u53cd\u7f16\u8bd1\u5668\u548c\u89e3\u6790\u5668\uff0c\u81ea\u52a8\u5316\u6570\u636e\u96c6\u751f\u6210\u3002", "result": "CodableLLM\u5728\u6570\u636e\u96c6\u751f\u6210\u4e2d\u8868\u73b0\u9ad8\u6548\u4e14\u7a33\u5065\uff0c\u4f18\u4e8e\u73b0\u6709\u5de5\u5177\u3002", "conclusion": "CodableLLM\u4e3a\u4ee3\u7801\u5bfc\u5411\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6570\u636e\u96c6\u751f\u6210\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.22071", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.22071", "abs": "https://arxiv.org/abs/2507.22071", "authors": ["Niels Glodny"], "title": "Analyzing and Evaluating the Behavior of Git Diff and Merge", "comment": "Bachelor's thesis", "summary": "Despite being widely used, the algorithms that enable collaboration with Git\nare not well understood. The diff and merge algorithms are particularly\ninteresting, as they could be applied in other contexts. In this thesis, I\ndocument the main functionalities of Git: how diffs are computed, how they are\nused to run merges, and how merges enable more complex operations. In the\nprocess, I show multiple unexpected behaviors in Git, including the following:\nThe histogram diff algorithm has pathological cases where a single-line change\ncan cause the entire rest of the file to be marked as changed. The default\nmerge strategy (ort) can result in merges requiring exponential time in the\nnumber of commits in the history. Merges and rebases are not commutative, and\neven when merges do not result in a conflict, the result is not specified but\ndepends on the diff algorithm used. And finally, sometimes when two sides of a\nmerge add different lines at the same position, the result is not a conflict,\nbut a merge containing both changes after each other, in arbitrary order.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86Git\u4e2ddiff\u548cmerge\u7b97\u6cd5\u7684\u529f\u80fd\u53ca\u5176\u610f\u5916\u884c\u4e3a\uff0c\u5305\u62ec\u5355\u884c\u66f4\u6539\u5bfc\u81f4\u6574\u4e2a\u6587\u4ef6\u6807\u8bb0\u4e3a\u66f4\u6539\u3001\u9ed8\u8ba4merge\u7b56\u7565\u53ef\u80fd\u5bfc\u81f4\u6307\u6570\u65f6\u95f4\u6d88\u8017\u7b49\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1Git\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u534f\u4f5c\u7b97\u6cd5\uff08\u5982diff\u548cmerge\uff09\u7684\u5177\u4f53\u884c\u4e3a\u5e76\u672a\u88ab\u5145\u5206\u7406\u89e3\uff0c\u8fd9\u4e9b\u7b97\u6cd5\u5728\u5176\u4ed6\u573a\u666f\u4e2d\u4e5f\u6709\u6f5c\u5728\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u6587\u6863\u5316Git\u7684\u4e3b\u8981\u529f\u80fd\uff08\u5982diff\u8ba1\u7b97\u3001merge\u8fd0\u884c\u65b9\u5f0f\uff09\u5e76\u5206\u6790\u5176\u884c\u4e3a\uff0c\u63ed\u793a\u4e86\u7b97\u6cd5\u4e2d\u7684\u591a\u4e2a\u610f\u5916\u73b0\u8c61\u3002", "result": "\u53d1\u73b0Git\u7684histogram diff\u7b97\u6cd5\u5b58\u5728\u6781\u7aef\u60c5\u51b5\uff0c\u9ed8\u8ba4merge\u7b56\u7565\uff08ort\uff09\u53ef\u80fd\u6d88\u8017\u6307\u6570\u65f6\u95f4\uff0cmerge\u548crebase\u4e0d\u6ee1\u8db3\u4ea4\u6362\u5f8b\uff0c\u4e14merge\u7ed3\u679c\u4f9d\u8d56\u4e8ediff\u7b97\u6cd5\u3002", "conclusion": "Git\u7684diff\u548cmerge\u7b97\u6cd5\u5b58\u5728\u672a\u9884\u671f\u7684\u884c\u4e3a\uff0c\u8fd9\u4e9b\u53d1\u73b0\u5bf9\u7406\u89e3\u548c\u6539\u8fdbGit\u53ca\u5176\u4ed6\u7c7b\u4f3c\u5de5\u5177\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2507.22080", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.22080", "abs": "https://arxiv.org/abs/2507.22080", "authors": ["Qiushi Sun", "Jinyang Gong", "Lei Li", "Qipeng Guo", "Fei Yuan"], "title": "CodeEvo: Interaction-Driven Synthesis of Code-centric Data through Hybrid and Iterative Feedback", "comment": "Work in progress", "summary": "Acquiring high-quality instruction-code pairs is essential for training Large\nLanguage Models (LLMs) for code generation. Manually curated data is expensive\nand inherently limited in scale, motivating the development of code-centric\nsynthesis methods. Yet, current approaches either focus on augmenting existing\ncode or rely on predefined heuristics, both lacking rigorous data validation,\nwhich results in synthetic data that is ungrounded, repetitive, or overly\nsimplistic. Inspired by collaborative programming practices, we propose\nCodeEvo, a framework that synthesizes code data through iterative interactions\nbetween two LLM agents: a Coder, which generates candidate code and test cases\nbased on given instructions, and a Reviewer, which guides the synthesis process\nby producing new instructions and feedback. We further introduce a hybrid\nfeedback mechanism that combines compiler determinism with the generative\nflexibility of agents, enabling automatic quality control throughout synthesis.\nExtensive experiments demonstrate that models fine-tuned on CodeEvo data\nsignificantly outperform established baselines across code generation\nbenchmarks with various difficulties. In-depth analyses further provide\ninsights from multiple perspectives into effective code-centric data synthesis.", "AI": {"tldr": "CodeEvo\u6846\u67b6\u901a\u8fc7\u4e24\u4e2aLLM\u4ee3\u7406\uff08Coder\u548cReviewer\uff09\u7684\u8fed\u4ee3\u4ea4\u4e92\u5408\u6210\u9ad8\u8d28\u91cf\u4ee3\u7801\u6570\u636e\uff0c\u7ed3\u5408\u7f16\u8bd1\u5668\u786e\u5b9a\u6027\u548c\u751f\u6210\u7075\u6d3b\u6027\uff0c\u663e\u8457\u63d0\u5347\u4ee3\u7801\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u624b\u52a8\u83b7\u53d6\u9ad8\u8d28\u91cf\u7684\u6307\u4ee4-\u4ee3\u7801\u5bf9\u6210\u672c\u9ad8\u4e14\u89c4\u6a21\u6709\u9650\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u4e25\u683c\u7684\u6570\u636e\u9a8c\u8bc1\uff0c\u5bfc\u81f4\u5408\u6210\u6570\u636e\u8d28\u91cf\u4e0d\u4f73\u3002", "method": "CodeEvo\u6846\u67b6\u901a\u8fc7Coder\u751f\u6210\u5019\u9009\u4ee3\u7801\u548c\u6d4b\u8bd5\u7528\u4f8b\uff0cReviewer\u63d0\u4f9b\u65b0\u6307\u4ee4\u548c\u53cd\u9988\uff0c\u7ed3\u5408\u6df7\u5408\u53cd\u9988\u673a\u5236\u8fdb\u884c\u8d28\u91cf\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8eCodeEvo\u6570\u636e\u5fae\u8c03\u7684\u6a21\u578b\u5728\u591a\u79cd\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "CodeEvo\u4e3a\u4ee3\u7801\u4e2d\u5fc3\u6570\u636e\u5408\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u591a\u89d2\u5ea6\u5206\u6790\u63d0\u4f9b\u4e86\u6df1\u5165\u89c1\u89e3\u3002"}}
{"id": "2507.22085", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.22085", "abs": "https://arxiv.org/abs/2507.22085", "authors": ["Vaani Goenka", "Aalok D. Thakkar"], "title": "BOOP: Write Right Code", "comment": null, "summary": "Novice programmers frequently adopt a syntax-specific and test-case-driven\napproach, writing code first and adjusting until programs compile and test\ncases pass, rather than developing correct solutions through systematic\nreasoning. AI coding tools exacerbate this challenge by providing syntactically\ncorrect but conceptually flawed solutions. In this paper, we introduce BOOP\n(Blueprint, Operations, OCaml, Proof), a structured framework requiring four\nmandatory phases: formal specification, language-agnostic algorithm\ndevelopment, implementation, and correctness proof. This shifts focus from\n``making code work'' to understanding why code is correct.\n  BOOP was implemented at our institution using a VS Code extension and\npreprocessor that enforces constraints and identifies counterproductive\npatterns. Initial evaluation shows improved algorithmic reasoning and reduced\ntrial-and-error debugging. Students reported better edge case understanding and\nproblem decomposition, though some initially found the format verbose.\nInstructors observed stronger foundational skills compared to traditional\napproaches.", "AI": {"tldr": "BOOP\u6846\u67b6\u901a\u8fc7\u5f3a\u5236\u56db\u4e2a\u9636\u6bb5\uff08\u89c4\u8303\u3001\u7b97\u6cd5\u5f00\u53d1\u3001\u5b9e\u73b0\u548c\u8bc1\u660e\uff09\u6765\u6539\u5584\u65b0\u624b\u7a0b\u5e8f\u5458\u7684\u7cfb\u7edf\u5316\u63a8\u7406\u80fd\u529b\uff0c\u51cf\u5c11\u8bd5\u9519\u8c03\u8bd5\u3002", "motivation": "\u65b0\u624b\u7a0b\u5e8f\u5458\u5e38\u4f9d\u8d56\u8bed\u6cd5\u548c\u6d4b\u8bd5\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u800cAI\u5de5\u5177\u53ef\u80fd\u63d0\u4f9b\u8bed\u6cd5\u6b63\u786e\u4f46\u6982\u5ff5\u9519\u8bef\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5bfc\u81f4\u7f3a\u4e4f\u7cfb\u7edf\u5316\u63a8\u7406\u3002", "method": "BOOP\u6846\u67b6\u5305\u542b\u56db\u4e2a\u5f3a\u5236\u9636\u6bb5\uff1a\u5f62\u5f0f\u5316\u89c4\u8303\u3001\u8bed\u8a00\u65e0\u5173\u7b97\u6cd5\u5f00\u53d1\u3001\u5b9e\u73b0\u548c\u6b63\u786e\u6027\u8bc1\u660e\uff0c\u901a\u8fc7VS Code\u6269\u5c55\u5f3a\u5236\u6267\u884c\u3002", "result": "\u521d\u6b65\u8bc4\u4f30\u663e\u793a\u5b66\u751f\u7b97\u6cd5\u63a8\u7406\u80fd\u529b\u63d0\u5347\uff0c\u8bd5\u9519\u51cf\u5c11\uff0c\u5bf9\u8fb9\u754c\u6761\u4ef6\u548c\u95ee\u9898\u5206\u89e3\u7684\u7406\u89e3\u589e\u5f3a\u3002", "conclusion": "BOOP\u6846\u67b6\u6709\u6548\u63d0\u5347\u5b66\u751f\u7684\u57fa\u7840\u6280\u80fd\uff0c\u5c3d\u7ba1\u521d\u671f\u53ef\u80fd\u663e\u5f97\u5197\u957f\uff0c\u4f46\u6548\u679c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2507.22223", "categories": ["cs.SE", "D.2; D.2.4; D.4.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.22223", "abs": "https://arxiv.org/abs/2507.22223", "authors": ["Kiana Kiashemshaki", "Mohammad Jalili Torkamani", "Negin Mahmoudi"], "title": "Secure coding for web applications: Frameworks, challenges, and the role of LLMs", "comment": "11 pages, 5 figures, 3 tables, 6 listings", "summary": "Secure coding is a critical yet often overlooked practice in software\ndevelopment. Despite extensive awareness efforts, real-world adoption remains\ninconsistent due to organizational, educational, and technical barriers. This\npaper provides a comprehensive review of secure coding practices across major\nframeworks and domains, including web development, DevSecOps, and cloud\nsecurity. It introduces a structured framework comparison and categorizes\nthreats aligned with the OWASP Top 10. Additionally, we explore the rising role\nof Large Language Models (LLMs) in evaluating and recommending secure code,\npresenting a reproducible case study across four major vulnerability types.\nThis paper offers practical insights for researchers, developers, and educators\non integrating secure coding into real-world development processes.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5b89\u5168\u7f16\u7801\u5b9e\u8df5\uff0c\u6bd4\u8f83\u4e86\u4e3b\u8981\u6846\u67b6\u548c\u9886\u57df\uff0c\u63a2\u8ba8\u4e86LLMs\u5728\u5b89\u5168\u4ee3\u7801\u8bc4\u4f30\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u9645\u6848\u4f8b\u7814\u7a76\u3002", "motivation": "\u5c3d\u7ba1\u5b89\u5168\u7f16\u7801\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u5b9e\u9645\u5e94\u7528\u4ecd\u56e0\u7ec4\u7ec7\u3001\u6559\u80b2\u548c\u6280\u672f\u969c\u788d\u800c\u4e0d\u4e00\u81f4\u3002", "method": "\u901a\u8fc7\u6846\u67b6\u6bd4\u8f83\u3001\u5a01\u80c1\u5206\u7c7b\uff08\u57fa\u4e8eOWASP Top 10\uff09\u548cLLMs\u7684\u5e94\u7528\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u6db5\u76d6\u56db\u79cd\u4e3b\u8981\u6f0f\u6d1e\u7c7b\u578b\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u5f00\u53d1\u8005\u63d0\u4f9b\u5b9e\u7528\u89c1\u89e3\u3002", "conclusion": "\u672c\u6587\u4e3a\u5c06\u5b89\u5168\u7f16\u7801\u6574\u5408\u5230\u5b9e\u9645\u5f00\u53d1\u6d41\u7a0b\u4e2d\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2507.22324", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22324", "abs": "https://arxiv.org/abs/2507.22324", "authors": ["Cameron S. Movassaghi", "Amanda Momenzadeh", "Jesse G. Meyer"], "title": "From Articles to Code: On-Demand Generation of Core Algorithms from Scientific Publications", "comment": null, "summary": "Maintaining software packages imposes significant costs due to dependency\nmanagement, bug fixes, and versioning. We show that rich method descriptions in\nscientific publications can serve as standalone specifications for modern large\nlanguage models (LLMs), enabling on-demand code generation that could supplant\nhuman-maintained libraries. We benchmark state-of-the-art models\n(GPT-o4-mini-high, Gemini Pro 2.5, Claude Sonnet 4) by tasking them with\nimplementing a diverse set of core algorithms drawn from original publications.\nOur results demonstrate that current LLMs can reliably reproduce package\nfunctionality with performance indistinguishable from conventional libraries.\nThese findings foreshadow a paradigm shift toward flexible, on-demand code\ngeneration and away from static, human-maintained packages, which will result\nin reduced maintenance overhead by leveraging published articles as sufficient\ncontext for the automated implementation of analytical workflows.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5229\u7528\u79d1\u5b66\u6587\u732e\u4e2d\u7684\u65b9\u6cd5\u63cf\u8ff0\u4f5c\u4e3aLLM\u7684\u72ec\u7acb\u89c4\u8303\uff0c\u5b9e\u73b0\u6309\u9700\u4ee3\u7801\u751f\u6210\uff0c\u53ef\u80fd\u66ff\u4ee3\u4eba\u5de5\u7ef4\u62a4\u7684\u8f6f\u4ef6\u5305\u3002", "motivation": "\u51cf\u5c11\u8f6f\u4ef6\u5305\u7ef4\u62a4\u6210\u672c\uff08\u5982\u4f9d\u8d56\u7ba1\u7406\u3001\u9519\u8bef\u4fee\u590d\u548c\u7248\u672c\u63a7\u5236\uff09\uff0c\u63a2\u7d22LLM\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u8ba9\u5148\u8fdbLLM\uff08\u5982GPT-4-mini-high\u3001Gemini Pro 2.5\u3001Claude Sonnet 4\uff09\u6839\u636e\u79d1\u5b66\u6587\u732e\u5b9e\u73b0\u6838\u5fc3\u7b97\u6cd5\uff0c\u5e76\u8fdb\u884c\u6027\u80fd\u6d4b\u8bd5\u3002", "result": "\u5f53\u524dLLM\u80fd\u53ef\u9760\u751f\u6210\u529f\u80fd\u4e0e\u5e38\u89c4\u5e93\u65e0\u5f02\u7684\u4ee3\u7801\uff0c\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "\u672a\u6765\u53ef\u80fd\u8f6c\u5411\u7075\u6d3b\u7684\u6309\u9700\u4ee3\u7801\u751f\u6210\uff0c\u51cf\u5c11\u5bf9\u9759\u6001\u4eba\u5de5\u7ef4\u62a4\u5305\u7684\u4f9d\u8d56\uff0c\u964d\u4f4e\u7ef4\u62a4\u6210\u672c\u3002"}}
{"id": "2507.22414", "categories": ["cs.SE", "D.2; I.2"], "pdf": "https://arxiv.org/pdf/2507.22414", "abs": "https://arxiv.org/abs/2507.22414", "authors": ["Sungmin Kang", "Haifeng Ruan", "Abhik Roychoudhury"], "title": "AutoCodeSherpa: Symbolic Explanations in AI Coding Agents", "comment": null, "summary": "Large Language Model (LLM) agents autonomously use external tools on top of\none or more LLMs to accomplish specific tasks. Lately LLM agents for software\nengineering tasks have become popular. These agents can benefit from the use of\nprogram analysis tools working on program representations. This is demonstrated\nby existing agentic AI solutions such as AutoCodeRover or SpecRover which\nperform automated program repair. Specifically the goal of these works is to\nuse program analysis to improve the patch quality. These agents are currently\nbeing used to automatically fix static analysis issues from the widely used\nSonarQube static analyzer.\n  Nevertheless, for the agents to be deployed in a production environment,\nagents need to suggest software artifacts, such as patches, with evidence and\nwith high confidence. In this work, we provide a workflow where an agent\nprovides explanations of the bug in the form of symbolic formulae. The\nexplanations are in the form of input conditions, infection conditions and\noutput conditions, implemented as property based tests (PBT) and\nprogram-internal symbolic expressions. These can help in human developer\ncognition of the agent outputs as well as in achieving completely automated\nagentic workflows for software. The human developer can benefit from the input\ncondition, represented as a PBT, to generate various concrete inputs showing a\ngiven issue. Furthermore, since the PBTs are executable, our explanations are\nexecutable as well. We can thus also use the explanations in a completely\nautomated issue resolution environment for accepting or rejecting the patches\nthat are suggested by patching agents such as AutoCodeRover. Finally, as\nagentic AI approaches continue to develop, the program analysis driven\nexplanations can be provided to other LLM-based repair techniques such as\nAgentless to improve their output.", "AI": {"tldr": "LLM\u4ee3\u7406\u901a\u8fc7\u7a0b\u5e8f\u5206\u6790\u5de5\u5177\u63d0\u5347\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u8865\u4e01\u8d28\u91cf\uff0c\u5e76\u63d0\u4f9b\u7b26\u53f7\u516c\u5f0f\u5f62\u5f0f\u7684\u89e3\u91ca\u4ee5\u589e\u5f3a\u5f00\u53d1\u8005\u7406\u89e3\u548c\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u7a0b\u3002", "motivation": "\u4e3a\u4e86\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u90e8\u7f72LLM\u4ee3\u7406\uff0c\u9700\u8981\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u8f6f\u4ef6\u5de5\u4ef6\uff08\u5982\u8865\u4e01\uff09\u53ca\u5176\u89e3\u91ca\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5de5\u4f5c\u6d41\uff0c\u4ee3\u7406\u901a\u8fc7\u5c5e\u6027\u6d4b\u8bd5\uff08PBT\uff09\u548c\u7a0b\u5e8f\u5185\u90e8\u7b26\u53f7\u8868\u8fbe\u5f0f\u63d0\u4f9b\u9519\u8bef\u89e3\u91ca\uff08\u8f93\u5165\u3001\u611f\u67d3\u548c\u8f93\u51fa\u6761\u4ef6\uff09\u3002", "result": "\u89e3\u91ca\u53ef\u6267\u884c\uff0c\u652f\u6301\u5f00\u53d1\u8005\u751f\u6210\u5177\u4f53\u8f93\u5165\u4ee5\u91cd\u73b0\u95ee\u9898\uff0c\u5e76\u7528\u4e8e\u81ea\u52a8\u5316\u8865\u4e01\u9a8c\u8bc1\u3002", "conclusion": "\u7a0b\u5e8f\u5206\u6790\u9a71\u52a8\u7684\u89e3\u91ca\u53ef\u63d0\u5347LLM\u4fee\u590d\u6280\u672f\u7684\u8f93\u51fa\u8d28\u91cf\uff0c\u9002\u7528\u4e8e\u81ea\u52a8\u5316\u73af\u5883\u3002"}}
{"id": "2507.22442", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.22442", "abs": "https://arxiv.org/abs/2507.22442", "authors": ["Yukai Zhao", "Shaohua Wang", "Jue Wang", "Xing Hu", "Xin Xia"], "title": "Ensemble Fuzzing with Dynamic Resource Scheduling and Multidimensional Seed Evaluation", "comment": "first submit", "summary": "Fuzzing is widely used for detecting bugs and vulnerabilities, with various\ntechniques proposed to enhance its effectiveness. To combine the advantages of\nmultiple technologies, researchers proposed ensemble fuzzing, which integrates\nmultiple base fuzzers. Despite promising results, state-of-the-art ensemble\nfuzzing techniques face limitations in resource scheduling and performance\nevaluation, leading to unnecessary resource waste. In this paper, we propose\nLegion, a novel ensemble fuzzing framework that dynamically schedules resources\nduring the ensemble fuzzing campaign. We designed a novel resource scheduling\nalgorithm based on the upper confidence bound algorithm to reduce the resource\nconsumption of ineffective base fuzzers. Additionally, we introduce a\nmultidimensional seed evaluation strategy, which considers multiple metrics to\nachieve more comprehensive fine-grained performance evaluation. We implemented\nLegion as a prototype tool and evaluated its effectiveness on Google's\nfuzzer-test-suite as well as real-world open-source projects. Results show that\nLegion outperforms existing state-of-the-art base fuzzers and ensemble fuzzing\ntechniques, detecting 20 vulnerabilities in real-world open-source\nprojects-five previously unknown and three classified as CVEs.", "AI": {"tldr": "Legion\u662f\u4e00\u4e2a\u65b0\u578b\u7684\u96c6\u6210\u6a21\u7cca\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8d44\u6e90\u8c03\u5ea6\u548c\u591a\u7ef4\u79cd\u5b50\u8bc4\u4f30\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u7cca\u6d4b\u8bd5\u7684\u6548\u7387\u548c\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u96c6\u6210\u6a21\u7cca\u6d4b\u8bd5\u6280\u672f\u5728\u8d44\u6e90\u8c03\u5ea6\u548c\u6027\u80fd\u8bc4\u4f30\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5bfc\u81f4\u8d44\u6e90\u6d6a\u8d39\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4e0a\u7f6e\u4fe1\u754c\u7b97\u6cd5\u7684\u8d44\u6e90\u8c03\u5ea6\u7b97\u6cd5\u548c\u591a\u7ef4\u79cd\u5b50\u8bc4\u4f30\u7b56\u7565\uff0c\u52a8\u6001\u4f18\u5316\u8d44\u6e90\u5206\u914d\u3002", "result": "\u5728Google\u6a21\u7cca\u6d4b\u8bd5\u5957\u4ef6\u548c\u5b9e\u9645\u5f00\u6e90\u9879\u76ee\u4e2d\uff0cLegion\u68c0\u6d4b\u523020\u4e2a\u6f0f\u6d1e\uff0c\u5305\u62ec5\u4e2a\u672a\u77e5\u6f0f\u6d1e\u548c3\u4e2aCVE\u6f0f\u6d1e\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "Legion\u901a\u8fc7\u52a8\u6001\u8d44\u6e90\u8c03\u5ea6\u548c\u5168\u9762\u6027\u80fd\u8bc4\u4f30\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96c6\u6210\u6a21\u7cca\u6d4b\u8bd5\u7684\u6548\u679c\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.22538", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.22538", "abs": "https://arxiv.org/abs/2507.22538", "authors": ["Matilde Gargiani", "Robin Sieber", "Philip Pawlowsky", "John Lygeros"], "title": "Inside madupite: Technical Design and Performance", "comment": null, "summary": "In this work, we introduce and benchmark madupite, a newly proposed\nhigh-performance solver designed for large-scale discounted infinite-horizon\nMarkov decision processes with finite state and action spaces. After a brief\noverview of the class of mathematical optimization methods on which madupite\nrelies, we provide details on implementation choices, technical design and\ndeployment. We then demonstrate its scalability and efficiency by showcasing\nits performance on the solution of Markov decision processes arising from\ndifferent application areas, including epidemiology and classical control.\nMadupite sets a new standard as, to the best of our knowledge, it is the only\nsolver capable of efficiently computing exact solutions for large-scale Markov\ndecision processes, even when these exceed the memory capacity of modern\nlaptops and operate in near-undiscounted settings. This is possible as madupite\ncan work in a fully distributed manner and therefore leverage the memory\nstorage and computation capabilities of modern high-performance computing\nclusters. This key feature enables the solver to efficiently handle problems of\nmedium to large size in an exact manner instead of necessarily resorting to\nfunction approximations. Moreover, madupite is unique in allowing users to\ncustomize the solution algorithm to better exploit the specific structure of\ntheir problem, significantly accelerating convergence especially in\nlarge-discount factor settings. Overall, madupite represents a significant\nadvancement, offering unmatched scalability and flexibility in solving\nlarge-scale Markov decision processes.", "AI": {"tldr": "Madupite\u662f\u4e00\u79cd\u65b0\u578b\u9ad8\u6027\u80fd\u6c42\u89e3\u5668\uff0c\u4e13\u4e3a\u5927\u89c4\u6a21\u6298\u6263\u65e0\u9650\u65f6\u57df\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u8bbe\u8ba1\uff0c\u5177\u6709\u5206\u5e03\u5f0f\u8ba1\u7b97\u548c\u5b9a\u5236\u5316\u7b97\u6cd5\u7684\u7279\u70b9\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6c42\u89e3\u5668\u65e0\u6cd5\u9ad8\u6548\u5904\u7406\u5927\u89c4\u6a21\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u5185\u5b58\u53d7\u9650\u548c\u63a5\u8fd1\u65e0\u6298\u6263\u8bbe\u7f6e\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u57fa\u4e8e\u6570\u5b66\u4f18\u5316\u65b9\u6cd5\uff0c\u91c7\u7528\u5206\u5e03\u5f0f\u8ba1\u7b97\u6280\u672f\uff0c\u652f\u6301\u7528\u6237\u5b9a\u5236\u7b97\u6cd5\u4ee5\u52a0\u901f\u6536\u655b\u3002", "result": "Madupite\u80fd\u591f\u9ad8\u6548\u8ba1\u7b97\u7cbe\u786e\u89e3\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u95ee\u9898\uff0c\u5e76\u5728\u6d41\u884c\u75c5\u5b66\u548c\u63a7\u5236\u7b49\u9886\u57df\u5c55\u793a\u4e86\u5353\u8d8a\u6027\u80fd\u3002", "conclusion": "Madupite\u5728\u89e3\u51b3\u5927\u89c4\u6a21\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u63d0\u4f9b\u4e86\u524d\u6240\u672a\u6709\u7684\u53ef\u6269\u5c55\u6027\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2507.22580", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22580", "abs": "https://arxiv.org/abs/2507.22580", "authors": ["Marcos Fuster-Pena", "David de-Fitero-Dominguez", "Antonio Garcia-Cabot", "Eva Garcia-Lopez"], "title": "RePaCA: Leveraging Reasoning Large Language Models for Static Automated Patch Correctness Assessment", "comment": null, "summary": "Automated Program Repair (APR) seeks to automatically correct software bugs\nwithout requiring human intervention. However, existing tools tend to generate\npatches that satisfy test cases without fixing the underlying bug, those are\nknown as overfitting patches. To address this issue, Automated Patch\nCorrectness Assessment (APCA) attempts to identify overfitting patches\ngenerated by APR tools. It can be solved as a static approach, meaning that no\nadditional information is needed beyond the original and fixed code snippets.\nCurrent static techniques often struggle with reliability, flexibility and\ntransparency. To address these issues, we introduce RePaCA, a novel static APCA\ntechnique that leverages Large Language Models (LLMs) specialized in thinking\ntasks. Our model is prompted with both buggy and fixed code snippets and guided\nto generate a Chain of Thought that analyses code differences, reasons about\nhow the patch addresses the root cause, and ultimately provides a binary\nclassification: correct or overfitting. To enhance these reasoning capabilities\nfor the APCA task specifically, the LLM is finetuned using Reinforcement\nLearning with the Group Relative Policy Optimization algorithm. When evaluated\non a standard Defects4J-derived test, our approach achieves state-of-the-art\nperformance, with 83.1% accuracy and an 84.8% F1-score. Furthermore, our model\ndemonstrates superior generalization capabilities when trained on different\ndatasets, outperforming the leading technique. This reasoning capability also\nprovides enhanced explainability for the patch assessment. These findings\nunderscore the considerable promise of finetuned, reasoning LLMs to advance\nstatic APCA by enhancing accuracy, generalization, and explainability.", "AI": {"tldr": "RePaCA\u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u9759\u6001APCA\u6280\u672f\uff0c\u901a\u8fc7\u5206\u6790\u4ee3\u7801\u5dee\u5f02\u548c\u63a8\u7406\u8865\u4e01\u7684\u6b63\u786e\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u9759\u6001APCA\u6280\u672f\u5728\u53ef\u9760\u6027\u3001\u7075\u6d3b\u6027\u548c\u900f\u660e\u5ea6\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u8bc6\u522b\u8fc7\u62df\u5408\u8865\u4e01\u3002", "method": "\u5229\u7528\u4e13\u95e8\u7528\u4e8e\u601d\u8003\u4efb\u52a1\u7684LLM\uff0c\u901a\u8fc7\u63d0\u793a\u8f93\u5165\u9519\u8bef\u548c\u4fee\u590d\u540e\u7684\u4ee3\u7801\u7247\u6bb5\uff0c\u751f\u6210\u601d\u7ef4\u94fe\u5206\u6790\u8865\u4e01\uff0c\u5e76\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728\u6807\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523083.1%\u7684\u51c6\u786e\u7387\u548c84.8%\u7684F1\u5206\u6570\uff0c\u5e76\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u5fae\u8c03\u540e\u7684LLM\u5728\u9759\u6001APCA\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2507.22610", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22610", "abs": "https://arxiv.org/abs/2507.22610", "authors": ["Ali Asgari", "Milan de Koning", "Pouria Derakhshanfar", "Annibale Panichella"], "title": "Metamorphic Testing of Deep Code Models: A Systematic Literature Review", "comment": null, "summary": "Large language models and deep learning models designed for code intelligence\nhave revolutionized the software engineering field due to their ability to\nperform various code-related tasks. These models can process source code and\nsoftware artifacts with high accuracy in tasks such as code completion, defect\ndetection, and code summarization; therefore, they can potentially become an\nintegral part of modern software engineering practices. Despite these\ncapabilities, robustness remains a critical quality attribute for deep-code\nmodels as they may produce different results under varied and adversarial\nconditions (e.g., variable renaming). Metamorphic testing has become a widely\nused approach to evaluate models' robustness by applying semantic-preserving\ntransformations to input programs and analyzing the stability of model outputs.\nWhile prior research has explored testing deep learning models, this systematic\nliterature review focuses specifically on metamorphic testing for deep code\nmodels. By studying 45 primary papers, we analyze the transformations,\ntechniques, and evaluation methods used to assess robustness. Our review\nsummarizes the current landscape, identifying frequently evaluated models,\nprogramming tasks, datasets, target languages, and evaluation metrics, and\nhighlights key challenges and future directions for advancing the field.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u53d8\u5f62\u6d4b\u8bd5\u5728\u6df1\u5ea6\u4ee3\u7801\u6a21\u578b\u4e2d\u7684\u5e94\u7528\uff0c\u5206\u6790\u4e8645\u7bc7\u8bba\u6587\uff0c\u603b\u7ed3\u4e86\u5f53\u524d\u7684\u7814\u7a76\u73b0\u72b6\u3001\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u6df1\u5ea6\u4ee3\u7801\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u9c81\u68d2\u6027\u4ecd\u9700\u9a8c\u8bc1\uff0c\u5c24\u5176\u662f\u5728\u5bf9\u6297\u6027\u6761\u4ef6\u4e0b\u3002\u53d8\u5f62\u6d4b\u8bd5\u6210\u4e3a\u8bc4\u4f30\u6a21\u578b\u9c81\u68d2\u6027\u7684\u91cd\u8981\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff0c\u5206\u679045\u7bc7\u8bba\u6587\u4e2d\u7684\u53d8\u5f62\u6d4b\u8bd5\u6280\u672f\u3001\u8bc4\u4f30\u65b9\u6cd5\u548c\u5e94\u7528\u573a\u666f\u3002", "result": "\u603b\u7ed3\u4e86\u5e38\u7528\u7684\u6a21\u578b\u3001\u7f16\u7a0b\u4efb\u52a1\u3001\u6570\u636e\u96c6\u3001\u76ee\u6807\u8bed\u8a00\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u8bc6\u522b\u4e86\u5f53\u524d\u7814\u7a76\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002", "conclusion": "\u53d8\u5f62\u6d4b\u8bd5\u662f\u8bc4\u4f30\u6df1\u5ea6\u4ee3\u7801\u6a21\u578b\u9c81\u68d2\u6027\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u672a\u6765\u7814\u7a76\u9700\u8fdb\u4e00\u6b65\u89e3\u51b3\u73b0\u6709\u6311\u6218\u5e76\u62d3\u5c55\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2507.22659", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22659", "abs": "https://arxiv.org/abs/2507.22659", "authors": ["Sabrina Kaniewski", "Fabian Schmidt", "Markus Enzweiler", "Michael Menth", "Tobias Heer"], "title": "A Systematic Literature Review on Detecting Software Vulnerabilities with Large Language Models", "comment": "36 pages + 17 pages references, 6 tables, 10 figures", "summary": "The increasing adoption of Large Language Models (LLMs) in software\nengineering has sparked interest in their use for software vulnerability\ndetection. However, the rapid development of this field has resulted in a\nfragmented research landscape, with diverse studies that are difficult to\ncompare due to differences in, e.g., system designs and dataset usage. This\nfragmentation makes it difficult to obtain a clear overview of the\nstate-of-the-art or compare and categorize studies meaningfully. In this work,\nwe present a comprehensive systematic literature review (SLR) of LLM-based\nsoftware vulnerability detection. We analyze 227 studies published between\nJanuary 2020 and June 2025, categorizing them by task formulation, input\nrepresentation, system architecture, and adaptation techniques. Further, we\nanalyze the datasets used, including their characteristics, vulnerability\ncoverage, and diversity. We present a fine-grained taxonomy of vulnerability\ndetection approaches, identify key limitations, and outline actionable future\nresearch opportunities. By providing a structured overview of the field, this\nreview improves transparency and serves as a practical guide for researchers\nand practitioners aiming to conduct more comparable and reproducible research.\nWe publicly release all artifacts and maintain a living repository of LLM-based\nsoftware vulnerability detection studies.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u5206\u6790\u4e86\u57fa\u4e8eLLM\u7684\u8f6f\u4ef6\u6f0f\u6d1e\u68c0\u6d4b\u7814\u7a76\uff0c\u603b\u7ed3\u4e86227\u9879\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u7531\u4e8eLLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u76f8\u5173\u7814\u7a76\u788e\u7247\u5316\u4e25\u91cd\uff0c\u96be\u4ee5\u6bd4\u8f83\u548c\u603b\u7ed3\u73b0\u72b6\u3002", "method": "\u5bf92020\u5e74\u81f32025\u5e74\u95f4\u7684227\u9879\u7814\u7a76\u8fdb\u884c\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff0c\u6309\u4efb\u52a1\u3001\u8f93\u5165\u3001\u67b6\u6784\u548c\u6280\u672f\u5206\u7c7b\uff0c\u5e76\u5206\u6790\u6570\u636e\u96c6\u3002", "result": "\u63d0\u51fa\u4e86\u6f0f\u6d1e\u68c0\u6d4b\u65b9\u6cd5\u7684\u7ec6\u7c92\u5ea6\u5206\u7c7b\uff0c\u8bc6\u522b\u4e86\u5173\u952e\u9650\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u5316\u7efc\u8ff0\uff0c\u63d0\u9ad8\u4e86\u7814\u7a76\u900f\u660e\u5ea6\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\uff0c\u5e76\u516c\u5f00\u4e86\u6240\u6709\u8d44\u6e90\u3002"}}
{"id": "2507.22664", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22664", "abs": "https://arxiv.org/abs/2507.22664", "authors": ["Mashal Afzal Memon", "Gianluca Filippone", "Gian Luca Scoccia", "Marco Autili", "Paola Inverardi"], "title": "RobEthiChor: Automated Context-aware Ethics-based Negotiation for Autonomous Robots", "comment": null, "summary": "The presence of autonomous systems is growing at a fast pace and it is\nimpacting many aspects of our lives. Designed to learn and act independently,\nthese systems operate and perform decision-making without human intervention.\nHowever, they lack the ability to incorporate users' ethical preferences, which\nare unique for each individual in society and are required to personalize the\ndecision-making processes. This reduces user trust and prevents autonomous\nsystems from behaving according to the moral beliefs of their end-users. When\nmultiple systems interact with differing ethical preferences, they must\nnegotiate to reach an agreement that satisfies the ethical beliefs of all the\nparties involved and adjust their behavior consequently. To address this\nchallenge, this paper proposes RobEthiChor, an approach that enables autonomous\nsystems to incorporate user ethical preferences and contextual factors into\ntheir decision-making through ethics-based negotiation. RobEthiChor features a\ndomain-agnostic reference architecture for designing autonomous systems capable\nof ethic-based negotiating. The paper also presents RobEthiChor-Ros, an\nimplementation of RobEthiChor within the Robot Operating System (ROS), which\ncan be deployed on robots to provide them with ethics-based negotiation\ncapabilities. To evaluate our approach, we deployed RobEthiChor-Ros on real\nrobots and ran scenarios where a pair of robots negotiate upon resource\ncontention. Experimental results demonstrate the feasibility and effectiveness\nof the system in realizing ethics-based negotiation. RobEthiChor allowed robots\nto reach an agreement in more than 73\\% of the scenarios with an acceptable\nnegotiation time (0.67s on average). Experiments also demonstrate that the\nnegotiation approach implemented in RobEthiChor is scalable.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRobEthiChor\u65b9\u6cd5\uff0c\u4f7f\u81ea\u4e3b\u7cfb\u7edf\u80fd\u901a\u8fc7\u57fa\u4e8e\u4f26\u7406\u7684\u534f\u5546\u6574\u5408\u7528\u6237\u4f26\u7406\u504f\u597d\u548c\u60c5\u5883\u56e0\u7d20\uff0c\u63d0\u5347\u51b3\u7b56\u4e2a\u6027\u5316\u3002", "motivation": "\u81ea\u4e3b\u7cfb\u7edf\u7f3a\u4e4f\u6574\u5408\u7528\u6237\u4f26\u7406\u504f\u597d\u7684\u80fd\u529b\uff0c\u5f71\u54cd\u7528\u6237\u4fe1\u4efb\u548c\u4e2a\u6027\u5316\u51b3\u7b56\u3002", "method": "\u63d0\u51faRobEthiChor\u67b6\u6784\u53ca\u5176\u5b9e\u73b0\u5728ROS\u4e2d\u7684RobEthiChor-Ros\uff0c\u652f\u6301\u4f26\u7406\u534f\u5546\u3002", "result": "\u5b9e\u9a8c\u663e\u793a73%\u573a\u666f\u4e0b\u673a\u5668\u4eba\u80fd\u8fbe\u6210\u534f\u8bae\uff0c\u5e73\u5747\u534f\u5546\u65f6\u95f40.67\u79d2\uff0c\u65b9\u6cd5\u5177\u6709\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "RobEthiChor\u6709\u6548\u5b9e\u73b0\u4e86\u57fa\u4e8e\u4f26\u7406\u7684\u534f\u5546\uff0c\u63d0\u5347\u4e86\u81ea\u4e3b\u7cfb\u7edf\u7684\u4f26\u7406\u51b3\u7b56\u80fd\u529b\u3002"}}
{"id": "2507.22800", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2507.22800", "abs": "https://arxiv.org/abs/2507.22800", "authors": ["Rui Ren"], "title": "The Multi-Agent Fault Localization System Based on Monte Carlo Tree Search Approach", "comment": null, "summary": "In real-world scenarios, due to the highly decoupled and flexible nature of\nmicroservices, it poses greater challenges to system reliability. The more\nfrequent occurrence of incidents has created a demand for Root Cause\nAnalysis(RCA) methods that enable rapid identification and recovery of\nincidents. Large language model (LLM) provides a new path for quickly locating\nand recovering from incidents by leveraging their powerful generalization\nability combined with expert experience. Current LLM for RCA frameworks are\nbased on ideas like ReAct and Chain-of-Thought, but the hallucination of LLM\nand the propagation nature of anomalies often lead to incorrect localization\nresults. Moreover, the massive amount of anomalous information generated in\nlarge, complex systems presents a huge challenge for the context window length\nof LLMs. To address these challenges, we propose KnowledgeMind, an innovative\nLLM multi-agent system based on Monte Carlo Tree Search and a knowledge base\nreward mechanism for standardized service-by-service reasoning. Compared to\nState-Of-The-Art(SOTA) LLM for RCA methods, our service-by-service exploration\napproach significantly reduces the burden on the maximum context window length,\nrequiring only one-tenth of its size. Additionally, by incorporating a\nrule-based real-time reward mechanism, our method effectively mitigates\nhallucinations during the inference process. Compared to the SOTA LLM for RCA\nframework, our method achieves a 49.29% to 128.35% improvement in root cause\nlocalization accuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faKnowledgeMind\uff0c\u4e00\u79cd\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u548c\u77e5\u8bc6\u5e93\u5956\u52b1\u673a\u5236\u7684LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u89e3\u51b3\u5fae\u670d\u52a1\u7cfb\u7edf\u4e2d\u6839\u56e0\u5206\u6790\u7684\u6311\u6218\u3002", "motivation": "\u5fae\u670d\u52a1\u7cfb\u7edf\u7684\u7075\u6d3b\u6027\u548c\u89e3\u8026\u6027\u589e\u52a0\u4e86\u53ef\u9760\u6027\u6311\u6218\uff0c\u73b0\u6709LLM\u65b9\u6cd5\u56e0\u5e7b\u89c9\u548c\u5f02\u5e38\u4f20\u64ad\u5bfc\u81f4\u5b9a\u4f4d\u4e0d\u51c6\uff0c\u4e14\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u5927\u3002", "method": "\u7ed3\u5408\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u548c\u77e5\u8bc6\u5e93\u5956\u52b1\u673a\u5236\uff0c\u5b9e\u73b0\u6807\u51c6\u5316\u670d\u52a1\u7ea7\u63a8\u7406\uff0c\u51cf\u5c11\u4e0a\u4e0b\u6587\u7a97\u53e3\u8d1f\u62c5\u5e76\u6291\u5236\u5e7b\u89c9\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u4e0a\u4e0b\u6587\u7a97\u53e3\u9700\u6c42\u51cf\u5c1190%\uff0c\u6839\u56e0\u5b9a\u4f4d\u51c6\u786e\u7387\u63d0\u534749.29%\u81f3128.35%\u3002", "conclusion": "KnowledgeMind\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u548c\u5956\u52b1\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5fae\u670d\u52a1\u7cfb\u7edf\u4e2d\u6839\u56e0\u5206\u6790\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2507.22853", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22853", "abs": "https://arxiv.org/abs/2507.22853", "authors": ["Haichuan Hu", "Xiaochen Xie", "Quanjun Zhang"], "title": "Repair-R1: Better Test Before Repair", "comment": null, "summary": "APR (Automated Program Repair) aims to automatically locate program defects,\ngenerate patches and validate the repairs. Existing techniques for APR are\noften combined with LLMs (Large Language Models), which leverages the\ncode-related knowledge of LLMs to improve repair effectiveness. Current\nLLM-based APR methods typically utilize test cases only during the inference\nstage, adopting an iterative approach that performs repair first and validates\nit through test execution afterward. This conventional paradigm neglects two\nimportant aspects: the potential contribution of test cases in the training\nphase, and the possibility of leveraging testing prior to repair. To address\nthis, we propose Repair-R1, which introduces test cases into the model's\ntraining phase and shifts test generation to precede repair. The model is\nrequired to first generate discriminative test cases that can distinguish\ndefective behaviors, and then perform repair based on these tests. This enables\nthe model to better locate defects and understand the underlying causes of\ndefects, thereby improving repair effectiveness. We implement Repair-R1 with\nthree different backbone models, using RL (reinforcement learning) to\nco-optimize test generation and bug repair. Experimental results on four widely\nadopted benchmarks demonstrate the superiority of Repair-R1. Specially,\ncompared to vanilla models, Repair-R1 improves repair success rate by 2.68\\% to\n48.29\\%, test generation success rate by 16.38\\% to 53.28\\%, and test coverage\nby 0.78\\% to 53.96\\%. We publish the code and weights at\nhttps://github.com/Tomsawyerhu/APR-RL and\nhttps://huggingface.co/tomhu/Qwen3-4B-RL-5000-step.", "AI": {"tldr": "Repair-R1\u662f\u4e00\u79cd\u6539\u8fdb\u7684\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\uff08APR\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u9636\u6bb5\u5f15\u5165\u6d4b\u8bd5\u7528\u4f8b\u5e76\u4f18\u5148\u751f\u6210\u6d4b\u8bd5\uff0c\u63d0\u9ad8\u4e86\u4fee\u590d\u6548\u679c\u3002", "motivation": "\u73b0\u6709LLM-based APR\u65b9\u6cd5\u5728\u8bad\u7ec3\u9636\u6bb5\u672a\u5229\u7528\u6d4b\u8bd5\u7528\u4f8b\uff0c\u4e14\u6d4b\u8bd5\u9a8c\u8bc1\u6ede\u540e\uff0c\u9650\u5236\u4e86\u4fee\u590d\u6548\u679c\u3002", "method": "Repair-R1\u5728\u8bad\u7ec3\u9636\u6bb5\u5f15\u5165\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5148\u751f\u6210\u533a\u5206\u6027\u6d4b\u8bd5\u7528\u4f8b\uff0c\u518d\u8fdb\u884c\u4fee\u590d\uff0c\u5e76\u4f7f\u7528RL\u534f\u540c\u4f18\u5316\u6d4b\u8bd5\u751f\u6210\u548c\u4fee\u590d\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRepair-R1\u663e\u8457\u63d0\u9ad8\u4e86\u4fee\u590d\u6210\u529f\u7387\uff082.68%\u81f348.29%\uff09\u3001\u6d4b\u8bd5\u751f\u6210\u6210\u529f\u7387\uff0816.38%\u81f353.28%\uff09\u548c\u6d4b\u8bd5\u8986\u76d6\u7387\uff080.78%\u81f353.96%\uff09\u3002", "conclusion": "Repair-R1\u901a\u8fc7\u4f18\u5316\u6d4b\u8bd5\u7528\u4f8b\u7684\u4f7f\u7528\u987a\u5e8f\u548c\u65b9\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86APR\u7684\u6548\u679c\u3002"}}
{"id": "2507.22871", "categories": ["cs.SE", "cs.DL", "D.2.13"], "pdf": "https://arxiv.org/pdf/2507.22871", "abs": "https://arxiv.org/abs/2507.22871", "authors": ["Domhnall Carlin", "Austen Rainer"], "title": "Tracking research software outputs in the UK", "comment": null, "summary": "Research software is crucial in the research process and the growth of Open\nScience underscores the importance of accessing research artifacts, like data\nand code, raising traceability challenges among outputs. While it is a clear\nprinciple that research code, along with other essential outputs, should be\nrecognised as artifacts of the research process, the how of this principle\nremains variable. This study examines where UK academic institutions store and\nregister software as a unique research output, searching the UKRI's Gateway to\nResearch (GtR) metadata for publicly funded research software in the UK. The\nquantity of software reported as research outcomes remains low in proportion to\nother categories. Artifact sharing appears low, with one-quarter of the\nreported software having no links and 45% having either a missing or erroneous\nURL. Of the valid URLs, we find the single largest category is Public\nCommercial Code Repository, with GitHub being the host of 18% of all publicly\nfunded research software listed. These observations are contrasted with past\nfindings from 2023 and finally, we discuss the lack of artifact sharing in UK\nresearch, with resulting implications for the maintenance and evolution of\nresearch software. Without dissemination, research software risks demotion to a\ntransient artifact, useful only to meet short term research demands but\nultimately lost to the broader enterprise of science.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u82f1\u56fd\u5b66\u672f\u673a\u6784\u5982\u4f55\u5b58\u50a8\u548c\u6ce8\u518c\u7814\u7a76\u8f6f\u4ef6\uff0c\u53d1\u73b0\u8f6f\u4ef6\u4f5c\u4e3a\u7814\u7a76\u6210\u679c\u7684\u62a5\u544a\u6bd4\u4f8b\u8f83\u4f4e\uff0c\u4e14\u5171\u4eab\u7387\u4e0d\u9ad8\uff0c\u591a\u6570\u94fe\u63a5\u65e0\u6548\u6216\u7f3a\u5931\u3002", "motivation": "\u5f00\u653e\u79d1\u5b66\u7684\u53d1\u5c55\u51f8\u663e\u4e86\u7814\u7a76\u8f6f\u4ef6\u7684\u91cd\u8981\u6027\uff0c\u4f46\u5982\u4f55\u5c06\u5176\u4f5c\u4e3a\u7814\u7a76\u4ea7\u51fa\u8fdb\u884c\u8bb0\u5f55\u548c\u5171\u4eab\u4ecd\u5b58\u5728\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5206\u6790\u82f1\u56fd\u7814\u7a76\u521b\u65b0\u7f72\uff08UKRI\uff09\u7684Gateway to Research\uff08GtR\uff09\u5143\u6570\u636e\uff0c\u8c03\u67e5\u82f1\u56fd\u516c\u5171\u8d44\u52a9\u7684\u7814\u7a76\u8f6f\u4ef6\u7684\u5b58\u50a8\u548c\u6ce8\u518c\u60c5\u51b5\u3002", "result": "\u8f6f\u4ef6\u4f5c\u4e3a\u7814\u7a76\u6210\u679c\u7684\u62a5\u544a\u6bd4\u4f8b\u4f4e\uff0c45%\u7684\u94fe\u63a5\u65e0\u6548\u6216\u7f3a\u5931\uff0cGitHub\u662f\u4e3b\u8981\u6258\u7ba1\u5e73\u53f0\u3002", "conclusion": "\u7f3a\u4e4f\u5171\u4eab\u5bfc\u81f4\u7814\u7a76\u8f6f\u4ef6\u53ef\u80fd\u6ca6\u4e3a\u77ed\u671f\u5de5\u5177\uff0c\u5f71\u54cd\u79d1\u5b66\u7684\u957f\u671f\u53d1\u5c55\u3002"}}
