<div id=toc></div>

# Table of Contents

- [cs.FL](#cs.FL) [Total: 1]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.SE](#cs.SE) [Total: 27]
- [cs.PL](#cs.PL) [Total: 4]


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [1] [A Unifying Approach to Picture Automata](https://arxiv.org/abs/2509.12077)
*Yvo Ad Meeres,František Mráz*

Main category: cs.FL

TL;DR: 本文提出使用DAG自动机通过将二维输入编码为有向无环图来识别图片语言，展示了输入无关和输入相关编码方法在不同自动机模型中的表达能力


<details>
  <summary>Details</summary>
Motivation: 研究如何利用有向无环图(DAG)来表示二维字符串或图片，并探索通过不同的编码方式使用DAG自动机来识别图片语言的能力

Method: 提出两种编码方法：输入无关编码（仅基于输入大小）和输入相关编码（依赖于符号内容）。使用DAG自动机处理这些编码后的图结构来识别语言

Result: 三种不同的输入无关编码分别刻画了返回有限自动机、牛耕式自动机和在线镶嵌自动机接受的图片语言类。输入相关编码使DAG自动机能够识别某些上下文相关的字符串语言，并在二维情况下优于在线镶嵌自动机

Conclusion: DAG自动机通过适当的编码策略能够有效识别图片语言，输入相关编码提供了更强的表达能力，超越了传统自动机模型在二维语言识别方面的性能

Abstract: A directed acyclic graph (DAG) can represent a two-dimensional string or
picture. We propose recognizing picture languages using DAG automata by
encoding 2D inputs into DAGs. An encoding can be input-agnostic (based on input
size only) or input-driven (depending on symbols). Three distinct
input-agnostic encodings characterize classes of picture languages accepted by
returning finite automata, boustrophedon automata, and online tessellation
automata. Encoding a string as a simple directed path limits recognition to
regular languages. However, input-driven encodings allow DAG automata to
recognize some context-sensitive string languages and outperform online
tessellation automata in two dimensions.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [2] [Proceedings 9th edition of Working Formal Methods Symposium](https://arxiv.org/abs/2509.11877)
*Andrei Arusoaie,Horaţiu Cheval,Radu Iosif*

Main category: cs.LO

TL;DR: 第九届形式化方法研讨会论文集，2025年9月在罗马尼亚雅西大学举行


<details>
  <summary>Details</summary>
Motivation: 记录和传播第九届形式化方法研讨会的研究成果，促进形式化方法领域的学术交流

Method: 会议论文集形式，收录研讨会期间发表的学术论文

Result: 成功举办了研讨会并出版了会议论文集，汇集了形式化方法领域的最新研究成果

Conclusion: 该论文集为形式化方法研究社区提供了重要的学术资源，推动了该领域的发展

Abstract: This volume contains the proceedings of the 9th Working Formal Methods
Symposium, which was held at the Alexandru Ioan Cuza University, Ia\c{s}i,
Romania on September 17-19, 2025.

</details>


### [3] [A Tree Clock Data Structure for Causal Orderings in Concurrent Executions](https://arxiv.org/abs/2201.06325)
*Umang Mathur,Andreas Pavlogiannis,Hünkar Can Tunç,Mahesh Viswanathan*

Main category: cs.LO

TL;DR: 树钟是一种新的数据结构，用于替代向量钟来计算并发程序执行中的因果排序，并在性能上实现了显著提升


<details>
  <summary>Details</summary>
Motivation: 向量钟的基本操作（join和copy）需要θ(k)时间，k为线程数，当k较大时成为计算瓶颈

Method: 提出树钟数据结构，join和copy操作的时间大约与修改的条目数成正比，避免了θ(k)的每次操作成本

Result: 在计算happens-before等偏序关系时，树钟达到了最优运行时间，平均性能提升2.02倍（MAZ）到2.97倍（HB）

Conclusion: 树钟具有成为并发分析领域标准数据结构的潜力，能够广泛应用于各种偏序关系的计算

Abstract: Dynamic techniques are a scalable and effective way to analyze concurrent
programs. Instead of analyzing all behaviors of a program, these techniques
detect errors by focusing on a single program execution. Often a crucial step
in these techniques is to define a causal ordering between events in the
execution, which is then computed using vector clocks, a simple data structure
that stores logical times of threads. The two basic operations of vector
clocks, namely join and copy, require $\Theta(k)$ time, where $k$ is the number
of threads. Thus they are a computational bottleneck when $k$ is large.
  In this work, we introduce tree clocks, a new data structure that replaces
vector clocks for computing causal orderings in program executions. Joining and
copying tree clocks takes time that is roughly proportional to the number of
entries being modified, and hence the two operations do not suffer the a-priori
$\Theta(k)$ cost per application. We show that when used to compute the classic
happens-before (HB) partial order, tree clocks are optimal, in the sense that
no other data structure can lead to smaller asymptotic running time. Moreover,
we demonstrate that tree clocks can be used to compute other partial orders,
such as schedulable-happens-before (SHB) and the standard Mazurkiewicz (MAZ)
partial order, and thus are a versatile data structure. Our experiments show
that just by replacing vector clocks with tree clocks, the computation becomes
from $2.02 \times$ faster (MAZ) to $2.66 \times$ (SHB) and $2.97 \times$ (HB)
on average per benchmark. These results illustrate that tree clocks have the
potential to become a standard data structure with wide applications in
concurrent analyses.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [4] [Quality Assessment of Tabular Data using Large Language Models and Code Generation](https://arxiv.org/abs/2509.10572)
*Ashlesha Akella,Akshar Kaul,Krishnasuri Narayanam,Sameep Mehta*

Main category: cs.SE

TL;DR: 提出一个三阶段框架，结合统计异常值检测和LLM驱动的规则与代码生成，用于自动化表格数据质量验证


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的数据质量验证方法存在效率低下、需要人工干预和计算成本高的问题，需要更智能的自动化解决方案

Method: 三阶段框架：1）传统聚类过滤数据样本；2）迭代提示LLM生成语义有效的质量规则；3）通过代码生成LLM合成可执行验证器。使用RAG技术增强LLM，利用外部知识源和领域特定示例

Result: 在基准数据集上的广泛评估证实了该方法的有效性

Conclusion: 该框架能够可靠地自动化生成数据质量规则和验证代码，显著提高表格数据质量验证的效率和准确性

Abstract: Reliable data quality is crucial for downstream analysis of tabular datasets,
yet rule-based validation often struggles with inefficiency, human
intervention, and high computational costs. We present a three-stage framework
that combines statistical inliner detection with LLM-driven rule and code
generation. After filtering data samples through traditional clustering, we
iteratively prompt LLMs to produce semantically valid quality rules and
synthesize their executable validators through code-generating LLMs. To
generate reliable quality rules, we aid LLMs with retrieval-augmented
generation (RAG) by leveraging external knowledge sources and domain-specific
few-shot examples. Robust guardrails ensure the accuracy and consistency of
both rules and code snippets. Extensive evaluations on benchmark datasets
confirm the effectiveness of our approach.

</details>


### [5] [Reasonable Experiments in Model-Based Systems Engineering](https://arxiv.org/abs/2509.10649)
*Johan Cederbladh,Loek Cleophas,Eduard Kamburjan,Lucas Lima,Rakshit Mittal,Hans Vangheluwe*

Main category: cs.SE

TL;DR: 提出一个基于案例推理的实验管理框架，通过智能重用实验数据来避免不必要的重复实验，加速系统工程设计过程


<details>
  <summary>Details</summary>
Motivation: 随着基于模型的系统工程向数字工程和早期验证验证发展，实验配置元数据和结果管理对加速整体设计工作至关重要，需要智能重用实验相关数据来节省时间和资源

Method: 开发了一个管理数字/物理资产实验的框架，重点采用基于案例推理和领域知识的方法，通过判断已有实验是否能回答新问题来决定是否重用实验数据

Result: 提供了通用实验管理器架构，并通过工业车辆能源系统设计案例研究验证了方法的有效性

Conclusion: 该框架能够有效重用实验数据，避免设置和执行新实验，显著提高系统工程实验管理的效率和资源利用率

Abstract: With the current trend in Model-Based Systems Engineering towards Digital
Engineering and early Validation & Verification, experiments are increasingly
used to estimate system parameters and explore design decisions. Managing such
experimental configuration metadata and results is of utmost importance in
accelerating overall design effort. In particular, we observe it is important
to 'intelligent-ly' reuse experiment-related data to save time and effort by
not performing potentially superfluous, time-consuming, and resource-intensive
experiments. In this work, we present a framework for managing experiments on
digital and/or physical assets with a focus on case-based reasoning with domain
knowledge to reuse experimental data efficiently by deciding whether an
already-performed experiment (or associated answer) can be reused to answer a
new (potentially different) question from the engineer/user without having to
set up and perform a new experiment. We provide the general architecture for
such an experiment manager and validate our approach using an industrial
vehicular energy system-design case study.

</details>


### [6] [Arguzz: Testing zkVMs for Soundness and Completeness Bugs](https://arxiv.org/abs/2509.10819)
*Christoph Hochrainer,Valentin Wüstholz,Maria Christakis*

Main category: cs.SE

TL;DR: Arguzz是首个自动化测试zkVM（零知识虚拟机）的工具，通过结合变形测试和故障注入来检测zkVM中的正确性和完整性漏洞，在6个真实zkVM中发现了11个漏洞


<details>
  <summary>Details</summary>
Motivation: zkVM在区块链和去中心化应用中广泛使用，但其约束系统和执行逻辑中的漏洞可能导致严重的安全问题（接受无效执行或拒绝有效执行），现有审计方法可能遗漏关键漏洞

Method: 采用新颖的变形测试变体与故障注入相结合的方法：生成语义等价的程序对，合并为具有已知输出的Rust程序，在zkVM中运行并通过故障注入模拟恶意证明者来发现约束过弱的问题

Result: 测试了6个真实zkVM（RISC Zero、Nexus、Jolt、SP1、OpenVM和Pico），在其中3个系统中发现了11个漏洞，其中一个RISC Zero漏洞获得了5万美元的漏洞赏金

Conclusion: 尽管经过审计，zkVM仍存在严重漏洞，系统化测试工具如Arguzz对于确保zkVM安全性至关重要

Abstract: Zero-knowledge virtual machines (zkVMs) are increasingly deployed in
decentralized applications and blockchain rollups since they enable verifiable
off-chain computation. These VMs execute general-purpose programs, frequently
written in Rust, and produce succinct cryptographic proofs. However, zkVMs are
complex, and bugs in their constraint systems or execution logic can cause
critical soundness (accepting invalid executions) or completeness (rejecting
valid ones) issues.
  We present Arguzz, the first automated tool for testing zkVMs for soundness
and completeness bugs. To detect such bugs, Arguzz combines a novel variant of
metamorphic testing with fault injection. In particular, it generates
semantically equivalent program pairs, merges them into a single Rust program
with a known output, and runs it inside a zkVM. By injecting faults into the
VM, Arguzz mimics malicious or buggy provers to uncover overly weak
constraints.
  We used Arguzz to test six real-world zkVMs (RISC Zero, Nexus, Jolt, SP1,
OpenVM, and Pico) and found eleven bugs in three of them. One RISC Zero bug
resulted in a $50,000 bounty, despite prior audits, demonstrating the critical
need for systematic testing of zkVMs.

</details>


### [7] [TPSQLi: Test Prioritization for SQL Injection Vulnerability Detection in Web Applications](https://arxiv.org/abs/2509.10920)
*Guan-Yan Yang,Farn Wang,You-Zong Gu,Ya-Wen Teng,Kuo-Hui Yeh,Ping-Hsueh Ho,Wei-Ling Wen*

Main category: cs.SE

TL;DR: 提出一种基于历史测试结果动态调整防御强度向量的SQL注入漏洞测试优先级排序方法，以提高测试效率和漏洞检测效果


<details>
  <summary>Details</summary>
Motivation: 网络攻击快速增长，SQL注入位列OWASP Top 10前三，传统测试方法难以应对敏捷开发需求，需要更高效的漏洞检测工具

Method: 利用先前测试结果调整后续测试的防御强度向量，通过动态调整机制优化测试流程，并根据软件特定需求定制防御机制

Result: 该方法能够提高SQL注入漏洞检测的效率和有效性，通过灵活的框架实现动态调整并考虑漏洞暴露的时间因素

Conclusion: 所提出的测试优先级排序方法为SQL注入漏洞检测提供了更高效的解决方案，能够支持敏捷开发周期并提升软件安全性

Abstract: The rapid proliferation of network applications has led to a significant
increase in network attacks. According to the OWASP Top 10 Projects report
released in 2021, injection attacks rank among the top three vulnerabilities in
software projects. This growing threat landscape has increased the complexity
and workload of software testing, necessitating advanced tools to support agile
development cycles. This paper introduces a novel test prioritization method
for SQL injection vulnerabilities to enhance testing efficiency. By leveraging
previous test outcomes, our method adjusts defense strength vectors for
subsequent tests, optimizing the testing workflow and tailoring defense
mechanisms to specific software needs. This approach aims to improve the
effectiveness and efficiency of vulnerability detection and mitigation through
a flexible framework that incorporates dynamic adjustments and considers the
temporal aspects of vulnerability exposure.

</details>


### [8] [When the Code Autopilot Breaks: Why LLMs Falter in Embedded Machine Learning](https://arxiv.org/abs/2509.10946)
*Roberto Morabito,Guanghan Wu*

Main category: cs.SE

TL;DR: 对LLM驱动的嵌入式机器学习工作流中代码生成失败模式的实证研究，揭示了提示格式、模型行为和结构假设如何影响成功率和失败特征，提出了失败分类法并分析了多个LLM的常见错误根源。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在嵌入式机器学习工作流中自动化软件生成的广泛应用，其输出经常出现静默失败或不可预测行为，需要系统性地研究这些失败模式以提高可靠性。

Method: 基于自动驾驶框架进行实证研究，该框架协调数据预处理、模型转换和片上推理代码生成，分析提示格式、模型行为和结构假设对LLM输出的影响。

Result: 研究揭示了多种易出错行为，包括格式引起的误解和编译通过但破坏下游运行的代码，建立了失败分类法并识别了跨多个LLM的常见系统性脆弱性。

Conclusion: 尽管基于特定设备，但研究揭示了LLM代码生成面临的广泛挑战，提出了改进LLM驱动嵌入式ML系统可靠性和可追溯性的方向。

Abstract: Large Language Models (LLMs) are increasingly used to automate software
generation in embedded machine learning workflows, yet their outputs often fail
silently or behave unpredictably. This article presents an empirical
investigation of failure modes in LLM-powered ML pipelines, based on an
autopilot framework that orchestrates data preprocessing, model conversion, and
on-device inference code generation. We show how prompt format, model behavior,
and structural assumptions influence both success rates and failure
characteristics, often in ways that standard validation pipelines fail to
detect. Our analysis reveals a diverse set of error-prone behaviors, including
format-induced misinterpretations and runtime-disruptive code that compiles but
breaks downstream. We derive a taxonomy of failure categories and analyze
errors across multiple LLMs, highlighting common root causes and systemic
fragilities. Though grounded in specific devices, our study reveals broader
challenges in LLM-based code generation. We conclude by discussing directions
for improving reliability and traceability in LLM-powered embedded ML systems.

</details>


### [9] [Hardness, Structural Knowledge, and Opportunity: An Analytical Framework for Modular Performance Modeling](https://arxiv.org/abs/2509.11000)
*Omid Gheibi,Christian Kästner,Pooyan Jamshidi*

Main category: cs.SE

TL;DR: 本文研究了系统结构特征和结构知识水平如何影响性能建模的难度和改进机会，发现模块数量和配置选项是主要影响因素，并为系统设计者提供了实用指导


<details>
  <summary>Details</summary>
Motivation: 性能影响模型有助于理解配置如何影响系统性能，但由于配置空间呈指数级增长，创建这些模型具有挑战性。虽然灰盒方法利用选择性结构知识来改进建模，但这种知识与系统特征及模型改进潜力之间的关系尚未得到充分理解

Method: 通过控制实验使用合成系统模型，引入并量化建模"硬度"概念，建立分析矩阵来测量结构特征变化和结构知识水平对模块化性能建模机会的影响

Result: 研究发现建模硬度主要由模块数量和每个模块的配置选项驱动。更高的结构知识水平和增加的建模硬度都能显著提升改进机会，但影响因性能指标而异：对于排名准确性，结构知识更占主导；对于预测准确性，硬度起更强作用

Conclusion: 研究结果为系统设计者提供了可操作的见解，指导他们根据系统特征和任务目标战略性地分配时间并选择合适的建模方法

Abstract: Performance-influence models are beneficial for understanding how
configurations affect system performance, but their creation is challenging due
to the exponential growth of configuration spaces. While gray-box approaches
leverage selective "structural knowledge" (like the module execution graph of
the system) to improve modeling, the relationship between this knowledge, a
system's characteristics (we call them "structural aspects"), and potential
model improvements is not well understood. This paper addresses this gap by
formally investigating how variations in structural aspects (e.g., the number
of modules and options per module) and the level of structural knowledge impact
the creation of "opportunities" for improved "modular performance modeling". We
introduce and quantify the concept of modeling "hardness", defined as the
inherent difficulty of performance modeling. Through controlled experiments
with synthetic system models, we establish an "analytical matrix" to measure
these concepts. Our findings show that modeling hardness is primarily driven by
the number of modules and configuration options per module. More importantly,
we demonstrate that both higher levels of structural knowledge and increased
modeling hardness significantly enhance the opportunity for improvement. The
impact of these factors varies by performance metric; for ranking accuracy
(e.g., in debugging task), structural knowledge is more dominant, while for
prediction accuracy (e.g., in resource management task), hardness plays a
stronger role. These results provide actionable insights for system designers,
guiding them to strategically allocate time and select appropriate modeling
approaches based on a system's characteristics and a given task's objectives.

</details>


### [10] [ViScratch: Using Large Language Models and Gameplay Videos for Automated Feedback in Scratch](https://arxiv.org/abs/2509.11065)
*Yuan Si,Daming Li,Hanyuan Shi,Jialu Zhang*

Main category: cs.SE

TL;DR: ViScratch是一个用于Scratch编程环境的多模态调试系统，通过结合代码块和游戏视频来诊断和修复bug，显著优于现有的基于LLM的工具。


<details>
  <summary>Details</summary>
Motivation: 现有的Scratch调试工具主要依赖预定义规则或用户手动输入，忽略了平台的视觉特性，无法有效处理语义错误。

Method: 采用两阶段流水线：首先使用视觉语言模型将视觉症状与代码结构对齐以识别关键问题，然后提出最小化的AST级别修复并在Scratch虚拟机中验证。

Result: ViScratch在真实Scratch项目上显著优于最先进的基于LLM的工具和人工测试者，游戏视频成为关键的调试信号。

Conclusion: 视频可以作为可视化编程环境中的一等规范，为基于LLM的调试开辟了超越纯符号代码的新方向。

Abstract: Block-based programming environments such as Scratch are increasingly popular
in programming education, in particular for young learners. While the use of
blocks helps prevent syntax errors, semantic bugs remain common and difficult
to debug. Existing tools for Scratch debugging rely heavily on predefined rules
or user manual inputs, and crucially, they ignore the platform's inherently
visual nature.
  We introduce ViScratch, the first multimodal feedback generation system for
Scratch that leverages both the project's block code and its generated gameplay
video to diagnose and repair bugs. ViScratch uses a two-stage pipeline: a
vision-language model first aligns visual symptoms with code structure to
identify a single critical issue, then proposes minimal, abstract syntax tree
level repairs that are verified via execution in the Scratch virtual machine.
  We evaluate ViScratch on a set of real-world Scratch projects against
state-of-the-art LLM-based tools and human testers. Results show that gameplay
video is a crucial debugging signal: ViScratch substantially outperforms prior
tools in both bug identification and repair quality, even without access to
project descriptions or goals. This work demonstrates that video can serve as a
first-class specification in visual programming environments, opening new
directions for LLM-based debugging beyond symbolic code alone.

</details>


### [11] [Rethinking Technology Stack Selection with AI Coding Proficiency](https://arxiv.org/abs/2509.11132)
*Xiaoyu Zhang,Weipeng Jiang,Juan Zhai,Shiqing Ma,Qingshuang Bao,Chenhao Lin,Chao Shen,Tianlin Li,Yang Liu*

Main category: cs.SE

TL;DR: 本文提出了AI编程熟练度的概念，评估LLMs使用不同技术库生成高质量代码的能力，发现不同库之间的AI熟练度差异高达84%，这会影响技术选择和生态系统多样性。


<details>
  <summary>Details</summary>
Motivation: 传统技术栈选择方法只关注技术本身属性，忽视了LLMs是否能有效利用所选技术。现有LLMs在使用流行库时经常生成低质量代码，导致高调试成本和技术债务。

Method: 首次对170个第三方库和61个任务场景进行全面的实证研究，评估6个广泛使用的LLMs的AI编程熟练度。

Result: 功能相似的库在LLM生成代码质量得分上差异高达84%，不同模型使用相同库时也存在质量差距。这些差距转化为实际工程成本，可能导致开发者只选择AI熟练度高的少数库。

Conclusion: 呼吁社区将AI熟练度评估整合到技术选择框架中，制定缓解策略，以保持AI驱动开发中的竞争平衡和技术多样性。

Abstract: Large language models (LLMs) are now an integral part of software development
workflows and are reshaping the whole process. Traditional technology stack
selection has not caught up. Most of the existing selection methods focus
solely on the inherent attributes of the technology, overlooking whether the
LLM can effectively leverage the chosen technology. For example, when
generating code snippets using popular libraries like Selenium (one of the most
widely used test automation tools with over 33k GitHub stars), existing LLMs
frequently generate low-quality code snippets (e.g., using deprecated APIs and
methods, or containing syntax errors). As such, teams using LLM assistants risk
choosing technologies that cannot be used effectively by LLMs, yielding high
debugging effort and mounting technical debt. We foresee a practical question
in the LLM era, is a technology ready for AI-assisted development? In this
paper, we first propose the concept, AI coding proficiency, the degree to which
LLMs can utilize a given technology to generate high-quality code snippets. We
conduct the first comprehensive empirical study examining AI proficiency across
170 third-party libraries and 61 task scenarios, evaluating six widely used
LLMs. Our findings reveal that libraries with similar functionalities can
exhibit up to 84% differences in the quality score of LLM-generated code, while
different models also exhibit quality gaps among their generation results using
the same library. These gaps translate into real engineering costs and can
steer developer choices toward a narrow set of libraries with high AI coding
proficiency, threatening technological diversity in the ecosystem. We call on
the community to integrate AI proficiency assessments into technology selection
frameworks and develop mitigation strategies, preserving competitive balance in
AI-driven development.

</details>


### [12] [UserTrace: User-Level Requirements Generation and Traceability Recovery from Software Project Repositories](https://arxiv.org/abs/2509.11238)
*Dongming Jin,Zhi Jin,Yiran Zhang,Zheng Fang,Linyu Li,Yuanpeng He,Xiaohong Chen,Weisong Sun*

Main category: cs.SE

TL;DR: UserTrace是一个多智能体系统，通过自动化生成用户级需求(URs)和恢复从URs到实现级需求(IRs)再到代码的实时追踪链接，解决了软件维护中的需求质量和追踪性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化代码摘要(ACS)方法主要生成面向开发者的实现级需求，而需求追踪(RT)技术往往忽略项目演进的影响，导致用户级需求和实时追踪链接未被充分探索，这对支持用户理解和验证AI生成软件是否符合用户意图至关重要。

Method: UserTrace协调四个专业智能体（代码审查员、搜索器、编写器和验证器），通过三阶段流程：结构化仓库依赖、为代码单元推导实现级需求、以及结合领域特定上下文合成用户级需求。

Result: 比较评估显示，UserTrace生成的用户级需求在完整性、正确性和有用性方面优于现有基线，在追踪链接恢复精度上优于五种最先进的需求追踪方法。用户研究进一步证明UserTrace能帮助最终用户验证AI生成的仓库是否符合其意图。

Conclusion: UserTrace系统有效解决了用户级需求生成和实时追踪链接恢复的问题，为软件维护和AI生成软件的验证提供了重要支持。

Abstract: Software maintainability critically depends on high-quality requirements
descriptions and explicit traceability between requirements and code. Although
automated code summarization (ACS) and requirements traceability (RT)
techniques have been widely studied, existing ACS methods mainly generate
implementation-level (i.e., developer-oriented) requirements (IRs) for
fine-grained units (e.g., methods), while RT techniques often overlook the
impact of project evolution. As a result, user-level (i.e., end user-oriented)
requirements (URs) and live trace links remain underexplored, despite their
importance for supporting user understanding and for validating whether
AI-generated software aligns with user intent. To address this gap, we propose
UserTrace, a multi-agent system that automatically generates URs and recovers
live trace links (from URs to IRs to code) from software repositories.
UserTrace coordinates four specialized agents (i.e., Code Reviewer, Searcher,
Writer, and Verifier) through a three-phase process: structuring repository
dependencies, deriving IRs for code units, and synthesizing URs with
domain-specific context. Our comparative evaluation shows that UserTrace
produces URs with higher completeness, correctness, and helpfulness than an
established baseline, and achieves superior precision in trace link recovery
compared to five state-of-the-art RT approaches. A user study further
demonstrates that UserTrace helps end users validate whether the AI-generated
repositories align with their intent.

</details>


### [13] [Beyond Autoregression: An Empirical Study of Diffusion Large Language Models for Code Generation](https://arxiv.org/abs/2509.11252)
*Chengze li,Yitong Zhang,Jia Li,Liyi Cai,Ge Li*

Main category: cs.SE

TL;DR: 本文首次系统研究扩散LLM在代码生成中的应用，发现扩散LLM在性能上与自回归LLM相当，但在长代码理解和长度外推方面表现更好，并提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 现有自回归LLM在代码生成中存在效率低下和生成顺序固定的局限性，而扩散LLM通过多token预测和灵活生成顺序有望解决这些问题，但缺乏系统研究。

Method: 对9个代表性扩散LLM在4个广泛使用的基准测试上进行实证研究，比较其与自回归LLM的性能差异，并分析影响效果和效率的因素。

Result: 扩散LLM与同等规模的自回归LLM性能相当；在长度外推和长代码理解方面表现更优；识别了影响扩散LLM效果和效率的关键因素。

Conclusion: 扩散LLM是代码生成的有前景替代方案，研究为其改进提供了实用指导，并指出了未来发展方向。

Abstract: LLMs have become the mainstream approaches to code generation. Existing LLMs
mainly employ autoregressive generation, i.e. generating code token-by-token
from left to right. However, the underlying autoregressive generation has two
limitations in code generation. First, autoregressive LLMs only generate a
token at each step, showing low efficiency in practice. Second, programming is
a non-sequential process involving back-and-forth editing, while autoregressive
LLMs only employ the left-to-right generation order. These two intrinsic
limitations hinder the further development of LLMs in code generation.
Recently, diffusion LLMs have emerged as a promising alternative. Diffusion
LLMs address the above limitations with two advances, including multi-token
prediction (i.e. generating multiple tokens at each step) and flexible
generation order (i.e. flexibly determining which positions to generate
tokens). However, there is no systematic study exploring diffusion LLMs in code
generation. To bridge the knowledge gap, we present the first empirical study
of diffusion LLMs for code generation. Our study involves 9 representative
diffusion LLMs and conduct experiments on 4 widely used benchmarks. Based on
the results, we summarize the following findings. (1) Existing diffusion LLMs
are competitive with autoregressive LLMs with similar sizes. (2) Diffusion LLMs
have a stronger length extrapolation ability than autoregressive LLMs and
perform better in long code understanding. (3) We explore factors impacting the
effectiveness and efficiency of diffusion LLMs, and provide practical guidance.
(4) We discuss several promising further directions to improve diffusion LLMs
on code generation. We open-source all source code, data, and results to
facilitate the following research. The code is publicly available at
https://github.com/zhangyitonggg/dllm4code.

</details>


### [14] [A Web-Based Environment for the Specification and Generation of Smart Legal Contracts](https://arxiv.org/abs/2509.11258)
*Regan Meloche,Durga Sivakumar,Amal A. Anda,Sofana Alfuhaid,Daniel Amyot,Luigi Logrippo,John Mylopoulos*

Main category: cs.SE

TL;DR: 基于Web的环境，支持用户辅助完善Symboleo规范，自动生成可部署在Hyperledger Fabric平台上的监控智能合约，用于法律合同合规性监测


<details>
  <summary>Details</summary>
Motivation: 自然语言合同与智能合约实现之间存在巨大差距，需要有效监测合同履行合规性以尽早发现违约行为

Method: 开发Web环境支持用户辅助完善Symboleo规范，然后自动生成可部署在Hyperledger Fabric平台的监控智能合约

Result: 通过交易能源领域的示例合同展示了该环境在法律合规背景下加速智能合约开发的潜力

Conclusion: 该Web环境部分填补了自然语言合同与智能合约实现之间的差距，为法律合规监测提供了有效工具

Abstract: Monitoring the compliance of contract performance against legal obligations
is important in order to detect violations, ideally, as soon as they occur.
Such monitoring can nowadays be achieved through the use of smart contracts,
which provide protection against tampering as well as some level of automation
in handling violations. However, there exists a large gap between natural
language contracts and smart contract implementations. This paper introduces a
Web-based environment that partly fills that gap by supporting the
user-assisted refinement of Symboleo specifications corresponding to legal
contract templates, followed by the automated generation of monitoring smart
contracts deployable on the Hyperledger Fabric platform. This environment,
illustrated using a sample contract from the transactive energy domain, shows
much potential in accelerating the development of smart contracts in a legal
compliance context.

</details>


### [15] [Weakly Supervised Vulnerability Localization via Multiple Instance Learning](https://arxiv.org/abs/2509.11312)
*Wenchao Gu,Yupan Chen,Yanlin Wang,Hongyu Zhang,Cuiyun Gao,Michael R. Lyu*

Main category: cs.SE

TL;DR: WAVES是一种基于多示例学习的弱监督漏洞定位方法，无需语句级标注即可同时实现漏洞检测和精确定位


<details>
  <summary>Details</summary>
Motivation: 现有漏洞检测方法主要在函数或文件级别进行粗粒度检测，开发者仍需手动检查大量代码来定位具体漏洞语句。语句级标注需要专家知识且成本高昂，因此需要无需额外语句级标注的方法

Method: 基于多示例学习概念，将函数级真实标签转换为语句级伪标签，无需额外语句级标注。利用这些伪标签训练函数级表示向量的分类器

Result: 在三个流行基准数据集上的实验表明，该方法在漏洞检测方面达到可比性能，在语句级漏洞定位方面达到最先进性能

Conclusion: WAVES方法成功解决了无需语句级标注的漏洞定位问题，为软件安全领域提供了有效的弱监督学习解决方案

Abstract: Software vulnerability detection has emerged as a significant concern in the
field of software security recently, capturing the attention of numerous
researchers and developers. Most previous approaches focus on coarse-grained
vulnerability detection, such as at the function or file level. However, the
developers would still encounter the challenge of manually inspecting a large
volume of code inside the vulnerable function to identify the specific
vulnerable statements for modification, indicating the importance of
vulnerability localization. Training the model for vulnerability localization
usually requires ground-truth labels at the statement-level, and labeling
vulnerable statements demands expert knowledge, which incurs high costs. Hence,
the demand for an approach that eliminates the need for additional labeling at
the statement-level is on the rise. To tackle this problem, we propose a novel
approach called WAVES for WeAkly supervised Vulnerability Localization via
multiplE inStance learning, which does not need the additional statement-level
labels during the training. WAVES has the capability to determine whether a
function is vulnerable (i.e., vulnerability detection) and pinpoint the
vulnerable statements (i.e., vulnerability localization). Specifically,
inspired by the concept of multiple instance learning, WAVES converts the
ground-truth label at the function-level into pseudo labels for individual
statements, eliminating the need for additional statement-level labeling. These
pseudo labels are utilized to train the classifiers for the function-level
representation vectors. Extensive experimentation on three popular benchmark
datasets demonstrates that, in comparison to previous baselines, our approach
achieves comparable performance in vulnerability detection and state-of-the-art
performance in statement-level vulnerability localization.

</details>


### [16] [Large Language Models (LLMs) for Requirements Engineering (RE): A Systematic Literature Review](https://arxiv.org/abs/2509.11446)
*Mohammad Amin Zadenoori,Jacek Dąbrowski,Waad Alhoshan,Liping Zhao,Alessio Ferrari*

Main category: cs.SE

TL;DR: 本文对2023-2024年间74项关于大语言模型在需求工程中应用的研究进行了系统性文献综述，分析了应用模式、研究趋势和未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在各个领域的广泛应用，需求工程作为语言密集型任务也开始受益于LLM的能力，需要系统性地了解当前研究现状和应用模式。

Method: 采用系统性文献综述方法，对74篇主要研究进行分析，从发表趋势、RE活动、提示策略、评估方法等多个维度进行分类研究。

Result: 研究发现LLM在需求工程中的应用主要集中在需求获取和验证，而非传统的缺陷检测；主要使用GPT模型和零样本/少样本提示；研究多在受控环境中进行，工业应用有限。

Conclusion: LLM为需求工程带来了新的机遇，但需要进一步探索较少研究的任务、改进提示方法、加强工业环境测试，并扩展RE在软件工程中的影响力。

Abstract: Large Language Models (LLMs) are finding applications in numerous domains,
and Requirements Engineering (RE) is increasingly benefiting from their
capabilities to assist with complex, language-intensive tasks. This paper
presents a systematic literature review of 74 primary studies published between
2023 and 2024, examining how LLMs are being applied in RE. The study
categorizes the literature according to several dimensions, including
publication trends, RE activities, prompting strategies, and evaluation
methods. Our findings indicate notable patterns, among which we observe
substantial differences compared to previous works leveraging standard Natural
Language Processing (NLP) techniques. Most of the studies focus on using LLMs
for requirements elicitation and validation, rather than defect detection and
classification, which were dominant in the past. Researchers have also
broadened their focus and addressed novel tasks, e.g., test generation,
exploring the integration of RE with other software engineering (SE)
disciplines. Although requirements specifications remain the primary focus,
other artifacts are increasingly considered, including issues from issue
tracking systems, regulations, and technical manuals. The studies mostly rely
on GPT-based models, and often use Zero-shot or Few-shot prompting. They are
usually evaluated in controlled environments, with limited use in industry
settings and limited integration in complex workflows. Our study outlines
important future directions, such as leveraging the potential to expand the
influence of RE in SE, exploring less-studied tasks, improving prompting
methods, and testing in real-world environments. Our contribution also helps
researchers and practitioners use LLMs more effectively in RE, by providing a
list of identified tools leveraging LLMs for RE, as well as datasets.

</details>


### [17] [VulAgent: Hypothesis-Validation based Multi-Agent Vulnerability Detection](https://arxiv.org/abs/2509.11523)
*Ziliang Wang,Ge Li,Jia Li,Hao Zhu,Zhi Jin*

Main category: cs.SE

TL;DR: VulAgent是一个基于假设验证的多智能体漏洞检测框架，通过模拟人类代码审计过程，提高项目级漏洞检测的准确性和覆盖率，降低误报率。


<details>
  <summary>Details</summary>
Motivation: 语言模型在项目级漏洞检测中面临准确识别安全敏感代码和复杂程序上下文推理的双重挑战，需要更有效的检测方法。

Method: 采用多智能体协作框架，每个智能体专注于特定分析视角（如内存、授权），通过假设验证范式：形成漏洞假设、构建触发路径、验证相关程序上下文和防御检查。

Result: 在两个数据集上平均提高整体准确率6.6%，正确识别漏洞-修复代码对的比率最高提升450%（平均246%），误报率降低约36%。

Conclusion: VulAgent通过语义敏感的多视角检测流水线和假设验证范式，显著提升了基于LLM的漏洞检测性能，为项目级安全分析提供了有效解决方案。

Abstract: The application of language models to project-level vulnerability detection
remains challenging, owing to the dual requirement of accurately localizing
security-sensitive code and correctly correlating and reasoning over complex
program context. We present VulAgent, a multi-agent vulnerability detection
framework based on hypothesis validation. Our design is inspired by how human
auditors review code: when noticing a sensitive operation, they form a
hypothesis about a possible vulnerability, consider potential trigger paths,
and then verify the hypothesis against the surrounding context. VulAgent
implements a semantics-sensitive, multi-view detection pipeline: specialized
agents, each aligned to a specific analysis perspective (e.g., memory,
authorization), collaboratively surface and precisely localize sensitive code
sites with higher coverage. Building on this, VulAgent adopts a
hypothesis-validation paradigm: for each vulnerability report, it builds
hypothesis conditions and a trigger path, steering the LLM to target the
relevant program context and defensive checks during verification, which
reduces false positives. On average across the two datasets, VulAgent improves
overall accuracy by 6.6%, increases the correct identification rate of
vulnerable--fixed code pairs by up to 450% (246% on average), and reduces the
false positive rate by about 36% compared with state-of-the-art LLM-based
baselines.

</details>


### [18] [Sedeve-Kit, a Specification-Driven Development Framework for Building Distributed Systems](https://arxiv.org/abs/2509.11566)
*Hua Guo,Yunhong Ji,Xuan Zhou*

Main category: cs.SE

TL;DR: 提出基于TLA+的规范驱动开发框架，通过三阶段方法确保分布式系统质量：规范定义与模型检查、代码实现、测试验证


<details>
  <summary>Details</summary>
Motivation: 解决分布式系统中非确定性并发和故障带来的复杂性挑战，确保系统正确性和可靠性

Method: 三阶段开发框架：1) 使用TLA+定义规范和不变式，进行模型检查并生成测试用例；2) 基于规范编写代码；3) 使用第一阶段生成的测试用例进行严格测试

Result: 建立了抽象设计与具体实现之间的强连接，通过持续验证确保系统质量

Conclusion: 该规范驱动开发框架能有效应对分布式系统开发的复杂性，提高系统正确性和可靠性

Abstract: Developing distributed systems presents significant challenges, primarily due
to the complexity introduced by non-deterministic concurrency and faults. To
address these, we propose a specification-driven development framework. Our
method encompasses three key stages. The first stage defines system
specifications and invariants using TLA${^+}$. It allows us to perform model
checking on the algorithm's correctness and generate test cases for subsequent
development phases. In the second stage, based on the established
specifications, we write code to ensure consistency and accuracy in the
implementation. Finally, after completing the coding process, we rigorously
test the system using the test cases generated in the initial stage. This
process ensures system quality by maintaining a strong connection between the
abstract design and the concrete implementation through continuous
verification.

</details>


### [19] [Automated Creation and Enrichment Framework for Improved Invocation of Enterprise APIs as Tools](https://arxiv.org/abs/2509.11626)
*Prerna Agarwal,Himanshu Gupta,Soujanya Soni,Rohith Vallam,Renuka Sindhgatta,Sameep Mehta*

Main category: cs.SE

TL;DR: ACE是一个自动化工具创建和增强框架，将企业API转换为LLM兼容工具，通过生成丰富的工具规范和动态筛选机制提高工具选择和调用准确性


<details>
  <summary>Details</summary>
Motivation: 企业环境中API工具使用存在文档质量差、输入输出模式复杂、操作数量多等问题，导致工具选择困难且有效负载形成准确率下降高达25%

Method: ACE框架(i)生成包含参数描述和示例的增强工具规范，(ii)集成动态筛选机制在运行时过滤相关工具，降低提示复杂度同时保持可扩展性

Result: 在专有和开源API上验证了框架有效性，并展示了与代理框架的集成能力

Conclusion: ACE是首个端到端自动化企业API工具创建、增强和动态选择的框架，为LLM代理提供了更好的工具兼容性和使用效率

Abstract: Recent advancements in Large Language Models (LLMs) has lead to the
development of agents capable of complex reasoning and interaction with
external tools. In enterprise contexts, the effective use of such tools that
are often enabled by application programming interfaces (APIs), is hindered by
poor documentation, complex input or output schema, and large number of
operations. These challenges make tool selection difficult and reduce the
accuracy of payload formation by up to 25%. We propose ACE, an automated tool
creation and enrichment framework that transforms enterprise APIs into
LLM-compatible tools. ACE, (i) generates enriched tool specifications with
parameter descriptions and examples to improve selection and invocation
accuracy, and (ii) incorporates a dynamic shortlisting mechanism that filters
relevant tools at runtime, reducing prompt complexity while maintaining
scalability. We validate our framework on both proprietary and open-source APIs
and demonstrate its integration with agentic frameworks. To the best of our
knowledge, ACE is the first end-to-end framework that automates the creation,
enrichment, and dynamic selection of enterprise API tools for LLM agents.

</details>


### [20] [Do Code Semantics Help? A Comprehensive Study on Execution Trace-Based Information for Code Large Language Models](https://arxiv.org/abs/2509.11686)
*Jian Wang,Xiaofei Xie,Qiang Hu,Shangqing Liu,Yi Li*

Main category: cs.SE

TL;DR: 本文研究发现代码大语言模型在推理程序运行时行为方面存在局限，语义信息（如执行轨迹）对监督微调和推理阶段的提升效果有限，与先前研究结论相反。


<details>
  <summary>Details</summary>
Motivation: 代码大语言模型在理解程序实际功能和推理运行时行为方面存在显著缺陷，现有方法对语义信息的表示不一致且碎片化，需要系统性的方法来增强其推理能力。

Method: 引入一个通用框架来整合语义信息（如执行轨迹）到代码任务相关的提示中，并全面研究语义信息在提升代码大语言模型推理能力中的作用，特别关注基于轨迹的语义信息在监督微调和推理阶段的效果。

Result: 实验结果与先前研究相反，表明语义信息对代码大语言模型的监督微调和测试时扩展的用处有限。

Conclusion: 语义信息在提升代码大语言模型的推理能力方面效果有限，需要探索其他更有效的方法来增强模型对程序运行时行为的理解能力。

Abstract: Code Large Language Models (Code LLMs) have opened a new era in programming
with their impressive capabilities. However, recent research has revealed
critical limitations in their ability to reason about runtime behavior and
understand the actual functionality of programs, which poses significant
challenges for their post-training and practical deployment. Specifically, Code
LLMs encounter two principal issues: (1) a lack of proficiency in reasoning
about program execution behavior, as they struggle to interpret what programs
actually do during runtime, and (2) the inconsistent and fragmented
representation of semantic information, such as execution traces, across
existing methods, which hinders their ability to generalize and reason
effectively. These challenges underscore the necessity for more systematic
approaches to enhance the reasoning capabilities of Code LLMs. To address these
issues, we introduce a generic framework to support integrating semantic
information~(e.g., execution trace) to code task-relevant prompts, and conduct
a comprehensive study to explore the role of semantic information in enhancing
the reasoning ability of Code LLMs accordingly. Specifically, we focus on
investigating the usefulness of trace-based semantic information in boosting
supervised fine-tuning~(SFT) and post-phase inference of Code LLMs. The
experimental results surprisingly disagree with previous works and demonstrate
that semantic information has limited usefulness for SFT and test time scaling
of Code LLM.

</details>


### [21] [AI Asset Management for Manufacturing (AIM4M): Development of a Process Model for Operationalization](https://arxiv.org/abs/2509.11691)
*Lukas Rauh,Mel-Rick Süner,Daniel Schel,Thomas Bauernhansl*

Main category: cs.SE

TL;DR: 提出基于MLOps原则的新流程模型，用于制造业AI资产全生命周期管理，解决CPPS环境中的实施挑战


<details>
  <summary>Details</summary>
Motivation: 制造业采用AI的好处明显，但将AI从原型扩展到实际运营，特别是在网络物理生产系统(CPPS)中，面临技术复杂性高、缺乏实施标准和碎片化组织流程等挑战

Method: 基于机器学习运维(MLOps)原则构建流程模型，针对CPPS领域的特定需求细化三个方面，支持AI资产的全生命周期管理

Result: 开发了一个理论贡献性的流程模型，旨在帮助组织在实践中系统化地开发、部署和管理AI资产

Conclusion: 该流程模型能够使AI资产在整个生命周期中与CPPS特定约束和监管要求保持一致，促进制造业AI的有效运营化

Abstract: The benefits of adopting artificial intelligence (AI) in manufacturing are
undeniable. However, operationalizing AI beyond the prototype, especially when
involved with cyber-physical production systems (CPPS), remains a significant
challenge due to the technical system complexity, a lack of implementation
standards and fragmented organizational processes. To this end, this paper
proposes a new process model for the lifecycle management of AI assets designed
to address challenges in manufacturing and facilitate effective
operationalization throughout the entire AI lifecycle. The process model, as a
theoretical contribution, builds on machine learning operations (MLOps)
principles and refines three aspects to address the domain-specific
requirements from the CPPS context. As a result, the proposed process model
aims to support organizations in practice to systematically develop, deploy and
manage AI assets across their full lifecycle while aligning with CPPS-specific
constraints and regulatory demands.

</details>


### [22] [From Evaluation to Enhancement: Large Language Models for Zero-Knowledge Proof Code Generation](https://arxiv.org/abs/2509.11708)
*Zhantong Xue,Pingchuan Ma,Zhaoyu Wang,Shuai Wang*

Main category: cs.SE

TL;DR: 本文提出了ZK-Eval评估框架和ZK-Coder代理框架，用于评估和提升大语言模型在零知识证明编程中的能力，显著提高了代码生成成功率。


<details>
  <summary>Details</summary>
Motivation: 零知识证明编程具有知识密集和易错的特点，现有大语言模型在通用编程领域表现优异，但在ZK编程中的有效性尚未被探索，需要专门的评估和改进方法。

Method: 提出ZK-Eval三级评估管道（语言知识、组件能力、端到端程序生成），并开发ZK-Coder代理框架，包含约束草图、引导检索和交互式修复三个组件。

Result: 对四个先进LLM的评估显示模型在表面语法上表现良好，但在组件使用和语义正确性上存在困难。ZK-Coder在Circom和Noir上分别将成功率从17.35%提升到83.38%和从32.21%提升到90.05%。

Conclusion: 通过ZK-Eval和ZK-Coder，为系统化测量和增强LLM在ZK代码生成中的能力奠定了基础，有助于降低实践门槛并推进可信计算发展。

Abstract: Zero-knowledge proofs (ZKPs) are increasingly deployed in domains such as
privacy-preserving authentication, blockchain scalability, and secure finance.
However, authoring ZK programs remains challenging: unlike mainstream
programming, ZK development requires reasoning about finite field arithmetic,
constraint systems, and gadgets, making it knowledge-intensive and error-prone.
While large language models (LLMs) have demonstrated strong code generation
capabilities in general-purpose languages, their effectiveness for ZK
programming, where correctness hinges on both language mastery and gadget-level
reasoning, remains unexplored. To address this gap, we propose
\textsc{ZK-Eval}, a domain-specific evaluation pipeline that probes LLM
capabilities at three levels: language knowledge, gadget competence, and
end-to-end program generation. Our evaluation of four state-of-the-art LLMs
reveals that models excel at surface-level syntax but struggle with gadget
usage and semantic correctness, often yielding incorrect programs. Based on
these insights, we introduce \textsc{ZK-Coder}, an agentic framework that
augments LLMs with constraint sketching, guided retrieval, and interactive
repair. Experiments on Circom and Noir show substantial gains, with success
rates improving from 17.35\% to 83.38\% and from 32.21\% to 90.05\%,
respectively. With \textsc{ZK-Eval} and \textsc{ZK-Coder}, we establish a
foundation for systematically measuring and augmenting LLMs in ZK code
generation to lower barriers for practitioners and advance trustworthy
computation.

</details>


### [23] [Toward Greener Background Processes -- Measuring Energy Cost of Autosave Feature](https://arxiv.org/abs/2509.11738)
*Maria Küüsvek,Hina Anwar*

Main category: cs.SE

TL;DR: 提出了一个可重用的三阶段流程来评估桌面应用后台功能的能耗行为，通过Python文本编辑器的自动保存功能案例研究，识别出影响能耗的关键设计因素并给出绿色实现建议


<details>
  <summary>Details</summary>
Motivation: 桌面应用的后台进程在能耗研究中常被忽视，但这些持续运行的自动化工作负载具有显著的累积影响，需要系统性的评估方法

Method: 三阶段流程：1) 将后台功能分解为核心操作 2) 操作隔离 3) 受控测量实现比较分析。在三个开源Python文本编辑器的自动保存实现中进行案例研究，进行了900次基于软件的能耗测量

Result: 识别出影响能耗的关键设计因素：保存频率、缓冲策略以及变更检测等辅助逻辑

Conclusion: 提出了四个可操作的建议，支持Python中自动保存功能的绿色实现，促进可持续软件开发实践

Abstract: Background processes in desktop applications are often overlooked in energy
consumption studies, yet they represent continuous, automated workloads with
significant cumulative impact. This paper introduces a reusable process for
evaluating the energy behavior of such features at the level of operational
design. The process works in three phases: 1) decomposing background
functionality into core operations, 2) operational isolation, and 3) controlled
measurements enabling comparative profiling. We instantiate the process in a
case study of autosave implementations across three open-source Python-based
text editors. Using 900 empirical software-based energy measurements, we
identify key design factors affecting energy use, including save frequency,
buffering strategy, and auxiliary logic such as change detection. We give four
actionable recommendations for greener implementations of autosave features in
Python to support sustainable software practices.

</details>


### [24] [Analysing Python Machine Learning Notebooks with Moose](https://arxiv.org/abs/2509.11748)
*Marius Mignard,Steven Costiou,Nicolas Anquetil,Anne Etien*

Main category: cs.SE

TL;DR: Vespucci Linter是一个多层次的静态分析工具，专门用于检测机器学习笔记本代码中的质量问题，涵盖Python编码规范、笔记本组织结构和ML特定问题三个层面。


<details>
  <summary>Details</summary>
Motivation: 机器学习代码（特别是在笔记本中）通常质量较低，存在三个层面的不良实践：通用Python编码规范、笔记本组织结构以及ML特定方面（如可重现性和正确API使用）。现有工具通常只关注其中一个层面，难以捕捉ML特定语义。

Method: 基于Moose构建的静态分析工具，采用元建模方法统一笔记本结构元素和Python代码实体，实现了22个基于文献的linting规则，并在Kaggle平台的5000个笔记本语料库上应用。

Result: 在所有三个层面都发现了违规行为，验证了多层次方法的相关性，证明了Vespucci Linter在提高笔记本环境中ML开发质量和可靠性方面的潜力。

Conclusion: Vespucci Linter通过多层次分析方法有效识别机器学习笔记本代码中的质量问题，为解决ML开发中的代码质量挑战提供了有力工具。

Abstract: Machine Learning (ML) code, particularly within notebooks, often exhibits
lower quality compared to traditional software. Bad practices arise at three
distinct levels: general Python coding conventions, the organizational
structure of the notebook itself, and ML-specific aspects such as
reproducibility and correct API usage. However, existing analysis tools
typically focus on only one of these levels and struggle to capture ML-specific
semantics, limiting their ability to detect issues. This paper introduces
Vespucci Linter, a static analysis tool with multi-level capabilities, built on
Moose and designed to address this challenge. Leveraging a metamodeling
approach that unifies the notebook's structural elements with Python code
entities, our linter enables a more contextualized analysis to identify issues
across all three levels. We implemented 22 linting rules derived from the
literature and applied our tool to a corpus of 5,000 notebooks from the Kaggle
platform. The results reveal violations at all levels, validating the relevance
of our multi-level approach and demonstrating Vespucci Linter's potential to
improve the quality and reliability of ML development in notebook environments.

</details>


### [25] [CodeCureAgent: Automatic Classification and Repair of Static Analysis Warnings](https://arxiv.org/abs/2509.11787)
*Pascal Joos,Islem Bouzenia,Michael Pradel*

Main category: cs.SE

TL;DR: CodeCureAgent是一个基于LLM的智能体方法，能够自动分析、分类和修复静态分析警告，无需预定义算法，通过迭代调用工具收集代码库信息并编辑代码来解决问题。


<details>
  <summary>Details</summary>
Motivation: 传统上开发者需要手动处理静态分析工具产生的警告，这个过程繁琐且容易导致警告积累和代码质量下降，因此需要自动化解决方案。

Method: 采用基于LLM的智能体框架，迭代调用工具收集代码库信息（如代码搜索），编辑代码库解决警告，并使用三步启发式方法验证补丁：构建项目、验证警告消失且无新警告、运行测试套件。

Result: 在106个Java项目的1000个SonarQube警告上评估，产生96.8%的合理修复，比最先进基线方法分别高出30.7%和29.2%，正确修复率达到86.3%，每个警告处理成本约2.9美分，处理时间约4分钟。

Conclusion: CodeCureAgent能够可靠地修复静态分析警告，有助于清理现有代码库并集成到CI/CD管道中防止警告积累。

Abstract: Static analysis tools are widely used to detect bugs, vulnerabilities, and
code smells. Traditionally, developers must resolve these warnings manually.
Because this process is tedious, developers sometimes ignore warnings, leading
to an accumulation of warnings and a degradation of code quality. This paper
presents CodeCureAgent, an approach that harnesses LLM-based agents to
automatically analyze, classify, and repair static analysis warnings. Unlike
previous work, our method does not follow a predetermined algorithm. Instead,
we adopt an agentic framework that iteratively invokes tools to gather
additional information from the codebase (e.g., via code search) and edit the
codebase to resolve the warning. CodeCureAgent detects and suppresses false
positives, while fixing true positives when identified. We equip CodeCureAgent
with a three-step heuristic to approve patches: (1) build the project, (2)
verify that the warning disappears without introducing new warnings, and (3)
run the test suite. We evaluate CodeCureAgent on a dataset of 1,000 SonarQube
warnings found in 106 Java projects and covering 291 distinct rules. Our
approach produces plausible fixes for 96.8% of the warnings, outperforming
state-of-the-art baseline approaches by 30.7% and 29.2% in plausible-fix rate,
respectively. Manual inspection of 291 cases reveals a correct-fix rate of
86.3%, showing that CodeCureAgent can reliably repair static analysis warnings.
The approach incurs LLM costs of about 2.9 cents (USD) and an end-to-end
processing time of about four minutes per warning. We envision CodeCureAgent
helping to clean existing codebases and being integrated into CI/CD pipelines
to prevent the accumulation of static analysis warnings.

</details>


### [26] [MMORE: Massive Multimodal Open RAG & Extraction](https://arxiv.org/abs/2509.11937)
*Alexandre Sallinen,Stefan Krsteski,Paul Teiletche,Marc-Antoine Allard,Baptiste Lecoeur,Michael Zhang,Fabrice Nemo,David Kalajdzic,Matthias Meyer,Mary-Anne Hartley*

Main category: cs.SE

TL;DR: MMORE是一个开源的多模态文档处理管道，支持15+种文件格式，提供统一的文档处理、混合检索和RAG功能，在速度和准确性上优于基准方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的文档数据具有多模态和异构特性，需要一种能够统一处理各种格式文档并支持大规模检索增强生成的系统。

Method: 采用模块化分布式架构，支持CPU和GPU并行处理，将不同格式文档转换为统一格式，集成稠密-稀疏混合检索技术，提供API和批量RAG端点。

Result: 在处理基准测试中比单节点基线快3.8倍，在扫描PDF上比Docling准确率高40%，在PubMedQA上医学LLM的问答准确性随检索深度增加而提升。

Conclusion: MMORE为在多模态真实世界数据上部署任务无关的RAG系统提供了强大且可扩展的基础设施。

Abstract: We introduce MMORE, an open-source pipeline for Massive Multimodal Open
RetrievalAugmented Generation and Extraction, designed to ingest, transform,
and retrieve knowledge from heterogeneous document formats at scale. MMORE
supports more than fifteen file types, including text, tables, images, emails,
audio, and video, and processes them into a unified format to enable downstream
applications for LLMs. The architecture offers modular, distributed processing,
enabling scalable parallelization across CPUs and GPUs. On processing
benchmarks, MMORE demonstrates a 3.8-fold speedup over single-node baselines
and 40% higher accuracy than Docling on scanned PDFs. The pipeline integrates
hybrid dense-sparse retrieval and supports both interactive APIs and batch RAG
endpoints. Evaluated on PubMedQA, MMORE-augmented medical LLMs improve
biomedical QA accuracy with increasing retrieval depth. MMORE provides a
robust, extensible foundation for deploying task-agnostic RAG systems on
diverse, real-world multimodal data. The codebase is available at
https://github.com/swiss-ai/mmore.

</details>


### [27] [VisDocSketcher: Towards Scalable Visual Documentation with Agentic Systems](https://arxiv.org/abs/2509.11942)
*Luís F. Gomes,Xin Zhou,David Lo,Rui Abreu*

Main category: cs.SE

TL;DR: 本文提出了VisDocSketcher，首个基于LLM代理的系统，通过结合静态分析和LLM代理自动从代码生成可视化文档，并开发了AutoSketchEval评估框架来量化评估生成质量。


<details>
  <summary>Details</summary>
Motivation: 可视化文档能有效降低开发者理解陌生代码的认知障碍，但手动创建耗时且难以评估。目前没有方法能自动从代码生成高质量的可视化文档，评估也缺乏标准化方法。

Method: 结合静态分析与LLM代理识别代码关键元素并生成可视化表示，提出AutoSketchEval评估框架使用代码级指标评估生成质量。

Result: 能为74.4%的样本生成有效可视化文档，相比基于模板的基线方法提升26.7-39.8%。评估框架能可靠区分高质量和低质量文档，AUC超过0.87。

Conclusion: 这项工作为自动化可视化文档研究奠定了基础，提供了既能生成有效可视化表示又能可靠评估质量的实用工具。

Abstract: Visual documentation is an effective tool for reducing the cognitive barrier
developers face when understanding unfamiliar code, enabling more intuitive
comprehension. Compared to textual documentation, it provides a higher-level
understanding of the system structure and data flow. Developers usually prefer
visual representations over lengthy textual descriptions for large software
systems. Visual documentation is both difficult to produce and challenging to
evaluate. Manually creating it is time-consuming, and currently, no existing
approach can automatically generate high-level visual documentation directly
from code. Its evaluation is often subjective, making it difficult to
standardize and automate. To address these challenges, this paper presents the
first exploration of using agentic LLM systems to automatically generate visual
documentation. We introduce VisDocSketcher, the first agent-based approach that
combines static analysis with LLM agents to identify key elements in the code
and produce corresponding visual representations. We propose a novel evaluation
framework, AutoSketchEval, for assessing the quality of generated visual
documentation using code-level metrics. The experimental results show that our
approach can valid visual documentation for 74.4% of the samples. It shows an
improvement of 26.7-39.8% over a simple template-based baseline. Our evaluation
framework can reliably distinguish high-quality (code-aligned) visual
documentation from low-quality (non-aligned) ones, achieving an AUC exceeding
0.87. Our work lays the foundation for future research on automated visual
documentation by introducing practical tools that not only generate valid
visual representations but also reliably assess their quality.

</details>


### [28] [LitterBox+: An Extensible Framework for LLM-enhanced Scratch Static Code Analysis](https://arxiv.org/abs/2509.12021)
*Benedikt Fein,Florian Obermüller,Gordon Fraser*

Main category: cs.SE

TL;DR: LitterBox+框架将Scratch积木编程转换为文本表示，结合LLMs为学习者提供代码查询、质量问题和修复建议，直接在Scratch环境中集成AI辅助功能


<details>
  <summary>Details</summary>
Motivation: 解决积木式编程环境Scratch中LLMs无法直接处理图形化代码的问题，为学习者提供AI编程辅助支持

Method: 扩展LitterBox静态分析工具，将积木代码转换为适合LLMs的文本表示，提供API和用户界面集成，支持多种提示词和LLM提供商

Result: 开发出可直接在Scratch环境中使用的LLM集成框架，支持程序查询、质量问题分析和代码修复生成

Conclusion: LitterBox+成功克服了图形化编程环境中使用LLMs的技术障碍，为编程教育提供了创新的AI辅助工具，框架具有良好的可扩展性

Abstract: Large language models (LLMs) have become an essential tool to support
developers using traditional text-based programming languages, but the
graphical notation of the block-based Scratch programming environment inhibits
the use of LLMs. To overcome this limitation, we propose the LitterBox+
framework that extends the Scratch static code analysis tool LitterBox with the
generative abilities of LLMs. By converting block-based code to a textual
representation suitable for LLMs, LitterBox+ allows users to query LLMs about
their programs, about quality issues reported by LitterBox, and it allows
generating code fixes. Besides offering a programmatic API for these
functionalities, LitterBox+ also extends the Scratch user interface to make
these functionalities available directly in the environment familiar to
learners. The framework is designed to be easily extensible with other prompts,
LLM providers, and new features combining the program analysis capabilities of
LitterBox with the generative features of LLMs. We provide a screencast
demonstrating the tool at https://youtu.be/RZ6E0xgrIgQ.

</details>


### [29] [A New Benchmark for Evaluating Code Translation with Third-Party Libraries](https://arxiv.org/abs/2509.12087)
*Pengyu Xue,Kunwu Zheng,Zhen Yang,Yifei Pei,Linhao Wu,Jiahui Dong,Xiapu Luo,Yan Xiao,Fei Liu,Yuxuan Zhang,Xiran Lyu,Xianhang Li,Xuanyu Zhu,Chengyi Wang*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In recent years, Large Language Models (LLMs) have been widely studied in the
code translation field on the method, class, and even repository levels.
However, most of these benchmarks are limited in terms of Third-Party Library
(TPL) categories and scales, making TPL-related errors hard to expose and
hindering the development of targeted solutions. Considering the high
dependence (over 90%) on TPLs in practical programming, demystifying and
analyzing LLMs' code translation performance involving various TPLs becomes
imperative. To address this gap, we construct TransLibEval, the first benchmark
dedicated to library-centric code translation. It consists of 200 real-world
tasks across Python, Java, and C++, each explicitly involving TPLs from diverse
categories such as data processing, machine learning, and web development, with
comprehensive dependency coverage and high-coverage test suites. We evaluate
seven recent LLMs of commercial, general, and code-specialized families under
six translation strategies of three categories: Direct, IR-guided, and
Retrieval-augmented. Experimental results show a dramatic performance drop
compared with library-free settings (average CA decline over 60%), while
diverse strategies demonstrate heterogeneous advantages. Furthermore, we
analyze 4,831 failed cases from GPT-4o, one of the State-of-the-Art (SOTA)
LLMs, revealing numerous third-party reference errors that were obscured
previously. These findings highlight the unique challenges of library-centric
translation and provide practical guidance for improving TPL-aware code
intelligence.

</details>


### [30] [EfficientUICoder: Efficient MLLM-based UI Code Generation via Input and Output Token Compression](https://arxiv.org/abs/2509.12159)
*Jingyu Xiao,Zhongyi Zhang,Yuxuan Wan,Yintong Huo,Yang Liu,Michael R. Lyu*

Main category: cs.SE

TL;DR: EfficientUICoder是一个用于UI代码生成的高效压缩框架，通过元素感知压缩、区域感知细化和自适应重复抑制，在保持网页质量的同时实现55%-60%的压缩比，显著降低计算成本和时间开销。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在UI转代码任务中计算开销巨大，存在大量图像和代码令牌冗余，导致计算复杂度高、关键UI元素关注不足，生成冗长且无效的HTML文件。

Method: 提出三组件压缩框架：1)元素和布局感知令牌压缩，检测元素区域并构建UI元素树；2)区域感知令牌细化，利用注意力分数筛选重要令牌；3)自适应重复令牌抑制，动态减少HTML/CSS结构重复生成。

Result: 实现55%-60%压缩比，计算成本降低44.9%，生成令牌减少41.4%，预填充时间减少46.6%，推理时间减少48.8%，且不损害网页质量。

Conclusion: EfficientUICoder有效解决了UI代码生成中的冗余问题，显著提升了多模态大语言模型在UI2Code任务中的效率，为高效网站开发提供了实用解决方案。

Abstract: Multimodal Large Language Models have demonstrated exceptional performance in
UI2Code tasks, significantly enhancing website development efficiency. However,
these tasks incur substantially higher computational overhead than traditional
code generation due to the large number of input image tokens and extensive
output code tokens required. Our comprehensive study identifies significant
redundancies in both image and code tokens that exacerbate computational
complexity and hinder focus on key UI elements, resulting in excessively
lengthy and often invalid HTML files. We propose EfficientUICoder, a
compression framework for efficient UI code generation with three key
components. First, Element and Layout-aware Token Compression preserves
essential UI information by detecting element regions and constructing UI
element trees. Second, Region-aware Token Refinement leverages attention scores
to discard low-attention tokens from selected regions while integrating
high-attention tokens from unselected regions. Third, Adaptive Duplicate Token
Suppression dynamically reduces repetitive generation by tracking HTML/CSS
structure frequencies and applying exponential penalties. Extensive experiments
show EfficientUICoderachieves a 55%-60% compression ratio without compromising
webpage quality and delivers superior efficiency improvements: reducing
computational cost by 44.9%, generated tokens by 41.4%, prefill time by 46.6%,
and inference time by 48.8% on 34B-level MLLMs. Code is available at
https://github.com/WebPAI/EfficientUICoder.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [31] [Mechanizing Synthetic Tait Computability in Istari](https://arxiv.org/abs/2509.11418)
*Runming Li,Yue Yao,Robert Harper*

Main category: cs.PL

TL;DR: 在Istari证明助手中实现了合成Tait可计算性(STC)的形式化，包括模态、扩展类型和严格胶合类型，并应用于两个案例研究：依赖类型理论的典范性模型和成本感知逻辑框架的Kripke典范性模型


<details>
  <summary>Details</summary>
Motivation: 范畴胶合是证明类型理论元定理（如典范性和归一化）的有力技术，STC通过将胶合范畴内化到具有相位区分的模态依赖类型理论中，为复杂胶合模型提供抽象处理

Method: 在Istari证明助手中开发可重用的合成相位区分库，包括模态、扩展类型和严格胶合类型，然后应用于两个具体案例研究

Result: 核心STC构造可以在Istari中几乎逐字形式化，保持了纸上论证的优雅性同时确保机器检查的正确性

Conclusion: 这项工作展示了在具有等式反射的扩展类型理论中成功实现STC形式化，为类型理论元定理的机器验证提供了有效途径

Abstract: Categorical gluing is a powerful technique for proving meta-theorems of type
theories such as canonicity and normalization. Synthetic Tait Computability
(STC) provides an abstract treatment of the complex gluing models by
internalizing the gluing category into a modal dependent type theory with a
phase distinction. This work presents a mechanization of STC in the Istari
proof assistant. Istari is a Martin-L\"{o}f-style extensional type theory with
equality reflection. Equality reflection eliminates the nuisance of transport
reasoning typically found in intensional proof assistants. This work develops a
reusable library for synthetic phase distinction, including modalities,
extension types, and strict glue types, and applies it to two case studies: (1)
a canonicity model for dependent type theory with dependent products and
booleans with large elimination, and (2) a Kripke canonicity model for the
cost-aware logical framework. Our results demonstrate that the core STC
constructions can be formalized essentially verbatim in Istari, preserving the
elegance of the on-paper arguments while ensuring machine-checked correctness.

</details>


### [32] [Efficient Decrease-and-Conquer Linearizability Monitoring](https://arxiv.org/abs/2410.04581)
*Lee Zheng Han,Umang Mathur*

Main category: cs.PL

TL;DR: 提出了一个统一的"decrease-and-conquer"算法框架用于线性化监控，通过识别线性化保持值来高效检测并发数据结构实现的正确性


<details>
  <summary>Details</summary>
Motivation: 线性化已成为并发数据结构实现的正确性标准，但形式化验证仍然困难。线性化监控作为开发过程中的重要第一步，能够早期发现问题，但目前对何时该问题可高效解决缺乏系统理解

Method: 提出了一个统一的"decrease-and-conquer"算法框架，核心是识别线性化保持值——这些值的存在使得移除后产生等价线性化的子历史，其缺失表明非线性化。将该框架实例化到集合、栈、队列和优先队列等常用数据类型

Result: 为多种数据类型开发了多项式时间算法，在无歧义限制下（每个插入操作添加不同值）实现了最优的对数线性时间复杂度。实验表明该方法可扩展到大型历史记录并优于现有工具

Conclusion: 该框架为线性化监控提供了系统化的理论基础，证明了识别线性化保持值问题的复杂度与监控问题复杂度等价，并为多种数据结构提供了高效监控算法

Abstract: Linearizability has become the de facto correctness specification for
implementations of concurrent data structures. While formally verifying such
implementations remains challenging, linearizability monitoring has emerged as
a promising first step to rule out early problems in the development of custom
implementations, and serves as a key component in approaches that stress test
such implementations. In this work, we investigate linearizability monitoring
-- check if an execution history of an implementation is linearizable. While
this problem is intractable in general, a systematic understanding of when it
becomes tractable has remained elusive. We revisit this problem and first
present a unified `decrease-and-conquer' algorithmic framework for
linearizability monitoring. At its heart, this framework asks to identify
special linearizability-preserving values in a given history -- values whose
presence yields an equilinearizable sub-history when removed, and whose absence
indicates non-linearizability. We prove that a polynomial time algorithm for
the problem of identifying linearizability-preserving values, yields a
polynomial time algorithm for linearizability monitoring, while conversely,
intractability of this problem implies intractability of the monitoring
problem. We demonstrate our framework's effectiveness by instantiating it for
several popular data types -- sets, stacks, queues and priority queues --
deriving polynomial time algorithms for each, with the unambiguity restriction,
where each insertion to the underlying data structure adds a distinct value. We
optimize these algorithms to achieve the optimal log-linear time complexity by
amortizing the cost of solving sub-problems through efficient data structures.
Our implementation and evaluation on publicly available implementations show
that our approach scales to large histories and outperforms existing tools.

</details>


### [33] [Expressive Power of One-Shot Control Operators and Coroutines](https://arxiv.org/abs/2509.11901)
*Kentaro Kobayashi,Yukiyoshi Kameyama*

Main category: cs.PL

TL;DR: 本文对单次控制操作符（包括异常处理、效果处理程序、分隔延续和不对称协程）的表达能力进行了严格的数学比较，填补了单次控制操作符研究的空白。


<details>
  <summary>Details</summary>
Motivation: 虽然大多数理论研究关注多次控制操作符，但单次控制操作符在表达能力和效率之间提供了更好的平衡，需要对其表达能力进行系统研究。

Method: 采用Felleisen的宏表达能力作为衡量标准，对单次效果处理程序、单次分隔控制操作符和不对称协程进行数学严谨的比较分析。

Result: 验证了单次效果处理程序和单次分隔控制操作符可以通过不对称协程进行宏表达，但反之不成立。修正了先前非正式论证的缺陷并提供了有效的宏转换方法。

Conclusion: 单次控制操作符之间存在明确的表达能力层次关系，不对称协程在表达能力上更强，为控制操作符的理论研究提供了重要贡献。

Abstract: Control operators, such as exceptions and effect handlers, provide a means of
representing computational effects in programs abstractly and modularly. While
most theoretical studies have focused on multi-shot control operators, one-shot
control operators -- which restrict the use of captured continuations to at
most once -- are gaining attention for their balance between expressiveness and
efficiency. This study aims to fill the gap. We present a mathematically
rigorous comparison of the expressive power among one-shot control operators,
including effect handlers, delimited continuations, and even asymmetric
coroutines. Following previous studies on multi-shot control operators, we
adopt Felleisen's macro-expressiveness as our measure of expressiveness. We
verify the folklore that one-shot effect handlers and one-shot
delimited-control operators can be macro-expressed by asymmetric coroutines,
but not vice versa. We explain why a previous informal argument fails, and how
to revise it to make a valid macro-translation.

</details>


### [34] [Enhanced Data Race Prediction Through Modular Reasoning](https://arxiv.org/abs/2504.10813)
*Zhendong Ang,Azadeh Farzan,Umang Mathur*

Main category: cs.PL

TL;DR: 本文系统化分析了并发程序数据竞争预测的两种正交方法（交换性和前缀推理），提出了新的粒度前缀竞争类别，并设计了高效的常数空间流式预测算法


<details>
  <summary>Details</summary>
Motivation: 现有数据竞争预测方法（交换性推理和前缀推理）大多是临时设计的，缺乏统一的理论基础，且各自存在局限性。本文旨在形式化这些方法的原理，并探索将它们结合以获得更强预测能力的方法

Method: 1) 形式化了交换性推理可预测的三种竞争类别并进行比较；2) 识别了三种前缀推理风格并证明它们预测相同的竞争类别；3) 提出了结合前缀推理和交换性推理的模块化方法，引入粒度前缀竞争新类别；4) 基于反链思想改进了前缀推理的常数空间算法

Result: 证明了前缀推理可预测的竞争类别包含所有交换性推理可预测的竞争。提出的粒度前缀竞争类别可在常数空间和线性时间内以流式方式预测，且包含所有交换性和前缀推理技术可预测的竞争

Conclusion: 通过系统化分析和结合两种正交方法，本文提出了更强大的数据竞争预测框架，为高效并发程序分析提供了新的理论基础和实用算法

Abstract: There are two orthogonal methodologies for efficient prediction of data races
from concurrent program runs: commutativity and prefix reasoning. There are
several instances of each methodology in the literature, with the goal of
predicting data races using a streaming algorithm where the required memory
does not grow proportional to the length of the observed run, but these
instances were mostly created in an ad hoc manner, without much attention to
their unifying underlying principles. In this paper, we identify and formalize
these principles for each category with the ultimate goal of paving the way for
combining them into a new algorithm which shares their efficiency
characteristics but offers strictly more prediction power. In particular, we
formalize three distinct classes of races predictable using commutativity
reasoning, and compare them. We identify three different styles of prefix
reasoning, and prove that they predict the same class of races, which provably
contains all races predictable by any commutativity reasoning technique.
  Our key contribution is combining prefix reasoning and commutativity
reasoning in a modular way to introduce a new class of races, granular prefix
races, that are predictable in constant-space and linear time, in a streaming
fashion. This class of races includes all races predictable using commutativity
and prefix reasoning techniques. We present an improved constant-space
algorithm for prefix reasoning alone based on the idea of antichains (from
language theory). This improved algorithm is the stepping stone that is
required to devise an efficient algorithm for prediction of granular prefix
races. We present experimental results to demonstrate the expressive power and
performance of our new algorithm.

</details>
