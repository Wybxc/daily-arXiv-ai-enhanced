<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]
- [cs.PL](#cs.PL) [Total: 4]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.LO](#cs.LO) [Total: 6]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Is Research Software Science a Metascience?](https://arxiv.org/abs/2509.13436)
*Evan Eisinger,Michael A. Heroux*

Main category: cs.SE

TL;DR: 本文探讨研究软件科学(RSS)是否应被视为元科学的一种形式，分析了两者的关系、重叠领域以及分类对认可度、资金和研究整合的影响。


<details>
  <summary>Details</summary>
Motivation: 随着研究日益依赖计算方法，科学结果的可靠性取决于研究软件的质量、可重复性和透明度。确保这些品质对科学完整性和发现至关重要，需要明确RSS在科学体系中的定位。

Method: 通过定义元科学和研究软件科学，比较两者的原则和目标，检验它们的重叠领域，分析支持与反对将RSS归类为元科学的论点。

Result: 分析发现RSS推进了元科学的核心目标，特别是在计算可重复性方面，并连接了研究的技术、社会和认知层面。RSS是否属于元科学取决于采用广义还是狭义的元科学定义。

Conclusion: RSS最好被理解为一个独特的跨学科领域，与元科学保持一致并在某些定义下属于元科学。无论分类如何，将科学严谨性应用于研究软件能确保发现工具符合发现本身的标准。

Abstract: As research increasingly relies on computational methods, the reliability of
scientific results depends on the quality, reproducibility, and transparency of
research software. Ensuring these qualities is critical for scientific
integrity and discovery. This paper asks whether Research Software Science
(RSS)--the empirical study of how research software is developed and
used--should be considered a form of metascience, the science of science.
Classification matters because it could affect recognition, funding, and
integration of RSS into research improvement. We define metascience and RSS,
compare their principles and objectives, and examine their overlaps. Arguments
for classification highlight shared commitments to reproducibility,
transparency, and empirical study of research processes. Arguments against
portraying RSS as a specialized domain focused on a tool rather than the
broader scientific enterprise. Our analysis finds RSS advances core goals of
metascience, especially in computational reproducibility, and bridges
technical, social, and cognitive aspects of research. Its classification
depends on whether one adopts a broad definition of metascience--any empirical
effort to improve science--or a narrow one focused on systemic and
epistemological structures. We argue RSS is best understood as a distinct
interdisciplinary domain that aligns with, and in some definitions fits within,
metascience. Recognizing it as such can strengthen its role in improving
reliability, justify funding, and elevate software development in research
institutions. Regardless of classification, applying scientific rigor to
research software ensures the tools of discovery meet the standards of the
discoveries themselves.

</details>


### [2] [An LLM Agentic Approach for Legal-Critical Software: A Case Study for Tax Prep Software](https://arxiv.org/abs/2509.13471)
*Sina Gogani-Khiabani,Ashutosh Trivedi,Diptikalyan Saha,Saeid Tizpaz-Niari*

Main category: cs.SE

TL;DR: 本文提出了一种基于智能代理的方法，使用高阶蜕变关系来测试法律关键软件，通过LLM驱动的框架自动化测试生成和代码合成，在复杂税法任务中表现优于前沿模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在将自然语言法规转换为可执行逻辑方面显示潜力，但在法律关键环境中的可靠性仍面临歧义和幻觉的挑战，特别是在美国联邦税务准备这样的案例中。

Method: 采用基于角色的多智能体框架，使用高阶蜕变关系进行蜕变测试，比较相似个体结构化变化下的系统输出，并利用LLM自动化测试生成和代码合成。

Result: 使用较小模型(GPT-4o-mini)的框架在最坏情况下达到45%的通过率，优于前沿模型(GPT-4o和Claude 3.5的9-15%)在复杂税法任务上的表现。

Conclusion: 智能代理LLM方法为从自然语言规范开发稳健、可信赖的法律关键软件提供了一条可行路径。

Abstract: Large language models (LLMs) show promise for translating natural-language
statutes into executable logic, but reliability in legally critical settings
remains challenging due to ambiguity and hallucinations. We present an agentic
approach for developing legal-critical software, using U.S. federal tax
preparation as a case study. The key challenge is test-case generation under
the oracle problem, where correct outputs require interpreting law. Building on
metamorphic testing, we introduce higher-order metamorphic relations that
compare system outputs across structured shifts among similar individuals.
Because authoring such relations is tedious and error-prone, we use an
LLM-driven, role-based framework to automate test generation and code
synthesis. We implement a multi-agent system that translates tax code into
executable software and incorporates a metamorphic-testing agent that searches
for counterexamples. In experiments, our framework using a smaller model
(GPT-4o-mini) achieves a worst-case pass rate of 45%, outperforming frontier
models (GPT-4o and Claude 3.5, 9-15%) on complex tax-code tasks. These results
support agentic LLM methodologies as a path to robust, trustworthy
legal-critical software from natural-language specifications.

</details>


### [3] [Prompt2DAG: A Modular Methodology for LLM-Based Data Enrichment Pipeline Generation](https://arxiv.org/abs/2509.13487)
*Abubakari Alidu,Michele Ciavotta,Flavio DePaoli*

Main category: cs.SE

TL;DR: Prompt2DAG是一种将自然语言描述转换为可执行Apache Airflow DAG的方法，通过混合方法实现了78.5%的成功率，显著优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 开发可靠的数据丰富管道需要大量工程专业知识，需要一种方法来自动化将自然语言描述转换为可执行工作流。

Method: 评估了四种生成方法（直接、仅LLM、混合和基于模板），在260个实验中使用13个LLM和5个案例研究，采用惩罚评分框架衡量可靠性、代码质量、结构完整性和可执行性。

Result: 混合方法是最佳生成方法，成功率达78.5%，质量得分稳健（SAT:6.79, DST:7.67, PCT:7.76），显著优于仅LLM方法（66.2%）和直接方法（29.2%）。可靠性是主要区分因素，混合方法的成本效益是直接提示的两倍以上。

Conclusion: 结构化混合方法对于在自动化工作流生成中平衡灵活性和可靠性至关重要，为民主化数据管道开发提供了可行路径。

Abstract: Developing reliable data enrichment pipelines demands significant engineering
expertise. We present Prompt2DAG, a methodology that transforms natural
language descriptions into executable Apache Airflow DAGs. We evaluate four
generation approaches -- Direct, LLM-only, Hybrid, and Template-based -- across
260 experiments using thirteen LLMs and five case studies to identify optimal
strategies for production-grade automation. Performance is measured using a
penalized scoring framework that combines reliability with code quality (SAT),
structural integrity (DST), and executability (PCT). The Hybrid approach
emerges as the optimal generative method, achieving a 78.5% success rate with
robust quality scores (SAT: 6.79, DST: 7.67, PCT: 7.76). This significantly
outperforms the LLM-only (66.2% success) and Direct (29.2% success) methods.
Our findings show that reliability, not intrinsic code quality, is the primary
differentiator. Cost-effectiveness analysis reveals the Hybrid method is over
twice as efficient as Direct prompting per successful DAG. We conclude that a
structured, hybrid approach is essential for balancing flexibility and
reliability in automated workflow generation, offering a viable path to
democratize data pipeline development.

</details>


### [4] [Crash Report Enhancement with Large Language Models: An Empirical Study](https://arxiv.org/abs/2509.13535)
*S M Farah Al Fahim,Md Nakhla Rafi,Zeyang Ma,Dong Jae Kim,Tse-Hsun,Chen*

Main category: cs.SE

TL;DR: 使用大语言模型增强崩溃报告，通过添加故障位置、根因解释和修复建议，显著提升调试效率


<details>
  <summary>Details</summary>
Motivation: 崩溃报告通常缺乏足够的诊断细节，导致开发者调试效率低下，需要探索如何利用LLM增强崩溃报告的有用性

Method: 研究两种增强策略：Direct-LLM（单次使用堆栈跟踪上下文）和Agentic-LLM（迭代探索代码库获取额外证据）

Result: 在492个真实崩溃报告数据集上，LLM增强报告将Top-1问题定位准确率从10.6%提升至40.2-43.1%，修复建议与开发者补丁高度相似（CodeBLEU约56-57%）

Conclusion: 为LLM提供堆栈跟踪和代码库信息可以生成显著更有用的增强崩溃报告，特别是Agentic-LLM在根因解释和修复指导方面表现更优

Abstract: Crash reports are central to software maintenance, yet many lack the
diagnostic detail developers need to debug efficiently. We examine whether
large language models can enhance crash reports by adding fault locations,
root-cause explanations, and repair suggestions. We study two enhancement
strategies: Direct-LLM, a single-shot approach that uses stack-trace context,
and Agentic-LLM, an iterative approach that explores the repository for
additional evidence. On a dataset of 492 real-world crash reports, LLM-enhanced
reports improve Top-1 problem-localization accuracy from 10.6% (original
reports) to 40.2-43.1%, and produce suggested fixes that closely resemble
developer patches (CodeBLEU around 56-57%). Both our manual evaluations and
LLM-as-a-judge assessment show that Agentic-LLM delivers stronger root-cause
explanations and more actionable repair guidance. A user study with 16
participants further confirms that enhanced reports make crashes easier to
understand and resolve, with the largest improvement in repair guidance. These
results indicate that supplying LLMs with stack traces and repository code
yields enhanced crash reports that are substantially more useful for debugging.

</details>


### [5] [GitHub's Copilot Code Review: Can AI Spot Security Flaws Before You Commit?](https://arxiv.org/abs/2509.13650)
*Amena Amro,Manar H. Alalfi*

Main category: cs.SE

TL;DR: GitHub Copilot的代码审查功能在检测安全漏洞方面效果不佳，主要关注低严重性问题而非关键安全漏洞


<details>
  <summary>Details</summary>
Motivation: 随着AI工具在软件开发中的广泛应用，评估AI辅助代码审查工具在安全编码支持方面的实际有效性变得至关重要

Method: 使用来自多个编程语言和应用领域的开源项目中精选的标记漏洞代码样本，系统评估Copilot检测安全漏洞的能力

Result: Copilot代码审查经常无法检测SQL注入、XSS和不安全反序列化等关键漏洞，主要反馈集中在编码风格和拼写错误等低严重性问题

Conclusion: AI辅助代码审查的实际效果与预期能力存在显著差距，仍需专用安全工具和人工代码审计来确保软件安全

Abstract: As software development practices increasingly adopt AI-powered tools,
ensuring that such tools can support secure coding has become critical. This
study evaluates the effectiveness of GitHub Copilot's recently introduced code
review feature in detecting security vulnerabilities. Using a curated set of
labeled vulnerable code samples drawn from diverse open-source projects
spanning multiple programming languages and application domains, we
systematically assessed Copilot's ability to identify and provide feedback on
common security flaws. Contrary to expectations, our results reveal that
Copilot's code review frequently fails to detect critical vulnerabilities such
as SQL injection, cross-site scripting (XSS), and insecure deserialization.
Instead, its feedback primarily addresses low-severity issues, such as coding
style and typographical errors. These findings expose a significant gap between
the perceived capabilities of AI-assisted code review and its actual
effectiveness in supporting secure development practices. Our results highlight
the continued necessity of dedicated security tools and manual code audits to
ensure robust software security.

</details>


### [6] [A Regression Testing Framework with Automated Assertion Generation for Machine Learning Notebooks](https://arxiv.org/abs/2509.13656)
*Yingao Elaine Yao,Vedant Nimje,Varun Viswanath,Saikat Dutta*

Main category: cs.SE

TL;DR: NBTest是一个针对Jupyter笔记本的回归测试框架，提供单元格级断言和自动化断言生成，旨在提高机器学习笔记本的可靠性而不增加开发者负担。


<details>
  <summary>Details</summary>
Motivation: Jupyter笔记本在机器学习开发中广泛使用，但缺乏测试支持，导致许多不易察觉的bug和性能回归问题难以被发现。

Method: 开发NBTest框架，包括断言API库、JupyterLab插件和自动化断言生成方法，支持在pytest和CI流水线中运行笔记本测试。

Result: 在592个Kaggle笔记本上生成21,163个断言（平均每个笔记本35.75个），变异得分为0.57，能够捕获回归bug，用户研究显示易用性评分4.3/5，实用性评分4.24/5。

Conclusion: NBTest有效提高了机器学习笔记本的测试能力，通过统计技术减少非确定性计算带来的测试不稳定性，已被实际ML库采用。

Abstract: Notebooks have become the de-facto choice for data scientists and machine
learning engineers for prototyping and experimenting with machine learning (ML)
pipelines. Notebooks provide an interactive interface for code, data, and
visualization. However, notebooks provide very limited support for testing.
Thus, during continuous development, many subtle bugs that do not lead to
crashes often go unnoticed and cause silent errors that manifest as performance
regressions.
  To address this, we introduce NBTest - the first regression testing framework
that allows developers to write cell-level assertions in notebooks and run such
notebooks in pytest or in continuous integration (CI) pipelines. NBTest offers
a library of assertion APIs, and a JupyterLab plugin that enables executing
assertions. We also develop the first automated approach for generating
cell-level assertions for key components in ML notebooks, such as data
processing, model building, and model evaluation. NBTest aims to improve the
reliability and maintainability of ML notebooks without adding developer
burden.
  We evaluate NBTest on 592 Kaggle notebooks. Overall, NBTest generates 21163
assertions (35.75 on average per notebook). The generated assertions obtain a
mutation score of 0.57 in killing ML-specific mutations. NBTest can catch
regression bugs in previous versions of the Kaggle notebooks using assertions
generated for the latest versions. Because ML pipelines involve non
deterministic computations, the assertions can be flaky. Hence, we also show
how NBTest leverages statistical techniques to minimize flakiness while
retaining high fault-detection effectiveness. NBTest has been adopted in the CI
of a popular ML library. Further, we perform a user study with 17 participants
that shows that notebook users find NBTest intuitive (Rating 4.3/5) and useful
in writing assertions and testing notebooks (Rating 4.24/5).

</details>


### [7] [Prompt Stability in Code LLMs: Measuring Sensitivity across Emotion- and Personality-Driven Variations](https://arxiv.org/abs/2509.13680)
*Wei Ma,Yixiao Yang,Jingquan Ge,Xiaofei Xie,Lingxiao Jiang*

Main category: cs.SE

TL;DR: PromptSE框架用于评估代码生成模型对提示词表达的敏感性，通过创建语义相同但情感和风格不同的提示变体，发现性能与稳定性是解耦的优化目标


<details>
  <summary>Details</summary>
Motivation: 代码生成模型在软件开发中广泛应用，但其对提示词表达的敏感性未被充分研究，相同需求用不同情感或沟通风格表达会产生不同输出，而现有基准主要关注峰值性能

Method: 提出PromptSE框架，使用情感和个性模板创建语义等价的提示变体，采用概率感知连续评分或二元通过率评估稳定性，并使用AUC-E指标进行跨模型比较

Result: 在14个模型（Llama、Qwen、DeepSeek）上的研究表明，性能和稳定性是解耦的优化目标，揭示了架构和规模相关的模式，挑战了关于模型鲁棒性的常见假设

Conclusion: PromptSE能够量化性能与稳定性的权衡，支持部署和模型选择，将提示稳定性定位为与性能和公平性互补的评估维度，有助于构建更可信的AI辅助软件开发工具

Abstract: Code generation models are widely used in software development, yet their
sensitivity to prompt phrasing remains under-examined. Identical requirements
expressed with different emotions or communication styles can yield divergent
outputs, while most benchmarks emphasize only peak performance. We present
PromptSE (Prompt Sensitivity Evaluation), a framework that creates semantically
equivalent prompt variants with emotion and personality templates, and that
evaluates stability using probability aware continuous scoring or using binary
pass rates when logits are unavailable. The results are aggregated into a
proposed area under curve metric (AUC-E) for cross model comparison. Across 14
models from three families (Llama, Qwen, and DeepSeek), our study shows that
performance and stability behave as largely decoupled optimization objectives,
and it reveals architectural and scale related patterns that challenge common
assumptions about model robustness. The framework supports rapid screening for
closed-source models as well as detailed stability analysis in research
settings. PromptSE enables practitioners to quantify performance stability
trade offs for deployment and model selection, positioning prompt stability as
a complementary evaluation dimension alongside performance and fairness, and
contributing to more trustworthy AI-assisted software development tools.

</details>


### [8] [Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning](https://arxiv.org/abs/2509.13755)
*Zhaoyang Chu,Yao Wan,Zhikun Zhang,Di Wang,Zhou Yang,Hongyu Zhang,Pan Zhou,Xuanhua Shi,Hai Jin,David Lo*

Main category: cs.SE

TL;DR: 本文提出CodeEraser方法，通过机器遗忘技术有效擦除代码语言模型中敏感信息的记忆，无需完整重新训练，在保持模型功能性的同时解决隐私泄露问题。


<details>
  <summary>Details</summary>
Motivation: 现有代码语言模型存在敏感训练数据记忆问题，可能导致机密信息泄露。现有解决方案需要完整重新训练，计算成本高昂，因此需要寻找更高效的敏感信息擦除方法。

Method: 采用机器遗忘技术，首先量化CLM训练数据中的记忆风险并构建5万个高风险样本数据集，研究基于梯度上升的遗忘方法，开发CodeEraser选择性擦除代码中敏感记忆片段同时保持代码结构完整性和功能正确性。

Result: 在CodeParrot、CodeGen-Mono和Qwen2.5-Coder三个CLM家族上的广泛实验验证了CodeEraser在擦除目标敏感记忆方面的有效性和效率，同时保持了模型实用性。

Conclusion: CodeEraser提供了一种高效的后处理方法，能够有效解决代码语言模型的隐私漏洞问题，为实际部署中的隐私保护提供了可行解决方案。

Abstract: While Code Language Models (CLMs) have demonstrated superior performance in
software engineering tasks such as code generation and summarization, recent
empirical studies reveal a critical privacy vulnerability: these models exhibit
unintended memorization of sensitive training data, enabling verbatim
reproduction of confidential information when specifically prompted. To address
this issue, several approaches, including training data de-duplication and
differential privacy augmentation, have been proposed. However, these methods
require full-model retraining for deployed CLMs, which incurs substantial
computational costs. In this paper, we aim to answer the following research
question: Can sensitive information memorized by CLMs be erased effectively and
efficiently?
  We conduct a pioneering investigation into erasing sensitive memorization in
CLMs through machine unlearning - a post-hoc modification method that removes
specific information from trained models without requiring full retraining.
Specifically, we first quantify the memorization risks of sensitive data within
CLM training datasets and curate a high-risk dataset of 50,000 sensitive
memorized samples as unlearning targets. We study two widely used gradient
ascent-based unlearning approaches: the vanilla and constraint-based methods,
and introduce CodeEraser, an advanced variant that selectively unlearns
sensitive memorized segments in code while preserving the structural integrity
and functional correctness of the surrounding code. Extensive experiments on
three families of CLMs, i.e., CodeParrot, CodeGen-Mono, and Qwen2.5-Coder,
validate the effectiveness and efficiency of CodeEraser in erasing targeted
sensitive memorization while maintaining model utility.

</details>


### [9] [A Study on Thinking Patterns of Large Reasoning Models in Code Generation](https://arxiv.org/abs/2509.13758)
*Kevin Halim,Sin G. Teo,Ruitao Feng,Zhenpeng Chen,Yang Gu,Chong Wang,Yang Liu*

Main category: cs.SE

TL;DR: 本文系统分析了大型推理模型(LRMs)在代码生成中的推理行为模式，通过人工标注推理轨迹建立了包含15种推理行为的分类法，揭示了不同模型的推理特点及其与代码正确性的关系，并提出了基于推理的提示策略改进方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型(LRMs)在代码生成任务中表现出强大的多步推理能力，但缺乏对这些模型推理模式的系统性分析，以及这些模式如何影响生成代码质量的研究。

Method: 使用多个先进LRM模型进行代码生成任务，采用开放式编码方法手动标注推理轨迹，从中推导出包含15种推理行为的分类法，涵盖四个推理阶段。

Result: 发现LRMs遵循类人编码工作流，复杂任务引发额外推理行为；不同模型推理模式差异显著(Qwen3迭代式，DeepSeek-R1-7B瀑布式)；单元测试创建和脚手架生成等行为与代码正确性强相关；基于推理的提示策略能有效改进代码质量。

Conclusion: 研究揭示了LRMs的推理行为模式及其对代码生成的影响，为改进自动代码生成提供了实践指导，展示了基于上下文和推理导向的提示策略的潜力。

Abstract: Currently, many large language models (LLMs) are utilized for software
engineering tasks such as code generation. The emergence of more advanced
models known as large reasoning models (LRMs), such as OpenAI's o3, DeepSeek
R1, and Qwen3. They have demonstrated the capability of performing multi-step
reasoning. Despite the advancement in LRMs, little attention has been paid to
systematically analyzing the reasoning patterns these models exhibit and how
such patterns influence the generated code. This paper presents a comprehensive
study aimed at investigating and uncovering the reasoning behavior of LRMs
during code generation. We prompted several state-of-the-art LRMs of varying
sizes with code generation tasks and applied open coding to manually annotate
the reasoning traces. From this analysis, we derive a taxonomy of LRM reasoning
behaviors, encompassing 15 reasoning actions across four phases.
  Our empirical study based on the taxonomy reveals a series of findings.
First, we identify common reasoning patterns, showing that LRMs generally
follow a human-like coding workflow, with more complex tasks eliciting
additional actions such as scaffolding, flaw detection, and style checks.
Second, we compare reasoning across models, finding that Qwen3 exhibits
iterative reasoning while DeepSeek-R1-7B follows a more linear, waterfall-like
approach. Third, we analyze the relationship between reasoning and code
correctness, showing that actions such as unit test creation and scaffold
generation strongly support functional outcomes, with LRMs adapting strategies
based on task context. Finally, we evaluate lightweight prompting strategies
informed by these findings, demonstrating the potential of context- and
reasoning-oriented prompts to improve LRM-generated code. Our results offer
insights and practical implications for advancing automatic code generation.

</details>


### [10] [Who is Introducing the Failure? Automatically Attributing Failures of Multi-Agent Systems via Spectrum Analysis](https://arxiv.org/abs/2509.13782)
*Yu Ge,Linna Xie,Zhong Li,Yu Pei,Tian Zhang*

Main category: cs.SE

TL;DR: FAMAS是首个基于频谱的多智能体系统故障归因方法，通过轨迹重放和频谱分析来识别导致故障的具体智能体行为


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在自动化复杂任务时存在故障，但故障归因困难且人工密集，阻碍了系统调试和改进

Method: 提出FAMAS方法，通过系统性的轨迹重放和抽象，然后进行频谱分析，估计每个智能体行为导致故障的可能性。设计了专门针对MAS的怀疑度公式，整合智能体行为组和动作行为组两个关键因素

Result: 在Who and When基准测试中与12个基线方法比较，FAMAS表现出优越性能，优于所有对比方法

Conclusion: FAMAS为多智能体系统提供了一种有效的自动化故障归因解决方案，有助于系统调试和改进

Abstract: Large Language Model Powered Multi-Agent Systems (MASs) are increasingly
employed to automate complex real-world problems, such as programming and
scientific discovery. Despite their promising, MASs are not without their
flaws. However, failure attribution in MASs - pinpointing the specific agent
actions responsible for failures - remains underexplored and labor-intensive,
posing significant challenges for debugging and system improvement. To bridge
this gap, we propose FAMAS, the first spectrum-based failure attribution
approach for MASs, which operates through systematic trajectory replay and
abstraction, followed by spectrum analysis.The core idea of FAMAS is to
estimate, from variations across repeated MAS executions, the likelihood that
each agent action is responsible for the failure. In particular, we propose a
novel suspiciousness formula tailored to MASs, which integrates two key factor
groups, namely the agent behavior group and the action behavior group, to
account for the agent activation patterns and the action activation patterns
within the execution trajectories of MASs. Through expensive evaluations
against 12 baselines on the Who and When benchmark, FAMAS demonstrates superior
performance by outperforming all the methods in comparison.

</details>


### [11] [Trace Sampling 2.0: Code Knowledge Enhanced Span-level Sampling for Distributed Tracing](https://arxiv.org/abs/2509.13852)
*Yulun Wu,Guangba Yu,Zhihan Jiang,Yichen Li,Michael R. Lyu*

Main category: cs.SE

TL;DR: Trace Sampling 2.0通过在span级别进行采样，在保持trace结构一致性的同时显著减少存储开销，Autoscope实现了81.2%的trace大小减少和98.1%的故障span覆盖率。


<details>
  <summary>Details</summary>
Motivation: 分布式追踪在微服务系统中是重要的诊断工具，但海量trace数据给后端存储带来巨大负担。传统trace采样方法通常会丢弃有价值的正常trace信息，影响对比分析。

Method: 提出了Trace Sampling 2.0概念，在span级别进行采样同时保持trace结构一致性。设计实现了Autoscope系统，利用静态分析提取执行逻辑，确保关键span被保留而不破坏结构完整性。

Result: 在两个开源微服务上评估显示：trace大小减少81.2%，故障span覆盖率达到98.1%，优于现有trace级别采样方法。在根因分析中平均提升8.3%的效果。

Conclusion: Autoscope能够显著提升微服务系统的可观测性和存储效率，为性能监控提供了强大的解决方案。

Abstract: Distributed tracing is an essential diagnostic tool in microservice systems,
but the sheer volume of traces places a significant burden on backend storage.
A common approach to mitigating this issue is trace sampling, which selectively
retains traces based on specific criteria, often preserving only anomalous
ones. However, this method frequently discards valuable information, including
normal traces that are essential for comparative analysis. To address this
limitation, we introduce Trace Sampling 2.0, which operates at the span level
while maintaining trace structure consistency. This approach allows for the
retention of all traces while significantly reducing storage overhead. Based on
this concept, we design and implement Autoscope, a span-level sampling method
that leverages static analysis to extract execution logic, ensuring that
critical spans are preserved without compromising structural integrity. We
evaluated Autoscope on two open-source microservices. Our results show that it
reduces trace size by 81.2% while maintaining 98.1% faulty span coverage,
outperforming existing trace-level sampling methods. Furthermore, we
demonstrate its effectiveness in root cause analysis, achieving an average
improvement of 8.3%. These findings indicate that Autoscope can significantly
enhance observability and storage efficiency in microservices, offering a
robust solution for performance monitoring.

</details>


### [12] [Are Prompts All You Need? Evaluating Prompt-Based Large Language Models (LLM)s for Software Requirements Classification](https://arxiv.org/abs/2509.13868)
*Manal Binkhonain,Reem Alfayaz*

Main category: cs.SE

TL;DR: 本研究测试基于提示的大语言模型是否能减少需求分类中的数据需求，通过多种提示方式在多个任务上验证，发现few shot提示的LLM可以匹配或超越传统微调模型，减少对大量标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 传统需求分类模型依赖监督学习，需要大量标注数据，成本高、创建慢、领域依赖性强，且泛化能力差。本研究旨在探索基于提示的大语言模型是否能减少数据需求并提高泛化能力。

Method: 在PROMISE和SecReq两个英文数据集上，对多种大语言模型和提示风格（zero shot、few shot、persona、chain of thought）进行基准测试，并与强微调transformer基线进行比较。

Result: 结果显示基于提示的LLM，特别是few shot提示，可以匹配或超越基线模型。添加persona或persona加chain of thought可以带来进一步的性能提升。

Conclusion: 基于提示的大语言模型是实用且可扩展的选择，减少了对大量标注数据的依赖，并能提高跨任务的泛化能力。

Abstract: Requirements classification assigns natural language requirements to
predefined classes, such as functional and non functional. Accurate
classification reduces risk and improves software quality. Most existing models
rely on supervised learning, which needs large labeled data that are costly,
slow to create, and domain dependent; they also generalize poorly and often
require retraining for each task. This study tests whether prompt based large
language models can reduce data needs. We benchmark several models and
prompting styles (zero shot, few shot, persona, and chain of thought) across
multiple tasks on two English datasets, PROMISE and SecReq. For each task we
compare model prompt configurations and then compare the best LLM setups with a
strong fine tuned transformer baseline. Results show that prompt based LLMs,
especially with few shot prompts, can match or exceed the baseline. Adding a
persona, or persona plus chain of thought, can yield further gains. We conclude
that prompt based LLMs are a practical and scalable option that reduces
dependence on large annotations and can improve generalizability across tasks.

</details>


### [13] [Mind the Ethics! The Overlooked Ethical Dimensions of GenAI in Software Modeling Education](https://arxiv.org/abs/2509.13896)
*Shalini Chakraborty,Lola Burgueño,Nathalie Moreno,Javier Troya,Paula Muñoz*

Main category: cs.SE

TL;DR: 本文通过系统性文献综述发现，生成式AI在软件建模教育中的伦理考量严重缺失，在1386篇相关论文中仅有3篇明确讨论伦理问题


<details>
  <summary>Details</summary>
Motivation: 生成式AI在软件建模教育中迅速普及，但缺乏明确的伦理监督和教学指导，其伦理影响尚未得到充分探索

Method: 对计算机科学六大数字图书馆（ACM、IEEE、Scopus等）进行系统性文献综述，识别讨论GenAI在软件建模教育中伦理方面的研究

Result: 从1386篇论文中仅发现3篇明确讨论伦理考量，突显了该领域伦理讨论的严重缺失

Conclusion: 迫切需要建立结构化伦理框架，负责任地将AI整合到建模课程中，并需要更多研究来应对这一新兴教育领域的伦理挑战

Abstract: Generative Artificial Intelligence (GenAI) is rapidly gaining momentum in
software modeling education, embraced by both students and educators. As GenAI
assists with interpreting requirements, formalizing models, and translating
students' mental models into structured notations, it increasingly shapes core
learning outcomes such as domain comprehension, diagrammatic thinking, and
modeling fluency without clear ethical oversight or pedagogical guidelines.
Yet, the ethical implications of this integration remain underexplored.
  In this paper, we conduct a systematic literature review across six major
digital libraries in computer science (ACM Digital Library, IEEE Xplore,
Scopus, ScienceDirect, SpringerLink, and Web of Science). Our aim is to
identify studies discussing the ethical aspects of GenAI in software modeling
education, including responsibility, fairness, transparency, diversity, and
inclusion among others.
  Out of 1,386 unique papers initially retrieved, only three explicitly
addressed ethical considerations. This scarcity highlights the critical absence
of ethical discourse surrounding GenAI in modeling education and raises urgent
questions about the responsible integration of AI in modeling curricula, as
well as it evinces the pressing need for structured ethical frameworks in this
emerging educational landscape. We examine these three studies and explore the
emerging research opportunities as well as the challenges that have arisen in
this field.

</details>


### [14] [An Empirical Study on Failures in Automated Issue Solving](https://arxiv.org/abs/2509.13941)
*Simiao Liu,Fang Liu,Liehao Li,Xin Tan,Yinghao Zhu,Xiaoli Lian,Li Zhang*

Main category: cs.SE

TL;DR: 本文分析了自动化问题解决中LLM代理工具的失败模式，提出了一个包含3个阶段、9个类别和25个子类别的失败分类法，并设计了一个专家-执行者协作框架来解决推理缺陷和认知死锁问题。


<details>
  <summary>Details</summary>
Motivation: 当前自动化问题解决评估主要报告聚合成功率，难以诊断模型弱点或指导针对性改进。需要深入分析失败原因，为构建更强大的代理提供诊断性评估和协作设计方法。

Method: 首先分析三种SOTA工具在SWE-Bench-Verified上的性能和效率，然后对150个失败实例进行系统手动分析，建立失败模式分类法，最后提出专家-执行者协作框架来纠正推理缺陷和认知死锁。

Result: 研究发现两种架构范式具有不同的失败特征，多数代理失败源于推理缺陷和认知死锁。提出的协作框架解决了领先单代理22.2%的先前无法解决的问题。

Conclusion: 通过诊断性评估和协作设计，可以构建更强大的自动化问题解决代理，专家-执行者框架有效解决了单代理的主要失败模式。

Abstract: Automated issue solving seeks to autonomously identify and repair defective
code snippets across an entire codebase. SWE-Bench has emerged as the most
widely adopted benchmark for evaluating progress in this area. While LLM-based
agentic tools show great promise, they still fail on a substantial portion of
tasks. Moreover, current evaluations primarily report aggregate issue-solving
rates, which obscure the underlying causes of success and failure, making it
challenging to diagnose model weaknesses or guide targeted improvements. To
bridge this gap, we first analyze the performance and efficiency of three SOTA
tools, spanning both pipeline-based and agentic architectures, in automated
issue solving tasks of SWE-Bench-Verified under varying task characteristics.
Furthermore, to move from high-level performance metrics to underlying cause
analysis, we conducted a systematic manual analysis of 150 failed instances.
From this analysis, we developed a comprehensive taxonomy of failure modes
comprising 3 primary phases, 9 main categories, and 25 fine-grained
subcategories. Then we systematically analyze the distribution of the
identified failure modes, the results reveal distinct failure fingerprints
between the two architectural paradigms, with the majority of agentic failures
stemming from flawed reasoning and cognitive deadlocks. Motivated by these
insights, we propose a collaborative Expert-Executor framework. It introduces a
supervisory Expert agent tasked with providing strategic oversight and
course-correction for a primary Executor agent. This architecture is designed
to correct flawed reasoning and break the cognitive deadlocks that frequently
lead to failure. Experiments show that our framework solves 22.2% of previously
intractable issues for a leading single agent. These findings pave the way for
building more robust agents through diagnostic evaluation and collaborative
design.

</details>


### [15] [Evaluating Classical Software Process Models as Coordination Mechanisms for LLM-Based Software Generation](https://arxiv.org/abs/2509.13942)
*Duc Minh Ha,Phu Trac Kien,Tho Quan,Anh Nguyen-Duc*

Main category: cs.SE

TL;DR: 本研究探讨如何将传统软件开发流程（瀑布模型、V模型、敏捷）作为协调框架应用于基于大语言模型的多智能体系统，分析不同流程对代码质量、成本和生产率的影响。


<details>
  <summary>Details</summary>
Motivation: 大语言模型驱动的多智能体系统正在改变软件开发方式，传统软件开发流程提供了结构化的协调模式，可以重新用于指导这些智能体之间的交互。

Method: 在三种流程模型和四种GPT变体下执行了11个不同的软件项目，共132次运行。使用标准化指标评估输出结果，包括规模（文件数、代码行数）、成本（执行时间、token使用量）和质量（代码异味、AI和人工检测的bug）。

Result: 流程模型和LLM选择都显著影响系统性能。瀑布模型最有效率，V模型产生最冗长的代码，敏捷方法实现最高代码质量但计算成本更高。

Conclusion: 传统软件流程可以有效地在基于LLM的多智能体系统中实例化，但每种流程在质量、成本和适应性方面都存在权衡。流程选择应反映项目目标，无论是优先考虑效率、健壮性还是结构化验证。

Abstract: [Background] Large Language Model (LLM)-based multi-agent systems (MAS) are
transforming software development by enabling autonomous collaboration.
Classical software processes such asWaterfall, V-Model, and Agile offer
structured coordination patterns that can be repurposed to guide these agent
interactions. [Aims] This study explores how traditional software development
processes can be adapted as coordination scaffolds for LLM based MAS and
examines their impact on code quality, cost, and productivity. [Method] We
executed 11 diverse software projects under three process models and four GPT
variants, totaling 132 runs. Each output was evaluated using standardized
metrics for size (files, LOC), cost (execution time, token usage), and quality
(code smells, AI- and human detected bugs). [Results] Both process model and
LLM choice significantly affected system performance. Waterfall was most
efficient, V-Model produced the most verbose code, and Agile achieved the
highest code quality, albeit at higher computational cost. [Conclusions]
Classical software processes can be effectively instantiated in LLM-based MAS,
but each entails trade-offs across quality, cost, and adaptability. Process
selection should reflect project goals, whether prioritizing efficiency,
robustness, or structured validation.

</details>


### [16] [Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A Self-Optimizing Framework](https://arxiv.org/abs/2509.14093)
*Kerui Huang,Shuhan Liu,Xing Hu,Tongtong Xu,Lingfeng Bao,Xin Xia*

Main category: cs.SE

TL;DR: Chain-of-Thought推理虽然提升LLM性能但带来高昂计算成本，研究发现过长推理反而有害。提出SEER框架自适应压缩CoT，在保持精度的同时减少42.1%推理长度并消除无限循环。


<details>
  <summary>Details</summary>
Motivation: CoT推理虽然能提高LLM在算术、逻辑和常识任务中的准确性和鲁棒性，但会导致计算成本激增（延迟增加、内存使用上升、KV缓存需求增大），特别是在需要简洁确定性输出的软件工程任务中。需要研究这种权衡关系并找到优化方案。

Method: 提出SEER（Self-Enhancing Efficient Reasoning）自适应框架，结合Best-of-N采样和任务感知自适应过滤，通过预推理输出动态调整阈值来压缩CoT推理过程，减少冗余和计算开销。

Result: 在三个软件工程任务和一个数学任务上的评估显示：SEER平均缩短CoT推理42.1%，通过减少截断提高准确性，并消除了大多数无限循环问题。

Conclusion: SEER是一种实用方法，能使CoT增强的LLM更加高效和鲁棒，即使在资源受限环境下也能保持性能，挑战了"推理越长越好"的假设。

Abstract: Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by
prompting intermediate steps, improving accuracy and robustness in arithmetic,
logic, and commonsense tasks. However, this benefit comes with high
computational costs: longer outputs increase latency, memory usage, and
KV-cache demands. These issues are especially critical in software engineering
tasks where concise and deterministic outputs are required. To investigate
these trade-offs, we conduct an empirical study based on code generation
benchmarks. The results reveal that longer CoT does not always help. Excessive
reasoning often causes truncation, accuracy drops, and latency up to five times
higher, with failed outputs consistently longer than successful ones. These
findings challenge the assumption that longer reasoning is inherently better
and highlight the need for adaptive CoT control. Motivated by this, we propose
SEER (Self-Enhancing Efficient Reasoning), an adaptive framework that
compresses CoT while preserving accuracy. SEER combines Best-of-N sampling with
task-aware adaptive filtering, dynamically adjusting thresholds based on
pre-inference outputs to reduce verbosity and computational overhead. We then
evaluate SEER on three software engineering tasks and one math task. On
average, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,
and eliminates most infinite loops. These results demonstrate SEER as a
practical method to make CoT-enhanced LLMs more efficient and robust, even
under resource constraints.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [17] [Catalpa: GC for a Low-Variance Software Stack](https://arxiv.org/abs/2509.13429)
*Anthony Arnold,Mark Marron*

Main category: cs.PL

TL;DR: 本文提出了一种名为Catalpa的新型垃圾收集器设计，专为Bosque编程语言和运行时系统开发，旨在通过利用语言特性实现低延迟、低变异性的高性能垃圾回收。


<details>
  <summary>Details</summary>
Motivation: 实际应用中性能往往被视为二元函数（要么足够快，要么用户可感知延迟），工业开发者更关注95%和99%百分位尾延迟而非平均响应时间。需要构建能够主动支持这些需求的软件栈。

Method: 利用Bosque语言的不变性和无引用环特性，设计无需屏障或应用代码同步的垃圾收集器，实现有界收集暂停和固定常数内存开销。

Result: Catalpa收集器能够最小化延迟和变异性，同时保持高吞吐量和小内存开销，具有有界收集暂停和固定常数内存开销特性。

Conclusion: 通过编程语言和运行时系统设计可以构建出能够满足工业级性能需求的垃圾收集器，Catalpa收集器为Bosque语言提供了高性能的内存管理解决方案。

Abstract: The performance of an application/runtime is usually conceptualized as a
continuous function where, the lower the amount of memory/time used on a given
workload, then the better the compiler/runtime is. However, in practice, good
performance of an application is viewed as more of a binary function - either
the application responds in under, say 100 ms, and is fast enough for a user to
barely notice, or it takes a noticeable amount of time, leaving the user
waiting and potentially abandoning the task. Thus, performance really means how
often the application is fast enough to be usable, leading industrial
developers to focus on the 95th and 99th percentile tail-latencies as heavily,
or moreso, than average response time. Our vision is to create a software stack
that actively supports these needs via programming language and runtime system
design. In this paper we present a novel garbage-collector design, the Catalpa
collector, for the Bosque programming language and runtime. This allocator is
designed to minimize latency and variability while maintaining high-throughput
and incurring small memory overheads. To achieve these goals we leverage
various features of the Bosque language, including immutability and
reference-cycle freedom, to construct a collector that has bounded collection
pauses, incurs fixed-constant memory overheads, and does not require any
barriers or synchronization with application code.

</details>


### [18] [Extended Abstract: Towards a Performance Comparison of Syntax and Type-Directed NbE](https://arxiv.org/abs/2509.13489)
*Chester J. F. Gould,William J. Bowman*

Main category: cs.PL

TL;DR: 本文提出了一个用于直接比较语法导向和类型导向两种类型相等性检查方法的实验平台，旨在量化类型导向方法的性能差异并分析改进途径


<details>
  <summary>Details</summary>
Motivation: 依赖类型检查器中类型相等性检查是关键部分，现有研究声称语法导向方法性能更好而类型导向方法更表达性强，但缺乏直接比较的实验平台

Method: 开发一个现实的实验平台，支持对两种方法进行直接、公平的比较，量化类型导向检查的性能差异，并分析改进方法

Result: 工作正在进行中，旨在提供量化数据和性能分析

Conclusion: 通过建立直接比较平台，可以精确评估两种类型相等性检查方法的性能差异，为优化类型导向方法提供依据

Abstract: A key part of any dependent type-checker is the method for checking whether
two types are equal. A common claim is that syntax-directed equality is more
performant, although type-directed equality is more expressive. However, this
claim is difficult to make precise, since implementations choose only one or
the other approach, making a direct comparison impossible. We present some
work-in-progress developing a realistic platform for direct, apples-to-apples,
comparison of the two approaches, quantifying how much slower type-directed
equality checking is, and analyzing why and how it can be improved.

</details>


### [19] [CLMTracing: Black-box User-level Watermarking for Code Language Model Tracing](https://arxiv.org/abs/2509.13982)
*Boyu Zhang,Ping He,Tianyu Du,Xuhong Zhang,Lei Yun,Kingsum Chow,Jianwei Yin*

Main category: cs.PL

TL;DR: CLMTracing是一个黑盒代码语言模型水印框架，通过规则水印和保持效用的注入方法实现用户级追踪，具有强鲁棒性对抗移除攻击


<details>
  <summary>Details</summary>
Motivation: 随着开源代码语言模型的广泛采用，知识产权保护变得日益重要。现有水印技术在面对黑盒设置下的用户级追踪需求时存在局限

Method: 采用基于规则的水印和保持效用的注入方法，结合对鲁棒水印敏感的参数选择算法和对抗训练来增强鲁棒性

Result: 在多个最先进代码语言模型上有效，相比现有基线显著无害改进，对各种移除攻击表现出强鲁棒性

Conclusion: CLMTracing为代码语言模型提供了有效的黑盒用户级追踪解决方案，解决了实际应用中的知识产权保护需求

Abstract: With the widespread adoption of open-source code language models (code LMs),
intellectual property (IP) protection has become an increasingly critical
concern. While current watermarking techniques have the potential to identify
the code LM to protect its IP, they have limitations when facing the more
practical and complex demand, i.e., offering the individual user-level tracing
in the black-box setting. This work presents CLMTracing, a black-box code LM
watermarking framework employing the rule-based watermarks and
utility-preserving injection method for user-level model tracing. CLMTracing
further incorporates a parameter selection algorithm sensitive to the robust
watermark and adversarial training to enhance the robustness against watermark
removal attacks. Comprehensive evaluations demonstrate CLMTracing is effective
across multiple state-of-the-art (SOTA) code LMs, showing significant harmless
improvements compared to existing SOTA baselines and strong robustness against
various removal attacks.

</details>


### [20] [Parallelizable Feynman-Kac Models for Universal Probabilistic Programming](https://arxiv.org/abs/2509.14092)
*Michele Boreale,Luisa Collodi*

Main category: cs.PL

TL;DR: 本文为概率程序开发了基于Sequential Monte Carlo的向量化粒子滤波算法VPF，并证明了其在无限执行轨迹上的语义一致性和有限近似定理。


<details>
  <summary>Details</summary>
Motivation: 为了解决通用概率程序中任意测度采样和无界循环条件/重加权操作的推理问题，需要建立形式化的操作语义并开发可证明正确高效的推理算法。

Method: 首先为概率程序图(PPG)建立基于无限执行轨迹的期望语义，然后证明有限近似定理，将语义框架纳入Feynman-Kac模型，确保粒子滤波算法的语义一致性，最后提出向量化粒子滤波算法VPF。

Result: 实验证明VPF算法相比现有最先进的概率程序推理工具表现出非常有前景的结果。

Conclusion: 本文为概率程序提供了形式化的语义基础和可证明正确的SMC推理算法，VPF算法在实验中显示出优越性能。

Abstract: We study provably correct and efficient instantiations of Sequential Monte
Carlo (SMC) inference in the context of formal operational semantics of
Probabilistic Programs (PPs). We focus on universal PPs featuring sampling from
arbitrary measures and conditioning/reweighting in unbounded loops. We first
equip Probabilistic Program Graphs (PPGs), an automata-theoretic description
format of PPs, with an expectation-based semantics over infinite execution
traces, which also incorporates trace weights. We then prove a finite
approximation theorem that provides bounds to this semantics based on
expectations taken over finite, fixed-length traces. This enables us to frame
our semantics within a Feynman-Kac (FK) model, and ensures the consistency of
the Particle Filtering (PF) algorithm, an instance of SMC, with respect to our
semantics. Building on these results, we introduce VPF, a vectorized version of
the PF algorithm tailored to PPGs and our semantics. Experiments conducted with
a proof-of-concept implementation of VPF show very promising results compared
to state-of-the-art PP inference tools.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [21] [How Concise are Chains of co-Büchi Automata?](https://arxiv.org/abs/2509.14087)
*Rüdiger Ehlers*

Main category: cs.FL

TL;DR: COCOA（co-Büchi自动机链）是ω-正则语言的新规范表示模型，本文分析其简洁性，证明即使所有自动机都是确定性的，COCOA也比确定性奇偶自动机指数级更简洁，但布尔运算会导致指数级增长。


<details>
  <summary>Details</summary>
Motivation: 研究COCOA作为ω-正则语言规范表示模型的简洁性特性，特别是与早期自动机模型的关系以及布尔运算对其简洁性的影响。

Method: 通过理论分析和构造性证明，比较COCOA与确定性奇偶自动机在表示相同语言时的状态复杂度，并分析布尔运算（析取和合取）对COCOA大小的影响。

Result: 发现确定性COCOA可以指数级更简洁地表示某些语言，但布尔运算会导致COCOA大小的指数级增长，而同样的运算在确定性奇偶自动机上只需要多项式增长。

Conclusion: 虽然COCOA在单独表示语言时具有指数级简洁性优势，但这种优势在进行布尔运算时无法保持，限制了其在需要复杂运算的应用中的实用性。

Abstract: Chains of co-B\"uchi automata (COCOA) have recently been introduced as a new
canonical model for representing arbitrary omega-regular languages. They can be
minimized in polynomial time and are hence an attractive language
representation for applications in which normally, deterministic omega-automata
are used. While it is known how to build COCOA from deterministic parity
automata, little is currently known about their relationship to automaton
models introduced earlier than COCOA.
  In this paper, we analyze the conciseness of chains of co-B\"uchi automata.
We show that even in the case that all automata in the chain are deterministic,
chains of co-B\"uchi automata can be exponentially more concise than
deterministic parity automata. We then answer the question if this conciseness
is retained when performing Boolean operations (such as disjunction and
conjunction) over COCOA by showing that there exist families of languages for
which these operations lead to an exponential growth of the sizes of the
automata. The families have the property that when representing them using
deterministic parity automata, taking the disjunction or conjunction of them
only requires a polynomial blow-up, which shows that Boolean operations over
COCOA do not retain their conciseness in general.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [22] [Multi-Threaded Software Model Checking via Parallel Trace Abstraction Refinement](https://arxiv.org/abs/2509.13699)
*Max Barth,Marie-Christine Jakobs*

Main category: cs.LO

TL;DR: 本文提出了一种并行化trace abstraction的方法，通过并行分析可能违反安全属性的不同程序路径来加速软件模型检测，在Ultimate Automizer工具中实现并验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 自动软件验证（特别是软件模型检测）耗时严重，阻碍了在实际应用（如持续集成）中的使用。为利用多核CPU优势，需要减少验证过程的响应时间。

Method: 并行化trace abstraction的抽象精化过程，具体通过并行分析可能违反安全属性的不同程序路径（语法路径）来实现。在Ultimate Automizer验证工具中实现了该并行化版本。

Result: 评估表明，并行化方法比顺序trace abstraction更有效，在许多耗时任务上能显著更快地提供结果。同时比最近的并行抽象软件模型检测方法DSS更有效。

Conclusion: 并行化trace abstraction是解决软件模型检测耗时问题的有效方案，能够充分利用多核CPU资源，显著提升验证效率。

Abstract: Automatic software verification is a valuable means for software quality
assurance. However, automatic verification and in particular software model
checking can be time-consuming, which hinders their practical applicability
e.g., the use in continuous integration. One solution to address the issue is
to reduce the response time of the verification procedure by leveraging today's
multi-core CPUs.
  In this paper, we propose a solution to parallelize trace abstraction, an
abstraction-based approach to software model checking. The underlying idea of
our approach is to parallelize the abstraction refinement. More concretely, our
approach analyzes different traces (syntactic program paths) that could violate
the safety property in parallel. We realize our parallelized version of trace
abstraction in the verification tool Ulti mate Automizer and perform a thorough
evaluation. Our evaluation shows that our parallelization is more effective
than sequential trace abstraction and can provide results significantly faster
on many time-consuming tasks. Also, our approach is more effective than DSS, a
recent parallel approach to abstraction-based software model checking.

</details>


### [23] [Algorithmic Perspective on Toda's Theorem](https://arxiv.org/abs/2509.13871)
*Dror Fried,Etay Segal,Gad E. Yaron*

Main category: cs.LO

TL;DR: 本文从算法角度重新审视Toda定理的归约过程，将其转化为具体算法，并通过理论和算法改进提升性能，揭示了将理论归约转化为实用QBF求解器的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着模型计数工具的进步，研究是否可以利用Toda的理论归约来开发高效的QBF求解算法

Method: 首先将Toda归约转化为具体算法，给定QBF公式和概率度量产生置信结果；然后通过理论和算法改进优化原型

Result: 相比初始原型取得了显著进展，但同时也更清楚地认识到将Toda归约转化为有竞争力求解器所面临的挑战

Conclusion: 研究为理解Toda归约的细节提供了新视角，指出了理论归约实用化过程中需要解决的关键问题

Abstract: Toda's Theorem is a fundamental result in computational complexity theory,
whose proof relies on a reduction from a QBF problem with a constant number of
quantifiers to a model counting problem. While this reduction, henceforth
called Toda's reduction, is of a purely theoretical nature, the recent progress
of model counting tools raises the question of whether the reduction can be
utilized to an efficient algorithm for solving QBF. In this work, we address
this question by looking at Toda's reduction from an algorithmic perspective.
We first convert the reduction into a concrete algorithm that given a QBF
formula and a probability measure, produces the correct result with a
confidence level corresponding to the given measure. Beyond obtaining a naive
prototype, our algorithm and the analysis that follows shed light on the fine
details of the reduction that are so far left elusive. Then, we improve this
prototype through various theoretical and algorithmic refinements. While our
results show a significant progress over the naive prototype, they also provide
a clearer understanding of the remaining challenges in turning Toda's reduction
into a competitive solver.

</details>


### [24] [The Complexity of Deciding Characteristic Formulae Modulo Nested Simulation (extended abstract)](https://arxiv.org/abs/2509.14089)
*Luca Aceto,Antonis Achilleos,Aggeliki Chalki,Anna Ingólfsdóttir*

Main category: cs.LO

TL;DR: 本文研究了模态逻辑中公式是否为某个过程的特征公式的复杂性，该问题等价于判断公式是否可满足且为素公式。主要结果是：判断公式在2-嵌套模拟预序模态逻辑中是否为素公式是coNP完全的，在n-嵌套模拟预序中（n≥3）是PSPACE完全的。


<details>
  <summary>Details</summary>
Motivation: 研究模态逻辑中特征公式判定问题的计算复杂性，特别是针对嵌套模拟语义的特征公式判定，这对于理解形式验证和进程代数的计算复杂度有重要意义。

Method: 通过理论分析和复杂度证明，研究了不同嵌套层次（2-嵌套和n≥3嵌套）模拟预序模态逻辑中公式素性判定问题的复杂度分类。

Result: 2-嵌套模拟预序的素公式判定是coNP完全的；n≥3嵌套模拟预序的素公式判定是PSPACE完全的；2-嵌套模拟语义的特征公式判定问题属于DP复杂度类。

Conclusion: 对于n≥3的嵌套模拟语义，判定特征公式是PSPACE完全的，这为相关模态逻辑问题的复杂度边界提供了明确的理论结果。

Abstract: This paper studies the complexity of determining whether a formula in the
modal logics characterizing the nested-simulation semantics is characteristic
for some process, which is equivalent to determining whether the formula is
satisfiable and prime. The main results are that the problem of determining
whether a formula is prime in the modal logic characterizing the
2-nested-simulation preorder is coNP-complete and is PSPACE-complete in the
case of the n-nested-simulation preorder, when n>=3. This establishes that
deciding characteristic formulae for the n-nested simulation semantics is
PSPACE-complete, when n>=3. In the case of the 2-nested simulation semantics,
that problem lies in the complexity class DP, which consists of languages that
can be expressed as the intersection of one language in NP and of one in coNP.

</details>


### [25] [An Automaton-based Characterisation of First-Order Logic over Infinite Trees](https://arxiv.org/abs/2509.14090)
*Massimo Benerecetti,Dario Della Monica,Angelo Matteo,Fabio Mogavero,Gabriele Puppis*

Main category: cs.LO

TL;DR: 本文通过引入两类犹豫树自动机，为无限树上的FO逻辑提供了自动机理论特征，并建立了与分支时序逻辑polcCTLp和cCTL*[f]的等价关系。


<details>
  <summary>Details</summary>
Motivation: 研究无限树上的一阶逻辑(FO)及其与分支时序逻辑的关系，旨在通过自动机理论来刻画FO的表达能力。

Method: 引入两类犹豫树自动机，证明它们能精确捕捉两个分支时序逻辑polcCTLp和cCTL*[f]的表达能力，这两个逻辑已被证明与无限树上的FO等价。

Result: 获得了FO在无限树上的自动机特征，为两个时序逻辑提供了正规形式，并揭示FO只能表达树分支上的安全性或共安全性性质。

Conclusion: 通过自动机理论成功刻画了FO在无限树上的表达能力，建立了与分支时序逻辑的紧密联系，并揭示了FO表达能力的本质限制。

Abstract: In this paper, we study First Order Logic (FO) over (unordered) infinite
trees and its connection with branching-time temporal logics. More
specifically, we provide an automata-theoretic characterisation of FO
interpreted over infinite trees. To this end, two different classes of hesitant
tree automata are introduced and proved to capture precisely the expressive
power of two branching time temporal logics, denoted polcCTLp and cCTL*[f],
which are, respectively, a restricted version of counting CTL with past and
counting CTL* over finite paths, both of which have been previously shown
equivalent to FO over infinite trees. The two automata characterisations
naturally lead to normal forms for the two temporal logics, and highlight the
fact that FO can only express properties of the tree branches which are either
safety or co-safety in nature.

</details>


### [26] [Metric Equational Theories](https://arxiv.org/abs/2509.14094)
*Radu Mardare,Neil Ghani,Eigil Rischel*

Main category: cs.LO

TL;DR: 将定量等式理论(QET)扩展到度量等式理论(MET)，其中操作的元数来自可数度量空间而非有限集，并为此类代数结构构建了完备的证明系统


<details>
  <summary>Details</summary>
Motivation: 受丰富Lawvere理论启发，需要将QET扩展到更一般的度量空间代数结构，其中操作元数来自可数度量空间，这导致项的有效性不再独立于等式的有效性

Method: 结合定量等式理论和丰富Lawvere理论，利用度量空间的特定结构，将QET的完备证明系统适配到更一般的MET中

Result: 成功解决了MET中项有效性依赖等式有效性的问题，为度量空间代数结构构建了适当的完备证明系统

Conclusion: 该方法有效扩展了QET到更一般的度量空间代数结构，为这类结构的推理提供了理论基础和实用工具

Abstract: This paper proposes appropriate sound and complete proof systems for
algebraic structures over metric spaces by combining the development of
Quantitative Equational Theories (QET) with the Enriched Lawvere Theories. We
extend QETs to Metric Equational Theories (METs) where operations no longer
have finite sets as arities (as in QETs and the general theory of universal
algebras), but arities are now drawn from countable metric spaces. This
extension is inspired by the theory of Enriched Lawvere Theories, which
suggests that the arities of operations should be the lambda-presentable
objects of the underlying lambda-accessible category. In this setting, the
validity of terms in METs can no longer be guaranteed independently of the
validity of equations, as is the case with QET. We solve this problem, and
adapt the sound and complete proof system for QETs to these more general METs,
taking advantage of the specific structure of metric spaces.

</details>


### [27] [The Complexity of Generalized HyperLTL with Stuttering and Contexts](https://arxiv.org/abs/2509.14095)
*Gaëtan Regaud,Martin Zimmermann*

Main category: cs.LO

TL;DR: 本文研究了广义HyperLTL（包含停顿和上下文）的满足性和模型检测复杂度，这是一个用于表达异步超属性的强大逻辑。


<details>
  <summary>Details</summary>
Motivation: HyperLTL只能表达同步超属性，无法处理异步超属性。为了填补这一空白，研究者引入了包含停顿和上下文的广义HyperLTL来支持异步超属性的规范。

Method: 通过理论分析和复杂度证明，研究了广义HyperLTL的满足性问题和模型检测问题的计算复杂度。

Result: 满足性问题为Σ₁¹-完全，与HyperLTL复杂度相同；模型检测问题等价于二阶算术的真值问题，比HyperLTL的模型检测问题困难得多。

Conclusion: 广义HyperLTL虽然表达能力更强，但模型检测复杂度显著增加，即使只允许停顿或只允许上下文，下界仍然成立。

Abstract: We settle the complexity of satisfiability and model-checking for generalized
HyperLTL with stuttering and contexts, an expressive logic for the
specification of asynchronous hyperproperties. Such properties cannot be
specified in HyperLTL, as it is restricted to synchronous hyperproperties.
Nevertheless, we prove that satisfiability is $\Sigma_1^1$-complete and thus
not harder than for HyperLTL. On the other hand, we prove that model-checking
is equivalent to truth in second-order arithmetic, and thus much harder than
the decidable HyperLTL model-checking problem. The lower bounds for the
model-checking problem hold even when only allowing stuttering or only allowing
contexts.

</details>
